{
    "ID": "W99-0613",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "W99-0613",
            "refer_sids": [
                121
            ],
            "refer_text": "They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page, and other pages pointing to the page).",
            "cite_ID": "N01-1023",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "W99-0613",
            "refer_sids": [
                252
            ],
            "refer_text": "The method uses a &quot;soft&quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.",
            "cite_ID": "N01-1023",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "W99-0613",
            "refer_sids": [
                91
            ],
            "refer_text": "There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.",
            "cite_ID": "W03-1509",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "W99-0613",
            "refer_sids": [
                91
            ],
            "refer_text": "There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.",
            "cite_ID": "C02-1154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "W99-0613",
            "refer_sids": [
                213
            ],
            "refer_text": "Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.",
            "cite_ID": "C02-1154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "W99-0613",
            "refer_sids": [
                250
            ],
            "refer_text": "Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.",
            "cite_ID": "W06-2204",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "W99-0613",
            "refer_sids": [
                39
            ],
            "refer_text": "(Brin 98) ,describes a system for extracting (author, book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.",
            "cite_ID": "W09-1116",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "W99-0613",
            "refer_sids": [
                202
            ],
            "refer_text": "The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.",
            "cite_ID": "W03-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "W99-0613",
            "refer_sids": [
                61
            ],
            "refer_text": "The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.",
            "cite_ID": "E09-1018",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "W99-0613",
            "refer_sids": [
                176
            ],
            "refer_text": "(7) is at 0 when: 1) Vi : sign(gi (xi)) = sign(g2 (xi)); 2) Ig3(xi)l oo; and 3) sign(gi (xi)) = yi for i = 1, , m. In fact, Zco provides a bound on the sum of the classification error of the labeled examples and the number of disagreements between the two classifiers on the unlabeled examples.",
            "cite_ID": "W10-3504",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labelled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "W99-0613",
            "refer_sids": [
                108
            ],
            "refer_text": "In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.",
            "cite_ID": "W07-1712",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "W99-0613",
            "refer_sids": [
                27,
                28
            ],
            "refer_text": "The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).\n(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.",
            "cite_ID": "W09-2208",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "W99-0613",
            "refer_sids": [
                7
            ],
            "refer_text": "Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "W99-0613",
            "refer_sids": [
                172
            ],
            "refer_text": "To see this, note thai the first two terms in the above equation correspond to the function that AdaBoost attempts to minimize in the standard supervised setting (Equ.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(4 )prec (p, y)= count (p, y) count (p) (5) where prec (p, y) is the raw precision of pattern p in the set of documents labeled with category y. Criterion 2: Collins This criterion was used in a lightly-supervised NE recognizer (Collins and Singer, 1999)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "W99-0613",
            "refer_sids": [
                85
            ],
            "refer_text": "(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "W99-0613",
            "refer_sids": [
                214
            ],
            "refer_text": "This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating \u2014 this deserves more theoretical investigation.",
            "cite_ID": "P12-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "3This is not clearly specified in Collins and Singer (1999), 3.2 Yarowsky-cautious",
            "label": [
                "Method Citation"
            ]
        }
    ]
}