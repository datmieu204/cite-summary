{
    "ID": "A00-2018",
    "citations": [
        {
            "Number": 2,
            "refer_ID": "A00-2018",
            "refer_sids": [
                17
            ],
            "refer_text": "In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.",
            "cite_ID": "N10-1002",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "A00-2018",
            "refer_sids": [
                5
            ],
            "refer_text": "We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.",
            "cite_ID": "W11-0610",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "A00-2018",
            "refer_sids": [
                17
            ],
            "refer_text": "In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.",
            "cite_ID": "W06-3119",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "A00-2018",
            "refer_sids": [
                120
            ],
            "refer_text": "Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &quot;correct&quot;, and statistics were collected on the resulting parses.",
            "cite_ID": "N03-2024",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "A00-2018",
            "refer_sids": [
                119
            ],
            "refer_text": "(It is &quot;soft&quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)",
            "cite_ID": "N06-1039",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "A00-2018",
            "refer_sids": [
                95
            ],
            "refer_text": "We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.",
            "cite_ID": "C04-1180",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "A00-2018",
            "refer_sids": [
                175
            ],
            "refer_text": "This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].",
            "cite_ID": "W05-0638",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "A00-2018",
            "refer_sids": [
                92
            ],
            "refer_text": "For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.",
            "cite_ID": "P05-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "A00-2018",
            "refer_sids": [
                1
            ],
            "refer_text": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.",
            "cite_ID": "P05-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "A00-2018",
            "refer_sids": [
                126
            ],
            "refer_text": "This is indicated in Figure 2, where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.",
            "cite_ID": "P04-1040",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "A00-2018",
            "refer_sids": [
                12
            ],
            "refer_text": "The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &quot;tag&quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).",
            "cite_ID": "P04-1040",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "A00-2018",
            "refer_sids": [
                174
            ],
            "refer_text": "We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.",
            "cite_ID": "P04-1040",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "A00-2018",
            "refer_sids": [
                63
            ],
            "refer_text": "As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &quot;before&quot;), and the label of the grandparent of c (la).",
            "cite_ID": "N06-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "A00-2018",
            "refer_sids": [
                78
            ],
            "refer_text": "With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.",
            "cite_ID": "N06-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "A00-2018",
            "refer_sids": [
                91
            ],
            "refer_text": "As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.",
            "cite_ID": "H05-1035",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "A00-2018",
            "refer_sids": [
                180
            ],
            "refer_text": "From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.",
            "cite_ID": "P04-1042",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size",
            "label": [
                "Method Citation"
            ]
        }
    ]
}