{
    "ID": "P04-1036",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "P04-1036",
            "refer_sids": [
                8
            ],
            "refer_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding con text into account (McCarthy et al, 2004)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "P04-1036",
            "refer_sids": [
                82
            ],
            "refer_text": "We also calculate the WSD accuracy that would be obtained on SemCor, when using our first sense in all contexts ( ).",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41Table 2: The SENSEVAL-2 first sense on the SEN SEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al, 2004) and also because there will be words that occur with insufficient frequency inthe hand-tagged resources available",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "P04-1036",
            "refer_sids": [
                64
            ],
            "refer_text": "We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).",
            "cite_ID": "W04-0837",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The method is described in (McCarthy et al, 2004), which we summarise here",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "P04-1036",
            "refer_sids": [
                172
            ],
            "refer_text": "We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD.We build upon this previous research, and pro pose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "P04-1036",
            "refer_sids": [
                153
            ],
            "refer_text": "Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Previous re search in inducing sense rankings from an untagged corpus (McCarthy et al, 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "P04-1036",
            "refer_sids": [
                1
            ],
            "refer_text": "word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.",
            "cite_ID": "I08-2105",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "P04-1036",
            "refer_sids": [
                165
            ],
            "refer_text": "Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.",
            "cite_ID": "P06-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Research by (McCarthy et al, 2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "P04-1036",
            "refer_sids": [
                171
            ],
            "refer_text": "In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.",
            "cite_ID": "P06-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, we implemented the unsupervised method of (McCarthy et al, 2004), which cal cu lates a prevalence score for each sense of a word to predict the predominant sense",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "P04-1036",
            "refer_sids": [
                89
            ],
            "refer_text": "Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.",
            "cite_ID": "P10-1155",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "McCarthy et al (2004) reported that the best results were obtained using k= 50 neighbors and the Wordnet Similarityjcn measure (Jiang and Conrath, 1997)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "P04-1036",
            "refer_sids": [
                172
            ],
            "refer_text": "We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al (2004)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "P04-1036",
            "refer_sids": [
                189
            ],
            "refer_text": "Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To define an appropriate categorical distribution over synsets for each 2 lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x. We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s? Sx, following the approach of McCarthy et al (2004)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 14,
            "refer_ID": "P04-1036",
            "refer_sids": [
                87
            ],
            "refer_text": "Again, the automatic ranking outperforms this by a large margin.",
            "cite_ID": "W12-3401",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As explained in Section 2.2, ASR is performed using the method of McCarthy et al (2004)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "P04-1036",
            "refer_sids": [
                1
            ],
            "refer_text": "word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.",
            "cite_ID": "S12-1097",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "P04-1036",
            "refer_sids": [
                89
            ],
            "refer_text": "Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.",
            "cite_ID": "W10-2803",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al, 2004), or to work directly with para phrases (McCarthy and Navigli, 2009)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "P04-1036",
            "refer_sids": [
                137
            ],
            "refer_text": "Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.",
            "cite_ID": "W08-2107",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Word net resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al (2004))",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "P04-1036",
            "refer_sids": [
                63
            ],
            "refer_text": "We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.",
            "cite_ID": "D07-1026",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "P04-1036",
            "refer_sids": [
                159
            ],
            "refer_text": "Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.",
            "cite_ID": "W12-2429",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The first, most frequent sense (MFS) (McCarthy et al, 2004), is widely used baseline for supervised WSD systems",
            "label": [
                "Method Citation"
            ]
        }
    ]
}