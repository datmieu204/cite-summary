{
    "ID": "P08-1102",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "P08-1102",
            "refer_sids": [
                32
            ],
            "refer_text": "We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.",
            "cite_ID": "C08-1049",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "P08-1102",
            "refer_sids": [
                32
            ],
            "refer_text": "We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.",
            "cite_ID": "C08-1049",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "P08-1102",
            "refer_sids": [
                38
            ],
            "refer_text": "Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.",
            "cite_ID": "C08-1049",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "plates called lexical-target in the column below areintroduced by Jiang et al (2008)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "P08-1102",
            "refer_sids": [
                92
            ],
            "refer_text": "The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.",
            "cite_ID": "D12-1126",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "P08-1102",
            "refer_sids": [
                36
            ],
            "refer_text": "All feature templates and their instances are shown in Table 1.",
            "cite_ID": "C10-1135",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use the feature templates the same as Jiang et al, (2008) to extract features form E model",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "P08-1102",
            "refer_sids": [
                32
            ],
            "refer_text": "We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.",
            "cite_ID": "P12-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "approach, where basic processing units are characters which compose words (Jiangetal., 2008a)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "P08-1102",
            "refer_sids": [
                92
            ],
            "refer_text": "The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.",
            "cite_ID": "C10-2096",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "P08-1102",
            "refer_sids": [
                92
            ],
            "refer_text": "The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.",
            "cite_ID": "C10-2096",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "P08-1102",
            "refer_sids": [
                97
            ],
            "refer_text": "Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "P08-1102",
            "refer_sids": [
                91
            ],
            "refer_text": "The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])",
            "label": [
                "Aim_Citation"
            ]
        },
        {
            "Number": 14,
            "refer_ID": "P08-1102",
            "refer_sids": [
                16
            ],
            "refer_text": "Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "P08-1102",
            "refer_sids": [
                34
            ],
            "refer_text": "The feature templates we adopted are selected from those of Ng and Low (2004).",
            "cite_ID": "C10-1132",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "P08-1102",
            "refer_sids": [
                92
            ],
            "refer_text": "The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.",
            "cite_ID": "D12-1046",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)",
            "label": [
                "Method_Citation"
            ]
        }
    ]
}