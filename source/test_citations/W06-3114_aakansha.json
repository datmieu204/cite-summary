{
    "ID": "W06-3114",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "W06-3114",
            "refer_sids": [
                170
            ],
            "refer_text": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "cite_ID": "W06-3120",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "W06-3114",
            "refer_sids": [
                8
            ],
            "refer_text": "The evaluation framework for the shared task is similar to the one used in last year\u2019s shared task.",
            "cite_ID": "D07-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "W06-3114",
            "refer_sids": [
                9
            ],
            "refer_text": "Training and testing is based on the Europarl corpus.",
            "cite_ID": "C08-1074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "W06-3114",
            "refer_sids": [
                36
            ],
            "refer_text": "The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.",
            "cite_ID": "W07-0718",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "W06-3114",
            "refer_sids": [
                9
            ],
            "refer_text": "Training and testing is based on the Europarl corpus.",
            "cite_ID": "P07-1083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "W06-3114",
            "refer_sids": [
                36
            ],
            "refer_text": "The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "W06-3114",
            "refer_sids": [
                140
            ],
            "refer_text": "We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "W06-3114",
            "refer_sids": [
                140
            ],
            "refer_text": "We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "W06-3114",
            "refer_sids": [
                140
            ],
            "refer_text": "We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "W06-3114",
            "refer_sids": [
                102
            ],
            "refer_text": "Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.",
            "cite_ID": "D07-1030",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use the same method described in (Koehn and Monz, 2006) to perform the significance test",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "W06-3114",
            "refer_sids": [
                84
            ],
            "refer_text": "The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:",
            "cite_ID": "D07-1030",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "W06-3114",
            "refer_sids": [
                11
            ],
            "refer_text": "To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.",
            "cite_ID": "W08-0406",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "W06-3114",
            "refer_sids": [
                140
            ],
            "refer_text": "We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.",
            "cite_ID": "W11-1002",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 14,
            "refer_ID": "W06-3114",
            "refer_sids": [
                126
            ],
            "refer_text": "The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.",
            "cite_ID": "D07-1091",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "W06-3114",
            "refer_sids": [
                15
            ],
            "refer_text": "Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.",
            "cite_ID": "D07-1091",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "W06-3114",
            "refer_sids": [
                8
            ],
            "refer_text": "The evaluation framework for the shared task is similar to the one used in last year\u2019s shared task.",
            "cite_ID": "P07-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "W06-3114",
            "refer_sids": [
                90
            ],
            "refer_text": "Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.",
            "cite_ID": "E12-3010",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "W06-3114",
            "refer_sids": [
                5,
                6
            ],
            "refer_text": "\u2022 We evaluated translation from English, in addition to into English.\nEnglish was again paired with German, French, and Spanish.",
            "cite_ID": "W09-0402",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)",
            "label": [
                "Method_Citation"
            ]
        }
    ]
}