{
    "ID": "D09-1092",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "D09-1092",
            "refer_sids": [
                32
            ],
            "refer_text": "Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.",
            "cite_ID": "P14-1004",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "D09-1092",
            "refer_sids": [
                39
            ],
            "refer_text": "Additionally, PLTM assumes that each \u201ctopic\u201d consists of a set of discrete distributions over words\u2014one for each language l = 1, ... , L. In other words, rather than using a single set of topics \u03a6 = {\u03c61, ... , \u03c6T}, as in LDA, there are L sets of language-specific topics, \u03a61, ... , \u03a6L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter \u03b2l.",
            "cite_ID": "P10-1044",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "D09-1092",
            "refer_sids": [
                138
            ],
            "refer_text": "We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.",
            "cite_ID": "P11-2084",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "D09-1092",
            "refer_sids": [
                196
            ],
            "refer_text": "When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.",
            "cite_ID": "E12-1014",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "D09-1092",
            "refer_sids": [
                111
            ],
            "refer_text": "In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts \u2013 i.e., we put each of these documents in a single-document tuple.",
            "cite_ID": "D11-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "of English document and the second half of its aligned foreign language document (Mimno et al,2009)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "D09-1092",
            "refer_sids": [
                128
            ],
            "refer_text": "Although the PLTM is clearly not a substitute for a machine translation system\u2014it has no way to represent syntax or even multi-word phrases\u2014it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.",
            "cite_ID": "N12-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "D09-1092",
            "refer_sids": [
                122
            ],
            "refer_text": "Divergence drops significantly when the proportion of \u201cglue\u201d tuples increases from 0.01 to 0.25.",
            "cite_ID": "N12-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "D09-1092",
            "refer_sids": [
                35
            ],
            "refer_text": "The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "D09-1092",
            "refer_sids": [
                110
            ],
            "refer_text": "Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "D09-1092",
            "refer_sids": [
                30
            ],
            "refer_text": "However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "D09-1092",
            "refer_sids": [
                146
            ],
            "refer_text": "In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "D09-1092",
            "refer_sids": [
                77
            ],
            "refer_text": "Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "D09-1092",
            "refer_sids": [
                156
            ],
            "refer_text": "We use both Jensen-Shannon divergence and cosine distance.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 14,
            "refer_ID": "D09-1092",
            "refer_sids": [
                31
            ],
            "refer_text": "They also provide little analysis of the differences between polylingual and single-language topic models.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Documents are defined as speeches by a single speaker, as in (Mimno et al, 2009) .4 For the Wikipedia set, we use 43,380 training documents, 8,675 development documents, and 8,675 final test set documents. For both corpora, the terms are extracted by word breaking all documents, removing the top 50 most frequent terms and keeping the next 20,000 most frequent terms",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "D09-1092",
            "refer_sids": [
                19
            ],
            "refer_text": "We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "D09-1092",
            "refer_sids": [
                131
            ],
            "refer_text": "We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).",
            "cite_ID": "W12-3117",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "D09-1092",
            "refer_sids": [
                196
            ],
            "refer_text": "When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.",
            "cite_ID": "W11-2133",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "D09-1092",
            "refer_sids": [
                192
            ],
            "refer_text": "We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.",
            "cite_ID": "W11-2133",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "D09-1092",
            "refer_sids": [
                195
            ],
            "refer_text": "Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.",
            "cite_ID": "P14-2110",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "D09-1092",
            "refer_sids": [
                6
            ],
            "refer_text": "Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).",
            "cite_ID": "P14-2110",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language",
            "label": [
                "Method Citation"
            ]
        }
    ]
}