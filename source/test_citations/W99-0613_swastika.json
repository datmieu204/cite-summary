{
    "ID": "W99-0613",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "W99-0613",
            "refer_sids": [
                9.0
            ],
            "refer_text": "This paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "cite_ID": "N01-1023",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)",
            "label": [
                "Aim_Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "W99-0613",
            "refer_sids": [
                159.0
            ],
            "refer_text": "To prevent this we &quot;smooth&quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.",
            "cite_ID": "N01-1023",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "W99-0613",
            "refer_sids": [
                137.0
            ],
            "refer_text": "The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.",
            "cite_ID": "W03-1509",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "W99-0613",
            "refer_sids": [
                91.0
            ],
            "refer_text": "There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.",
            "cite_ID": "C02-1154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "W99-0613",
            "refer_sids": [
                213.0
            ],
            "refer_text": "Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.",
            "cite_ID": "C02-1154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "W99-0613",
            "refer_sids": [
                250.0
            ],
            "refer_text": "Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.",
            "cite_ID": "W06-2204",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "W99-0613",
            "refer_sids": [
                213.0
            ],
            "refer_text": "Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.",
            "cite_ID": "W03-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "W99-0613",
            "refer_sids": [
                9.0
            ],
            "refer_text": "This paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "cite_ID": "E09-1018",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)",
            "label": [
                "Aim_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "W99-0613",
            "refer_sids": [
                36.0
            ],
            "refer_text": "Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.",
            "cite_ID": "W10-3504",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labelled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999)",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "W99-0613",
            "refer_sids": [
                29.0
            ],
            "refer_text": "Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function.",
            "cite_ID": "W09-2208",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "W99-0613",
            "refer_sids": [
                7.0
            ],
            "refer_text": "Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)",
            "label": [
                "Aim_Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "W99-0613",
            "refer_sids": [
                85.0
            ],
            "refer_text": "(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "W99-0613",
            "refer_sids": [
                95.0
            ],
            "refer_text": "(Specifically, the limit n starts at 5 and increases by 5 at each iteration.)",
            "cite_ID": "P12-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "W99-0613",
            "refer_sids": [
                213.0
            ],
            "refer_text": "Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.",
            "cite_ID": "P12-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "3This is not clearly specified in Collins and Singer (1999), 3.2 Yarowsky-cautious",
            "label": [
                "Method_Citation"
            ]
        }
    ]
}