{
    "ID": "J01-2004",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "J01-2004",
            "refer_sids": [
                372
            ],
            "refer_text": "The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.",
            "cite_ID": "W05-0104",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "J01-2004",
            "refer_sids": [
                40,
                41,
                42
            ],
            "refer_text": "The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.\nThere will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.\nThree parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.",
            "cite_ID": "P08-1013",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "J01-2004",
            "refer_sids": [
                25
            ],
            "refer_text": "A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "J01-2004",
            "refer_sids": [
                364
            ],
            "refer_text": "We follow Chelba (2000) in dealing with this problem: for parsing purposes, we use the Penn Treebank tokenization; for interpolation with the provided trigram model, and for evaluation, the lattice tokenization is used.",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "J01-2004",
            "refer_sids": [
                302
            ],
            "refer_text": "In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "J01-2004",
            "refer_sids": [
                31
            ],
            "refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "J01-2004",
            "refer_sids": [
                231
            ],
            "refer_text": "Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).",
            "cite_ID": "P04-1015",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "J01-2004",
            "refer_sids": [
                297
            ],
            "refer_text": "The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.",
            "cite_ID": "P05-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "J01-2004",
            "refer_sids": [
                133
            ],
            "refer_text": "Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model.",
            "cite_ID": "P05-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "J01-2004",
            "refer_sids": [
                291
            ],
            "refer_text": "Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length < 100.",
            "cite_ID": "P05-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "J01-2004",
            "refer_sids": [
                355
            ],
            "refer_text": "In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.",
            "cite_ID": "P04-1006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 14,
            "refer_ID": "J01-2004",
            "refer_sids": [
                59
            ],
            "refer_text": "A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.",
            "cite_ID": "P05-1063",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "J01-2004",
            "refer_sids": [
                100
            ],
            "refer_text": "The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.",
            "cite_ID": "W10-2009",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "J01-2004",
            "refer_sids": [
                108
            ],
            "refer_text": "Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &quot;surface&quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model.",
            "cite_ID": "D09-1034",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "J01-2004",
            "refer_sids": [
                31
            ],
            "refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).",
            "cite_ID": "D09-1034",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here",
            "label": [
                "Method citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "J01-2004",
            "refer_sids": [
                31
            ],
            "refer_text": "Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).",
            "cite_ID": "D09-1034",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed",
            "label": [
                "Method citation"
            ]
        }
    ]
}