{
    "ID": "W99-0613",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "W99-0613",
            "refer_sids": [
                9
            ],
            "refer_text": "This paper discusses the use of unlabeled examples for the problem of named entity classification.",
            "cite_ID": "N01-1023",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "W99-0613",
            "refer_sids": [
                35
            ],
            "refer_text": "AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.",
            "cite_ID": "N01-1023",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "W99-0613",
            "refer_sids": [
                134
            ],
            "refer_text": "This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.",
            "cite_ID": "W03-1509",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "W99-0613",
            "refer_sids": [
                236
            ],
            "refer_text": "We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.",
            "cite_ID": "C02-1154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "W99-0613",
            "refer_sids": [
                8
            ],
            "refer_text": "Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.",
            "cite_ID": "W06-2204",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "W99-0613",
            "refer_sids": [
                42
            ],
            "refer_text": "(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &quot;vehicle&quot; or &quot;weapon&quot; categories).",
            "cite_ID": "W09-1116",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "W99-0613",
            "refer_sids": [
                236
            ],
            "refer_text": "We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.",
            "cite_ID": "W03-1022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "W99-0613",
            "refer_sids": [
                222
            ],
            "refer_text": "The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.",
            "cite_ID": "E09-1018",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "W99-0613",
            "refer_sids": [
                30
            ],
            "refer_text": "(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of can help classification, and suggest an objective function when training with unlabeled examples.",
            "cite_ID": "W10-3504",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labelled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "W99-0613",
            "refer_sids": [
                26
            ],
            "refer_text": "We present two algorithms.",
            "cite_ID": "W09-2208",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "W99-0613",
            "refer_sids": [
                7
            ],
            "refer_text": "Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "W99-0613",
            "refer_sids": [
                32
            ],
            "refer_text": "The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.",
            "cite_ID": "W06-2207",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(4 )prec (p, y)= count (p, y) count (p) (5) where prec (p, y) is the raw precision of pattern p in the set of documents labeled with category y. Criterion 2: Collins This criterion was used in a lightly-supervised NE recognizer (Collins and Singer, 1999)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "W99-0613",
            "refer_sids": [
                47
            ],
            "refer_text": "971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).",
            "cite_ID": "P12-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "W99-0613",
            "refer_sids": [
                127
            ],
            "refer_text": "The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.",
            "cite_ID": "P12-1065",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "3This is not clearly specified in Collins and Singer (1999), 3.2 Yarowsky-cautious",
            "label": [
                "Method_Citation"
            ]
        }
    ]
}