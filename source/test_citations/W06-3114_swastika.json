{
    "ID": "W06-3114",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "W06-3114",
            "refer_sids": [
                47.0
            ],
            "refer_text": "Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.",
            "cite_ID": "W06-3120",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "W06-3114",
            "refer_sids": [
                8.0
            ],
            "refer_text": "The evaluation framework for the shared task is similar to the one used in last year\u2019s shared task.",
            "cite_ID": "D07-1092",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "W06-3114",
            "refer_sids": [
                18.0
            ],
            "refer_text": "In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.",
            "cite_ID": "C08-1074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "W06-3114",
            "refer_sids": [
                18.0
            ],
            "refer_text": "In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.",
            "cite_ID": "P07-1083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "W06-3114",
            "refer_sids": [
                144.0
            ],
            "refer_text": "Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "W06-3114",
            "refer_sids": [
                145.0
            ],
            "refer_text": "This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "W06-3114",
            "refer_sids": [
                103.0
            ],
            "refer_text": "Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \u00b7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.",
            "cite_ID": "W07-0738",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "W06-3114",
            "refer_sids": [
                50.0
            ],
            "refer_text": "Following this method, we repeatedly \u2014 say, 1000 times \u2014 sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.",
            "cite_ID": "D07-1030",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We use the same method described in (Koehn and Monz, 2006) to perform the significance test",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "W06-3114",
            "refer_sids": [
                68.0
            ],
            "refer_text": "We asked participants to each judge 200\u2013300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.",
            "cite_ID": "D07-1030",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "W06-3114",
            "refer_sids": [
                170.0
            ],
            "refer_text": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "cite_ID": "W08-0406",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "W06-3114",
            "refer_sids": [
                18.0
            ],
            "refer_text": "In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.",
            "cite_ID": "D07-1091",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "W06-3114",
            "refer_sids": [
                170.0
            ],
            "refer_text": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "cite_ID": "D07-1091",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "W06-3114",
            "refer_sids": [
                170.0
            ],
            "refer_text": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.",
            "cite_ID": "P07-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "W06-3114",
            "refer_sids": [
                92.0
            ],
            "refer_text": "The way judgements are collected, human judges tend to use the scores to rank systems against each other.",
            "cite_ID": "E12-3010",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "W06-3114",
            "refer_sids": [
                145.0
            ],
            "refer_text": "This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.",
            "cite_ID": "W09-0402",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)",
            "label": [
                "Method_Citation"
            ]
        }
    ]
}