{
    "ID": "D10-1044",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "D10-1044",
            "refer_sids": [
                4
            ],
            "refer_text": "Domain adaptation is a common concern when optimizing empirical NLP applications.",
            "cite_ID": "P11-2074",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Another popular task in SMT is domain adaptation (Foster et al, 2010)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "D10-1044",
            "refer_sids": [
                132
            ],
            "refer_text": "We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.",
            "cite_ID": "P12-1048",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "D10-1044",
            "refer_sids": [
                7
            ],
            "refer_text": "For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.",
            "cite_ID": "D12-1129",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010) .Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "D10-1044",
            "refer_sids": [
                62
            ],
            "refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "cite_ID": "P14-2093",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "D10-1044",
            "refer_sids": [
                50
            ],
            "refer_text": "Linear weights are difficult to incorporate into the standard MERT procedure because they are \u201chidden\u201d within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn, 2007), we circumvent this problem by choosing weights to optimize corpus loglikelihood, which is roughly speaking the training criterion used by the LM and TM themselves.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs, and merely varies? .Our main technical contributions are as fol lows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMTtranslation model: the phrase translation probabilities p (t|s) and p (s|t), and the lexical weights lex (t|s) and lex (s|t)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "D10-1044",
            "refer_sids": [
                152
            ],
            "refer_text": "We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) ex tend this approach by weighting individual phrase pairs",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "D10-1044",
            "refer_sids": [
                144
            ],
            "refer_text": "In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al (2010) combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "D10-1044",
            "refer_sids": [
                9
            ],
            "refer_text": "In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material\u2014though adequate for reasonable performance\u2014is also available.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN) Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "D10-1044",
            "refer_sids": [
                75
            ],
            "refer_text": "However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.",
            "cite_ID": "E12-1055",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translationmodels.15 We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "D10-1044",
            "refer_sids": [
                28
            ],
            "refer_text": "We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "D10-1044",
            "refer_sids": [
                97
            ],
            "refer_text": "We carried out translation experiments in two different settings.",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "m ?mpm (e? |f?) Our technique for setting? m is similar to that outlined in Foster et al (2010)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "D10-1044",
            "refer_sids": [
                75
            ],
            "refer_text": "However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "m ?mpm (e? |f?) For efficiency and stability, we use the EMalgorithm to find??, rather than L-BFGS as in (Foster et al., 2010)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "D10-1044",
            "refer_sids": [
                143
            ],
            "refer_text": "Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008).",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010), however, uses a different approach to select related sentences from OUT",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 14,
            "refer_ID": "D10-1044",
            "refer_sids": [
                153
            ],
            "refer_text": "Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.",
            "cite_ID": "P12-1099",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010) propose asimilar method for machine translation that uses features to capture degrees of generality",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "D10-1044",
            "refer_sids": [
                144
            ],
            "refer_text": "In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.",
            "cite_ID": "P13-1126",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "As in (Foster et al, 2010), this approach works at the level of phrase pairs",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "D10-1044",
            "refer_sids": [
                62
            ],
            "refer_text": "To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.",
            "cite_ID": "D11-1033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "D10-1044",
            "refer_sids": [
                141
            ],
            "refer_text": "Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L\u00a8u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L\u00a8u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).",
            "cite_ID": "D11-1033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and re port a decrease in performance",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "D10-1044",
            "refer_sids": [
                28
            ],
            "refer_text": "We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.",
            "cite_ID": "D11-1033",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Foster et al (2010) further perform this on extracted phrase pairs, not just sentences",
            "label": [
                "Method Citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "D10-1044",
            "refer_sids": [
                37
            ],
            "refer_text": "Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.",
            "cite_ID": "P14-1012",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments",
            "label": [
                "Method Citation"
            ]
        }
    ]
}