{
    "ID": "W11-2123",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "W11-2123",
            "refer_sids": [
                7
            ],
            "refer_text": "This paper presents methods to query N-gram language models, minimizing time and space costs.",
            "cite_ID": "W11-2138",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "W11-2123",
            "refer_sids": [
                45
            ],
            "refer_text": "The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.",
            "cite_ID": "P14-2022",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "W11-2123",
            "refer_sids": [
                136
            ],
            "refer_text": "We offer a state function s(wn1) = wn\ufffd where substring wn\ufffd is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.",
            "cite_ID": "W12-3145",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W12-3131",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "W11-2123",
            "refer_sids": [
                7
            ],
            "refer_text": "This paper presents methods to query N-gram language models, minimizing time and space costs.",
            "cite_ID": "W12-3154",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "W11-2123",
            "refer_sids": [
                8
            ],
            "refer_text": "Queries take the form p(wn|wn\u22121 1 ) where wn1 is an n-gram.",
            "cite_ID": "P12-2058",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W11-2139",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Inference was carried out using the language modeling library described by Heafield (2011)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "W11-2123",
            "refer_sids": [
                205
            ],
            "refer_text": "We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).",
            "cite_ID": "P13-2003",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "W11-2123",
            "refer_sids": [
                274
            ],
            "refer_text": "We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.",
            "cite_ID": "W12-3134",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "W11-2123",
            "refer_sids": [
                204
            ],
            "refer_text": "For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 256.",
            "cite_ID": "W12-3134",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "W11-2123",
            "refer_sids": [
                274
            ],
            "refer_text": "We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.",
            "cite_ID": "W12-3134",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W12-3160",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This was used to create a KenLM (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W12-3706",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 14,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "W11-2147",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "E12-1083",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "W11-2123",
            "refer_sids": [
                229
            ],
            "refer_text": "Then we ran binary search to determine the least amount of memory with which it would run.",
            "cite_ID": "P12-1002",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "D12-1108",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "W11-2123",
            "refer_sids": [
                93
            ],
            "refer_text": "The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.",
            "cite_ID": "P12-2006",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "W11-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.",
            "cite_ID": "P13-2073",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "W11-2123",
            "refer_sids": [
                199
            ],
            "refer_text": "For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.",
            "cite_ID": "P13-1109",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing",
            "label": [
                "Method_Citation"
            ]
        }
    ]
}