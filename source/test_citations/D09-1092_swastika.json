{
    "ID": "D09-1092",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "D09-1092",
            "refer_sids": [
                193
            ],
            "refer_text": "We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.",
            "cite_ID": "P14-1004",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "D09-1092",
            "refer_sids": [
                114
            ],
            "refer_text": "Ideally, the \u201cglue\u201d documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.",
            "cite_ID": "P10-1044",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)",
            "label": [
                "Aim_Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "D09-1092",
            "refer_sids": [
                138
            ],
            "refer_text": "We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.",
            "cite_ID": "P11-2084",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "D09-1092",
            "refer_sids": [
                18
            ],
            "refer_text": "In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.",
            "cite_ID": "E12-1014",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "D09-1092",
            "refer_sids": [
                55
            ],
            "refer_text": "The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.",
            "cite_ID": "D11-1086",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "of English document and the second half of its aligned foreign language document (Mimno et al,2009)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "D09-1092",
            "refer_sids": [
                192
            ],
            "refer_text": "We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.",
            "cite_ID": "N12-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "D09-1092",
            "refer_sids": [
                119
            ],
            "refer_text": "The lower the divergence, the more similar the distributions are to each other.",
            "cite_ID": "N12-1007",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "D09-1092",
            "refer_sids": [
                148
            ],
            "refer_text": "To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "D09-1092",
            "refer_sids": [
                193
            ],
            "refer_text": "We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "D09-1092",
            "refer_sids": [
                184
            ],
            "refer_text": "Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "D09-1092",
            "refer_sids": [
                193
            ],
            "refer_text": "We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "D09-1092",
            "refer_sids": [
                163
            ],
            "refer_text": "Performance continues to improve with longer documents, most likely due to better topic inference.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "D09-1092",
            "refer_sids": [
                184
            ],
            "refer_text": "Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "D09-1092",
            "refer_sids": [
                148
            ],
            "refer_text": "To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.",
            "cite_ID": "D10-1025",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "D09-1092",
            "refer_sids": [
                184
            ],
            "refer_text": "Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.",
            "cite_ID": "W12-3117",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "D09-1092",
            "refer_sids": [
                193
            ],
            "refer_text": "We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.",
            "cite_ID": "W11-2133",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "D09-1092",
            "refer_sids": [
                192
            ],
            "refer_text": "We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.",
            "cite_ID": "W11-2133",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora",
            "label": [
                "Result_Citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "D09-1092",
            "refer_sids": [
                105
            ],
            "refer_text": "An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.",
            "cite_ID": "P14-2110",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "D09-1092",
            "refer_sids": [
                10
            ],
            "refer_text": "We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).",
            "cite_ID": "P14-2110",
            "cite_maker_sids": [
                0
            ],
            "cite_sids": [
                0
            ],
            "cite_text": "3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language",
            "label": [
                "Method_Citation"
            ]
        }
    ]
}