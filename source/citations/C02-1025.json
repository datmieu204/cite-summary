{
    "ID": "C02-1025",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "C02-1025",
            "refer_sids": [
                63
            ],
            "refer_text": "Global features are extracted from other occurrences of the same token in the whole document.",
            "cite_ID": "C10-2104",
            "cite_maker_sids": [
                115
            ],
            "cite_sids": [
                115
            ],
            "cite_text": "In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "C02-1025",
            "refer_sids": [
                4
            ],
            "refer_text": "In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.",
            "cite_ID": "C10-2167",
            "cite_maker_sids": [
                65
            ],
            "cite_sids": [
                65
            ],
            "cite_text": "In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "C02-1025",
            "refer_sids": [
                4
            ],
            "refer_text": "In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.",
            "cite_ID": "I05-3013",
            "cite_maker_sids": [
                88
            ],
            "cite_sids": [
                88
            ],
            "cite_text": "The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "C02-1025",
            "refer_sids": [
                196
            ],
            "refer_text": "We have shown that the maximum entropy framework is able to use global information directly.",
            "cite_ID": "I05-3030",
            "cite_maker_sids": [
                33
            ],
            "cite_sids": [
                33
            ],
            "cite_text": "model\u00e2\u20ac\u2122s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "C02-1025",
            "refer_sids": [
                86
            ],
            "refer_text": "Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.",
            "cite_ID": "P02-1061",
            "cite_maker_sids": [
                51
            ],
            "cite_sids": [
                51
            ],
            "cite_text": "More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "C02-1025",
            "refer_sids": [
                6
            ],
            "refer_text": "A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.",
            "cite_ID": "P03-1028",
            "cite_maker_sids": [
                161
            ],
            "cite_sids": [
                161
            ],
            "cite_text": "They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).",
            "label": [
                "Implication_Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "C02-1025",
            "refer_sids": [
                2,
                3
            ],
            "refer_text": "It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier.",
            "cite_ID": "P03-1028",
            "cite_maker_sids": [
                52
            ],
            "cite_sids": [
                52
            ],
            "cite_text": "Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "C02-1025",
            "refer_sids": [
                63
            ],
            "refer_text": "Global features are extracted from other occurrences of the same token in the whole document.",
            "cite_ID": "P05-1045",
            "cite_maker_sids": [
                174
            ],
            "cite_sids": [
                174
            ],
            "cite_text": "Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "C02-1025",
            "refer_sids": [
                63
            ],
            "refer_text": "Global features are extracted from other occurrences of the same token in the whole document.",
            "cite_ID": "P05-1051",
            "cite_maker_sids": [
                27
            ],
            "cite_sids": [
                27
            ],
            "cite_text": "(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "C02-1025",
            "refer_sids": [
                4
            ],
            "refer_text": "In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.",
            "cite_ID": "W03-0423",
            "cite_maker_sids": [
                12
            ],
            "cite_sids": [
                12
            ],
            "cite_text": "Such global features enhance the performance of NER (Chieu and Ng, 2002b).",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "C02-1025",
            "refer_sids": [
                102,
                103,
                104
            ],
            "refer_text": "For all lists except locations, the lists are processed into a list of tokens (unigrams).Location list is processed into a list of unigrams and bigrams (e.g., New York).For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams.",
            "cite_ID": "W03-0423",
            "cite_maker_sids": [
                32
            ],
            "cite_sids": [
                32
            ],
            "cite_text": "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "C02-1025",
            "refer_sids": [
                61,
                62,
                63
            ],
            "refer_text": "The features we used can be divided into 2 classes: local and global.Local features are features that are based on neighboring tokens, as well as the token itself.Global features are extracted from other occurrences of the same token in the whole document.",
            "cite_ID": "W03-0423",
            "cite_maker_sids": [
                44
            ],
            "cite_sids": [
                44
            ],
            "cite_text": "The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "C02-1025",
            "refer_sids": [
                86
            ],
            "refer_text": "Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.",
            "cite_ID": "W03-0423",
            "cite_maker_sids": [
                62
            ],
            "cite_sids": [
                62
            ],
            "cite_text": "Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "C02-1025",
            "refer_sids": [
                61,
                62,
                63
            ],
            "refer_text": "The features we used can be divided into 2 classes: local and global.Local features are features that are based on neighboring tokens, as well as the token itself.Global features are extracted from other occurrences of the same token in the whole document.",
            "cite_ID": "W03-0423",
            "cite_maker_sids": [
                46
            ],
            "cite_sids": [
                46
            ],
            "cite_text": "In this paper, w\u2212i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "C02-1025",
            "refer_sids": [
                136,
                137
            ],
            "refer_text": "The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps.For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own.",
            "cite_ID": "W03-0432",
            "cite_maker_sids": [
                44
            ],
            "cite_sids": [
                44
            ],
            "cite_text": "Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a \u00e2\u20ac\u0153weaker\u00e2\u20ac\u009d classifier that did not use case information at all (Chieu and Ng, 2002b).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 21,
            "refer_ID": "C02-1025",
            "refer_sids": [
                11
            ],
            "refer_text": "We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information).",
            "cite_ID": "W04-0705",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                8
            ],
            "cite_text": "AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 22,
            "refer_ID": "C02-1025",
            "refer_sids": [
                63
            ],
            "refer_text": "Global features are extracted from other occurrences of the same token in the whole document.",
            "cite_ID": "W04-0705",
            "cite_maker_sids": [
                147
            ],
            "cite_sids": [
                147
            ],
            "cite_text": "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 23,
            "refer_ID": "C02-1025",
            "refer_sids": [
                4
            ],
            "refer_text": "In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.",
            "cite_ID": "W06-0119",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                11
            ],
            "cite_text": "To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;",
            "label": [
                "Results_Citation"
            ]
        }
    ]
}