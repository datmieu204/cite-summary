{
    "ID": "C00-2123",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "C00-2123",
            "refer_sids": [
                179
            ],
            "refer_text": "For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.",
            "cite_ID": "C02-1050",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                39,
                40,
                41
            ],
            "cite_text": "Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "C00-2123",
            "refer_sids": [
                1,
                94
            ],
            "refer_text": "In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence.",
            "cite_ID": "C02-1050",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                8
            ],
            "cite_text": "There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "C00-2123",
            "refer_sids": [
                46,
                47
            ],
            "refer_text": "The algorithm works due to the fact that not all permutations of cities have to be considered explicitly.For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored.",
            "cite_ID": "C02-1050",
            "cite_maker_sids": [
                43
            ],
            "cite_sids": [
                43
            ],
            "cite_text": "The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "C00-2123",
            "refer_sids": [
                94,
                139
            ],
            "refer_text": "When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence.This approach leads to a search procedure with complexity O(E3 J4).",
            "cite_ID": "C02-1050",
            "cite_maker_sids": [
                80
            ],
            "cite_sids": [
                80
            ],
            "cite_text": "The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "C00-2123",
            "refer_sids": [
                39
            ],
            "refer_text": "In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).",
            "cite_ID": "C04-1091",
            "cite_maker_sids": [
                22
            ],
            "cite_sids": [
                22
            ],
            "cite_text": "Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) \u2248 O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "C00-2123",
            "refer_sids": [
                6
            ],
            "refer_text": "We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability",
            "cite_ID": "E06-1004",
            "cite_maker_sids": [
                22
            ],
            "cite_sids": [
                21,
                22
            ],
            "cite_text": "\u00e2\u20ac\u00a2 Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "C00-2123",
            "refer_sids": [
                169
            ],
            "refer_text": "For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words.",
            "cite_ID": "H01-1062",
            "cite_maker_sids": [
                113
            ],
            "cite_sids": [
                113
            ],
            "cite_text": "To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: \u00e2\u20ac\u00a2 single-word based approach [20];",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "C00-2123",
            "refer_sids": [
                165
            ],
            "refer_text": "We apply a beam search concept as in speech recognition.",
            "cite_ID": "J03-1005",
            "cite_maker_sids": [
                117
            ],
            "cite_sids": [
                115,
                117
            ],
            "cite_text": "This article will present a DP-based beam search decoder for the IBM4 translation model.A preliminary version of the work presented here was published in Tillmann and Ney (2000).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "C00-2123",
            "refer_sids": [
                20
            ],
            "refer_text": "For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.",
            "cite_ID": "J04-2003",
            "cite_maker_sids": [
                35
            ],
            "cite_sids": [
                35
            ],
            "cite_text": "Many existing systems for statistical machine translation 1 1 (Garc\u00b4\u0131a-Varea and Casacuberta 2001; Germann et al. 2001; Nie\u00dfen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "C00-2123",
            "refer_sids": [
                179
            ],
            "refer_text": "For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.",
            "cite_ID": "J04-4002",
            "cite_maker_sids": [
                282
            ],
            "cite_sids": [
                282
            ],
            "cite_text": "We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "C00-2123",
            "refer_sids": [
                136,
                165
            ],
            "refer_text": "A dynamic programming recursion similar to the one in Eq. 2 is evaluated.We apply a beam search concept as in speech recognition.",
            "cite_ID": "N03-1010",
            "cite_maker_sids": [
                16
            ],
            "cite_sids": [
                16
            ],
            "cite_text": "Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "C00-2123",
            "refer_sids": [
                169
            ],
            "refer_text": "For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words.",
            "cite_ID": "P01-1027",
            "cite_maker_sids": [
                127
            ],
            "cite_sids": [
                127
            ],
            "cite_text": "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 14,
            "refer_ID": "C00-2123",
            "refer_sids": [
                165
            ],
            "refer_text": "We apply a beam search concept as in speech recognition.",
            "cite_ID": "P03-1039",
            "cite_maker_sids": [
                113
            ],
            "cite_sids": [
                113
            ],
            "cite_text": "The decoding algorithm employed for this chunk + weight \u00c3\u2014 j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "C00-2123",
            "refer_sids": [
                58
            ],
            "refer_text": "The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either \u00c6 = 0 or \u00c6 = 1 new target words.",
            "cite_ID": "P03-1039",
            "cite_maker_sids": [
                120
            ],
            "cite_sids": [
                120
            ],
            "cite_text": "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).",
            "label": [
                "Implication_Citation",
                "Method_Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "C00-2123",
            "refer_sids": [
                159
            ],
            "refer_text": "This measure has the advantage of being completely automatic.",
            "cite_ID": "W01-1404",
            "cite_maker_sids": [
                5
            ],
            "cite_sids": [
                5
            ],
            "cite_text": "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).",
            "label": [
                "Implication_Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "C00-2123",
            "refer_sids": [
                169
            ],
            "refer_text": "For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is su\u00c6cient to consider only the best 50 words.",
            "cite_ID": "W01-1407",
            "cite_maker_sids": [
                110
            ],
            "cite_sids": [
                110
            ],
            "cite_text": "We used a translation system called \u00e2\u20ac\u0153single- word based approach\u00e2\u20ac described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "C00-2123",
            "refer_sids": [
                165
            ],
            "refer_text": "We apply a beam search concept as in speech recognition.",
            "cite_ID": "W01-1408",
            "cite_maker_sids": [
                47
            ],
            "cite_sids": [
                47
            ],
            "cite_text": "Search algorithms We evaluate the following two search algorithms: \u00e2\u20ac\u00a2 beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "C00-2123",
            "refer_sids": [
                1
            ],
            "refer_text": "In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).",
            "cite_ID": "W02-1020",
            "cite_maker_sids": [
                62
            ],
            "cite_sids": [
                61,
                62
            ],
            "cite_text": "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J \u2014for in p(v1, w2, wm\u22121, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ]
        }
    ]
}