{
    "ID": "N09-1025",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "N09-1025",
            "refer_sids": [
                23
            ],
            "refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "cite_ID": "C10-2052",
            "cite_maker_sids": [
                22
            ],
            "cite_sids": [
                22
            ],
            "cite_text": "We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "N09-1025",
            "refer_sids": [
                9
            ],
            "refer_text": "We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "cite_ID": "D11-1081",
            "cite_maker_sids": [
                6
            ],
            "cite_sids": [
                6
            ],
            "cite_text": "Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "N09-1025",
            "refer_sids": [
                23
            ],
            "refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "cite_ID": "D11-1081",
            "cite_maker_sids": [
                9
            ],
            "cite_sids": [
                9
            ],
            "cite_text": "Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "N09-1025",
            "refer_sids": [
                51
            ],
            "refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "cite_ID": "D11-1081",
            "cite_maker_sids": [
                25
            ],
            "cite_sids": [
                25
            ],
            "cite_text": "(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "N09-1025",
            "refer_sids": [
                32,
                33
            ],
            "refer_text": "Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007).",
            "cite_ID": "D11-1081",
            "cite_maker_sids": [
                159
            ],
            "cite_sids": [
                159
            ],
            "cite_text": "Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "N09-1025",
            "refer_sids": [
                29
            ],
            "refer_text": "Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.",
            "cite_ID": "D11-1081",
            "cite_maker_sids": [
                239
            ],
            "cite_sids": [
                239
            ],
            "cite_text": "Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "N09-1025",
            "refer_sids": [
                51
            ],
            "refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "cite_ID": "D11-1125",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13
            ],
            "cite_text": "The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "N09-1025",
            "refer_sids": [
                14
            ],
            "refer_text": "Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.",
            "cite_ID": "D11-1125",
            "cite_maker_sids": [
                78,
                79
            ],
            "cite_sids": [
                77,
                78,
                79
            ],
            "cite_text": "The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al. (2007) and Chiang et al. (2008b; 2009).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "N09-1025",
            "refer_sids": [
                9
            ],
            "refer_text": "We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "cite_ID": "D11-1125",
            "cite_maker_sids": [
                208
            ],
            "cite_sids": [
                207,
                208,
                209
            ],
            "cite_text": "We used the following feature classes in SBMT and PBMT extended scenarios: \u2022 Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1) \u2022 Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "N09-1025",
            "refer_sids": [
                9
            ],
            "refer_text": "We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "cite_ID": "D11-1125",
            "cite_maker_sids": [
                210
            ],
            "cite_sids": [
                210,
                211
            ],
            "cite_text": "Chiang et al. (2009), Section 4.1):10 \u2022 Rule overlap features \u2022 Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "N09-1025",
            "refer_sids": [
                14
            ],
            "refer_text": "Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.",
            "cite_ID": "D11-1125",
            "cite_maker_sids": [
                227
            ],
            "cite_sids": [
                227,
                228,
                229
            ],
            "cite_text": "5.4.1 MERT We used David Chiang\u2019s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al. (2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "N09-1025",
            "refer_sids": [
                51
            ],
            "refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "cite_ID": "N12-1006",
            "cite_maker_sids": [
                102
            ],
            "cite_sids": [
                102,
                103
            ],
            "cite_text": "Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al.  (2009).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "N09-1025",
            "refer_sids": [
                14
            ],
            "refer_text": "Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.",
            "cite_ID": "P12-1001",
            "cite_maker_sids": [
                147
            ],
            "cite_sids": [
                147
            ],
            "cite_text": "Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 14,
            "refer_ID": "N09-1025",
            "refer_sids": [
                51
            ],
            "refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "cite_ID": "P13-1110",
            "cite_maker_sids": [
                36
            ],
            "cite_sids": [
                36
            ],
            "cite_text": "Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "N09-1025",
            "refer_sids": [
                14
            ],
            "refer_text": "Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.",
            "cite_ID": "P13-1110",
            "cite_maker_sids": [
                102
            ],
            "cite_sids": [
                102
            ],
            "cite_text": "task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "N09-1025",
            "refer_sids": [
                65
            ],
            "refer_text": "Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations.",
            "cite_ID": "P13-1110",
            "cite_maker_sids": [
                115
            ],
            "cite_sids": [
                115
            ],
            "cite_text": "The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "N09-1025",
            "refer_sids": [
                23
            ],
            "refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "cite_ID": "P13-1110",
            "cite_maker_sids": [
                120
            ],
            "cite_sids": [
                120
            ],
            "cite_text": "For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "N09-1025",
            "refer_sids": [
                29
            ],
            "refer_text": "Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.",
            "cite_ID": "P28_n09",
            "cite_maker_sids": [
                51
            ],
            "cite_sids": [
                51
            ],
            "cite_text": "Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "N09-1025",
            "refer_sids": [
                9
            ],
            "refer_text": "We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "cite_ID": "P134_n09",
            "cite_maker_sids": [
                27
            ],
            "cite_sids": [
                27,
                28
            ],
            "cite_text": "First, we used features proposed by Chiang et al. (2009): \u2022 phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) \u2022 target word insertion features \u2022 source word deletion features \u2022 word translation features \u2022 phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "N09-1025",
            "refer_sids": [
                9
            ],
            "refer_text": "We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "cite_ID": "Pmert_n09",
            "cite_maker_sids": [
                15
            ],
            "cite_sids": [
                15
            ],
            "cite_text": "These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 21,
            "refer_ID": "N09-1025",
            "refer_sids": [
                9
            ],
            "refer_text": "We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "cite_ID": "Pmert_n09",
            "cite_maker_sids": [
                16
            ],
            "cite_sids": [
                16
            ],
            "cite_text": "In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 22,
            "refer_ID": "N09-1025",
            "refer_sids": [
                51
            ],
            "refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "cite_ID": "PMTS_n09",
            "cite_maker_sids": [
                218
            ],
            "cite_sids": [
                218
            ],
            "cite_text": "This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 23,
            "refer_ID": "N09-1025",
            "refer_sids": [
                65
            ],
            "refer_text": "Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations.",
            "cite_ID": "PMTS_n09",
            "cite_maker_sids": [
                245
            ],
            "cite_sids": [
                245
            ],
            "cite_text": "The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 24,
            "refer_ID": "N09-1025",
            "refer_sids": [
                51
            ],
            "refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "cite_ID": "PMTS_n09",
            "cite_maker_sids": [
                252
            ],
            "cite_sids": [
                252
            ],
            "cite_text": "We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 25,
            "refer_ID": "N09-1025",
            "refer_sids": [
                29
            ],
            "refer_text": "Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.",
            "cite_ID": "PSMPT_n09",
            "cite_maker_sids": [
                73
            ],
            "cite_sids": [
                73,
                74
            ],
            "cite_text": "Chiang et w(X \u2192 (\u03b1, \u03b3, \u223c )) = \u03bbi\u03c6i (2) i al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 26,
            "refer_ID": "N09-1025",
            "refer_sids": [
                51
            ],
            "refer_text": "We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).",
            "cite_ID": "PTASL_n09",
            "cite_maker_sids": [
                79
            ],
            "cite_sids": [
                79
            ],
            "cite_text": "For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 27,
            "refer_ID": "N09-1025",
            "refer_sids": [
                29
            ],
            "refer_text": "Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.",
            "cite_ID": "PTASL_n09",
            "cite_maker_sids": [
                86
            ],
            "cite_sids": [
                86
            ],
            "cite_text": "Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 28,
            "refer_ID": "N09-1025",
            "refer_sids": [
                23
            ],
            "refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "cite_ID": "W10-1757",
            "cite_maker_sids": [
                48
            ],
            "cite_sids": [
                48
            ],
            "cite_text": "When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 29,
            "refer_ID": "N09-1025",
            "refer_sids": [
                32,
                33
            ],
            "refer_text": "Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007).",
            "cite_ID": "W10-1757",
            "cite_maker_sids": [
                209
            ],
            "cite_sids": [
                209
            ],
            "cite_text": "Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 30,
            "refer_ID": "N09-1025",
            "refer_sids": [
                9
            ],
            "refer_text": "We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.",
            "cite_ID": "W10-1761",
            "cite_maker_sids": [
                53
            ],
            "cite_sids": [
                53,
                54
            ],
            "cite_text": "Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 31,
            "refer_ID": "N09-1025",
            "refer_sids": [
                23
            ],
            "refer_text": "Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.",
            "cite_ID": "W10-1761",
            "cite_maker_sids": [
                92
            ],
            "cite_sids": [
                92
            ],
            "cite_text": "A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 32,
            "refer_ID": "N09-1025",
            "refer_sids": [
                32,
                33
            ],
            "refer_text": "Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007).",
            "cite_ID": "W10-1761",
            "cite_maker_sids": [
                97
            ],
            "cite_sids": [
                97
            ],
            "cite_text": "The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.",
            "label": [
                "Method_Citation"
            ]
        }
    ]
}