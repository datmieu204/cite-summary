{
    "ID": "P06-2124",
    "citations": [
        {
            "Number": 2,
            "refer_ID": "P06-2124",
            "refer_sids": [
                15
            ],
            "refer_text": "In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.",
            "cite_ID": "D11-1084",
            "cite_maker_sids": [
                50
            ],
            "cite_sids": [
                48,
                49,
                50,
                51
            ],
            "cite_text": "There are only a few studies on document-level SMT.Representative work includes Zhao et al.(2006), Tam et al.(2007), Carpuat (2009).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "P06-2124",
            "refer_sids": [
                2,
                15
            ],
            "refer_text": "Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.",
            "cite_ID": "D11-1084",
            "cite_maker_sids": [
                53
            ],
            "cite_sids": [
                52,
                53,
                54
            ],
            "cite_text": "Zhao et al.(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.",
            "label": [
                "Hypothesis_Citation",
                "Method_Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "P06-2124",
            "refer_sids": [
                21,
                37,
                152
            ],
            "refer_text": "We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.The translation lexicon p(f |e) is the key component in this generative process.Topic-specific translation lexicons are learned by a 3-topic BiTAM1.",
            "cite_ID": "P07-1066",
            "cite_maker_sids": [
                28
            ],
            "cite_sids": [
                28,
                29
            ],
            "cite_text": "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "P06-2124",
            "refer_sids": [
                152
            ],
            "refer_text": "Topic-specific translation lexicons are learned by a 3-topic BiTAM1.",
            "cite_ID": "P10-2025",
            "cite_maker_sids": [
                84
            ],
            "cite_sids": [
                84
            ],
            "cite_text": "We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "P06-2124",
            "refer_sids": [
                124,
                194
            ],
            "refer_text": "Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA).Inter takes the intersection of the two directions and generates high-precision alignments;",
            "cite_ID": "P10-2025",
            "cite_maker_sids": [
                86
            ],
            "cite_sids": [
                86
            ],
            "cite_text": "The alignment results for both directions were refined with \u2018GROW\u2019 heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "P06-2124",
            "refer_sids": [
                115,
                116,
                117
            ],
            "refer_text": "The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003).To reduce the data sparsity problem, we introduce two remedies in our models.First: Laplace smoothing.",
            "cite_ID": "P11-2032",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                11
            ],
            "cite_text": "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.",
            "label": [
                "Implication_Citation",
                "Method_Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "P06-2124",
            "refer_sids": [
                2,
                3,
                21
            ],
            "refer_text": "Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.",
            "cite_ID": "P12-1048",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.",
            "label": [
                "Hypothesis_Citation",
                "Method_Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "P06-2124",
            "refer_sids": [
                7,
                10
            ],
            "refer_text": "Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.",
            "cite_ID": "P12-1048",
            "cite_maker_sids": [
                149
            ],
            "cite_sids": [
                149
            ],
            "cite_text": "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.",
            "label": [
                "Implication_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "P06-2124",
            "refer_sids": [
                10,
                83
            ],
            "refer_text": "Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn ).",
            "cite_ID": "P12-1048",
            "cite_maker_sids": [
                160
            ],
            "cite_sids": [
                160
            ],
            "cite_text": "\u00e2\u20ac\u00a2 In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model \u00e2\u20ac\u201d HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "P06-2124",
            "refer_sids": [
                10,
                13
            ],
            "refer_text": "Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.For example, the word shot in \u00e2\u20ac\u0153It was a nice shot.\u00e2\u20ac\u009d should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing.",
            "cite_ID": "P12-1079",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                8
            ],
            "cite_text": "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.",
            "label": [
                "Implication_Citation"
            ]
        },
        {
            "Number": 14,
            "refer_ID": "P06-2124",
            "refer_sids": [
                21
            ],
            "refer_text": "We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.",
            "cite_ID": "P12-1079",
            "cite_maker_sids": [
                40
            ],
            "cite_sids": [
                40
            ],
            "cite_text": "Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).",
            "label": [
                "Hypothesis_Citation",
                "Aim_Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "P06-2124",
            "refer_sids": [
                17,
                21
            ],
            "refer_text": "Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.",
            "cite_ID": "P12-2023",
            "cite_maker_sids": [
                30
            ],
            "cite_sids": [
                30
            ],
            "cite_text": "Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.",
            "label": [
                "Hypothesis_Citation",
                "Aim_Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "P06-2124",
            "refer_sids": [
                21
            ],
            "refer_text": "We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.",
            "cite_ID": "P13-2122",
            "cite_maker_sids": [
                27
            ],
            "cite_sids": [
                27
            ],
            "cite_text": "To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or \u00e2\u20ac\u02dcbiTAM\u00e2\u20ac\u2122 (Zhao and Xing, 2006).",
            "label": [
                "Hypothesis_Citation",
                "Aim_Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "P06-2124",
            "refer_sids": [
                21,
                39,
                45,
                192,
                203
            ],
            "refer_text": "We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework.Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entityNotably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE).As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM1\u223c3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1.",
            "cite_ID": "W07-0722",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13,
                14,
                15
            ],
            "cite_text": "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "P06-2124",
            "refer_sids": [
                116,
                119,
                120
            ],
            "refer_text": "To reduce the data sparsity problem, we introduce two remedies in our models.Second: interpolation smoothing.Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting:",
            "cite_ID": "W07-0722",
            "cite_maker_sids": [
                62
            ],
            "cite_sids": [
                62
            ],
            "cite_text": "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).",
            "label": [
                "Method_Citation"
            ]
        }
    ]
}