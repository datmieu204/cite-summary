{
    "ID": "D10-1058",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "D10-1058",
            "refer_sids": [
                3,
                4,
                6
            ],
            "refer_text": "We built a fertility hidden Markov model by adding fertility to the hidden Markov model.This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.",
            "cite_ID": "C16-1060",
            "cite_maker_sids": [
                48
            ],
            "cite_sids": [
                48
            ],
            "cite_text": "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "D10-1058",
            "refer_sids": [
                18,
                26,
                33,
                34
            ],
            "refer_text": "Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4.Our model is a coherent generative model that combines the HMM and IBM Model 4.Our model is much faster than IBM Model 4.In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM.",
            "cite_ID": "P13-2002",
            "cite_maker_sids": [
                24
            ],
            "cite_sids": [
                24
            ],
            "cite_text": "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "D10-1058",
            "refer_sids": [
                6,
                46
            ],
            "refer_text": "We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.",
            "cite_ID": "P13-2002",
            "cite_maker_sids": [
                33
            ],
            "cite_sids": [
                33
            ],
            "cite_text": ", fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "D10-1058",
            "refer_sids": [
                86
            ],
            "refer_text": "Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (I , , aJ , f J PIPEe2I +1); 1 1 1 1 are further away from the mean have low probability.",
            "cite_ID": "P13-2002",
            "cite_maker_sids": [
                43
            ],
            "cite_sids": [
                43
            ],
            "cite_text": "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "D10-1058",
            "refer_sids": [
                88,
                90
            ],
            "refer_text": "Our model has only one parameter for each target word, which can be learned more reliably.i=1 i! 1 1 1 , ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable i, and we assume i follows a Poisson distribution Poisson(i; (ei)).",
            "cite_ID": "P13-2002",
            "cite_maker_sids": [
                82
            ],
            "cite_sids": [
                82
            ],
            "cite_text": "Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "D10-1058",
            "refer_sids": [
                110
            ],
            "refer_text": "1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.",
            "cite_ID": "P13-2002",
            "cite_maker_sids": [
                99
            ],
            "cite_sids": [
                99
            ],
            "cite_text": "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "D10-1058",
            "refer_sids": [
                113,
                114,
                115
            ],
            "refer_text": "This Gibbs sampling method updates parameters constantly, so it is an online learning algorithm.However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel.Instead, we do batch learning: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step).",
            "cite_ID": "P59105ca",
            "cite_maker_sids": [
                45
            ],
            "cite_sids": [
                45
            ],
            "cite_text": "Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "D10-1058",
            "refer_sids": [
                14,
                26,
                27
            ],
            "refer_text": "Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand.Our model is a coherent generative model that combines the HMM and IBM Model 4.It is easier to understand than IBM Model 4 (see Section 3).",
            "cite_ID": "P87-94",
            "cite_maker_sids": [
                79
            ],
            "cite_sids": [
                79
            ],
            "cite_text": "Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "D10-1058",
            "refer_sids": [
                29,
                30
            ],
            "refer_text": "We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596605, MIT, Massachusetts, USA, 911 October 2010.Qc 2010 Association for Computational Linguistics estimation.",
            "cite_ID": "Pbulletin",
            "cite_maker_sids": [
                104
            ],
            "cite_sids": [
                103,
                104
            ],
            "cite_text": "For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "D10-1058",
            "refer_sids": [
                29,
                30,
                107,
                108
            ],
            "refer_text": "We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596605, MIT, Massachusetts, USA, 911 October 2010.Qc 2010 Association for Computational Linguistics estimation.Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977).The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential.",
            "cite_ID": "Pbulletin",
            "cite_maker_sids": [
                131
            ],
            "cite_sids": [
                131
            ],
            "cite_text": "Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "D10-1058",
            "refer_sids": [
                3
            ],
            "refer_text": "We built a fertility hidden Markov model by adding fertility to the hidden Markov model.",
            "cite_ID": "Pcoling_D10",
            "cite_maker_sids": [
                232
            ],
            "cite_sids": [
                232
            ],
            "cite_text": "Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "D10-1058",
            "refer_sids": [
                3,
                46
            ],
            "refer_text": "We built a fertility hidden Markov model by adding fertility to the hidden Markov model.We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.",
            "cite_ID": "Pproc_D10",
            "cite_maker_sids": [
                24
            ],
            "cite_sids": [
                24
            ],
            "cite_text": "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "D10-1058",
            "refer_sids": [
                86
            ],
            "refer_text": "Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (I , , aJ , f J PIPEe2I +1); 1 1 1 1 are further away from the mean have low probability.",
            "cite_ID": "Pproc_D10",
            "cite_maker_sids": [
                38
            ],
            "cite_sids": [
                38
            ],
            "cite_text": "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 14,
            "refer_ID": "D10-1058",
            "refer_sids": [
                6,
                46
            ],
            "refer_text": "We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.",
            "cite_ID": "Pproc_D10",
            "cite_maker_sids": [
                39
            ],
            "cite_sids": [
                39
            ],
            "cite_text": "I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "D10-1058",
            "refer_sids": [
                88,
                90
            ],
            "refer_text": "Our model has only one parameter for each target word, which can be learned more reliably.i=1 i! 1 1 1 , ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable i, and we assume i follows a Poisson distribution Poisson(i; (ei)).",
            "cite_ID": "Pproc_D10",
            "cite_maker_sids": [
                86
            ],
            "cite_sids": [
                86
            ],
            "cite_text": "Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "D10-1058",
            "refer_sids": [
                110
            ],
            "refer_text": "1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.",
            "cite_ID": "Pproc_D10",
            "cite_maker_sids": [
                104
            ],
            "cite_sids": [
                104
            ],
            "cite_text": "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "D10-1058",
            "refer_sids": [
                3,
                4,
                5,
                6
            ],
            "refer_text": "We built a fertility hidden Markov model by adding fertility to the hidden Markov model.This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.It is similar in some ways to IBM Model 4, but is much easier to understand.We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.",
            "cite_ID": "Q13-1024",
            "cite_maker_sids": [
                60
            ],
            "cite_sids": [
                60
            ],
            "cite_text": "The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "D10-1058",
            "refer_sids": [
                46
            ],
            "refer_text": "We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.",
            "cite_ID": "Q13-1024",
            "cite_maker_sids": [
                130
            ],
            "cite_sids": [
                130
            ],
            "cite_text": "Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.",
            "label": [
                "Method_Citation"
            ]
        }
    ]
}