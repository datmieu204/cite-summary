{
    "ID": "J96-3004",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "J96-3004",
            "refer_sids": [
                89,
                90,
                91
            ],
            "refer_text": "Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexi\u00c2\u00ad cal rule-based approaches, and approaches that combine lexical information with sta\u00c2\u00ad tistical information.The present proposal falls into the last group.Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.",
            "cite_ID": "A00-2032",
            "cite_maker_sids": [
                142
            ],
            "cite_sids": [
                141,
                142
            ],
            "cite_text": "Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pub\u00c2\u00ad lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical ap\u00c2\u00ad proach.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "J96-3004",
            "refer_sids": [
                297,
                298
            ],
            "refer_text": "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.Thus, rather than give a single evaluative score, we prefer to compare the performance of our method with the judgments of several human subjects.",
            "cite_ID": "C00-2095",
            "cite_maker_sids": [
                80
            ],
            "cite_sids": [
                80
            ],
            "cite_text": "As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.",
            "label": [
                "Implication_Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108,
                109,
                112
            ],
            "refer_text": "The most popular approach to dealing with seg\u00c2\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin\u00ad ning) of the sentence is reached.The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.",
            "cite_ID": "C02-1049",
            "cite_maker_sids": [
                58
            ],
            "cite_sids": [
                58
            ],
            "cite_text": "Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen & Liu, 1992, Sproat et al, 1996).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "J96-3004",
            "refer_sids": [
                92,
                93
            ],
            "refer_text": "In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.",
            "cite_ID": "C02-1049",
            "cite_maker_sids": [
                127
            ],
            "cite_sids": [
                124,
                127
            ],
            "cite_text": "Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "J96-3004",
            "refer_sids": [
                51
            ],
            "refer_text": "Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation.",
            "cite_ID": "C02-1080",
            "cite_maker_sids": [
                20
            ],
            "cite_sids": [
                20,
                21
            ],
            "cite_text": "Chinese NE recognition is much more difficult than that in English due to two major problems.The first is the word segmentation problem (Sproat et al. 96, Palmer 97).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108
            ],
            "refer_text": "The most popular approach to dealing with seg\u00c2\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.",
            "cite_ID": "C02-1143",
            "cite_maker_sids": [
                107
            ],
            "cite_sids": [
                107
            ],
            "cite_text": "We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "J96-3004",
            "refer_sids": [
                297,
                325
            ],
            "refer_text": "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 On",
            "cite_ID": "E09-1063",
            "cite_maker_sids": [
                107
            ],
            "cite_sids": [
                107
            ],
            "cite_text": "First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).",
            "label": [
                "Implication_Citation",
                "Results_Citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "J96-3004",
            "refer_sids": [
                2,
                33
            ],
            "refer_text": "An initial step of any text\u00c2\u00ad analysis task is the tokenization of the input into words.Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.",
            "cite_ID": "I05-3031",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                6,
                7
            ],
            "cite_text": "The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "J96-3004",
            "refer_sids": [
                297,
                325
            ],
            "refer_text": "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15",
            "cite_ID": "J00-3004",
            "cite_maker_sids": [
                42
            ],
            "cite_sids": [
                42
            ],
            "cite_text": "According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the \"correct\" segmentation, and the figure reduces as more people become involved.",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "J96-3004",
            "refer_sids": [
                399
            ],
            "refer_text": "This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.",
            "cite_ID": "J00-3004",
            "cite_maker_sids": [
                96
            ],
            "cite_sids": [
                96,
                97
            ],
            "cite_text": "Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "J96-3004",
            "refer_sids": [
                89
            ],
            "refer_text": "Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexi\u00c2\u00ad cal rule-based approaches, and approaches that combine lexical information with sta\u00c2\u00ad tistical information.",
            "cite_ID": "J04-1004",
            "cite_maker_sids": [
                53
            ],
            "cite_sids": [
                53
            ],
            "cite_text": "In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108
            ],
            "refer_text": "The most popular approach to dealing with seg\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.",
            "cite_ID": "J04-1004",
            "cite_maker_sids": [
                113
            ],
            "cite_sids": [
                113
            ],
            "cite_text": "There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 14,
            "refer_ID": "J96-3004",
            "refer_sids": [
                133
            ],
            "refer_text": "Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.",
            "cite_ID": "J04-1004",
            "cite_maker_sids": [
                211
            ],
            "cite_sids": [
                211
            ],
            "cite_text": "In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).",
            "label": [
                "Implication_Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "J96-3004",
            "refer_sids": [
                297
            ],
            "refer_text": "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.",
            "cite_ID": "J04-1004",
            "cite_maker_sids": [
                321
            ],
            "cite_sids": [
                321
            ],
            "cite_text": "As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.",
            "label": [
                "Implication_Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "J96-3004",
            "refer_sids": [
                398
            ],
            "refer_text": "In this paper we have argued that Chinese word segmentation can be modeled ef\u00c2\u00ad fectively using weighted finite-state transducers.",
            "cite_ID": "J05-4005",
            "cite_maker_sids": [
                88
            ],
            "cite_sids": [
                88,
                89
            ],
            "cite_text": "A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "J96-3004",
            "refer_sids": [
                325
            ],
            "refer_text": "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75",
            "cite_ID": "J05-4005",
            "cite_maker_sids": [
                126
            ],
            "cite_sids": [
                125,
                126
            ],
            "cite_text": "As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "J96-3004",
            "refer_sids": [
                325
            ],
            "refer_text": "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75",
            "cite_ID": "J05-4005",
            "cite_maker_sids": [
                132
            ],
            "cite_sids": [
                131,
                132
            ],
            "cite_text": "Similarly, Sproat et al.(1996) also uses multiple human judges.",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 19,
            "refer_ID": "J96-3004",
            "refer_sids": [
                399
            ],
            "refer_text": "This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.",
            "cite_ID": "J05-4005",
            "cite_maker_sids": [
                490
            ],
            "cite_sids": [
                489,
                490
            ],
            "cite_text": "The Chinese person-name model is a modified version of that described in Sproat et al.(1996).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 20,
            "refer_ID": "J96-3004",
            "refer_sids": [
                325
            ],
            "refer_text": "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75",
            "cite_ID": "J11-1005",
            "cite_maker_sids": [
                123
            ],
            "cite_sids": [
                123
            ],
            "cite_text": "Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 21,
            "refer_ID": "J96-3004",
            "refer_sids": [
                296,
                297
            ],
            "refer_text": "Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.",
            "cite_ID": "J11-3001",
            "cite_maker_sids": [
                326
            ],
            "cite_sids": [
                326
            ],
            "cite_text": "Gold standards, however, 435 cannot be uni\ufb01ed into a single standard (Fung and Wu 1994; Sproat et al. 1996).",
            "label": [
                "Implication_Citation"
            ]
        },
        {
            "Number": 23,
            "refer_ID": "J96-3004",
            "refer_sids": [
                21,
                33
            ],
            "refer_text": "In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces;Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.",
            "cite_ID": "J97-4004",
            "cite_maker_sids": [
                9
            ],
            "cite_sids": [
                9
            ],
            "cite_text": "Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 25,
            "refer_ID": "J96-3004",
            "refer_sids": [
                138
            ],
            "refer_text": "More formally, we start by representing the dictionary D as a Weighted Finite State Trans\u00c2\u00ad ducer (WFST) (Pereira, Riley, and Sproat 1994).",
            "cite_ID": "J97-4004",
            "cite_maker_sids": [
                613
            ],
            "cite_sids": [
                612,
                613
            ],
            "cite_text": "The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 26,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108
            ],
            "refer_text": "The most popular approach to dealing with seg\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.",
            "cite_ID": "J97-4004",
            "cite_maker_sids": [
                621
            ],
            "cite_sids": [
                621,
                622
            ],
            "cite_text": "While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are ap\u00c2\u00ad parently employed neither in Sproat et al.(1996) nor in Ma (1996).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 27,
            "refer_ID": "J96-3004",
            "refer_sids": [
                138
            ],
            "refer_text": "More formally, we start by representing the dictionary D as a Weighted Finite State Trans\u00c2\u00ad ducer (WFST) (Pereira, Riley, and Sproat 1994).",
            "cite_ID": "N10-1068",
            "cite_maker_sids": [
                6
            ],
            "cite_sids": [
                6
            ],
            "cite_text": "Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:\u00e2\u20ac\u00a2 WFSTs provide a uniform knowledge represen tation.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 28,
            "refer_ID": "J96-3004",
            "refer_sids": [
                138
            ],
            "refer_text": "More formally, we start by representing the dictionary D as a Weighted Finite State Trans\u00c2\u00ad ducer (WFST) (Pereira, Riley, and Sproat 1994).",
            "cite_ID": "P03-1035",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                41,
                42
            ],
            "cite_text": "One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 29,
            "refer_ID": "J96-3004",
            "refer_sids": [
                419,
                420
            ],
            "refer_text": "The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.",
            "cite_ID": "P03-1035",
            "cite_maker_sids": [
                122
            ],
            "cite_sids": [
                122
            ],
            "cite_text": "Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 30,
            "refer_ID": "J96-3004",
            "refer_sids": [
                281
            ],
            "refer_text": "Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.",
            "cite_ID": "P03-1035",
            "cite_maker_sids": [
                155
            ],
            "cite_sids": [
                154,
                155
            ],
            "cite_text": "5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 31,
            "refer_ID": "J96-3004",
            "refer_sids": [
                283,
                284
            ],
            "refer_text": "Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal Chi\u00ad nese personal name, retains a foreign flavor because of liM.As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabil\u00ad ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.",
            "cite_ID": "P06-1010",
            "cite_maker_sids": [
                43
            ],
            "cite_sids": [
                42,
                43
            ],
            "cite_text": "Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 32,
            "refer_ID": "J96-3004",
            "refer_sids": [
                88
            ],
            "refer_text": "There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).",
            "cite_ID": "P06-1126",
            "cite_maker_sids": [
                7
            ],
            "cite_sids": [
                7
            ],
            "cite_text": "Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).",
            "label": [
                "Aim_Citation"
            ]
        },
        {
            "Number": 33,
            "refer_ID": "J96-3004",
            "refer_sids": [
                283,
                284
            ],
            "refer_text": "Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal Chi\u00ad nese personal name, retains a foreign flavor because of liM. As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabil\u00ad ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.",
            "cite_ID": "P07-1015",
            "cite_maker_sids": [
                113
            ],
            "cite_sids": [
                113
            ],
            "cite_text": "Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 34,
            "refer_ID": "J96-3004",
            "refer_sids": [
                284
            ],
            "refer_text": "As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabil\u00c2\u00ad ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.",
            "cite_ID": "P07-1016",
            "cite_maker_sids": [
                70
            ],
            "cite_sids": [
                70
            ],
            "cite_text": "As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 35,
            "refer_ID": "J96-3004",
            "refer_sids": [
                54
            ],
            "refer_text": "A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.",
            "cite_ID": "P12-1110",
            "cite_maker_sids": [
                105
            ],
            "cite_sids": [
                105
            ],
            "cite_text": "3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 36,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108,
                109
            ],
            "refer_text": "The most popular approach to dealing with seg\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin\u00ad ning) of the sentence is reached.",
            "cite_ID": "P12-1111",
            "cite_maker_sids": [
                91
            ],
            "cite_sids": [
                91
            ],
            "cite_text": "In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 37,
            "refer_ID": "J96-3004",
            "refer_sids": [
                87,
                88
            ],
            "refer_text": "Previous Work.There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).",
            "cite_ID": "P97-1041",
            "cite_maker_sids": [
                12
            ],
            "cite_sids": [
                12
            ],
            "cite_text": "For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 38,
            "refer_ID": "J96-3004",
            "refer_sids": [
                419,
                420
            ],
            "refer_text": "The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.",
            "cite_ID": "P98-1076",
            "cite_maker_sids": [
                145
            ],
            "cite_sids": [
                144,
                145
            ],
            "cite_text": "The actual implementation of the weighted finite\u00c2\u00ad state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.",
            "label": [
                "Implication_Citation"
            ]
        },
        {
            "Number": 39,
            "refer_ID": "J96-3004",
            "refer_sids": [
                419
            ],
            "refer_text": "The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.",
            "cite_ID": "P98-1076",
            "cite_maker_sids": [
                150
            ],
            "cite_sids": [
                149,
                150
            ],
            "cite_text": "utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 40,
            "refer_ID": "J96-3004",
            "refer_sids": [
                356
            ],
            "refer_text": "The performance was 80.99% recall and 61.83% precision.",
            "cite_ID": "P99-1036",
            "cite_maker_sids": [
                6
            ],
            "cite_sids": [
                5,
                6
            ],
            "cite_text": "In Japanese, around 95% word segmentation ac\u00c2\u00ad curacy is reported by using a word-based lan\u00c2\u00ad guage model and the Viterbi-like dynamic program\u00c2\u00ad ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Mat\u00c2\u00ad sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 41,
            "refer_ID": "J96-3004",
            "refer_sids": [
                415,
                416,
                417
            ],
            "refer_text": "The major problem for our seg\u00c2\u00ad menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).We have provided methods for handling certain classes of unknown words, and models for other classes could be provided, as we have noted.However, there will remain a large number of words that are not readily adduced to any produc\u00c2\u00ad tive pattern and that would simply have to be added to the dictionary.",
            "cite_ID": "P99-1036",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                8
            ],
            "cite_text": "There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).",
            "label": [
                "Implication_Citation"
            ]
        },
        {
            "Number": 42,
            "refer_ID": "J96-3004",
            "refer_sids": [
                398,
                399
            ],
            "refer_text": "In this paper we have argued that Chinese word segmentation can be modeled ef\u00c2\u00ad fectively using weighted finite-state transducers.This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.",
            "cite_ID": "P99-1036",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10
            ],
            "cite_text": "To improve word segmenta\u00c2\u00ad tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 43,
            "refer_ID": "J96-3004",
            "refer_sids": [
                128
            ],
            "refer_text": "Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recal",
            "cite_ID": "P99-1036",
            "cite_maker_sids": [
                178
            ],
            "cite_sids": [
                178
            ],
            "cite_text": "Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 44,
            "refer_ID": "J96-3004",
            "refer_sids": [
                116
            ],
            "refer_text": "Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).",
            "cite_ID": "W00-0803",
            "cite_maker_sids": [
                29
            ],
            "cite_sids": [
                29
            ],
            "cite_text": "Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 45,
            "refer_ID": "J96-3004",
            "refer_sids": [
                415,
                417
            ],
            "refer_text": "The major problem for our seg\u00c2\u00ad menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).However, there will remain a large number of words that are not readily adduced to any produc\u00c2\u00ad tive pattern and that would simply have to be added to the dictionary.",
            "cite_ID": "W00-1207",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10
            ],
            "cite_text": "Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.",
            "label": [
                "Implication_Citation"
            ]
        },
        {
            "Number": 46,
            "refer_ID": "J96-3004",
            "refer_sids": [
                20,
                33
            ],
            "refer_text": "Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writ\u00c2\u00ad ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.",
            "cite_ID": "W01-0513",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                40,
                41
            ],
            "cite_text": "The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).",
            "label": [
                "Implication_Citation"
            ]
        },
        {
            "Number": 47,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108,
                109
            ],
            "refer_text": "The most popular approach to dealing with seg\u00c2\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin\u00ad ning) of the sentence is reached.",
            "cite_ID": "W02-1117",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13
            ],
            "cite_text": "For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 48,
            "refer_ID": "J96-3004",
            "refer_sids": [
                65,
                168,
                169,
                170
            ],
            "refer_text": "In this paper we present a stochastic finite-state model for segmenting Chinese text into wordsWord frequencies are estimated by a re-estimation procedure that involves apply\u00ad ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of.newspaper material, but also including kungfu fiction, Buddhist tracts, and scientific material.This larger corpus was kindly provided to us by United Informatics Inc.,",
            "cite_ID": "W02-1808",
            "cite_maker_sids": [
                5
            ],
            "cite_sids": [
                5
            ],
            "cite_text": "Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 49,
            "refer_ID": "J96-3004",
            "refer_sids": [
                325
            ],
            "refer_text": "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement",
            "cite_ID": "W03-1025",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 50,
            "refer_ID": "J96-3004",
            "refer_sids": [
                0,
                325
            ],
            "refer_text": "A Stochastic Finite-State Word-Segmentation Algorithm for ChineseThe average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement",
            "cite_ID": "W03-1025",
            "cite_maker_sids": [
                180
            ],
            "cite_sids": [
                180
            ],
            "cite_text": "Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 51,
            "refer_ID": "J96-3004",
            "refer_sids": [
                138,
                149,
                150
            ],
            "refer_text": "More formally, we start by representing the dictionary D as a Weighted Finite State Trans\u00ad ducer (WFST) (Pereira, Riley, and Sproat 1994).This FSA I can be segmented into words by composing Id(I) with D*, to form the WFST shown in Figure 2(c), then selecting the best path through this WFST to produce the WFST in Figure 2(d).This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.",
            "cite_ID": "W03-1025",
            "cite_maker_sids": [
                187
            ],
            "cite_sids": [
                186,
                187
            ],
            "cite_text": "Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 52,
            "refer_ID": "J96-3004",
            "refer_sids": [
                65
            ],
            "refer_text": "In this paper we present a stochastic finite-state model for segmenting Chinese text into words",
            "cite_ID": "W03-1728",
            "cite_maker_sids": [
                3
            ],
            "cite_sids": [
                3
            ],
            "cite_text": "This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 53,
            "refer_ID": "J96-3004",
            "refer_sids": [
                419
            ],
            "refer_text": "The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.",
            "cite_ID": "W05-0709",
            "cite_maker_sids": [
                83
            ],
            "cite_sids": [
                83
            ],
            "cite_text": "In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 54,
            "refer_ID": "J96-3004",
            "refer_sids": [
                284
            ],
            "refer_text": "As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabil\u00c2\u00ad ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.",
            "cite_ID": "W06-1630",
            "cite_maker_sids": [
                118
            ],
            "cite_sids": [
                118
            ],
            "cite_text": "The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 55,
            "refer_ID": "J96-3004",
            "refer_sids": [
                305
            ],
            "refer_text": "A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.",
            "cite_ID": "W10-3212",
            "cite_maker_sids": [
                16
            ],
            "cite_sids": [
                16
            ],
            "cite_text": "In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 56,
            "refer_ID": "J96-3004",
            "refer_sids": [
                325
            ],
            "refer_text": "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement",
            "cite_ID": "W10-3708",
            "cite_maker_sids": [
                16
            ],
            "cite_sids": [
                16
            ],
            "cite_text": "Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 57,
            "refer_ID": "J96-3004",
            "refer_sids": [
                92,
                93,
                103
            ],
            "refer_text": "In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.Church and Hanks [1989]), and we have used lists of character pairs ranked by mutual information to expand our own dictionary.",
            "cite_ID": "W11-0823",
            "cite_maker_sids": [
                174
            ],
            "cite_sids": [
                174
            ],
            "cite_text": "There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 58,
            "refer_ID": "J96-3004",
            "refer_sids": [
                325
            ],
            "refer_text": "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement",
            "cite_ID": "W12-1011",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                41
            ],
            "cite_text": "Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 59,
            "refer_ID": "J96-3004",
            "refer_sids": [
                325
            ],
            "refer_text": "The average agreement among the human judges is .76",
            "cite_ID": "W12-1011",
            "cite_maker_sids": [
                204
            ],
            "cite_sids": [
                204
            ],
            "cite_text": "No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 60,
            "refer_ID": "J96-3004",
            "refer_sids": [
                158
            ],
            "refer_text": "The cost is computed as follows, where N is the corpus size and f is the frequency: (1) Besides actual words from the base dictionary, the lexicon contains all hanzi in the Big 5 Chinese code/ with their pronunciation(s), plus entries for other characters that can be found in Chinese text, such as Roman letters, numerals, and special symbols.",
            "cite_ID": "W12-2303",
            "cite_maker_sids": [
                12
            ],
            "cite_sids": [
                11,
                12
            ],
            "cite_text": "An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 61,
            "refer_ID": "J96-3004",
            "refer_sids": [
                398,
                399
            ],
            "refer_text": "In this paper we have argued that Chinese word segmentation can be modeled ef\u00c2\u00ad fectively using weighted finite-state transducers.This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.",
            "cite_ID": "W12-2303",
            "cite_maker_sids": [
                157
            ],
            "cite_sids": [
                155,
                156,
                157
            ],
            "cite_text": "There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.",
            "label": [
                "Implication_Citation"
            ]
        },
        {
            "Number": 62,
            "refer_ID": "J96-3004",
            "refer_sids": [
                67,
                68,
                69
            ],
            "refer_text": "The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.It also incorporates the Good-Turing method (Baayen 1989; Church and Gale 1991) in estimating the likelihoods of previously unseen con\u00ad structions, including morphological derivatives and personal names.We will evaluate various specific aspects of the segmentation, as well as the overall segmentation per\u00ad formance.",
            "cite_ID": "W97-0120",
            "cite_maker_sids": [
                26
            ],
            "cite_sids": [
                26
            ],
            "cite_text": "One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 63,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108,
                109
            ],
            "refer_text": "The most popular approach to dealing with seg\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin\u00ad ning) of the sentence is reached.",
            "cite_ID": "W97-0120",
            "cite_maker_sids": [
                69
            ],
            "cite_sids": [
                69
            ],
            "cite_text": "We used a simple greedy algorithm described in [Sproat et al., 1996].",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 64,
            "refer_ID": "J96-3004",
            "refer_sids": [
                190,
                191
            ],
            "refer_text": "The morphological anal\u00adysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.each word in the lexicon whether or not each string is actually an instance of the word in question.",
            "cite_ID": "W97-0120",
            "cite_maker_sids": [
                73
            ],
            "cite_sids": [
                73
            ],
            "cite_text": "[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 65,
            "refer_ID": "J96-3004",
            "refer_sids": [
                108,
                109
            ],
            "refer_text": "The most popular approach to dealing with seg\u00ad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin\u00ad ning) of the sentence is reached.",
            "cite_ID": "W97-0120",
            "cite_maker_sids": [
                86
            ],
            "cite_sids": [
                86
            ],
            "cite_text": "The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it \"maximum matching\", we call this method \"longest match\" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 66,
            "refer_ID": "J96-3004",
            "refer_sids": [
                142,
                143,
                356
            ],
            "refer_text": "We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items.selected; and that recall is defined to be the number of correct hits divided by the number of items that should have been selected.The performance was 80.99% recall and 61.83% precision.",
            "cite_ID": "W97-0120",
            "cite_maker_sids": [
                121
            ],
            "cite_sids": [
                121
            ],
            "cite_text": "Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).",
            "label": [
                "Method_Citation",
                "Results_Citation"
            ]
        },
        {
            "Number": 67,
            "refer_ID": "J96-3004",
            "refer_sids": [
                65
            ],
            "refer_text": "In this paper we present a stochastic finite-state model for segmenting Chinese text into words",
            "cite_ID": "W97-0316",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                10,
                11
            ],
            "cite_text": "Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).",
            "label": [
                "Method_Citation"
            ]
        }
    ]
}