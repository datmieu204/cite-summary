{
    "ID": "D10-1083",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "D10-1083",
            "refer_sids": [
                99
            ],
            "refer_text": "TheFigure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration",
            "cite_ID": "D11-1056",
            "cite_maker_sids": [
                264
            ],
            "cite_sids": [
                263,
                264
            ],
            "cite_text": "Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "D10-1083",
            "refer_sids": [
                22
            ],
            "refer_text": "In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.",
            "cite_ID": "D11-1059",
            "cite_maker_sids": [
                11
            ],
            "cite_sids": [
                10,
                11
            ],
            "cite_text": "This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "D10-1083",
            "refer_sids": [
                239
            ],
            "refer_text": "Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints.",
            "cite_ID": "D11-1059",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                16,
                17
            ],
            "cite_text": "More recently, Lee et al.(2010) presented a new type-based model, and also reported very good results.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "D10-1083",
            "refer_sids": [
                52
            ],
            "refer_text": "We consider the unsupervised POS induction problem without the use of a tagging dictionary.",
            "cite_ID": "D11-1059",
            "cite_maker_sids": [
                32
            ],
            "cite_sids": [
                32
            ],
            "cite_text": "As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "D10-1083",
            "refer_sids": [
                112
            ],
            "refer_text": "For all languages we do not make use of a tagging dictionary.",
            "cite_ID": "D11-1059",
            "cite_maker_sids": [
                91
            ],
            "cite_sids": [
                90,
                91
            ],
            "cite_text": "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.(2010).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "D10-1083",
            "refer_sids": [
                6
            ],
            "refer_text": "Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts.",
            "cite_ID": "D11-1059",
            "cite_maker_sids": [
                130
            ],
            "cite_sids": [
                129,
                130
            ],
            "cite_text": "Following Lee et al.(2010) we used only the training sections for each language.",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "D10-1083",
            "refer_sids": [
                9
            ],
            "refer_text": "Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank.",
            "cite_ID": "D12-1086",
            "cite_maker_sids": [
                74
            ],
            "cite_sids": [
                74
            ],
            "cite_text": "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "D10-1083",
            "refer_sids": [
                52
            ],
            "refer_text": "We consider the unsupervised POS induction problem without the use of a tagging dictionary.",
            "cite_ID": "D12-1125",
            "cite_maker_sids": [
                213
            ],
            "cite_sids": [
                213
            ],
            "cite_text": "vised POS induction algorithm (Lee et al., 2010)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "D10-1083",
            "refer_sids": [
                52
            ],
            "refer_text": "We consider the unsupervised POS induction problem without the use of a tagging dictionary.",
            "cite_ID": "D12-1127",
            "cite_maker_sids": [
                13
            ],
            "cite_sids": [
                13
            ],
            "cite_text": "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; Grac\u00c2\u00b8a et al., 2011; Lee et al., 2010).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "D10-1083",
            "refer_sids": [
                243
            ],
            "refer_text": "We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.",
            "cite_ID": "D13-1004",
            "cite_maker_sids": [
                27
            ],
            "cite_sids": [
                27
            ],
            "cite_text": "Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 13,
            "refer_ID": "D10-1083",
            "refer_sids": [
                243
            ],
            "refer_text": "We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.",
            "cite_ID": "N12-1045",
            "cite_maker_sids": [
                14
            ],
            "cite_sids": [
                14
            ],
            "cite_text": "Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)",
            "label": [
                "Aim_Citation"
            ]
        },
        {
            "Number": 14,
            "refer_ID": "D10-1083",
            "refer_sids": [
                27,
                85,
                97
            ],
            "refer_text": "First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns).Learned Tag Prior (PRIOR) We next assume there exists a single prior distribution \u03c8 over tag assignments drawn from DIRICHLET(\u03b2, K ).During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior:",
            "cite_ID": "P11-1087",
            "cite_maker_sids": [
                41
            ],
            "cite_sids": [
                40,
                41,
                42,
                43
            ],
            "cite_text": "Recently Lee et al.(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al.(1992)\u00e2\u20ac\u2122s one-class HMM.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 15,
            "refer_ID": "D10-1083",
            "refer_sids": [
                155
            ],
            "refer_text": "5 60.6 Table 3: Multilingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5).",
            "cite_ID": "P11-1087",
            "cite_maker_sids": [
                153
            ],
            "cite_sids": [
                152,
                153,
                154
            ],
            "cite_text": "It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.(2010).That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 16,
            "refer_ID": "D10-1083",
            "refer_sids": [
                236
            ],
            "refer_text": "We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.",
            "cite_ID": "P13-1150",
            "cite_maker_sids": [
                55
            ],
            "cite_sids": [
                55
            ],
            "cite_text": "Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 17,
            "refer_ID": "D10-1083",
            "refer_sids": [
                20,
                21
            ],
            "refer_text": "The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag.",
            "cite_ID": "W11-0301",
            "cite_maker_sids": [
                102
            ],
            "cite_sids": [
                102
            ],
            "cite_text": "Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 18,
            "refer_ID": "D10-1083",
            "refer_sids": [
                236
            ],
            "refer_text": "We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.",
            "cite_ID": "W12-1914",
            "cite_maker_sids": [
                8
            ],
            "cite_sids": [
                7,
                8
            ],
            "cite_text": "Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.(2010), Lamar et al.",
            "label": [
                "Aim_Citation",
                "Method_Citation"
            ]
        }
    ]
}