{
    "ID": "J98-2005",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "J98-2005",
            "refer_sids": [
                17
            ],
            "refer_text": "We will show that in both cases the estimated probability is tight.",
            "cite_ID": "J01-2004",
            "cite_maker_sids": [
                72
            ],
            "cite_sids": [
                72
            ],
            "cite_text": "Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.",
            "label": [
                "Aim_Citation"
            ]
        },
        {
            "Number": 2,
            "refer_ID": "J98-2005",
            "refer_sids": [
                45
            ],
            "refer_text": "(8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H <B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, \"relative frequency,\" estimator.",
            "cite_ID": "N03-1027",
            "cite_maker_sids": [
                56
            ],
            "cite_sids": [
                56
            ],
            "cite_text": "Chi and Geman (1998) proved that this con\u00addition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 3,
            "refer_ID": "J98-2005",
            "refer_sids": [
                17
            ],
            "refer_text": "We will show that in both cases the estimated probability is tight.",
            "cite_ID": "P01-1017",
            "cite_maker_sids": [
                79
            ],
            "cite_sids": [
                79
            ],
            "cite_text": "When a PCFG probability distribu\u00adtion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum\u00adming to one) probability distribution over strings [5], thus making them appropriate for language models.",
            "label": [
                "Aim_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "J98-2005",
            "refer_sids": [
                16,
                17
            ],
            "refer_text": "If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?We will show that in both cases the estimated probability is tight.",
            "cite_ID": "P13-1102",
            "cite_maker_sids": [
                23
            ],
            "cite_sids": [
                23
            ],
            "cite_text": "Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).",
            "label": [
                "Aim_Citation"
            ]
        }
    ]
}