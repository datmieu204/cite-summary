{
    "ID": "X96-1048",
    "citations": [
        {
            "Number": 1,
            "refer_ID": "X96-1048",
            "refer_sids": [
                357
            ],
            "refer_text": "In addition, there are plans to put evaluations on line, with public access, starting with the NE evaluation; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure.",
            "cite_ID": "A97-1028",
            "cite_maker_sids": [
                3
            ],
            "cite_sids": [
                3
            ],
            "cite_text": "Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).",
            "label": [
                "Implication_Citation"
            ]
        },
        {
            "Number": 4,
            "refer_ID": "X96-1048",
            "refer_sids": [
                4,
                113
            ],
            "refer_text": "The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus.",
            "cite_ID": "J00-4003",
            "cite_maker_sids": [
                20
            ],
            "cite_sids": [
                12,
                20
            ],
            "cite_text": "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic interpretation as well, for example, at the Sixth and Seventh Message Understa nd\u00ad ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 5,
            "refer_ID": "X96-1048",
            "refer_sids": [
                129,
                132,
                133
            ],
            "refer_text": "In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted.There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc..Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns.",
            "cite_ID": "J00-4003",
            "cite_maker_sids": [
                72
            ],
            "cite_sids": [
                71,
                72
            ],
            "cite_text": "The third main result was that we found very little agreement between our sub\u00ad jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for.example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 6,
            "refer_ID": "X96-1048",
            "refer_sids": [
                13
            ],
            "refer_text": "\u2022 Coreference (CO) --Insert SGML tags into the text to link strings that represent coreferring noun phrases.",
            "cite_ID": "W97-1307",
            "cite_maker_sids": [
                29
            ],
            "cite_sids": [
                29,
                30
            ],
            "cite_text": "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod.uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems' ability to recog nize coreference among noun phrases (Sund heim, 1995).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 7,
            "refer_ID": "X96-1048",
            "refer_sids": [
                174
            ],
            "refer_text": "5 The highest score for the PERSON object, 95% recall and 95% precision, is close to the highest score on the NE subcategorization for person, which was 98% recall and 99% precision..",
            "cite_ID": "P06-1059",
            "cite_maker_sids": [
                10
            ],
            "cite_sids": [
                10
            ],
            "cite_text": "For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 8,
            "refer_ID": "X96-1048",
            "refer_sids": [
                27,
                28,
                31,
                227
            ],
            "refer_text": "CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994.The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes, whose retrieval engine is based on a context-vector model, producing a ranked list of hits according to degree of match with a keyword search query.Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.",
            "cite_ID": "C04-1126",
            "cite_maker_sids": [
                49
            ],
            "cite_sids": [
                49,
                50,
                51
            ],
            "cite_text": "The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the an\u00adnotators sometimes found di.cult to interpret (Sundheim, 1995).Interannotator agreement was measured on 30 texts which were examined by two annotators.It was found to be 83% when one annotator\u2019s templates were assumed to be correct and compared with the other.",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 9,
            "refer_ID": "X96-1048",
            "refer_sids": [
                212,
                213
            ],
            "refer_text": "In this article, the management succession scenario will be used as the basis for discussion.The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure.",
            "cite_ID": "C04-1126",
            "cite_maker_sids": [
                34
            ],
            "cite_sids": [
                34
            ],
            "cite_text": "The test corpus consists of 100 Wall Street Jour\u00adnal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 10,
            "refer_ID": "X96-1048",
            "refer_sids": [
                54,
                57
            ],
            "refer_text": "When the outputs are scored in \"key-to-response\" mode, as though one annotator's output represented the \"key\" and the other the \"response,\" the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6%.Summary NE scores on primary metrics for the top 16 (out of 20) systems tested, in order of decreasing F-Measure (P&R) 1 1 Key to F-measure scores: BBN baseline configuration 93.65, BBN experimental configuration 92.88, Knight-Ridder 85.73, Lockheed-Martin 90.84, UManitoba 93.33, UMass 84.95, MITRE 91.2, NMSU CRL baseline configuration 85.82, NYU 88.19, USheffield 89.06, SRA baseline configuration 96.42, SRA \"fast\" configuration 95.66, SRA \"fastest\" configuration 92.61, SRA \"nonames\" configuration 94.92, SRI 94.0, Sterling Software 92.74..",
            "cite_ID": "W99-0612",
            "cite_maker_sids": [
                142
            ],
            "cite_sids": [
                141,
                142
            ],
            "cite_text": "It is not clear what resources are required to adapt systems to new languages.\"It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).",
            "label": [
                "Results_Citation"
            ]
        },
        {
            "Number": 11,
            "refer_ID": "X96-1048",
            "refer_sids": [
                72
            ],
            "refer_text": "Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc..",
            "cite_ID": "E99-1001",
            "cite_maker_sids": [
                17
            ],
            "cite_sids": [
                17
            ],
            "cite_text": "In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that \"common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks\" (Sundheim 1995: 16).",
            "label": [
                "Method_Citation"
            ]
        },
        {
            "Number": 12,
            "refer_ID": "X96-1048",
            "refer_sids": [
                245,
                246
            ],
            "refer_text": "No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks.The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.",
            "cite_ID": "M98-1003",
            "cite_maker_sids": [
                3
            ],
            "cite_sids": [
                3
            ],
            "cite_text": "The organizers of MUC 6 did not attempt to compare the difficulty of the MUC 6 task to the previous MUC tasks saying that the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed [Sundheim1995].",
            "label": [
                "Implication_Citation"
            ]
        }
    ]
}