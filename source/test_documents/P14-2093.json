{
    "ID": "P14-2093",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Effective Selection of Translation Model Training Data",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Data selection has been demonstrated to be an effective approach to addressing the lack of high-quality bitext for statistical machine translation in the domain of interest.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most current data selection methods solely use language models trained on a small scale in-domain data to select domain-relevant sentence pairs from general-domain parallel corpus.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By contrast, we argue that the relevance between a sentence pair and target domain can be better evaluated by the combination of language model and translation model.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we study and experiment with novel methods that apply translation models into domain-relevant data selection.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results show that our methods outperform previous methods.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When the selected sentence pairs are evaluated on an end-to-end MT task, our methods can increase the translation performance by 3 BLEU points.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Statistical machine translation depends heavily on large scale parallel corpora.",
                    "sid": 7,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The corpora are necessary priori knowledge for training effective translation model.",
                    "sid": 8,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, domain-specific machine translation has few parallel corpora for translation model training in the domain of interest.",
                    "sid": 9,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this, an effective approach is to automatically select and expand domain-specific sentence pairs from large scale general-domain parallel corpus.",
                    "sid": 10,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The approach is named Data Selection.",
                    "sid": 11,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Current data selection methods mostly use language models trained on small scale indomain data to measure domain relevance and select domain-relevant parallel sentence pairs to expand training corpora.",
                    "sid": 12,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Related work in literature has proven that the expanded corpora can substantially improve the performance of machine translation (Duh et al., 2010; Haddow and Koehn, 2012).",
                    "sid": 13,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the methods are still far from satisfactory for real application for the following reasons: In a word, current data selection methods can\u2019t well maintain both parallelism and domainrelevance of bitext.",
                    "sid": 14,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To overcome the problem, we first propose the method combining translation model with language model in data selection.",
                    "sid": 15,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The language model measures the domainspecific generation probability of sentences, being used to select domain-relevant sentences at both sides of source and target language.",
                    "sid": 16,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Meanwhile, the translation model measures the translation probability of sentence pair, being used to verify the parallelism of the selected domainrelevant bitext.",
                    "sid": 17,
                    "ssid": 11,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "2 related work",
            "number": "2",
            "sents": [
                {
                    "text": "The existing data selection methods are mostly based on language model.",
                    "sid": 18,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Yasuda et al. (2008) and Foster et al.",
                    "sid": 19,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models.",
                    "sid": 20,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Axelrod et al. (2011) improved the perplexitybased approach and proposed bilingual crossentropy difference as a ranking function with inand general- domain language models.",
                    "sid": 21,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Duh et al. (2013) employed the method of (Axelrod et al., Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 569\u2013573, Baltimore, Maryland, USA, June 23-25 2014. c\ufffd2014 Association for Computational Linguistics 2011) and further explored neural language model for data selection rather than the conventional n-gram language model.",
                    "sid": 22,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although previous works in data selection (Duh et al., 2013; Koehn and Haddow, 2012; Axelrod et al., 2011; Foster et al., 2010; Yasuda et al., 2008) have gained good performance, the methods which only adopt language models to score the sentence pairs are sub-optimal.",
                    "sid": 23,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The reason is that a sentence pair contains a source language sentence and a target language sentence, while the existing methods are incapable of evaluating the mutual translation probability of sentence pair in the target domain.",
                    "sid": 24,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, we propose novel methods which are based on translation model and language model for data selection.",
                    "sid": 25,
                    "ssid": 8,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 training data selection methods",
            "number": "3",
            "sents": [
                {
                    "text": "We present three data selection methods for ranking and selecting domain-relevant sentence pairs from general-domain corpus, with an eye towards improving domain-specific translation model performance.",
                    "sid": 26,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These methods are based on language model and translation model, which are trained on small in-domain parallel data.",
                    "sid": 27,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Translation model is a key component in statistical machine translation.",
                    "sid": 28,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is commonly used to translate the source language sentence into the target language sentence.",
                    "sid": 29,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, in this paper, we adopt the translation model to evaluate the translation probability of sentence pair and develop a simple but effective variant of translation model to rank the sentence pairs in the generaldomain corpus.",
                    "sid": 30,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The formulations are detailed as below: Where ( ) is the translation model, which is IBM Model 1 in this paper, it represents the translation probability of target language sentence conditioned on source language sentence . and are the number of words in sentence and respectively.",
                    "sid": 31,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "( ) is the translation probability of word conditioned on word and is estimated from the small in-domain parallel data.",
                    "sid": 32,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The parameter is a constant and is assigned with the value of 1.0. is the lengthnormalized IBM Model 1, which is used to score general-domain sentence pairs.",
                    "sid": 33,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The sentence pair with higher score is more likely to be generated by in-domain translation model, thus, it is more relevant to the in-domain corpus and will be remained to expand the training data.",
                    "sid": 34,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As described in section 1, the existing data selection methods which only adopt language model to score sentence pairs are unable to measure the mutual translation probability of sentence pairs.",
                    "sid": 35,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To solve the problem, we develop the second data selection method, which is based on the combination of translation model and language model.",
                    "sid": 36,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our method and ranking function are formulated as follows: Where ( ) is a joint probability of sentence and according to the translation model ( ) and language model ( ), whose parameters are estimated from the small in-domain text. is the improved ranking function and used to score the sentence pairs with the length-normalized translation model ( )and language model ( ).",
                    "sid": 37,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The sentence pair with higher score is more similar to in-domain corpus, and will be picked out.",
                    "sid": 38,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Combining Translation and Language Models As presented in subsection 3.2, the method combines translation model and language model to rank the sentence pairs in the general-domain corpus.",
                    "sid": 39,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, it does not evaluate the inverse translation probability of sentence pair and the probability of target language sentence.",
                    "sid": 40,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, we take bidirectional scores into account and simply sum the scores in both directions.",
                    "sid": 41,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Again, the sentence pairs with higher scores are presumed to be better and will be selected to incorporate into the domain-specific training data.",
                    "sid": 42,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This approach makes full use of two translation models and two language models for sentence pairs ranking.",
                    "sid": 43,
                    "ssid": 18,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 experiments",
            "number": "4",
            "sents": [
                {
                    "text": "We conduct our experiments on the Spoken Language Translation English-to-Chinese task.",
                    "sid": 44,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Two corpora are needed for the data selection.",
                    "sid": 45,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The indomain data is collected from CWMT09, which consists of spoken dialogues in a travel setting, containing approximately 50,000 parallel sentence pairs in English and Chinese.",
                    "sid": 46,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our generaldomain corpus mined from the Internet contains 16 million sentence pairs.",
                    "sid": 47,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both the in- and general- domain corpora are identically tokenized (in English) and segmented (in Chinese)1.",
                    "sid": 48,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The details of corpora are listed in Table 1.",
                    "sid": 49,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additionally, we evaluate our work on the 2004 test set of \u201c863\u201d Spoken Language Translation task (\u201c863\u201d SLT), which consists of 400 English sentences with 4 Chinese reference translations for each.",
                    "sid": 50,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Meanwhile, the 2005 test set of \u201c863\u201d SLT task, which contains 456 English sentences with 4 references each, is used as the development set to tune our systems.",
                    "sid": 51,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the NiuTrans 2 toolkit which adopts GIZA++ (Och and Ney, 2003) and MERT (Och, 2003) to train and tune the machine translation system.",
                    "sid": 52,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As NiuTrans integrates the mainstream translation engine, we select hierarchical phrasebased engine (Chiang, 2007) to extract the translation rules and carry out our experiments.",
                    "sid": 53,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, in the decoding process, we use the NiuTrans decoder to produce the best outputs, and score them with the widely used NIST mteval131a3 tool.",
                    "sid": 54,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This tool scores the outputs in several criterions, while the case-insensitive BLEU-4 (Papineni et al., 2002) is used as the evaluation for the machine translation system.",
                    "sid": 55,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our work relies on the use of in-domain language models and translation models to rank the sentence pairs from the general-domain bilingual training set.",
                    "sid": 56,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here, we employ ngram language model and IBM Model 1 for data selection.",
                    "sid": 57,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train the in-domain 4-gram language model with interpolated modified Kneser-Ney discounting (Chen and Goodman, 1998).",
                    "sid": 58,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The language model is only used to score the general-domain sentences.",
                    "sid": 59,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Meanwhile, we use the language model training scripts integrated in the NiuTrans toolkit to train another 4-gram language model, which is used in MT tuning and decoding.",
                    "sid": 60,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additionally, we adopt GIZA++ to get the word alignment of in-domain parallel data and form the word translation probability table.",
                    "sid": 61,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This table will be used to compute the translation probability of general-domain sentence pairs.",
                    "sid": 62,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As described above, by using the NiuTrans toolkit, we have built two baseline systems to fulfill \u201c863\u201d SLT task in our experiments.",
                    "sid": 63,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The In-domain baseline trained on spoken language corpus has 1.05 million rules in its hierarchicalphrase table.",
                    "sid": 64,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While, the General-domain baseline trained on 16 million sentence pairs has a hierarchical phrase table containing 1.7 billion translation rules.",
                    "sid": 65,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These two baseline systems are equipped with the same language model which is trained on large-scale monolingual target language corpus.",
                    "sid": 66,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The BLEU scores of the Indomain and General-domain baseline system are listed in Table 2.",
                    "sid": 67,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results show that General-domain system trained on a larger amount of bilingual resources outperforms the system trained on the in-domain corpus by over 12 BLEU points.",
                    "sid": 68,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The reason is that large scale parallel corpus maintains more bilingual knowledge and language phenomenon, while small in-domain corpus encounters data sparse problem, which degrades the translation performance.",
                    "sid": 69,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the performance of General-domain baseline can be improved further.",
                    "sid": 70,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use our three methods to refine the generaldomain corpus and improve the translation performance in the domain of interest.",
                    "sid": 71,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, we build several contrasting systems trained on refined training data selected by the following different methods.",
                    "sid": 72,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We adopt five methods for extracting domainrelevant parallel data from general-domain corpus.",
                    "sid": 73,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using the scoring methods, we rank the sentence pairs of the general-domain corpus and select only the top N = {50k, 100k, 200k, 400k, 600k, 800k, 1000k} sentence pairs as refined training data.",
                    "sid": 74,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "New MT systems are then trained on these small refined training data.",
                    "sid": 75,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1 shows the performances of systems trained on selected corpora from the general-domain corpus.",
                    "sid": 76,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The horizontal coordinate represents the number of selected sentence pairs and vertical coordinate is the BLEU scores of MT systems.",
                    "sid": 77,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From Figure 1, we conclude that these five data selection methods are effective for domainspecific translation.",
                    "sid": 78,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When top 600k sentence pairs are picked out from general-domain corpus to train machine translation systems, the systems perform higher than the General-domain baseline trained on 16 million parallel data.",
                    "sid": 79,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results indicate that more training data for translation model is not always better.",
                    "sid": 80,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When the domainspecific bilingual resources are deficient, the domain-relevant sentence pairs will play an important role in improving the translation performance.",
                    "sid": 81,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additionally, it turns out that our methods (TM, TM+LM and Bidirectional TM+LM) are indeed more effective in selecting domainrelevant sentence pairs.",
                    "sid": 82,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the end-to-end SMT evaluation, TM selects top 600k sentence pairs of general-domain corpus, but increases the translation performance by 2.7 BLEU points.",
                    "sid": 83,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Meanwhile, the TM+LM and Bidirectional TM+LM have gained 3.66 and 3.56 BLEU point improvements compared against the generaldomain baseline system.",
                    "sid": 84,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Compared with the mainstream methods (Igram and Ieural net), our methods increase translation performance by nearly 3 BLEU points, when the top 600k sentence pairs are picked out.",
                    "sid": 85,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although, in the figure 1, our three methods are not performing better than the existing methods in all cases, their overall performances are relatively higher.",
                    "sid": 86,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We therefore believe that combining in-domain translation model and language model to score the sentence pairs is well-suited for domainrelevant sentence pair selection.",
                    "sid": 87,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, we observe that the overall performance of our methods is gradually improved.",
                    "sid": 88,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is because our methods are combining more statistical characteristics of in-domain data in ranking and selecting sentence pairs.",
                    "sid": 89,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results have proven the effectiveness of our methods again.",
                    "sid": 90,
                    "ssid": 47,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5 conclusion",
            "number": "5",
            "sents": [
                {
                    "text": "We present three novel methods for translation model training data selection, which are based on the translation model and language model.",
                    "sid": 91,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Compared with the methods which only employ language model for data selection, we observe that our methods are able to select high-quality domain-relevant sentence pairs and improve the translation performance by nearly 3 BLEU points.",
                    "sid": 92,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, our methods make full use of the limited in-domain data and are easily implemented.",
                    "sid": 93,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the future, we are interested in applying our methods into domain adaptation task of statistical machine translation in model level.",
                    "sid": 94,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments",
            "number": "6",
            "sents": [
                {
                    "text": "This research work has been sponsored by two NSFC grants, No.61373097 and No.61272259, and one National Science Foundation of Suzhou (Grants No.",
                    "sid": 95,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "SH201212).",
                    "sid": 96,
                    "ssid": 2,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}