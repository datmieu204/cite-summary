{
    "ID": "N12-1007",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Entity Clustering Across Languages",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Standard entity clustering systems commonly rely on mention (string) matching, syntactic features, and linguistic resources like English WordNet.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When co-referent text mentions appear in different languages, these techniques cannot be easily applied.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consequently, we develop new methods for clustering text mentions across documents and languages simultaneously, producing cross-lingual entity clusters.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our approach extends standard clustering algorithms with cross-lingual mention and context similarity measures.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Crucially, we do not assume a pre-existing entity list (knowledge base), so entity characteristics are unknown.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On an Arabic-English corpus that contains seven different text genres, our best model yields a 24.3% F1 gain over the baseline.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 introduction",
            "number": "1",
            "sents": [
                {
                    "text": "This paper introduces techniques for clustering coreferent text mentions across documents and languages.",
                    "sid": 7,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the web today, a breaking news item may instantly result in mentions to a real-world entity in multiple text formats: news articles, blog posts, tweets, etc.",
                    "sid": 8,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Much NLP work has focused on model adaptation to these diverse text genres.",
                    "sid": 9,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the diversity of languages in which the mentions appear is a more significant challenge.",
                    "sid": 10,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This was particularly evident during the 2011 popular uprisings in the Arab world, in which electronic media played a prominent role.",
                    "sid": 11,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A key issue for the outside world was the aggregation of information that appeared simultaneously in English, French, and various Arabic dialects.",
                    "sid": 12,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To our knowledge, we are the first to consider clustering entity mentions across languages without a priori knowledge of the quantity or types of real-world entities (a knowledge base).",
                    "sid": 13,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The cross-lingual setting introduces several challenges.",
                    "sid": 14,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we cannot assume a prototypical name format.",
                    "sid": 15,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the Anglo-centric first/middle/last prototype used in previous name modeling work (cf.",
                    "sid": 16,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Charniak, 2001)) does not apply to Arabic names like Abdullah ibn Abd Al-Aziz Al-Saud or Chinese names like Hu Jintao (referred to as Mr. Hu, not Mr. Jintao).",
                    "sid": 17,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, organization names often require both transliteration and translation.",
                    "sid": 18,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the Arabic but a translation of \ufffd\u00e9\u00bbQ\u00e5\ufffd... \u2018Corporation\u2019.",
                    "sid": 19,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our models are organized as a pipeline.",
                    "sid": 20,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, for each document, we perform standard mention detection and coreference resolution.",
                    "sid": 21,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then, we use pairwise cross-lingual similarity models to measure both mention and context similarity.",
                    "sid": 22,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we cluster the mentions based on similarity.",
                    "sid": 23,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our work makes the following contributions: (1) introduction of the task, (2) novel models for crosslingual entity clustering of person and organization entities, (3) cross-lingual annotation of the NIST Automatic Content Extraction (ACE) 2008 Arabic-English evaluation set, and (4) experimental results using both gold and automatic within-document processing.",
                    "sid": 24,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will release our software and annotations to support future research.",
                    "sid": 25,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consider the toy corpus in Fig.",
                    "sid": 26,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.",
                    "sid": 27,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The English documents contain mentions of two people: Steven Paul Jobs and Mark Elliot Zuckerberg.",
                    "sid": 28,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Of course, the surface realization of Mr. Jobs\u2019 last name in English is also an ordinary nominal, hence the ambiguous mention string (absent context) in the second document.",
                    "sid": 29,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Arabic document introduces an organization entity (Apple Inc.) along with proper and pronominal references to Mr. Jobs.",
                    "sid": 30,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, the French document refers to Mr. Jobs by the honorific \u2018Monsieur,\u2019 and to Apple without its corporate designation.",
                    "sid": 31,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our goal is to automatically produce the crosslingual entity clusters E1 (Mark Elliot Zuckerberg), E2 (Apple Inc.), and E3 (Steven Paul Jobs).",
                    "sid": 32,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both the true number and characteristics of these entities are unobserved.",
                    "sid": 33,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our models require two pre-processing steps: mention detection and within-document coreference/anaphora resolution, shown in Fig.",
                    "sid": 34,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 by the text boxes and intra-document links, respectively.",
                    "sid": 35,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, in doc3, a within-document coreference system would pre-linker joobz \u2018Jobs\u2019 with the masculine pronoun o h `his'.",
                    "sid": 36,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, the mention detector determines that the surface form \u201cJobs\u201d in doc2 is not an entity reference.",
                    "sid": 37,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this within-document pre-processing we use Serif (Ramshaw et al., 2011).1 Our models measure cross-lingual similarity of the coreference chains to make clustering decisions (\u2022 in Fig.",
                    "sid": 38,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1).",
                    "sid": 39,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The similarity models (indicated by the = and =6 operators in Fig.",
                    "sid": 40,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1) consider both mention string and context similarity (\u00a72).",
                    "sid": 41,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the mention similarities as hard constraints, and the context similarities as soft constraints.",
                    "sid": 42,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this work, we investigate two standard constrained clustering algorithms (\u00a73).",
                    "sid": 43,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our methods can be used to extend existing systems for mono-lingual entity clustering (also known as \u201ccross-document coreference resolution\u201d) to the cross-lingual setting.",
                    "sid": 44,
                    "ssid": 38,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "2 mention and context similarity",
            "number": "2",
            "sents": [
                {
                    "text": "Our goal is to create cross-lingual sets of co-referent mentions to real-world entities (people, places, organizations, etc.).",
                    "sid": 45,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we adopt the following notation.",
                    "sid": 46,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let M be a set of distinct text mentions in a collection of documents; C is a partitioning of M into document-level sets of co-referent mentions (called coreference chains); E is a partitioning of C into sets of co-referent chains (called entities).",
                    "sid": 47,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let i, j be nonnegative integers less than or equal to |M |and a, b be non-negative integers less than or equal to |C|.",
                    "sid": 48,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our experiments use a separate within-document coreference system to create C, which is fixed.",
                    "sid": 49,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will learn E, which has size no greater than |C |since the set of mono-lingual chains is the largest valid partitioning.",
                    "sid": 50,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We define accessor functions to access properties of mentions and chains.",
                    "sid": 51,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For any mention mi, define the following functions: lang(mi) is the language; doc(mi) is the document containing mi; type(mi) is the semantic type, which is assigned by the withindocument coreference system.",
                    "sid": 52,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also extract a set of mention contexts S, which are the sentences containing each mention (i.e., |S |= |M|).",
                    "sid": 53,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We learn the partition E by considering mention and context similarity, which are measured with separate component models.",
                    "sid": 54,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use separate methods for within- and crosslanguage mention similarity.",
                    "sid": 55,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The pairwise similarity thographic representation.",
                    "sid": 56,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u201c0\u201d indicates a null mapping.",
                    "sid": 57,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For English, we also lowercase and remove determiners and punctuation.",
                    "sid": 58,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For Arabic, we remove the determiner \u00c8@ Al `the' and the elongation character tatwil `_'. of any two mentions mi and mj is: distance (Porter and Winkler, 1997).",
                    "sid": 59,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Jaro-Winkler rewards matching prefixes, the empirical justification being that less variation typically occurs at the beginning of names.2 The metric produces a score in the range [0,1], where 0 indicates equality.",
                    "sid": 60,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Maxent model (cross-language) When lang(mi) =\ufffd lang(mj), then the two mentions might be in different writing systems.",
                    "sid": 61,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Edit distance calculations no longer apply directly.",
                    "sid": 62,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One solution would be full-blown transliteration (Knight and Graehl, 1998), followed by application of Jaro-Winkler.",
                    "sid": 63,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, transliteration systems are complex and require significant training resources.",
                    "sid": 64,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We find that a simpler, low-resource approach works well in practice.",
                    "sid": 65,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we deterministically map both languages to a common phonetic representation (Tbl.",
                    "sid": 66,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1).3 Next, we align the mention pairs with the Hungarian algorithm, ken indices.",
                    "sid": 67,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Define the following functions for strings: cbigrams(\u00b7) returns the set of character bigrams; len(\u00b7) is the token length; Lev(\u00b7, \u00b7) is the Levenshtein edit distance between two strings.",
                    "sid": 68,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Prior to feature extraction, we add unique start and end symbols to the mention strings. which produces a word-to-word alignment Ary,.i\ufffd...j.4 Finally, we build a simple binary Maxent classifier p(y|mi, mj; A) that extracts features from the aligned mentions (Tbl.",
                    "sid": 69,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2).",
                    "sid": 70,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We learn the parameters A using a quasi-Newton procedure with Li (lasso) regularization (Andrew and Gao, 2007).",
                    "sid": 71,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Mention strings alone are not always sufficient for disambiguation.",
                    "sid": 72,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consider again the simple example in Fig.",
                    "sid": 73,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.",
                    "sid": 74,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both doc3 and doc4 reference \u201cSteve Jobs\u201d and \u201cApple\u201d in the same contexts.",
                    "sid": 75,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Context cooccurence and/or similarity can thus disambiguate these two entities from other entities with similar references (e.g., \u201cSteve Jones\u201d or \u201cApple Corps\u201d).",
                    "sid": 76,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As with the mention strings, the contexts may originate in different writing systems.",
                    "sid": 77,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We consider both highand low-resource approaches for mapping contexts to a common representation.",
                    "sid": 78,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Machine Translation (MT) For the high-resource setting, if lang(mi) =\ufffd English, then we translate both mi and its context si to English with an MT system.",
                    "sid": 79,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use Phrasal (Cer et al., 2010), a phrase-based system which, like most public MT systems, lacks a transliteration module.",
                    "sid": 80,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We believe that this approach yields the most accurate context mapping for highresource language pairs (like English-Arabic).",
                    "sid": 81,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Polylingual Topic Model (PLTM) The polylingual topic model (PLTM) (Mimno et al., 2009) is a generative process in which document tuples\u2014 groups of topically-similar documents\u2014share a topic distribution.",
                    "sid": 82,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The tuples need not be sentence-aligned, so training data is easier to obtain.",
                    "sid": 83,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, one document tuple might be the set of Wikipedia articles (in all languages) for Steve Jobs.",
                    "sid": 84,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let D be a set of document tuples, where there is one document in each tuple for each of L languages.",
                    "sid": 85,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each language has vocabulary V and each document dlt has Ni tokens.",
                    "sid": 86,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We specify a fixed-size set of topics K. The PLTM generates the document tuples as follows: For cross-lingual context mapping, we infer the 1best topic assignments for each token in all S mention contexts.",
                    "sid": 87,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This technique reduces V = k for all l. Moreover, all languages have a common vocabulary: the set of K topic indices.",
                    "sid": 88,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al., 2009) for more details.",
                    "sid": 89,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After mapping each mention context to a common representation, we measure context similarity based on the choice of clustering algorithm.",
                    "sid": 90,
                    "ssid": 46,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 clustering algorithms",
            "number": "3",
            "sents": [
                {
                    "text": "We incorporate the mention and context similarity measures into a clustering framework.",
                    "sid": 91,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We consider two algorithms.",
                    "sid": 92,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first is hierarchical agglomerative clustering (HAC), with which we assume basic familiarity (Manning et al., 2008).",
                    "sid": 93,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A shortcoming of HAC is that a stop threshold must be tuned.",
                    "sid": 94,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To avoid this requirement, we also consider non-parametric probabilistic clustering in the form of a Dirichlet process mixture model (DPMM) (Antoniak, 1974) .",
                    "sid": 95,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both clustering algorithms can be modified to accommodate pairwise constraints.",
                    "sid": 96,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have observed better results by encoding mention similarity as a hard constraint.",
                    "sid": 97,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Context similarity is thus the cluster distance measure.5 To turn the Jaro-Winkler distance into a hard boolean constraint, we tuned a threshold q on held-out data, i.e., jaro-winkler(mi, mj) G q =>. mi = mj.",
                    "sid": 98,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Likewise, the Maxent model is a binary classifier, so p(y = 1|mi, mj; A) > 0.5 =>. mi = mj.",
                    "sid": 99,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In both clustering algorithms, any two chains Ca and Cb cannot share the same cluster assignment if: The deterministic accessor function repr(Ca) returns the representative mention of a chain.",
                    "sid": 100,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The heuristic we used was \u201cfirst mention\u201d: the function returns the earliest mention that appears in the associated document.",
                    "sid": 101,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In many languages, the first mention is typically more complete than later mentions.",
                    "sid": 102,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This heuristic also makes our system less sensitive to withindocument coreference errors.6 The representative mention only has special status for mention similarity: context similarity considers all mention contexts.",
                    "sid": 103,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "HAC iteratively merges the \u201cnearest\u201d clusters according to context similarity.",
                    "sid": 104,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our system, each cluster context is a bag of words W formed from the contexts of all coreference chains in that cluster.",
                    "sid": 105,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each word in W we estimate a unigram Entity Language Model (ELM) (Raghavan et al., 2004): the corpus7 and p is a smoothing parameter.",
                    "sid": 106,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For any two entity clusters Ea and Eb, the distance between PE. and PEb is given by a metric based on the JensenShannon Divergence (JSD) (Endres and Schindelin, 2003): where KL(PE.||M) is the Kullback-Leibler divergence and M =1\ufffd(PE.",
                    "sid": 107,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "+ PEb).",
                    "sid": 108,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We initialize HAC to E = C, i.e., the initial clustering solution is just the set of all coreference chains.",
                    "sid": 109,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then we remove all links in the HAC proximity matrix that violate pairwise cannot-link constraints.",
                    "sid": 110,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During clustering, we do not merge Ea and Eb if any pair of chains violates a cannot-link constraint.",
                    "sid": 111,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This procedure propagates the cannot-link constraints (Klein et al., 2002).",
                    "sid": 112,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To output E, we stop clustering when the minimum JSD exceeds a stop threshold 'y, which is tuned on a development set.",
                    "sid": 113,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead of tuning a parameter like -y, it would be preferable to let the data dictate the number of entity clusters.",
                    "sid": 114,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We thus consider a non-parametric Bayesian mixture model where the mixtures are multinomial distributions over the entity contexts S. Specifically, we consider a DPMM, which automatically infers the number of mixtures.",
                    "sid": 115,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each Ca has an associated mixture Ba: where \u03b1 is the concentration parameter of the DP prior and G0 is the base distribution with support V .",
                    "sid": 116,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our experiments, we set G0 = Dir(7r1,... , 7rV ), where 7ri = PV (wi).",
                    "sid": 117,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For inference, we use the Gibbs sampler of Vlachos et al. (2009), which can incorporate pairwise constraints.",
                    "sid": 118,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The sampler is identical to a standard collapsed, token-based sampler, except the conditional probability p(Ea = E|E_a, Ca) = 0 if Ca cannot be merged with the chains in cluster E. This property makes the model non-exchangeable, but in practice non-exchangeable models are sometimes useful (Blei and Frazier, 2010).",
                    "sid": 119,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During sampling, we also learn \u03b1 using the auxiliary variable procedure of West (1995), so the only fixed parameters are those of the vague Gamma prior.",
                    "sid": 120,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, we found that these hyperparameters were not sensitive.",
                    "sid": 121,
                    "ssid": 31,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 training data and procedures",
            "number": "4",
            "sents": [
                {
                    "text": "We trained our system for Arabic-English crosslingual entity clustering.8 Maxent Mention Similarity The Maxent mention similarity model requires a parallel name list for training.",
                    "sid": 122,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Name pair lists can be obtained from the LDC (e.g., LDC2005T34 contains nearly 450,000 parallel Chinese-English names) or Wikipedia (Irvine et al., 2010).",
                    "sid": 123,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We extracted 12,860 name pairs from the parallel Arabic-English translation treebanks,9 although our experiments show that the model achieves high accuracy with significantly fewer training examples.",
                    "sid": 124,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We generated a uniform distribution of training examples by running a Bernoulli trial for each aligned name pair in the corpus.",
                    "sid": 125,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the coin was heads, we replaced the English name with another English name chosen randomly from the corpus.",
                    "sid": 126,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "MT Context Mapping For the MT context mapping method, we trained Phrasal with all data permitted under the NIST OpenMT Ar-En 2009 constrained track evaluation.",
                    "sid": 127,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We built a 5-gram language model from the Xinhua and AFP sections of the Gigaword corpus (LDC2007T07), in addition to all of the target side training data.",
                    "sid": 128,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to the baseline Phrasal feature set, we used the lexicalized re-ordering model of Galley and Manning (2008).",
                    "sid": 129,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "PLTM Context Mapping For PLTM training, we formed a corpus of 19,139 English-Arabic topicallyaligned Wikipedia articles.",
                    "sid": 130,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Cross-lingual links in Wikipedia are abundant: as of February 2010, there were 77.07M cross-lingual links among Wikipedia\ufffds 272 language editions (de Melo and Weikum, 2010).",
                    "sid": 131,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To increase vocabulary coverage for our ACE2008 evaluation corpus, we added 20,000 document singletons from the ACE2008 training corpus.",
                    "sid": 132,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The topically-aligned tuples served as \u201cglue\u201d to share topics between languages, while the ACE documents distribute those topics over in-domain vocabulary.10 We used the PLTM implementation in Mallet (McCallum, 2002).",
                    "sid": 133,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We ran the sampler for 10,000 iterations and set the number of topics K = 512.",
                    "sid": 134,
                    "ssid": 13,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5 task evaluation framework",
            "number": "5",
            "sents": [
                {
                    "text": "Our experimental design is a cross-lingual extension of the standard cross-document coreference resolution task, which appeared in ACE2008 (Strassel et al., 2008; NIST, 2008).",
                    "sid": 135,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluate name (NAM) mentions for cross-lingual person (PER) and organization (ORG) entities.",
                    "sid": 136,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base).",
                    "sid": 137,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We report results for both gold and automatic within-document mention detection and coreference resolution.",
                    "sid": 138,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions.",
                    "sid": 139,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the gold setting, we report: Information-theoretic measure that utilizes the entropy of the clusters and their mutual information.",
                    "sid": 140,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unlike the commonly-used Variation of Information (VI) metric, normalized VI (NVI) is not sensitive to the size of the data set.",
                    "sid": 141,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the automatic setting, we must apply a different metric since the number of system chains may differ from the reference.",
                    "sid": 142,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use B3sys (Cai and Strube, 2010), a variant of B3 that was shown to penalize both twinless reference chains and spurious system chains more fairly.",
                    "sid": 143,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Evaluation Corpus The automatic evaluation of cross-lingual coreference systems requires annotated 10Mimno et al. (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly. multilingual corpora.",
                    "sid": 144,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Cross-document annotation is expensive (Strassel et al., 2008), so we chose the ACE2008 Arabic-English evaluation corpus as a starting point for cross-lingual annotation.",
                    "sid": 145,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The corpus consists of seven genres sampled from independent sources over the course of a decade (Tbl.",
                    "sid": 146,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3).",
                    "sid": 147,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The corpus provides gold mono-lingual cross-document coreference annotations for both PER and ORG entities.",
                    "sid": 148,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using these annotations as a starting point, we found and annotated 216 cross-lingual entities.11 Because a similar corpus did not exist for development, we split the evaluation corpus into development and test sections.",
                    "sid": 149,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the usual method of splitting by document would not confine all mentions of each entity to one side of the split.",
                    "sid": 150,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We thus split the corpus by global entity id.",
                    "sid": 151,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We assigned one-third of the entities to development, and the remaining twothirds to test.",
                    "sid": 152,
                    "ssid": 18,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "6 comparison to related tasks and work",
            "number": "6",
            "sents": [
                {
                    "text": "Our modeling techniques and task formulation can be viewed as cross-lingual extensions to cross-document coreference resolution.",
                    "sid": 153,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975).",
                    "sid": 154,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering.",
                    "sid": 155,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007).",
                    "sid": 156,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types.",
                    "sid": 157,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do.",
                    "sid": 158,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More 11The annotators were the first author and another fluent speaker of Arabic.",
                    "sid": 159,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011).",
                    "sid": 160,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Cross-document work on languages other than English is scarce.",
                    "sid": 161,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names.",
                    "sid": 162,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al.",
                    "sid": 163,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2004).",
                    "sid": 164,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They clustered the entities incrementally using a binary classifier.",
                    "sid": 165,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness.",
                    "sid": 166,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching.",
                    "sid": 167,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To our knowledge, the cross-lingual entity clustering task is novel.",
                    "sid": 168,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, there is significant prior work on similar tasks: Our work incorporates elements of the first three tasks.",
                    "sid": 169,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most importantly, we avoid the key element of entity linking: a knowledge base.",
                    "sid": 170,
                    "ssid": 18,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "7 experiments",
            "number": "7",
            "sents": [
                {
                    "text": "We performed intrinsic evaluations for both mention and context similarity.",
                    "sid": 171,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For context similarity, we analyzed mono-lingual entity clustering, which also facilitated comparison to prior work on the ACE2008 set, gold within-document processing).",
                    "sid": 172,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Higher scores (T) are better for CEAF and B3, whereas lower (].) is better for NVI.",
                    "sid": 173,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "#gold indicates the number of reference entities, whereas #hyp is the size of E. evaluation set.",
                    "sid": 174,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our main results are for the new task: cross-lingual entity clustering.",
                    "sid": 175,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Cross-lingual Mention Matching We created a random 80/10/10 (train, development, test) split of the Maxent training corpus and evaluated binary classification accuracy (Tbl.",
                    "sid": 176,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4).",
                    "sid": 177,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Of the mis-classified examples, we observed three major error types.",
                    "sid": 178,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, the model learns that high edit distance is predictive of a mismatch.",
                    "sid": 179,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, singleton strings that do not match often have a lower edit distance than longer strings that do match.",
                    "sid": 180,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a result, singletons often cause false positives.",
                    "sid": 181,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, names that originate in a third language tend to violate the phonemic correspondences.",
                    "sid": 182,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the model gives a false negative for a German football team: vim,\ufffd , \ufffd.",
                    "sid": 183,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", v 1 (phonetic mapping: af s kazrslawtrn) versus \u201cFC Kaiserslautern.\u201d Finally, names that require translation are problematic.",
                    "sid": 184,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the classifier produces a false negative for (God, gd) ?",
                    "sid": 185,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(ail1, allh).",
                    "sid": 186,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CEAFT NVI]. applied to the subset of target cross-lingual entities in the test set.",
                    "sid": 187,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For CEAF and B3, SINGLEToN is the stronger baseline due to the high proportion of singleton entities in the corpus.",
                    "sid": 188,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Of course, cross-lingual entities have at least two chains, so No-CoNTExT is a better baseline for cross-lingual clustering.",
                    "sid": 189,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Mono-lingual Entity Clustering For comparison, we also evaluated our system on a standard monolingual cross-document coreference task (Arabic and English) (Tbl.",
                    "sid": 190,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5).",
                    "sid": 191,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We configured the system with HAC clustering and Jaro-Winkler (within-language) mention similarity.",
                    "sid": 192,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We built mono-lingual ELMs for context similarity.",
                    "sid": 193,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used two baselines: fore, E is the set of fully-connected components in C subject to the pairwise constraints.",
                    "sid": 194,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For HAC, we manually tuned the stop threshold -y, the Jaro-Winkler threshold q, and the ELM smoothing parameter p on the development set.",
                    "sid": 195,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the DPMM, no development tuning was necessary, and we evaluated a single sample of E taken after 3,000 iterations.",
                    "sid": 196,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To our knowledge, Baron and Freedman (2008) reported the only previous results on the ACE2008 data set.",
                    "sid": 197,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, they only gave gold results for English, and clustered the entire evaluation corpus (test+development).",
                    "sid": 198,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To control for the effect of within-document errors, we considered their gold input (mention detection and within-document coreference resolution) results.",
                    "sid": 199,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They reported B3 for the two entity types separately: ORG (91.5% F1) and PER (94.3% F1).",
                    "sid": 200,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The different experimental designs preclude a precise comparison, but the accuracy of (Serif) within-document processing).",
                    "sid": 201,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For HAC, we used the same parameters as the gold setting. the two systems are at least in the same range.",
                    "sid": 202,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluated four system configurations on the new task: HAC+MT, HAC+PLTM, DPMM+MT, and DPMM+PLTM.",
                    "sid": 203,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we established an upper bound by assuming gold within-document mention detection and coreference resolution (Tbl.",
                    "sid": 204,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6).",
                    "sid": 205,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This setting isolated the new cross-lingual clustering methods from within-document processing errors.",
                    "sid": 206,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then we evaluated with Serif (automatic) within-document processing (Tbl.",
                    "sid": 207,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7).",
                    "sid": 208,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This second experiment replicated an application setting.",
                    "sid": 209,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used the same baselines and tuning procedures as in the mono-lingual clustering experiment.",
                    "sid": 210,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Results In the gold setting, HAC+MT produces the best results, as expected.",
                    "sid": 211,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The dimensionality reduction of the vocabulary imposed by PLTM significantly reduces accuracy, but HAC+PLTM still exceeds the baseline.",
                    "sid": 212,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We tried increasing the number of PLTM topics k, but did not observe an improvement in task accuracy.",
                    "sid": 213,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For both context-mapping methods, the DPMM suffers from low-recall.",
                    "sid": 214,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Upon inspection, the clustering solution of DPMM+MT contains a high proportion of singleton hypotheses, suggesting that the model finds lower similarity in the presence of a larger vocabulary.",
                    "sid": 215,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When the context vocabulary consists of PLTM topics, larger clusters are discovered (DPMM+PLTM).",
                    "sid": 216,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The effect of dimensionality reduction is also apparent in the clustering solutions of the PLTM models.",
                    "sid": 217,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, for the Serif output, DPMM+PLTM produces a cluster consisting of \u201cWhite House\u201d, \u201cSenate\u201d, \u201cHouse of Representatives\u201d, and \u201cParliament\u201d.",
                    "sid": 218,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Arabic mentions of the latter three entities pass the pairwise mention similarity constraints due to the word 0_4A \u2018council\u2019, which appears in text mentions for all three legislative bodies.",
                    "sid": 219,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A cross-language matching error resulted in the linking of \u201cWhite House\u201d, and the reduced granularity of the contexts precluded further disambiguation.",
                    "sid": 220,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Of course, these entities probably appear in similar contexts.",
                    "sid": 221,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The caveat with the Serif results in Tbl.",
                    "sid": 222,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7 is that 3,251 of the 7,655 automatic coreference chains are not in the reference.",
                    "sid": 223,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consequently, the evaluation is dominated by the penalty for spurious system coreference chains.",
                    "sid": 224,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Nonetheless, all models except for DPMM+PLTM exceed the baselines, and the relationships between models depicted in the gold experiments hold for the this setting.",
                    "sid": 225,
                    "ssid": 55,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "8 conclusion",
            "number": "8",
            "sents": [
                {
                    "text": "Cross-lingual entity clustering is a natural step toward more robust natural language understanding.",
                    "sid": 226,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We proposed pipeline models that make clustering decisions based on cross-lingual similarity.",
                    "sid": 227,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We investigated two methods for mapping documents in different languages to a common representation: MT and the PLTM.",
                    "sid": 228,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although MT may achieve more accurate results for some language pairs, the PLTM training resources (e.g., Wikipedia) are readily available for many languages.",
                    "sid": 229,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As for the clustering algorithms, HAC appears to perform better than the DPMM on our dataset, but this may be due to the small corpus size.",
                    "sid": 230,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The instance-level constraints represent tendencies that could be learned from larger amounts of data.",
                    "sid": 231,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With more data, we might be able to relax the constraints and use an exchangeable DPMM, which might be more effective.",
                    "sid": 232,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we have shown that significant quantities of within-document errors cascade into the cross-lingual clustering phase.",
                    "sid": 233,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a result, we plan a model that clusters the mentions directly, thus removing the dependence on within-document coreference resolution.",
                    "sid": 234,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we have set baselines and proposed models that significantly exceeded those baselines.",
                    "sid": 235,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The best model improved upon the cross-lingual entity baseline by 24.3% F1.",
                    "sid": 236,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This result was achieved without a knowledge base, which is required by previous approaches to cross-lingual entity linking.",
                    "sid": 237,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More importantly, our techniques can be used to extend existing cross-document entity clustering systems for the increasingly multilingual web.",
                    "sid": 238,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Acknowledgments We thank Jason Eisner, David Mimno, Scott Miller, Jim Mayfield, and Paul McNamee for helpful discussions.",
                    "sid": 239,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This work was started during the SCALE 2010 summer workshop at Johns Hopkins.",
                    "sid": 240,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first author is supported by a National Science Foundation Graduate Fellowship.",
                    "sid": 241,
                    "ssid": 16,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}