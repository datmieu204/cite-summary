{
    "ID": "N12-1052",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While previous work has focused primarily on English, we extend these results to other languages along two dimensions.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we show that these results hold true for a number of languages across families.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 introduction",
            "number": "1",
            "sents": [
                {
                    "text": "The ability to predict the linguistic structure of sentences or documents is central to the field of natural language processing (NLP).",
                    "sid": 7,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Structures such as named-entity tag sequences (Bikel et al., 1999) or sentiment relations (Pang and Lee, 2008) are inherently useful in data mining, information retrieval and other user-facing technologies.",
                    "sid": 8,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More fundamental structures such as part-of-speech tag sequences (Ratnaparkhi, 1996) or syntactic parse trees (Collins, 1997; K\u00a8ubler et al., 2009), on the other hand, comprise the core linguistic analysis for many important downstream tasks such as machine translation (Chiang, * The majority of this work was performed while the author was an intern at Google, New York, NY.",
                    "sid": 9,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2005; Collins et al., 2005).",
                    "sid": 10,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Currently, supervised data-driven methods dominate the literature on linguistic structure prediction (Smith, 2011).",
                    "sid": 11,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Regrettably, the majority of studies on these methods have focused on evaluations specific to English, since it is the language with the most annotated resources.",
                    "sid": 12,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Notable exceptions include the CoNLL shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003; Buchholz and Marsi, 2006; Nivre et al., 2007) and subsequent studies on this data, as well as a number of focused studies on one or two specific languages, as discussed by Bender (2011).",
                    "sid": 13,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While annotated resources for parsing and several other tasks are available in a number of languages, we cannot expect to have access to labeled resources for all tasks in all languages.",
                    "sid": 14,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al., 2008) and transfer (Hwa et al., 2005) systems for prediction of linguistic structure.",
                    "sid": 15,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These methods all attempt to benefit from the plethora of unlabeled monolingual and/or cross-lingual data that has become available in the digital age.",
                    "sid": 16,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unsupervised methods are appealing in that they are often inherently language independent.",
                    "sid": 17,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is borne out by the many recent studies on unsupervised parsing that include evaluations covering a number of languages (Cohen and Smith, 2009; Gillenwater et al., 2010; Naseem et al., 2010; Spitkovsky et al., 2011).",
                    "sid": 18,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the performance for most languages is still well below that of supervised systems and recent work has established that the performance is also below simple methods of linguistic transfer (McDonald et al., 2011).",
                    "sid": 19,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this study we focus on semi-supervised and linguistic-transfer methods for multilingual structure prediction.",
                    "sid": 20,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, we pursue two lines of research around the use of word cluster features in discriminative models for structure prediction: guages for dependency parsing and 4 languages for named-entity recognition (NER).",
                    "sid": 21,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is the first study with such a broad view on this subject, in terms of language diversity.",
                    "sid": 22,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.",
                    "sid": 23,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Cross-lingual word cluster features for transferring linguistic structure from English to other languages.",
                    "sid": 24,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We develop an algorithm that generates cross-lingual word clusters; that is clusters of words that are consistent across languages.",
                    "sid": 25,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is achieved by means of a probabilistic model over large amounts of monolingual data in two languages, coupled with parallel data through which cross-lingual word-cluster constraints are enforced.",
                    "sid": 26,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We show that by augmenting the delexicalized direct transfer system of McDonald et al. (2011) with cross-lingual cluster features, we are able to reduce its error by up to 13% relative.",
                    "sid": 27,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Further, we show that by applying the same method to direct-transfer NER, we achieve a relative error reduction of 26%.",
                    "sid": 28,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In line with much previous work on word clusters for tasks such as dependency parsing and NER, for which local syntactic and semantic constraints are of importance, we induce word clusters by means of a probabilistic class-based language model (Brown et al., 1992; Clark, 2003).",
                    "sid": 29,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, rather than the more commonly used model of Brown et al. (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008).",
                    "sid": 30,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The two models are very similar, but whereas the former takes classto-class transitions into account, the latter directly models word-to-class transitions.",
                    "sid": 31,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By ignoring classto-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008).",
                    "sid": 32,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is a useful property, as we later develop an algorithm for inducing cross-lingual word clusters that calls this monolingual algorithm as a subroutine.",
                    "sid": 33,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More formally, let C : V H 1, ... , K be a (hard) clustering function that maps each word type from the vocabulary, V, to one of K cluster identities.",
                    "sid": 34,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With the model of Uszkoreit and Brants (2008), the likelihood of a sequence of word tokens, w = (wi)mi=1, with wi E V U {S}, where S is a designated start-ofsegment symbol, factors as Compare this to the model of Brown et al. (1992): By incorporating cross-lingual cluster features in a m linguistic transfer system, we are for the first time L'(w; C) = P(wi|C(wi))P(C(wi)|C(wi\u22121)) . combining SSL and cross-lingual transfer. i=1",
                    "sid": 35,
                    "ssid": 29,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "2 monolingual word cluster features",
            "number": "2",
            "sents": [
                {
                    "text": "Word cluster features have been shown to be useful in various tasks in natural language processing, including syntactic dependency parsing (Koo et al., 2008; Haffari et al., 2011; Tratz and Hovy, 2011), syntactic chunking (Turian et al., 2010), and NER (Freitag, 2004; Miller et al., 2004; Turian et al., 2010; Faruqui and Pad\u00b4o, 2010).",
                    "sid": 36,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Intuitively, the reason for the effectiveness of cluster features lie in their ability to aggregate local distributional information from large unlabeled corpora, which aid in conquering data sparsity in supervised training regimes as well as in mitigating cross-domain generalization issues.",
                    "sid": 37,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While the use of class-to-class transitions can lead to more compact models, which is often useful for conquering data sparsity, when clustering large data sets we can get reliable statistics directly on the wordto-class transitions (Uszkoreit and Brants, 2008).",
                    "sid": 38,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to the clustering model that we make use of in this study, a number of additional word clustering and embedding variants have been proposed.",
                    "sid": 39,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, Turian et al. (2010) assessed the effectiveness of the word embedding techniques of Collobert and Weston (2008) and Mnih and Hinton (2007) along with the word clustering technique of Brown et al.",
                    "sid": 40,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1992) for syntactic chunking and NER.",
                    "sid": 41,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recently, Dhillon et al. (2011) proposed a word embedding method based on canonical correlation analysis that provides state-of-the art results for wordbased SSL for English NER.",
                    "sid": 42,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER.",
                    "sid": 43,
                    "ssid": 8,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 monolingual cluster experiments",
            "number": "3",
            "sents": [
                {
                    "text": "Before moving on to the multilingual setting, we conduct a set of monolingual experiments where we evaluate the use of the monolingual word clusters just described as features for dependency parsing and NER.",
                    "sid": 44,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the parsing experiments, we study the following thirteen languages:1 Danish (DA), German (DE), Greek (EL), English (EN), Spanish (ES), French (FR), Italian (IT), Korean (KO), Dutch (NL), Portugese (PT), Russian (RU), Swedish (SV) and Chinese (ZH) \u2013 representing the Chinese, Germanic, Hellenic, Romance, Slavic, Altaic and Korean genera.",
                    "sid": 45,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the NER experiments, we study three Germanic languages: German (DE), English (EN) and Dutch (NL); and one Romance language: Spanish (ES).",
                    "sid": 46,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Details of the labeled and unlabeled data sets used are given in Appendix A.",
                    "sid": 47,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all experiments we fixed the number of clusters to 256 as this performed well on held-out data.",
                    "sid": 48,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, we only clustered the 1 million most frequent word types in each language for both efficiency and sparsity reasons.",
                    "sid": 49,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For languages in which our unlabeled data did not have at least 1 million types, we considered all types.",
                    "sid": 50,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008).",
                    "sid": 51,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all languages and settings, we use an arc-eager decoding strategy, with a beam of eight hypotheses, and perform ten epochs of the averaged structured perceptron algorithm (Zhang and Clark, 2008).",
                    "sid": 52,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We extend the state-of-the-art feature model recently introduced by Zhang and Nivre (2011) by adding an additional word cluster based feature template for each word based template.",
                    "sid": 53,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additionally, we add templates where one or more partof-speech feature is replaced with the corresponding cluster feature.",
                    "sid": 54,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The resulting set of additional feature templates are shown in Table 1.",
                    "sid": 55,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The expanded feature model includes all of the feature templates defined by Zhang and Nivre (2011), which we also use as the baseline model, whereas Table 1 only shows our new templates due to space limitations.",
                    "sid": 56,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all NER experiments, we use a sequential firstorder conditional random field (CRF) with a unit variance Normal prior, trained with L-BFGS until c-convergence (c = 0.0001, typically obtained after less than 400 iterations).",
                    "sid": 57,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The feature model used for the NER tagger is shown in Table 2.",
                    "sid": 58,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These are similar to the features used by Turian et al. (2010), with the main difference that we do not use any long range features and that we add templates that conjoin adjacent clusters and adjacent tags as well as templates that conjoin label transitions with tags, clusters and capitalization features.",
                    "sid": 59,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results of the parsing experiments, measured with labeled accuracy score (LAS) on all sentence lengths, excluding punctuation, are shown in Table 3.",
                    "sid": 60,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The baselines are all comparable to the state-of-theart.",
                    "sid": 61,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On average, the addition of word cluster features yields a 6% relative reduction in error and upwards of 15% (for RU).",
                    "sid": 62,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All languages improve except FR, which sees neither an increase nor a decrease in LAS.",
                    "sid": 63,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We observe an average absolute increase in LAS of approximately 1%, which is inline with previous observations (Koo et al., 2008).",
                    "sid": 64,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is perhaps not surprising that RU sees a large gain as it is a highly inflected language, making observations of lexical features far more sparse.",
                    "sid": 65,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some languages, e.g., FR, NL, and ZH see much smaller gains.",
                    "sid": 66,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One likely culprit is a divergence between the tokenization schemes used in the treebank and in our unlabeled data, which for Indo-European languages is closely related to the Penn Treebank tokenization.",
                    "sid": 67,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the NL treebank contains many multi-word tokens that are typically broken apart by our automatic tokenizer.",
                    "sid": 68,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The NER results, in terms of F1 measure, are listed in Table 4.",
                    "sid": 69,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Introducing word cluster features for NER reduces relative errors on the test set by 21% (39% on the development set) on average.",
                    "sid": 70,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Broken down per language, reductions on the test set vary from substantial for NL (30%) and EN (26%), down to more modest for DE (17%) and ES (12%).",
                    "sid": 71,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The addition of cluster features most markedly improve recognition of the PER category, with an average error reduction on the test set of 44%, while the reductions for ORG (19%), LOC (17%) and MISC (10%) are more modest, but still significant.",
                    "sid": 72,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although our results are below the best reported results for EN and DE (Lin and Wu, 2009; Faruqui and Pad\u00b4o, 2010), the relative improvements of adding word clusters are inline with previous results on NER for EN (Miller et al., 2004; Turian et al., 2010), who report error reductions of approximately 25% from adding word cluster features.",
                    "sid": 73,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Slightly higher reductions where achieved for DE by Faruqui and Pad\u00b4o (2010), who report a reduction of 22%.",
                    "sid": 74,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that we did not tune hyper-parameters of the supervised learning methods and of the clustering method, such as the number of clusters (Turian et al., 2010; Faruqui and Pad\u00b4o, 2010), and that we did not apply any heuristic for data cleaning such as that used by Turian et al. (2010).",
                    "sid": 75,
                    "ssid": 32,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 cross-lingual word cluster features",
            "number": "4",
            "sents": [
                {
                    "text": "All results of the previous section rely on the availability of large quantities of language specific annotations for each task.",
                    "sid": 76,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Cross-lingual transfer methods and unsupervised methods have recently been shown to hold promise as a way to at least partially sidestep the demand for labeled data.",
                    "sid": 77,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al., 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010).",
                    "sid": 78,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The aim of transfer methods is instead to use knowledge induced from labeled resources in one or more source languages to construct systems for target languages in which no or few such resources are available (Hwa et al., 2005).",
                    "sid": 79,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Currently, the performance of even the most simple direct transfer systems far exceeds that of unsupervised systems (Cohen et al., 2011; McDonald et al., 2011; S\u00f8gaard, 2011).",
                    "sid": 80,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our starting point is the delexicalized direct transfer method proposed by McDonald et al. (2011) based on work by Zeman and Resnik (2008).",
                    "sid": 81,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This method was shown to outperform a number of state-of-the-art unsupervised and transfer-based baselines.",
                    "sid": 82,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The method is simple; for a given training set, the learner ignores all lexical identities and only observes features over other characteristics, e.g., part-of-speech tags, orthographic features, direction of syntactic attachment, etc.",
                    "sid": 83,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The learner builds a model from an annotated source language data set, after which the model is used to directly make target language predictions.",
                    "sid": 84,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are three basic assumptions that drive this approach.",
                    "sid": 85,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, that high-level tasks, such as syntactic parsing, can be learned reliably using coarse-grained statistics, such as part-of-speech tags, in place of fine-grained statistics such as lexical word identities.",
                    "sid": 86,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, that the parameters of features over coarsegrained statistics are in some sense language independent, e.g., a feature that indicates that adjectives modify their closest noun is useful in all languages.",
                    "sid": 87,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Third, that these coarse-grained statistics are robustly available across languages.",
                    "sid": 88,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The approach proposed by McDonald et al. (2011) relies on these three assumptions.",
                    "sid": 89,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available.",
                    "sid": 90,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Below, we extend this approach to universal parsing by adding cross-lingual word cluster features.",
                    "sid": 91,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A cross-lingual word clustering is a clustering of words in two languages, in which the clusters correspond to some meaningful cross-lingual property.",
                    "sid": 92,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, prepositions from both languages should be in the same cluster, proper names from both languages in another cluster and so on.",
                    "sid": 93,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By adding features defined over these clusters, we can, to some degree, re-lexicalize the delexicalized models, while maintaining the \u201cuniversality\u201d of the features.",
                    "sid": 94,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This approach is outlined in Figure 1.",
                    "sid": 95,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Assuming that we have an algorithm for generating cross-lingual word clusters (see Section 4.2), we can augment the delexicalized parsing algorithm to use these word cluster features at training and testing time.",
                    "sid": 96,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to further motivate the proposed approach, consider the accuracy of the supervised English parser.",
                    "sid": 97,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A parser with lexical, part-of-speech and cluster features achieves 90.7% LAS (see Table 3).",
                    "sid": 98,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If we remove all lexical and cluster features, the same parser achieves 83.1%.",
                    "sid": 99,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, if we add back just the cluster features, the accuracy jumps back up to 89.5%, which is only 1.2% below the full system.",
                    "sid": 100,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, if we can accurately learn cross-lingual clusters, there is hope of regaining some of the accuracy lost due to the delexicalization process.",
                    "sid": 101,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our first method for inducing cross-lingual clusters has two stages.",
                    "sid": 102,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, it clusters a source language (S) as in the monolingual case, and then projects these clusters to a target language (T), using word alignments.",
                    "sid": 103,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given two aligned word sequences set of scored alignments from the source language to the target language, where (wTj , wSaj, sj,aj) \u2208 AT |S is an alignment from the ajth source word to the jth target word, with score sj,aj \u2265 \u03b4.2 We use the shorthand j \u2208 AT |S to denote those target words wTj that are aligned to some source word wSaj.",
                    "sid": 104,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Provided a clustering CS, we assign the target word t \u2208 VT to the cluster with which it is most often aligned: where [\u00b7\ufffd is the indicator function.",
                    "sid": 105,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We refer to the cross-lingual clusters induced in this way as PROJECTED CLUSTERS.",
                    "sid": 106,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This simple projection approach has two potential drawbacks.",
                    "sid": 107,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, it only provides a clustering of those target language words that occur in the word 2In our case, the alignment score corresponds to the conditional alignment probability p(wTj |wSa\ufffd ).",
                    "sid": 108,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All E-alignments are ignored and we use S = 0.95 throughout. aligned data, which is typically smaller than our monolingual data sets.",
                    "sid": 109,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, the mapped clustering may not necessarily correspond to an acceptable target language clustering in terms of monolingual likelihood.",
                    "sid": 110,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to tackle these issues, we propose the following more complex model.",
                    "sid": 111,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, to find clusterings that are good according to both the source and target language, and to make use of more unlabeled data, we model word sequences in each language by the monolingual language model with likelihood function defined by equation (1).",
                    "sid": 112,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Denote these likelihood functions respectively by LS(wS; CS) and LT (wT; CT), where we have overloaded notation so that the word sequences denoted by wS and wT include much more plentiful non-aligned data when taken as an argument of the monolingual likelihood functions.",
                    "sid": 113,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, we couple the clusterings defined by these individual models, by introducing additional factors based on word alignments, as proposed by Och (1999): and the symmetric LS|T(wS; AS|T, CS, CT).",
                    "sid": 114,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that the simple projection defined by equation (2) correspond to a hard assignment variant of this probabilistic formulation when the source clustering is fixed.",
                    "sid": 115,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Combining all four factors results in the joint monolingual and cross-lingual objective function The intuition of this approach is that the clusterings CS and CT are forced to jointly explain the source and target data, treating the word alignments as a form of soft constraints.",
                    "sid": 116,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We approximately optimize (3) with the alternating procedure in Algorithm 1, in which we iteratively maximize LS and LT , keeping the other factors fixed.",
                    "sid": 117,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this way we can generate cross-lingual clusterings using all the monolingual data while forcing the clusterings to obey the word alignment constraints.",
                    "sid": 118,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We refer to the clusters induced with this method as X-LINGUAL CLUSTERS.",
                    "sid": 119,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In practice we found that each unconstrained monolingual run of the exchange algorithm (lines \u2020 Optimized via the exchange algorithm keeping the cluster of projected words fixed and only clustering additional words not in the projection.",
                    "sid": 120,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 and 3) moves the clustering too far from those that obey the word alignment constraints, which causes the procedure to fail to converge.",
                    "sid": 121,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, we found that fixing the clustering of the words that are assigned clusters in the projection stages (lines 2 and 4) and only clustering the remaining words works well in practice.",
                    "sid": 122,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, we found that iterating the procedure has little effect on performance and set N = 1 for all subsequent experiments.",
                    "sid": 123,
                    "ssid": 48,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5 cross-lingual experiments",
            "number": "5",
            "sents": [
                {
                    "text": "In our first set of experiments on using cross-lingual cluster features, we evaluate direct transfer of our EN parser, trained on Stanford style dependencies (De Marneffe et al., 2006), to the the ten non-EN Indo-European languages listed in Section 3.",
                    "sid": 124,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We exclude KO and ZH as initial experiments proved direct transfer a poor technique when transferring parsers between such diverse languages.",
                    "sid": 125,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al. (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters.",
                    "sid": 126,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In both cases the feature models are the same as those used in Section 3.1, except that they are delexicalized by removing all lexical word-identity features.",
                    "sid": 127,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluate both the PROJECTED CLUSTERS and the X-LINGUAL CLUSTERS.",
                    "sid": 128,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For these experiments we train the perceptron for only five epochs in order to prevent over-fitting, which is an acute problem due to the divergence between the training and testing data sets in this setting.",
                    "sid": 129,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, in accordance to standard practices we only evaluate unlabeled attachment score (UAS) due to the fact that each treebank uses a different \u2013 possibly non-overlapping \u2013 label set.",
                    "sid": 130,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our second set of experiments, we evaluate direct transfer of a NER system trained on EN to DE, ES and NL.",
                    "sid": 131,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the same feature models as in the monolingual case, with the exception that we use universal part-of-speech tags for all languages and we remove the capitalization feature when transferring from EN to DE.",
                    "sid": 132,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Capitalization is both a prevalent and highly predictive feature of named-entities in EN, while in DE, capitalization is even more prevalent, but has very low predictive power.",
                    "sid": 133,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Interestingly, while delexicalization has shown to be important for direct transfer of dependency-parsers (McDonald et al., 2011), we noticed in preliminary experiments that it substantially degrades performance for NER.",
                    "sid": 134,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We hypothesize that this is because word features are predictive of common proper names and that these are often translated directly across languages, at least in the case of newswire text.",
                    "sid": 135,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As for the transfer parser, when training the source NER model, we regularize the model more heavily by setting Q = 0.1.",
                    "sid": 136,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Appendix A contains the details of the training, testing, unlabeled and parallel/aligned data sets.",
                    "sid": 137,
                    "ssid": 14,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5.1 results",
            "number": "6",
            "sents": [
                {
                    "text": "Table 5 lists the results of the transfer experiments for dependency parsing.",
                    "sid": 138,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The baseline results are comparable to those in McDonald et al. (2011) and thus also significantly outperform the results of recent unsupervised approaches (Berg-Kirkpatrick and Klein, 2010; Naseem et al., 2010).",
                    "sid": 139,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Importantly, crosslingual cluster features are helpful across the board and give a relative error reduction ranging from 3% for DA to 13% for PT, with an average reduction of 6%, in terms of unlabeled attachment score (UAS).",
                    "sid": 140,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This shows the utility of cross-lingual cluster features for syntactic transfer.",
                    "sid": 141,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, X-LINGUAL CLUSTERS provides roughly the same performance as PROJECTED CLUSTERS suggesting that even simple methods of cross-lingual clustering are sufficient for direct transfer dependency parsing.",
                    "sid": 142,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We would like to stress that these results are likely to be under-estimating the parsers\u2019 actual ability to predict Stanford-style dependencies in the target languages.",
                    "sid": 143,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is because the target language annotations that we use for evaluation differ from the Stanford dependency annotation.",
                    "sid": 144,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some of these differences are warranted in that certain target language phenomena are better captured by the native annotation.",
                    "sid": 145,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, differences such as choice of lexical versus functional head are more arbitrary.",
                    "sid": 146,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To highlight this point we run two additional experiments.",
                    "sid": 147,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we had linguists, who were also fluent speakers of German, re-annotate the DE test set so that unlabeled arcs are consistent with Stanfordstyle dependencies.",
                    "sid": 148,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using this data, NO CLUSTERS obtains 60.0% UAS, PROJECTED CLUSTERS 63.6% and X-LINGUAL CLUSTERS 64.4%.",
                    "sid": 149,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When compared to the scores on the original data set (48.9%, 50.3% and 50.7%, respectively) we can see that not only is the baseline system doing much better, but that the improvements from cross-lingual clustering are much more pronounced.",
                    "sid": 150,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Next, we investigated the accuracy of subject and object dependencies, as these are often annotated in similar ways across treebanks, typically modifying the main verb of the sentence.",
                    "sid": 151,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The bottom half of Table 5 gives the scores when restricted to such dependencies in the gold data.",
                    "sid": 152,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We measure the percentage of modifiers in subject and object dependencies that modify the correct word.",
                    "sid": 153,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Indeed, here we see the difference in performance become clearer, with the cross-lingual cluster model reducing errors by 14% relative to the non-cross-lingual model and upwards of 22% relative for IT.",
                    "sid": 154,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We now turn to the results of the transfer experiments for NER, listed in Table 6.",
                    "sid": 155,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While the performance of the transfer systems is very poor when no word clusters are used, adding cross-lingual word clusters give substantial improvements across all languages.",
                    "sid": 156,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The simple PROJECTED CLUSTERS work well, but the X-LINGUAL CLUSTERS provide even larger improvements.",
                    "sid": 157,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On average the latter reduce errors on the test set by 22% in terms of FI and up to 26% for ES.",
                    "sid": 158,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also measure how well the direct transfer NER systems are able to detect entity boundaries (ignoring the entity categories).",
                    "sid": 159,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here, on average, the best clusters provide a 24% relative error reduction on the test set (75.8 vs. 68.1 FI).",
                    "sid": 160,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To our knowledge there are no comparable results on transfer learning of NER systems.",
                    "sid": 161,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Based on the results of this first attempt at this scenario, we believe that transfer learning by multilingual word clusters could be developed into a practical way to construct NER systems for resource poor languages.",
                    "sid": 162,
                    "ssid": 25,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "6 conclusion",
            "number": "7",
            "sents": [
                {
                    "text": "In the first part of this study, we showed that word clusters induced from a simple class-based language model can be used to significantly improve on stateof-the-art supervised dependency parsing and NER for a wide range of languages and even across language families.",
                    "sid": 163,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although the improvements vary between languages, the addition of word cluster features never has a negative impact on performance.",
                    "sid": 164,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This result has important practical consequences as it allows practitioners to simply plug in word cluster features into their current feature models.",
                    "sid": 165,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given previous work on word clusters for various linguistic structure prediction tasks, these results are not too surprising.",
                    "sid": 166,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, to our knowledge this is the first study to apply the same type of word cluster features across languages and tasks.",
                    "sid": 167,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the second part, we provided two simple methods for inducing cross-lingual word clusters.",
                    "sid": 168,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first method works by projecting word clusters, induced from monolingual data, from a source language to a target language directly via word alignments.",
                    "sid": 169,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second method, on the other hand, makes use of monolingual data in both the source and the target language, together with word alignments that act as constraints on the joint clustering.",
                    "sid": 170,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then showed that by using these cross-lingual word clusters, we can significantly improve on direct transfer of discriminative models for both parsing and NER.",
                    "sid": 171,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As in the monolingual case, both types of cross-lingual word cluster features yield improvements across the board, with the more complex method providing a significantly larger improvement for NER.",
                    "sid": 172,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although the performance of transfer systems is still substantially below that of supervised systems, this research provides one step towards bridging this gap.",
                    "sid": 173,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Further, we believe that it opens up an avenue for future work on multilingual clustering methods, cross-lingual feature projection and domain adaptation for direct transfer of linguistic structure.",
                    "sid": 174,
                    "ssid": 12,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments",
            "number": "8",
            "sents": [
                {
                    "text": "We thank John DeNero for help with creating the word alignments; Reut Tsarfaty and Joakim Nivre for rewarding discussions on evaluation; Slav Petrov and Kuzman Ganchev for discussions on cross-lingual clustering; and the anonymous reviewers, along with Joakim Nivre, for valuable comments that helped improve the paper.",
                    "sid": 175,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first author is grateful for the financial support of the Swedish National Graduate School of Language Technology (GSLT).",
                    "sid": 176,
                    "ssid": 2,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "a data sets",
            "number": "9",
            "sents": [
                {
                    "text": "In the parsing experiments, we use the following data sets.",
                    "sid": 177,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For DA, DE, EL, ES, IT, NL, PT and SV, we use the predefined training and evaluation data sets from the CoNLL 2006/2007 data sets (Buchholz and Marsi, 2006; Nivre et al., 2007).",
                    "sid": 178,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For EN we use sections 02-21, 22, and 23 of the Penn WSJ Treebank (Marcus et al., 1993) for training, development and evaluation.",
                    "sid": 179,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For FR we used the French Treebank (Abeill\u00b4e and Barrier, 2004) with splits defined in Candito et al. (2010).",
                    "sid": 180,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For KO we use the Sejong Korean Treebank (Han et al., 2002) randomly splitting the data into 80% training, 10% development and 10% evaluation.",
                    "sid": 181,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For RU we use the SynTagRus Treebank (Boguslavsky et al., 2000; Apresjan et al., 2006) randomly splitting the data into 80% training, 10% development and 10% evaluation.",
                    "sid": 182,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For ZH we use the Penn Chinese Treebank v6 (Xue et al., 2005) using the proposed data splits from the documentation.",
                    "sid": 183,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both EN and ZH were converted to dependencies using v1.6.8 of the Stanford Converter (De Marneffe et al., 2006).",
                    "sid": 184,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "FR was converted using the procedure defined in Candito et al. (2010).",
                    "sid": 185,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "RU and KO are native dependency treebanks.",
                    "sid": 186,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the CoNLL data sets we use the part-of-speech tags provided with the data.",
                    "sid": 187,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all other data sets, we train a part-of-speech tagger on the training data in order to tag the development and evaluation data.",
                    "sid": 188,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the NER experiments we use the training, development and evaluation data sets from the CoNLL 2002/2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for all four languages (DE, EN, ES and NL).",
                    "sid": 189,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The data set for each language consists of newswire text annotated with four entity categories: Location (LOC), Miscellaneous (MISC), Organization (ORG) and Person (PER).",
                    "sid": 190,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the part-of-speech tags supplied with the data, except for ES where we instead use universal part-of-speech tags (Petrov et al., 2011).",
                    "sid": 191,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unlabeled data for training the monolingual cluster models was extracted from one year of newswire articles from multiple sources from a news aggregation website.",
                    "sid": 192,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This consists of 0.8 billion (DA) to 121.6 billion (EN) tokens per language.",
                    "sid": 193,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.",
                    "sid": 194,
                    "ssid": 18,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}