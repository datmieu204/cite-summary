{
    "ID": "C08-1074",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Random Restarts in Minimum Error Rate Training for Statistical Machine Translation",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Och?s (2003) minimum error rate training (MERT) procedure is the most commonly used method for training feature weights instatistical machine translation (SMT) models.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The use of multiple randomized start ing points in MERT is a well-established practice, although there seems to be nopublished systematic study of its benefits.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We compare several ways of perform ing random restarts with MERT.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We findthat all of our random restart methods out perform MERT without random restarts,and we develop some refinements of ran dom restarts that are superior to the most common approach with regard to resulting model quality and training time.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Och (2003) introduced minimum error rate training (MERT) for optimizing feature weights in sta tistical machine translation (SMT) models, and demonstrated that it produced higher translation quality scores than maximizing the conditional likelihood of a maximum entropy model using the same features.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Och?s method performs a series of one-dimensional optimizations of the feature weight vector, using an innovative line search thattakes advantage of special properties of the map ping from sets of feature weights to the resulting translation quality measurement.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Och?s line search is guaranteed to find a global optimum, whereas more general line search methods are guaranteed only to find a local optimum.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "c ? 2008.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some rights reserved.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Global optimization along one dimension at a time, however, does not insure global optimizationin all dimensions.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hence, as Och briefly men tions, ?to avoid finding a poor local optimum,?MERT can be augmented by trying multiple ran dom starting points for each optimization search within the overall algorithm.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, we are notaware of any published study of the effects of ran dom restarts in the MERT optimization search.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We first compare a variant of Och?s method withand without multiple starting points for the op timization search, selecting initial starting points randomly according to a uniform distribution.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Wefind that using multiple random restarts can sub stantially improve the resulting model in terms oftranslation quality as measured by the BLEU metric, but that training time also increases substan tially.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We next try selecting starting points by a random walk from the last local optimum reached,rather than by sampling from a uniform distrib ution.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We find this provides a slight additional improvement in BLEU score, and is significantly faster, although still slower than training without random restarts.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally we look at two methods for speeding uptraining by pruning the set of hypotheses consid ered.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We find that, under some circumstances, this can speed up training so that it takes very little additional time compared to the original method without restarts, with no significant reduction in BLEU score compared to the best training methods in our experiments.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "och?s mert procedure. ",
            "number": "2",
            "sents": [
                {
                    "text": "While minimum error rate training for SMT istheoretically possible by directly applying gen eral numerical optimization techniques, such as the downhill simplex method or Powell?s method 585 (Press, 2002), naive use of these techniques wouldinvolve repeated translation of the training sentences using hundreds or thousands of combinations of feature weights, which is clearly impracti cal given the speed of most SMT decoders.",
                    "sid": 20,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Och?s optimization method saves expensive SMT decoding time by generating lists of n-besttranslation hypotheses, and their feature values ac cording to the SMT model, and then optimizingfeature weights just with respect to those hypothe ses.",
                    "sid": 21,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this way, as many different feature weightsettings as necessary can be explored without rerunning the decoder.",
                    "sid": 22,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The translation quality mea surement for the training corpus can be estimatedfor a given point in feature weight space by find ing the highest scoring translation hypothesis, out of the current set of hypotheses, for each sentencein the training set.",
                    "sid": 23,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is typically orders of mag nitude faster than re-running the decoder for each combination of feature weights.Since the resulting feature weights are optimized only for one particular set of translation hypotheses, the decoder may actually produce different results when run with those weights.",
                    "sid": 24,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore Och iterates the process, re-running the decoder with the optimized feature weights to pro duce new sets of n-best translation hypotheses,merging these with the previous sets of hypothe ses, and re-optimizing the feature weights relativeto the expanded hypothesis sets.",
                    "sid": 25,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This process is re peated until no more new hypotheses are obtained for any sentence in the training set.Another innovation by Och is a method of nu merical optimization that takes advantage of the fact that, while translation quality metrics may have continous values, they are always applied to the discrete outputs of a translation decoder.",
                    "sid": 26,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This means that any measure of translation quality can change with variation in feature weights only at discrete points where the decoder output changes.",
                    "sid": 27,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Och takes advantage of this through an efficientprocedure for finding all the points along a one dimensional line in feature weight space at which the highest scoring translation hypothesis changes,given the current set of hypotheses for a particu lar sentence.",
                    "sid": 28,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By merging the lists of such points for all sentences in the training set, he finds all the points at which the highest scoring hypothesis changes for any training sentence.",
                    "sid": 29,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finding the optimal value of the feature weightsalong the line being optimized then requires sim ply evaluating the translation quality metric foreach range of values for the feature weights be tween two such consecutive points.",
                    "sid": 30,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This can be done efficiently by tracking incremental changesin the sufficient statistics for the translation qual ity metric as we iterate through the points where things change.",
                    "sid": 31,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Och uses this procedure as a linesearch method in an iterative optimization procedure, until no additional improvement in the trans lation quality metric is obtained, given the current sets of translation hypotheses.",
                    "sid": 32,
                    "ssid": 13,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "optimization with random restarts. ",
            "number": "3",
            "sents": [
                {
                    "text": "Although Och?s line search is globally optimal, this is not sufficient to guarantee that a series ofline searches will find the globally optimal com bination of all feature weights.",
                    "sid": 33,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To avoid getting stuck at an inferior local optimum during MERT, it is usual to perform multiple optimization searches over each expanded set of translation hypotheses starting from different initial points.",
                    "sid": 34,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Typically, oneof these points is the best point found while optimizing over the previous set of translation hypotheses.1 Additional starting points are then se lected by independently choosing initial values foreach feature weight according to a uniform distri bution over a fixed interval, say ?1.0 to +1.0.",
                    "sid": 35,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thebest point reached, starting from either the previ ous optimum or one of the random restart points,is selected as the optimum for the current set of hy potheses.",
                    "sid": 36,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This widely-used procedure is described by Koehn et al (2007, p. 50).",
                    "sid": 37,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.1 Preliminary evaluation.",
                    "sid": 38,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our first experiments, we compared a variantof Och?s MERT procedure with and without ran dom restarts as described above.",
                    "sid": 39,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference.",
                    "sid": 40,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We built a stan-.",
                    "sid": 41,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "dard baseline phrasal SMT system, as describedby Koehn et al (2003), for translating from Eng lish to French (E-to-F), using the word alignments and French target language model provided by the workshop organizers.We trained a model with the standard eight fea tures: E-to-F and F-to-E phrase translation log 1Since additional hypotheses have been added, initiatingan optimization search from this point on the new set of hy potheses will often lead to a higher local optimum.",
                    "sid": 42,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "586 probabilities, E-to-F and F-to-E phrase translationlexical scores, French language model log proba bilities, phrase pair count, French word count, and distortion score.",
                    "sid": 43,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Feature weight optimization was performed on the designated 2000-sentence-pair development set, and the resulting feature weightswere evaluated on the designated 2000-sentence pair development test set, using the BLEU-4 metric with one reference translation per sentence.",
                    "sid": 44,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "At each decoding iteration we generated the 100-best translation hypotheses found by our phrasal SMT decoder.",
                    "sid": 45,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To generate the initial 100-best list, we applied the policy of setting the weights for features we expected to be positivelycorrelated with BLEU to 1, the weights for fea tures we expected to be negatively correlated with BLEU to ?1, and the remaining weights to 0.",
                    "sid": 46,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this case, we set the initial distortion score weight to ?1, the phrase count weight to 0, and all other feature weights to 1.",
                    "sid": 47,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We made a common modification of the MERTprocedure described by Och, by replacing Powell?s method (Press, 2002) for selecting the direc tions in which to search the feature weight space, with simple co-ordinate ascent?following Koehn et al (2007)?repeatedly optimizing one featureweight at a time while holding the others fixed, un til all feature weights are at optimum values, given the values of the other feature weights.",
                    "sid": 48,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Powell?smethod is not designed to reach a better local optimum than co-ordinate ascent, but does have convergence guarantees under certain idealized condi tions.",
                    "sid": 49,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, we have observed informally that,in MERT, co-ordinate ascent always seems to converge relatively quickly, with Powell?s method of fering no clear advantage.",
                    "sid": 50,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also modified Och?s termination test slightly.",
                    "sid": 51,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As noted above, Och suggests terminating the overall procedure when n-best decoding fails to produce any hypotheses that have not already been seen.",
                    "sid": 52,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Without random restarts, this will guarantee convergence because the last set of feature weights selected will still be a local optimum.2 However, if we go through the coordinate ascent procedure without finding a better set of feature weights, thenwe do not have to perform the last iteration of n best decoding, because it will necessarily produce the same n-best lists as the previous iteration, as2With random restarts, there can be no guarantee of con vergence, unless we have a true global optimization method, or we enumerate all possible hypotheses permitted by the model.",
                    "sid": 53,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "long as the decoder is deterministic.",
                    "sid": 54,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus we can terminate the overall procedure if either we eitherfail to generate any new hypotheses in n-best de coding, or the optimium set of feature weights doesnot change in the coordinate ascent phase of train ing.",
                    "sid": 55,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In fact, we relax the termination test a bit more than this, and terminate if no feature weight changes by more than 1.0%.",
                    "sid": 56,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Without random restarts, we found that MERTconverged in 8 decoding iterations, with the result ing model producing a BLEU score of 31.12 on the development test set.",
                    "sid": 57,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the experiment with random restarts, after each iteration of 100-best decoding, we searched from 20 initial points, 19points selected by uniform sampling over the interval [?1, 1] for each feature weight, plus the optimum point found for the previous set of hypotheses.",
                    "sid": 58,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This procedure converged in 10 decoding iterations, with a BLEU score of 32.02 on the de velopment test set, an improvement of 0.90 BLEU, compared to MERT without random restarts.",
                    "sid": 59,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While the difference in BLEU score with and without random restarts was substantial, training with random restarts took much longer.",
                    "sid": 60,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Withour phrasal decoder and our MERT implementa tion, optimizing feature weights took 3894 seconds without random restarts and 12690 seconds with random restarts.3 We therefore asked the question whether there was some other way to invest extra time in training feature weights that might be justas effective as performing random restarts.",
                    "sid": 61,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The ob vious thing to try is using larger n-best lists, so we re-ran the training without random restarts, using n-best lists of 200 and 300.",
                    "sid": 62,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using n-best lists of 200 produced a noticeableimprovement in training without restarts, converg ing in 9 decoding iterations taking 7877 seconds,and producing a BLEU score of 31.83 on the development test set.",
                    "sid": 63,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using n-best lists of 300 converged in 8 decoding iterations taking 8973 sec onds, but the BLEU score on the development test set fell back to 31.16.",
                    "sid": 64,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, simply increasing thesize of the n-best list does not seem to be a reli able method for improving the results obtained by MERT without random restarts.",
                    "sid": 65,
                    "ssid": 33,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "random walk restarts. ",
            "number": "4",
            "sents": [
                {
                    "text": "In the procedure described above, the initial values for each feature weight are independently sampled3Timings are for single-threaded execution using a desk top PC with 3.60 GHz Intel Xeon processors.",
                    "sid": 66,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "587 from a uniform distribution over the range [?1, 1].",
                    "sid": 67,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have observed anecdotally, however, that if the selected starting point itself produces a BLEUscore much below the best we have seen so far, co ordinate ascent search is very unlikely to take us to a point that is better than the current best.",
                    "sid": 68,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to bias the selection of restarting points towards better scores, we select starting points by random walk from the ending point of the last coordinate ascent search.",
                    "sid": 69,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The idea is to perform a series of cautious steps in feature weight space guided by training set BLEU.",
                    "sid": 70,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We begin the walk at the ending point of the last coordinate ascent search; let us call this point ~w (0) . Each step updates the feature weights in a. manner inspired by Metropolis-Hastings sampling (Hastings, 1970).",
                    "sid": 71,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Starting from the current feature weight vector ~w(i), we sample a small update from a multivariate Gaussian distribution with mean of 0 and diagonal covariance matrix ?2I . This updateis added to the current value to produce a new po tential feature weight vector.",
                    "sid": 72,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The BLEU scores forthe old and the new feature weight vector are com pared.",
                    "sid": 73,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The new feature weight vector is always accepted if the BLEU score on the training set is improved; however if the BLEU score drops, thenew vector is accepted with a probability that de pends on how close the new BLEU score is to the previous one.",
                    "sid": 74,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After a fixed number of steps, the walk is terminated, and we produce a value to use as the initial point for the next round of coordinate ascent.",
                    "sid": 75,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are several wrinkles, however.",
                    "sid": 76,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we prefer that the scores not fall substantially duringthe random walk.",
                    "sid": 77,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore we establish a base line value of m = BLEU(~w(0)) ? 0.005 (i.e., 1/2BLEU point below the initial value) and do not allow a step to go below this baseline value.",
                    "sid": 78,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To en sure this, each step progresses as follows: ~ d (i) ? GAUSSIAN(0, ?2I) ~v (i) = ~w (i) + ~ d (i) u (i) ? UNIFORM(0, 1) ~w (i+1) = ? ?",
                    "sid": 79,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "~v (i) if BLEU(~v (i) )?m BLEU(~w(i))?m ? u (i) ~w (i) otherwise.",
                    "sid": 80,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With this update rule, we know that ~w(i+1) willnever go below m, since the initial value is not be low m, and any step moving below m will result in a negative ratio and therefore not be accepted.So far, ?2 is left as a free parameter.",
                    "sid": 81,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An initial value of 0.001 performs well in our experi ence, though in general it may result in steps thatare consistently too small (so that only a very lo cal neighborhood is explored) or too large (so that the vast majority of steps are rejected).",
                    "sid": 82,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore we devote the first half of the steps to ?burn-in?;that is, tuning the variance parameter so that ap proximately 60% of the steps are accepted.",
                    "sid": 83,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During burn-in, we compute the acceptance rate after each step.",
                    "sid": 84,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If it is less than 60%, we multiply ?2 by 0.99; if greater, we multiply by 1.01.",
                    "sid": 85,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The final twist is in selection of the point used for the next iteration of coordinate ascent.",
                    "sid": 86,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Rather than using the final point of the random walk ~w(n), we return the feature weight vector that achieved the highest BLEU score after burn-in: ~w? = argmax ~w (i) ,n/2<i?n BLEU(~w).",
                    "sid": 87,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This ensures that the new feature weight vector has a relatively high objective function value yet is likely very different from the initial point.",
                    "sid": 88,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.1 Preliminary evaluation.",
                    "sid": 89,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To evaluate the random walk selection procedure,we used a similar experimental set-up to the previous one, testing on the 2006 English-French Eu roparl corpus, using n-best lists of 100, and 20 starting points for each coordinate ascent search?",
                    "sid": 90,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "one being the best point found for the previous hypothesis set, and the other 19 selected by our random walk procedure.",
                    "sid": 91,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We set the number of steps to be used in each random walk to 500.",
                    "sid": 92,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thisprocedure converged in 6 decoding iterations tak ing 8458 seconds, with a BLEU score of 32.13 on the development test set.",
                    "sid": 93,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is an improvement of 0.11 BLEU over the uniform random restart method, and it also took only 67% as much time.",
                    "sid": 94,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The speed up was due to the fact that random walk method converged in 2 fewer decoding iterations, although the average time per iteration was greater (1410 seconds vs. 1269 seconds) because of the extra time needed for the random walk.",
                    "sid": 95,
                    "ssid": 30,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "hypothesis set pruning. ",
            "number": "5",
            "sents": [
                {
                    "text": "MERT with random walk restarts seems to pro duce better models than either MERT with uniform random restarts or with no restarts, but it is still slower than MERT with no restarts by more thana factor of 2.",
                    "sid": 96,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The difference between 3894 sec onds (1.08 hours) and 8458 seconds (2.35 hours) to optimize feature weights may not seem important,given how long the rest of the process of build 588 ing and training an SMT system takes; however, to truly optimize an SMT system would actually require performing feature weight training many times to find optimum values of hyper-parameters such as maximum phrase size and distortion limit.",
                    "sid": 97,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This kind of optimization is rarely done for every small model change, because of how long feature weight optimization takes; so it seems well worth the effort to speed up the optimization process as much as possible.To try to speed up the feature weight optimization process, we have tried pruning the set of hy potheses that MERT is applied to.",
                    "sid": 98,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The time taken by the random walk and coordinate ascent phasesof MERT with random walk restarts is roughly linear in the number of translation hypotheses exam ined.",
                    "sid": 99,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the experiment described in Section 4.1, after the first 100-best decoding iteration there were 196,319 hypotheses in the n-best lists, andMERT took 347 seconds.",
                    "sid": 100,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After merging all hy potheses from 6 iterations of 100-best decoding there were 800,580 hypotheses, and MERT took 1380 seconds.",
                    "sid": 101,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We conjectured that a large proportion of these hypotheses are both low scoring according to mostsubmodels and low in measured translation quality, so that omitting them would make little difference to the feature weight optimization optimization process.4 We attempt to identify such hypotheses by extracting some additional informa tion from Och?s line search procedure.",
                    "sid": 102,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Och?s line search procedure takes note of every hypothesis that is the highest scoring hypothesisfor a particular sentence for some value of the fea ture weight being optimized by the line search.",
                    "sid": 103,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The hypotheses that are never the highest scoring hypothesis for any combination of feature valuesexplored effectively play no role in the MERT pro cedure.",
                    "sid": 104,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We conjectured that hypotheses that are never selected as potentially highest scoring in a particular round of MERT could be pruned from the hypothesis set without adversely affecting the quality of the feature weights eventually produced.",
                    "sid": 105,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We tested two implementations of this type ofhypothesis pruning.",
                    "sid": 106,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the more conservative im plementation, after each decoding iteration, we note all the hypotheses that are ever ?touched?(i.e., ever the highest scoring) during the coordi nate ascent search either from the initial starting 4Hypotheses that are of low translation quality, but high scoring according to some submodels, need to be retained so that the feature weights are tuned to avoid selecting them.point or from one of the random restarts.",
                    "sid": 107,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Any hy pothesis that is never touched is pruned from the sets of hypotheses that are merged with the results of subsequent n-best decoding iterations.",
                    "sid": 108,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We refer to this as ?post-restart?",
                    "sid": 109,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "pruning.",
                    "sid": 110,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the more aggressive implementation, aftereach decoding iteration, we note all the hypothe ses that are touched during the coordinate ascentsearch from the initial starting point.",
                    "sid": 111,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The hypotheses that are not touched are pruned from the hy pothesis set before any random restarts.",
                    "sid": 112,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We refer to this as ?pre-restart?",
                    "sid": 113,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "pruning.",
                    "sid": 114,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.1 Preliminary evaluation.",
                    "sid": 115,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluated both post- and pre-restart pruningwith random walk restarts, under the same condi tions used to evaluate random walk restarts without pruning.",
                    "sid": 116,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With post-restart pruning, feature weight training converged in 8 decoding iterations taking 7790 seconds, with a BLEU score of 32.14 on the development test set.",
                    "sid": 117,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The last set of restarts ofMERT had 276,134 hypotheses to consider, a reduction of more than 65% compared to no pruning.",
                    "sid": 118,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With pre-restart pruning, feature weight train ing converged in 7 decoding iterations taking 4556seconds, with a BLEU score of 32.17 on the devel opment test set.",
                    "sid": 119,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The last set of restarts of MERThad only 64,346 hypotheses to consider, a reduc tion of 92% compared to no pruning.",
                    "sid": 120,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Neither method of pruning degraded translation quality as measured by BLEU; in fact, BLEU scoresincreased by a trivial amount with pruning.",
                    "sid": 121,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Post restart pruning speeded up training only slightly, primarily because it took more decoding iterationsto converge.",
                    "sid": 122,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Time per decoding iteration was reduced from 1409 seconds to 974 seconds.",
                    "sid": 123,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pre restart pruning was substantially faster overall, as well as in terms of time per decoding iteration, which was 650 seconds.",
                    "sid": 124,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additional insight into the differences between feature weight optimization methods can be gainedby evaluating the feature weight sets produced af ter each decoding iteration.",
                    "sid": 125,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1 plots the BLEU score obtained on the development test setas a function of the cumulative time taken to pro duce the corresponding feature weights, for each of the training runs we have described so far.",
                    "sid": 126,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We observe a clear gap between the results obtained from random walk restarts and those from either uniform random restarts or no restarts.Note in particular that, although the uniform ran 589 Restart Pruning Num Num Decoding Total MERT Dev-test Conf Method Method Starts N-best Iterations Seconds Seconds BLEU Level none none 1 100 8 3894 276 31.12 > 0.999 none none 1 300 8 8973 1173 31.17 > 0.999 none none 1 200 9 7877 717 31.83 0.999 uniform rand none 5 100 7 4294 917 31.95 0.993 uniform rand none 30 100 11 19345 13306 31.98 0.995 uniform rand none 20 100 10 12690 7613 32.02 0.984 uniform rand none 10 100 10 9059 3898 32.02 0.984 random walk pre-restart 30 100 12 9963 3016 32.04 0.999 random walk pre-restart 5 100 11 7696 619 32.10 0.962 random walk none 30 100 7 14055 10887 32.10 0.959 random walk pre-restart 10 100 14 8581 1254 32.10 0.986 random walk post-restart 10 100 9 6985 2236 32.11 0.938 random walk none 20 100 6 8458 5766 32.13 0.909 random walk post-restart 20 100 8 7790 3965 32.14 0.857 random walk none 10 100 8 8338 4280 32.15 0.840 random walk pre-restart 20 100 7 4556 1179 32.17 0.712 random walk post-restart 5 100 10 6103 1114 32.18 0.794 random walk post-restart 30 100 8 9811 6218 32.20 0.554 random walk none 5 100 10 7741 3047 32.21 0.000 Table 1: Extended Evaluation Results.",
                    "sid": 127,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "dom restart method eventually comes within 0.15 BLEU points of the best result using random walkrestarts, it takes far longer to get there.",
                    "sid": 128,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The uni form restart run produces noticably inferior BLEU scores until just before convergence, while with the random walk method, the BLEU score increasesquite quickly and then stays essentially flat for sev eral iterations before convergence.",
                    "sid": 129,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also note that there appears to be less real difference among our three variations on random walk restarts than there might seem to be from their times to convergence.",
                    "sid": 130,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although pre-restartpruning was much faster to convergence than either of the other variants, all three reached approx imately the same BLEU score in the same amount of time, if intermediate points are considered.",
                    "sid": 131,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thissuggests that our convergence test, while more lib eral than Och?s, still may be more conservative than necessary when using random walk restarts.",
                    "sid": 132,
                    "ssid": 37,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "extended evaluation. ",
            "number": "6",
            "sents": [
                {
                    "text": "We now extend the previous evaluations in twoways.",
                    "sid": 133,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we repeat all the experiments on opti mization with 20 starting points, using 5, 10, and 30 starting points, to see whether we can trade off training time for translation quality by changingthat parameter setting, and if so, whether any set tings seem clearly better than others.",
                    "sid": 134,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, we note that different optimization methods lead to convergence at different numbers of decoding iterations.",
                    "sid": 135,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This means that which method produces the shortest total training time will depend on the relative time taken by n-bestdecoding and the MERT procedure itself (includ ing the random walk selection procedure, if that is used).",
                    "sid": 136,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By co-incidence, these times happened to be roughly comparable in our experiments.5 In a situation where decoding is much slower than MERT, however, the main determinant of overalltraining time would be how many decoding iterations are needed.",
                    "sid": 137,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the other hand, if decoding was made much faster, say, through algorith mic improvements or by using a compute cluster, total training time would be dominated by MERTproper.",
                    "sid": 138,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We therefore report number of decoding iterations to convergence and pure MERT time (ex cluding decoding and hypothesis set merging) for each of our experiments, in addition to total feature weight training time.Table 1 reports these three measures of com5In our complete set of training experiments encompass ing 187 decoding iterations, decoding averaged 521 seconds per iteration, and MERT (excluding decoding and hypothesis set merging) averaged 421 seconds per (decoding) iteration.",
                    "sid": 139,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "590puational effort, plus BLEU score on the devel opment test set, sorted by ascending BLEU score, for 19 variations on MERT: 3 n-best list sizes for MERT without restarts, and 4 different numbers of restarts for 4 different versions of MERT with restarts (uniform random selection, random walk selection without pruning, random walk selectionwith post-restart pruning, and random walk selec tion with pre-restart pruning).",
                    "sid": 140,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The final column of Table 1 is a confidence score reflecting the estimated probability that the translation model produced (at convergence) by the MERT variant for that row of the table is not as good in terms of BLEU score as the variant that yielded the highest BLEU score (at convergence) that we observed in these experiments.",
                    "sid": 141,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These probabilities were estimated by Koehn?s (2004) paired bootstrap resampling method, run for at least 100,000 samples per comparison.",
                    "sid": 142,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The 11 models obtaining a BLEU score of 31.10 or less are all estimated to be at least 95% likely tohave worse translation quality than the best scor ing model.",
                    "sid": 143,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We therefore dismiss these models from further consideration,6 including all modelstrained without random restarts, as well as all mod els trained with uniform random restarts, leaving only models trained with random walk restarts.With random walk restarts, post-restart prun ing remains under consideration at all numbersof restarts tried.",
                    "sid": 144,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For random walk restarts without pruning, only the model produced by 30 start ing points has been eliminated, and for pre-restartpruning, only the model produced by 20 starts remains under consideration.",
                    "sid": 145,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This suggests that pre restart pruning may be too aggressive and, thus, overly sensitive to the number of restarts.To get a better picture of the remaining 8 mod els, see Figures 2?4.",
                    "sid": 146,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Despite convergence times ranging from 4456 to 9811 seconds, in Figure 2 we observe that, if feature weights after each decoding iteration are considered, the relationships betweentraining time and BLEU score are remarkably sim ilar.",
                    "sid": 147,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Figure 3, BLEU score varies considerably up to 3 decoding iterations, but above that, BLEU scores are very close, and almost flat.",
                    "sid": 148,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In fact, we see almost no benefit from more than 7 decoding iterations for any model.Finally, Figure 4 shows some noticeable differ ences between random walk variants in respect to 6We estimate the joint probability that these 11 models areall worse than our best scoring model to be 0.882, by multi plying the confidence scores for all these models.MERT time proper.",
                    "sid": 149,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, while the choice of random walk variant chosen may matter little if decoding is slow, it seems that it can have an important impact if decoding is fast.",
                    "sid": 150,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If we com bine the results shown here with the observation from Figure 3 that there seems to be no benefit to trying more than 7 decoding iterations, it appears that perhaps the best trade-off between translationquality and training time would be obtained by us ing post-restart pruning, with 5 starting points perdecoding iteration, cutting off training after 7 iter ations.",
                    "sid": 151,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This took a total of 4009 seconds to train, compared to 7741 seconds for the highest scoring model on the development test set considered in Table 1 (produced by random walk restarts with no pruning, 5 starting points per decoding iteration, at convergence after 10 iterations).",
                    "sid": 152,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To validate the proposal to use the suggested faster training procedure, we compared the two models under discussion on the 2000 in-domain sentence pairs for the designated final test set forour corpus.",
                    "sid": 153,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The model produced by the sug gested training procedure resulted in a BLEU score of 31.92, with the model that scored highest on the development test set scoring an insignificantly worse 31.89.",
                    "sid": 154,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contast, the highest scoring model of the three trained with no restarts produced a BLEU score of 31.55 on the final test set, which was worse than either of the random walk methods evaluated on the final test set, at confidence levels exceeding 0.996 according to the paired bootstrap resampling test.",
                    "sid": 155,
                    "ssid": 23,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusions. ",
            "number": "7",
            "sents": [
                {
                    "text": "We believe that our results show very convincingly that using random restarts in MERT improves the BLEU scores produced by the result ing models.",
                    "sid": 156,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They also seem to show that starting point selection by random walk is slightly superiorto uniform random selection.",
                    "sid": 157,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, our exper iments suggest that time to carry out MERT canbe significantly reduced by using as few as 5 starting points per decoding iteration, performing post restart pruning of hypothesis sets, and cutting off training after a fixed number of decoding iterations (perhaps 7) rather than waiting for convergence.",
                    "sid": 158,
                    "ssid": 3,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}