{
    "ID": "D12-1126",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Part-of-Speech Tagging for Chinese-English Mixed Texts with Dynamic Features",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In modern Chinese articles or conversations, it is very popular to involve a few English words, especially in emails and Internet literature.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, it becomes an important and challenging topic to analyze Chinese-English mixed texts.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The underlying problem is how to tag part-of-speech (POS) for the English words involved.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Due to the lack of specially annotated corpus, most of the English words are tagged as the oversimplified type, \u201cforeign words\u201d.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we present a method using dynamic features to tag POS of mixed texts.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Experiments show that our method achieves higher performance than traditional sequence labeling methods.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Meanwhile, our method also boosts the performance of POS tagging for pure Chinese texts.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Nowadays, Chinese-English mixed texts are prevalent in modern articles or emails.",
                    "sid": 8,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More and more English words are used in Chinese texts as names of organizations, products, terms and abbreviations, such as \u201ceBay\u201d, \u201ciPhone\u201d, \u201cGDP\u201d, \u201cAndroid\u201d etc.",
                    "sid": 9,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the other hand, it is also a common phenomenon to use Chinese-English mixed texts in daily conversation, especially in communication among employers in large international corporations.",
                    "sid": 10,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are some challenges for analyzing ChineseEnglish mixed texts: creases the difficulties to tag them.",
                    "sid": 11,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Currently, the mainstream method of Chinese POS tagging is joint segmentation & tagging with cross-labels, which can avoid the problem of error propagation and achieve higher performance on both subtasks(Ng and Low, 2004).",
                    "sid": 12,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each label is the crossproduct of a segmentation label and a tagging label, e.g.",
                    "sid": 13,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "{B-NN, I-NN, E-NN, S-NN, ...}.",
                    "sid": 14,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The features are generated by position-based templates on character-level.",
                    "sid": 15,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the main part of mixed texts is in Chinese and the role of English word is more like Chinese, we use Chinese POS tags (Xia, 2000) to tag English words.",
                    "sid": 16,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the categories of the most commonly used English words are nouns, verbs and adjectives, we can use \u201cNN\u201d, \u201cNR\u201d, \u201cVV\u201d, \u201cVA\u201d, \u201cJJ\u201d to label their POS tags.",
                    "sid": 17,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the English proper nouns and verbs, there are no significant differences in Chinese and English POS tags except that English features plural and tense forms.",
                    "sid": 18,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the English nouns, these are some English nouns used as verbs, such as \u201c\u6211\u5f88 [fan/VV] \u4ed6\u3002(I adore him very much.",
                    "sid": 19,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": ")\u201d where \u201cfan\u201d means \u201cadore\u201d and is used as a verb.",
                    "sid": 20,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the English adjectives, there are two corresponding Chinese POS tags \u201cVA\u201d and \u201cJJ\u201d.",
                    "sid": 21,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the roles of some English words in Table 1, such as \u201cprofessional\u201d and \u201chigh\u201d, are different with their original ones.",
                    "sid": 22,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, the POS tagging for mixed texts cannot be settled with simple methods, such as looking up in a dictionary.",
                    "sid": 23,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One of the main differences between Chinese and English in POS tagging is that the two languages have character-based features and word-based features respectively.",
                    "sid": 24,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To ensure the consistency of tagging models, we prefer to use word-level information in Chinese, which is both useful for ChineseEnglish mixed texts and Chinese-only texts.",
                    "sid": 25,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, in a sentence \u201cX Jr, Z;t Y ... (X or Y ...)\u201d, the word Y ought to have the same POS tag as the word X.",
                    "sid": 26,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Another example is that the word following a pronoun is usually a verb, and adjectives often describe nouns.",
                    "sid": 27,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some related works show that word-level features can improve the performance of Chinese POS tagging (Jiang et al., 2008; Sun, 2011).",
                    "sid": 28,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we propose a method to tag mixed texts with dynamic features.",
                    "sid": 29,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our method combines these dynamic features, which are dynamically generated at the decoding stage, with traditional static features.",
                    "sid": 30,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For Chinese-English mixed texts, the traditional features cannot yield a satisfied result due to lack of training data.",
                    "sid": 31,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The proposed dynamic features can improve the performance by using the information of a word, such as POS tag or length of the whole word, which is proven effective by experiments.",
                    "sid": 32,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The rest of the paper is organized as follows: In section 2, we introduce the sequence labeling models, then we describe our method of dynamic features in section 3 and analyze its complexity in section 4.",
                    "sid": 33,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Section 5 describes the training method.",
                    "sid": 34,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The experimental results are manifested in section 6.",
                    "sid": 35,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, We review the relevant research works in section 7 and conclude our work in section 8.",
                    "sid": 36,
                    "ssid": 29,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "2 sequence labeling models",
            "number": "2",
            "sents": [
                {
                    "text": "Sequence labeling is the task of assigning labels y = y1, ... , yn to an input sequence x = x1, ... , xn.",
                    "sid": 37,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a sample x, we define the feature 4b(x, y).",
                    "sid": 38,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, we can label x with a score function, where w is the parameter of function F(\u00b7).",
                    "sid": 39,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For sequence labeling, the feature can be denoted as \u03d5k(yZ, yZ\u22121, x, i), where i stands for the position in the sequence and k stands for the number of feature templates. we use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters.",
                    "sid": 40,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Following (Collins, 2002), the average strategy is used to avoid the overfitting problem.",
                    "sid": 41,
                    "ssid": 5,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 dynamic features",
            "number": "3",
            "sents": [
                {
                    "text": "The form of traditional features is shown in Table 2, where C represents a Chinese character, and T represents the character-based tag.",
                    "sid": 42,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The subscript i indicates its position related to the current character.",
                    "sid": 43,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Traditional features are generated by positionfixed templates.",
                    "sid": 44,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the length of Chinese word is unfixed, their meanings are incomplete.",
                    "sid": 45,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We categorize them as \u201cstatic\u201d features since they can be calculated before tagging (except \u201cT\u22121, T0\u201d).",
                    "sid": 46,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The form of dynamic features is shown in Table 3, where WORD represents a Chinese word, and POS (LEN) is the POS tag (length) of the word.",
                    "sid": 47,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The subscript of dynamic feature template indicates its position related to the current word.",
                    "sid": 48,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 4 shows an example.",
                    "sid": 49,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the current position is \u201c Apple\u201d, then {POS\u22121=CC, POS\u22122=NR, WORD\u22121=\u201c*p\u201d, LEN\u22122=2}.",
                    "sid": 50,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since these features are unavailable before tagging, we call them \u201cdynamic\u201d features.",
                    "sid": 51,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "POSi, POSj, T0(i, j = \u22122, \u22121, 0 and i =\ufffd j) POSi, WORDj, T0(i, j = \u22122, \u22121,0) WORDi, LENj, POSk, T0(i, j, k = \u22122, \u22121, 0) ...",
                    "sid": 52,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Dynamic features are more flexible because the number of involved characters is dependent on the length of previous words.",
                    "sid": 53,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unlike static features, dynamic features do not merely rely on the input sequence C1:n, so the weights of dynamic features, in which POS/LEN are involved, can be trained by Chinese-only texts and used by mixed texts, which resolve the problem of the lack of training data.",
                    "sid": 54,
                    "ssid": 13,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 tagging with dynamic features",
            "number": "4",
            "sents": [
                {
                    "text": "In the tagging stage, we use the current best result to approximately calculate the unknown tag information.",
                    "sid": 55,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For an input sequence C1:n, the current best tags from index 0 to i\u22121 can be calculated by Viterbi algorithm and they can be used to generate dynamic features for index i.",
                    "sid": 56,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The specific algorithm is shown in Algorithm 1.",
                    "sid": 57,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here is an example to explain the time complexity of the dynamic features.",
                    "sid": 58,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Normal template xi_2xi_1yi requires to look for the positions of i \u2212 2 and i \u2212 1 related to the current character xi, but dynamic template posi_2posi_1yi needs to know the pos tags of two words.",
                    "sid": 59,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the length of wordi_1/wordi_2 is 2, then the positions of i\u22124, i\u2212 3, i\u22122, i\u22121 are needed to generate the dynamic features.",
                    "sid": 60,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all dynamic features, it is unnecessary to repetitively calculate the POS/WORD/LEN array.",
                    "sid": 61,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Apart from that one time calculation of the array, no distinction can be found between the time complexity of the dynamic features and the traditional features.",
                    "sid": 62,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For input C1:n, the time complexity is O(n*[O(op.2)+(Ts.num+Td.num)*O(op.1)+ O(op.4)]), n.b.",
                    "sid": 63,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "O(op.1) = O(op.3).",
                    "sid": 64,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Universally the dynamic features only require the information of position i \u2212 2 and i \u2212 1, so the time complexity of calculating the POS/WORD/LEN array can be ignored as compared with the complexity of Viterbi algorithm and feature extraction.",
                    "sid": 65,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The approximate algorithm is thus faster than the Brute-Force way by input : character sequence C1:n static templates Ts dynamic templates Td number of labels m trans matrix M output: results Max & Vp Initialize: weight matrix W (n x m) viterbi score matrix Vs (n x m) viterbi path matrix Vp (n x m) the index of current best label Max using word-level information.",
                    "sid": 66,
                    "ssid": 12,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5 training",
            "number": "5",
            "sents": [
                {
                    "text": "Given an example (x, y), y\ufffd are denoted as the incorrect labels with the highest score Thus, we calculate the hinge loss \u2113(w; (x, y), (abbreviated as \u2113w) by where \u03be is a non-negative slack variable, and C is a positive parameter which controls the influence of the slack term on the objective function.",
                    "sid": 67,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Following the derivation in PA (Crammer et al., 2006), we can get the update rule, Our algorithm based on PA algorithm is shown in Algorithm 2.",
                    "sid": 68,
                    "ssid": 2,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "6 experiments",
            "number": "6",
            "sents": [
                {
                    "text": "We implement our system based on FudanNLP1.",
                    "sid": 69,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We employ the commonly used label set {B, I, E, S} for the segmentation part of cross-labels.",
                    "sid": 70,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "{B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation.",
                    "sid": 71,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The F1 score is used for evaluation, which is the harmonic mean of precision P (percentage of predict phrases that exactly match the reference phrases) and recall R (percentage of reference phrases that returned by system).",
                    "sid": 72,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The feature templates, which are used to extract features, are listed in Table 5.",
                    "sid": 73,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We set traditional method (static features) as the baseline.",
                    "sid": 74,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The detailed experimental settings and results are reported in the following subsections.",
                    "sid": 75,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Before the experiments on Chinese-English mixed texts, we evaluate the performance of our method on Chinese-only texts.",
                    "sid": 76,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the CTB dataset from the POS tagging task of the Fourth International Chinese Language Processing Bakeoff (SIGHAN Bakeoff 2008)(Jin and Chen, 2008).",
                    "sid": 77,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The details are shown in Table 6.",
                    "sid": 78,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The performance comparison on joint segmentation & POS tagging is shown in Table 7.",
                    "sid": 79,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our method obtains an error reduction of 6.7% over the baseline.",
                    "sid": 80,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The reason is that our dynamic features can utilize where word-level information effectively and the feature templates are more flexible. words but not the words themselves and overcome the problem of out-of-vocabulary (OOV) English words.",
                    "sid": 81,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our experiments, we just select 5% of the Chinese nouns and verbs from SIGHAN dataset, and replace them in the above two ways.",
                    "sid": 82,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After replacement, the training and test data have 12780 and 1254 English words, respectively.",
                    "sid": 83,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5189 words are generated by way of \u201cRespective Replacement\u201d.",
                    "sid": 84,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the test data, 326 words are OOV, which comprises 25% of the whole vocabulary.",
                    "sid": 85,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The information of generated data is shown in Table 8.",
                    "sid": 86,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Without annotated corpus for Chinese-English mixed texts, we use synthetic data as the alternative.",
                    "sid": 87,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Chinese-English mixed texts, English words of noun(NN/NR), verb(VV/VA) and adjective(JJ) categories are the most commonly used, so we randomly transform a certain percentage of Chinese words with these POS tags in the SIGHAN Bakeoff 2008 dataset(Jin and Chen, 2008) into their English counterparts.",
                    "sid": 88,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Before trying out an experiment, we first study how to generate the data of mixed texts.",
                    "sid": 89,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use two ways to produce the synthetic data: \u201cRespective Replacement\u201d and \u201cUnified Replacement\u201d.",
                    "sid": 90,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Respective Replacement We replace the selected Chinese words into their corresponding English counterparts.",
                    "sid": 91,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unified Replacement We replace the selected Chinese words with a unified label ENG.",
                    "sid": 92,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The reason we use the label ENG instead of real words is that we want to consider the context of these We use H1 to represent the dataset generated by way of \u201cRespective Replacement\u201d, and H2 for the dataset by way of with \u201cUnified Replacement\u201d.",
                    "sid": 93,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The experimental results on these two datasets are shown in Table 9.",
                    "sid": 94,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From Table 9, we can see that the \u201cUnified Replacement\u201d way is better than the \u201cRespective Replacement\u201d way for both the baseline and our method.",
                    "sid": 95,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The main reason is that the \u201cUnified Replacement\u201d way can greatly improve the tagging performance of OOV words.",
                    "sid": 96,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For detail comparisons of all situations of mixed texts, we design six synthetic datasets, A/B/C/D1/D2/E by randomly selecting 10% or 15% of Chinese words (\u201cNN/NR/VV/VA/JJ\u201d) in the above SIGHAN Bakeoff 2008 dataset, and replacing them with English label ENG.",
                    "sid": 97,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The differences of these datasets are as following: The detailed information of datasets A/B/C/D1/D2/E is shown in Table 10.",
                    "sid": 98,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results are shown in Table 11.",
                    "sid": 99,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On dataset E, our method achieves 6.78% higher performance on tagging ENG labels than traditional static features.",
                    "sid": 100,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This result is reasonable because our model can use more flexible feature templates to extract features and reduce the problem of being dependent on specific English words.",
                    "sid": 101,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Tables 12/13/14/15/16/17 show the detailed results on datasets A/B/C/D1/D2/E.",
                    "sid": 102,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Experiment on dataset A gets the best result because \u201cNN\u201d and \u201cVV\u201d can be easily distinguished by its context.",
                    "sid": 103,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sometimes, \u201cVA\u201d has the similar context with \u201cVV\u201d, experiment on dataset B shows its influence.",
                    "sid": 104,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The performances on datasets B/C/E descend in turn.",
                    "sid": 105,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The reason is that words with tag \u201cNN\u201d or \u201cNR/JJ\u201d have the similar usage/contexts in Chinese.",
                    "sid": 106,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since we use the same form ENG instead of real words, there are no differences between these words, which leads to some errors.",
                    "sid": 107,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Though the datasets is generated randomly, we can see our method perform better on every dataset than the baseline.",
                    "sid": 108,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To investigate the actual performance, we collect a real dataset from Web, which consists of 142 representative Chinese-English mixed sentences.",
                    "sid": 109,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This dataset contains 4, 238 Chinese characters and 275 English words.",
                    "sid": 110,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since we focus on the performance for English words, we only label the POS tags of the English words.",
                    "sid": 111,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 18 shows some examples in the real dataset of mixed texts.",
                    "sid": 112,
                    "ssid": 44,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "31-:rnmum !",
            "number": "7",
            "sents": [
                {
                    "text": "... very [COOL/VA]!",
                    "sid": 113,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The information of the real dataset is shown in Table 19.",
                    "sid": 114,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If all involved English words are tagging as \u201cNN\u201d, the precision is just 56%.",
                    "sid": 115,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since there is no noun-modifier \u201cJJ\u201d in our collected data.",
                    "sid": 116,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the models trained on dataset B and C to tag the real data.",
                    "sid": 117,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results are shown in Table 20.",
                    "sid": 118,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The difference between model B and C is that model B regards all words with tag \u201cNR\u201d as \u201cNN\u201d.",
                    "sid": 119,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since it is difficult to distinguish between \u201cNR\u201d and \u201cNN\u201d merely according to the context, model B performs better than model C. The detail results of model B and C are shown in Table 21 and 22.",
                    "sid": 120,
                    "ssid": 8,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "7 related works",
            "number": "8",
            "sents": [
                {
                    "text": "In recent years, POS tagging has undergone great development.",
                    "sid": 121,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The mainstream method is to regard POS tagging as sequence labeling problems (Rabiner, 1990; Xue, 2003; Peng et al., 2004; Ng and Low, 2004).",
                    "sid": 122,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the analysis of Chinese-English mixed texts is rarely involved in previous literature.",
                    "sid": 123,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the aspect of the general multilingual POS tagging, most works focus on modeling cross-lingual correlations and tagging multilingual POS on respective monolingual texts, not on mixed texts (Cucerzan and Yarowsky, 2002; Yarowsky et al., 2001; Naseem et al., 2009).",
                    "sid": 124,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since we choose to use dynamic word-level features to improve the performance of POS tagging, we also review some works on word-level features.",
                    "sid": 125,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Semi-Markov Conditional Random Fields (semiCRF) (Sarawagi and Cohen, 2004) is a model in which segmentation task is implicitly included into the decoding algorithm.",
                    "sid": 126,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this model, feature representation would be more flexible than traditional CRFs, since features can be extracted from the previous/the next segmentation within a window of variable size.",
                    "sid": 127,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The problem of this approach lies in that the decoding algorithm depends on the predefined window size to exploit the boundaries of segmentations but not the real length of words.",
                    "sid": 128,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Bunescu (2008) presents an improved pipeline model in which the output of the previous subtasks are considered as hidden variables, and the hidden variables together with their probabilities denoting the confidence are used as probabilistic features in the next subtasks.",
                    "sid": 129,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One shortcoming of this method is inefficiency caused by the calculation of marginal probabilities of features.",
                    "sid": 130,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The other disadvantages of the pipeline method are error propagation and the need of separate training of different subtasks in the pipeline.",
                    "sid": 131,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Another disadvantage of pipeline method is error propagation.",
                    "sid": 132,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Jiang et al. (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging.",
                    "sid": 133,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With a character-based perceptron as the core, combined with real-valued features such as language models, the cascaded model can efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.",
                    "sid": 134,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, they use POS tags or word information in a BruteForce way, which may suffer from the problem of time complexity.",
                    "sid": 135,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sun (2011) presents a stacked sub-word model for joint Chinese word segmentation and POS tagging.",
                    "sid": 136,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By merging the outputs of the three predictors (including one word-based segmenter) into sub-word sequences, rich contextual features can be approximately derived.",
                    "sid": 137,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The experiments are conducted to show the effectiveness of using word-based information.",
                    "sid": 138,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The difference between the above methods and ours is that our word-level features are dynamically generated in the decoding stage without exhaustive or preprocessed word segmentation.",
                    "sid": 139,
                    "ssid": 19,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "8 conclusion",
            "number": "9",
            "sents": [
                {
                    "text": "In this paper, we focus on Chinese-English mixed texts and use dynamic features for POS tagging.",
                    "sid": 140,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To overcome the problem of the lack of annotated corpus on mixed texts, our features use both local and non-local information and take advantage of the characteristics of Chinese-English mixed texts.",
                    "sid": 141,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The experiments demonstrate the effectiveness of our method.",
                    "sid": 142,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It should be noted that our method is also effective for the mixed texts of Chinese and any foreign languages since we use \u201cUnified Replacement\u201d.",
                    "sid": 143,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For future works, we plan to improve our approximate tagging algorithm to reduce error propagation.",
                    "sid": 144,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, we will refer to an English dictionary to generate some useful features to distinguish between \u201cNR\u201d and \u201cNN\u201d in Chinese-English mixed texts and add some statistical features derived from English resources, such as the most common tag of each English word.",
                    "sid": 145,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We would also like to investigate these features in more applications of natural language processing, such as name entity recognition, information extraction, etc.",
                    "sid": 146,
                    "ssid": 7,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgements",
            "number": "10",
            "sents": [
                {
                    "text": "We would like to thank the anonymous reviewers for their valuable comments.",
                    "sid": 147,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also thanks Amy Zhou for her help in spell and grammar checking.",
                    "sid": 148,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This work was funded by NSFC (No.61003091 and No.61073069), 863 Program (No.2011AA010604) and 973 Program (No.2010CB327900).",
                    "sid": 149,
                    "ssid": 3,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}