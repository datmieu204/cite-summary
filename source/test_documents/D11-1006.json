{
    "ID": "D11-1006",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Multi-Source Transfer of Delexicalized Dependency Parsers",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993).",
                    "sid": 6,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008).",
                    "sid": 7,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text.",
                    "sid": 8,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Subsequently, researchers have begun to look at both porting these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007).",
                    "sid": 9,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English.",
                    "sid": 10,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010).",
                    "sid": 11,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervised systems.",
                    "sid": 12,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, they are often trained and evaluated under idealized conditions, e.g., only on short sentences or assuming the existence of gold-standard part-ofspeech (POS) tags.1 The reason for these assumptions is clear.",
                    "sid": 13,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unsupervised grammar induction is difficult given the complexity of the analysis space.",
                    "sid": 14,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These assumptions help to give the model traction.",
                    "sid": 15,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The study of unsupervised grammar induction has many merits.",
                    "sid": 16,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most notably, it increases our understanding of how computers (and possibly humans) learn in the absence of any explicit feedback.",
                    "sid": 17,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the gold POS tag assumption weakens any conclusions that can be drawn, as part-of-speech are also a form of syntactic analysis, only shallower.",
                    "sid": 18,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, from a practical standpoint, it is rarely the case that we are completely devoid of resources for most languages.",
                    "sid": 19,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This point has been made by studies that transfer parsers to new languages by projecting syntax across word alignments extracted from parallel corpora (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009).",
                    "sid": 20,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although again, most of these studies also assume the existence of POS tags.",
                    "sid": 21,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this work we present a method for creating dependency parsers for languages for which no labeled training data is available.",
                    "sid": 22,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we train a source side English parser that, crucially, is delexicalized so that its predictions rely soley on the part-of-speech tags of the input sentence, in the same vein as Zeman and Resnik (2008).",
                    "sid": 23,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We empirically show that directly transferring delexicalized models (i.e. parsing a foreign language POS sequence with an English parser) already outperforms state-of-the-art unsupervised parsers by a significant margin.",
                    "sid": 24,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This result holds in the presence of both gold POS tags as well as automatic tags projected from English.",
                    "sid": 25,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This emphasizes that even for languages with no syntactic resources \u2013 or possibly even parallel data \u2013 simple transfer methods can already be more powerful than grammar induction systems.",
                    "sid": 26,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Next, we use this delexicalized English parser to seed a perceptron learner for the target language.",
                    "sid": 27,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The model is trained to update towards parses that are in high agreement with a source side English parse based on constraints drawn from alignments in the parallel data.",
                    "sid": 28,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the augmented-loss learning procedure (Hall et al., 2011) which is closely related to constraint driven learning (Chang et al., 2007; Chang et al., 2010).",
                    "sid": 29,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The resulting parser consistently improves on the directly transferred delexicalized parser, reducing relative errors by 8% on average, and as much as 18% on some languages.",
                    "sid": 30,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we show that by transferring parsers from multiple source languages we can further reduce errors by 16% over the directly transferred English baseline.",
                    "sid": 31,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is consistent with previous work on multilingual part-of-speech (Snyder et al., 2009) and grammar (Berg-Kirkpatrick and Klein, 2010; Cohen and Smith, 2009) induction, that shows that adding languages leads to improvements.",
                    "sid": 32,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We present a comprehensive set of experiments on eight Indo-European languages for which a significant amount of parallel data exists.",
                    "sid": 33,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We make no language specific enhancements in our experiments.",
                    "sid": 34,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We report results for sentences of all lengths, as well as with gold and automatically induced part-of-speech tags.",
                    "sid": 35,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also report results on sentences of length 10 or less with gold part-of-speech tags to compare with previous work.",
                    "sid": 36,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our results consistently outperform the previous state-of-the-art across all languages and training configurations.",
                    "sid": 37,
                    "ssid": 32,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "2 preliminaries",
            "number": "2",
            "sents": [
                {
                    "text": "In this paper we focus on transferring dependency parsers between languages.",
                    "sid": 38,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A dependency parser takes a tokenized input sentence (optionally part-ofspeech tagged) and produces a connected tree where directed arcs represent a syntactic head-modifier relationship.",
                    "sid": 39,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An example of such a tree is given in Figure 1.",
                    "sid": 40,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Dependency tree arcs are often labeled with the role of the syntactic relationship, e.g., is to hearing might be labeled as SUBJECT.",
                    "sid": 41,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, we focus on unlabeled parsing in order to reduce problems that arise due to different treebank annotation schemes.",
                    "sid": 42,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Of course, even for unlabeled dependencies, significant variations in the annotation schemes remain.",
                    "sid": 43,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, in the Danish treebank determiners govern adjectives and nouns in noun phrases, while in most other treebanks the noun is the head of the noun phrase.",
                    "sid": 44,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unlike previous work (Zeman and Resnik, 2008; Smith and Eisner, 2009), we do not apply any transformations to the treebanks, which makes our results easier to reproduce, but systematically underestimates accuracy.",
                    "sid": 45,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The treebank data in our experiments are from the CoNLL shared-tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007).",
                    "sid": 46,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use English (en) only as a source language throughout the paper.",
                    "sid": 47,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additionally, we use the following eight languages as both source and target languages: Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es) and Swedish (sv).",
                    "sid": 48,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For languages that were included in both the 2006 and 2007 tasks, we used the treebank from the latter.",
                    "sid": 49,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We focused on this subset of languages because they are Indo-European and a significant amount of parallel data exists for each language.",
                    "sid": 50,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By presenting results on eight languages our study is already more comprehensive than most previous work in this area.",
                    "sid": 51,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the restriction to Indo-European languages does make the results less conclusive when one wishes to transfer a parser from English to Chinese, for example.",
                    "sid": 52,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To account for this, we report additional results in the discussion for non-IndoEuropean languages.",
                    "sid": 53,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all data sets we used the predefined training and testing splits.",
                    "sid": 54,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our approach relies on a consistent set of partof-speech tags across languages and treebanks.",
                    "sid": 55,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this we used the universal tagset from Petrov et al. (2011), which includes: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions orpostpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all tag).",
                    "sid": 56,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similar tagsets are used by other studies on grammar induction and projection (Naseem et al., 2010; Zeman and Resnik, 2008).",
                    "sid": 57,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all our experiments we replaced the language specific part-of-speech tags in the treebanks with these universal tags.",
                    "sid": 58,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Like all treebank projection studies we require a corpus of parallel text for each pair of languages we study.",
                    "sid": 59,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this we used the Europarl corpus version 5 (Koehn, 2005).",
                    "sid": 60,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The corpus was preprocessed in standard ways and word aligned by running six iterations of IBM Model 1 (Brown et al., 1993), followed by six iterations of the HMM model (Vogel et al., 1996) in both directions.",
                    "sid": 61,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then intersect word alignments to generate one-to-one alignments.",
                    "sid": 62,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All of our parsing models are based on the transition-based dependency parsing paradigm (Nivre, 2008).",
                    "sid": 63,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Specifically, all models use an arc-eager transition strategy and are trained using the averaged perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8.",
                    "sid": 64,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available).",
                    "sid": 65,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All feature conjunctions are included.",
                    "sid": 66,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For treebanks with non-projective trees we use the pseudo-projective parsing technique to transform the treebank into projective structures (Nivre and Nilsson, 2005).",
                    "sid": 67,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We focus on using this parsing system for two reasons.",
                    "sid": 68,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, the parser is near state-of-the-art on English parsing benchmarks and second, and more importantly, the parser is extremely fast to train and run, making it easy to run a large number of experiments.",
                    "sid": 69,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Preliminary experiments using a different dependency parser \u2013 MSTParser (McDonald et al., 2005) \u2013 resulted in similar empirical observations.",
                    "sid": 70,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All systems are evaluated using unlabeled attachment score (UAS), which is the percentage of words (ignoring punctuation tokens) in a corpus that modify the correct head (Buchholz and Marsi, 2006).",
                    "sid": 71,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-ofspeech tags from the projected part-of-speech tagger of Das and Petrov (2011).2 This tagger relies only on labeled training data for English, and achieves accuracies around 85% on the languages that we consider.",
                    "sid": 72,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluate in the former setting to compare to previous studies that make this assumption.",
                    "sid": 73,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluate in the latter setting to measure performance in a more realistic scenario \u2013 when no target language resources are available.",
                    "sid": 74,
                    "ssid": 37,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 transferring from english",
            "number": "3",
            "sents": [
                {
                    "text": "To simplify discussion, we first focus on the most common instantiation of parser transfer in the literature: transferring from English to other languages.",
                    "sid": 75,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the next section we expand our system to allow for the inclusion of multiple source languages.",
                    "sid": 76,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We start with the observation that discriminatively trained dependency parsers rely heavily on part-ofspeech tagging features.",
                    "sid": 77,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, when training and testing a parser on our English data, a parser with all features obtains an UAS of 89.3%3 whereas a delexicalized parser \u2013 a parser that only has nonlexical features \u2013 obtains an UAS of 82.5%.",
                    "sid": 78,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The key observation is that part-of-speech tags contain a significant amount of information for unlabeled dependency parsing.",
                    "sid": 79,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This observation combined with our universal part-of-speech tagset, leads to the idea of direct transfer, i.e., directly parsing the target language with the source language parser without relying on parallel corpora.",
                    "sid": 80,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This idea has been previously explored by Zeman and Resnik (2008) and recently by S\u00f8gaard (2011).",
                    "sid": 81,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Because we use a mapping of the treebank specific part-of-speech tags to a common tagset, the performance of a such a system is easy to measure \u2013 simply parse the target language data set with a delexicalized parser trained on the source language data.",
                    "sid": 82,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We conducted two experiments.",
                    "sid": 83,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language.",
                    "sid": 84,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "UAS for all sentence lengths without punctuation are given in Table 1.",
                    "sid": 85,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We report results for both the English direct transfer parser (en-dir.)",
                    "sid": 86,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "as well as a baseline unsupervised grammar induction system \u2013 the dependency model with valence (DMV) of Klein and Manning (2004), as obtained by the implementation of Ganchev et al. (2010).",
                    "sid": 87,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We trained on sentences of length 10 or less and evaluated on all sentences from the test set.4 For DMV, we reversed the direction of all dependencies if this led to higher performance.",
                    "sid": 88,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From this table we can see that direct transfer is a very strong baseline and is over 20% absolute better than the DMV model for both gold and predicted POS tags.",
                    "sid": 89,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 4, which we will discuss in more detail later, further shows that the direct transfer parser also significantly outperforms stateof-the-art unsupervised grammar induction models, but in a more limited setting of sentences of length less than 10.",
                    "sid": 90,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Direct transfer works for a couple of reasons.",
                    "sid": 91,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, part-of-speech tags contain a significant amount of information for parsing unlabeled dependencies.",
                    "sid": 92,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, this information can be transferred, to some degree, across languages and treebank standards.",
                    "sid": 93,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is because, at least for Indo-European languages, there is some regularity in how syntax is expressed, e.g., primarily SVO, prepositional, etc.",
                    "sid": 94,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Even though there are some differences with respect to relative location of certain word classes, strong head-modifier POS tag preferences can still help resolve these, especially when no other viable alternatives are available.",
                    "sid": 95,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consider for example an artificial sentence with a tag sequence: \u2018VERB NOUN ADJ DET PUNC\u2019.",
                    "sid": 96,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The English parser still predicts that the NOUN and PUNC modify the VERB and the ADJ and DET modify the NOUN, even though in the English data such noun phrases are unlikely.5 Unlike most language transfer systems for parsers, the direct transfer approach does not rely on projecting syntax across aligned parallel corpora (modulo the fact that non-gold tags come from a system that uses parallel corpora).",
                    "sid": 97,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section we describe a simple mechanism for projecting from the direct transfer system using large amounts of parallel data in a similar vein to Hwa et al. (2005), Ganchev et al.",
                    "sid": 98,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2009), Smith and Eisner (2009) inter alia.",
                    "sid": 99,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The algorithm is based on the work of Hall et al. (2011) for training extrinsic parser objective functions and borrows heavily from ideas in learning with weak supervision including work on learning with constraints (Chang et al., 2007) and posterior regularization (Ganchev et al., 2010).",
                    "sid": 100,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al. (2010).",
                    "sid": 101,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The algorithm is given in Figure 2.",
                    "sid": 102,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It starts by labeling a set of target language sentences with a parser, which in our case is the direct transfer parser from the previous section (line 1).",
                    "sid": 103,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Next, it uses these parsed target sentences to \u2018seed\u2019 a new parser by training a parameter vector using the predicted parses as a gold standard via standard perceptron updates for J rounds (lines 3-6).",
                    "sid": 104,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This generates a parser that emulates the direct transfer parser, but DP: dependency parser, i.e., DP : x -+ y that are considered \u2018good\u2019 by some external metric.",
                    "sid": 105,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The algorithm then updates towards that output.",
                    "sid": 106,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this case \u2018goodness\u2019 is determined through the pre-specified sentence alignment and how well the target language parse aligns with the English parse.",
                    "sid": 107,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a result, the model will, ideally, converge to a state where it predicts target parses that align as closely as possible with the corresponding English parses.",
                    "sid": 108,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, since we seed the learner with the direct transfer parser, we bias the parameters to select parses that both align well and also have high scores under the direct transfer model.",
                    "sid": 109,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This helps to not only constrain the search space at the start of learning, but also helps to bias dependencies between words that are not part of the alignment.",
                    "sid": 110,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "So far we have not defined the ALIGN function that is used to score potential parses.",
                    "sid": 111,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let a = {(s(1), t(1)), ... , (s(n), t(n))} be an alignment where s(i) is a word in the source sentence xs (not necessarily the ith word) and t(i) is similarly a word in the target sentence xt (again, not necessarily the ith word).",
                    "sid": 112,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The notation (s(i), t(i)) E a indicates two words are the ith aligned pair in a.",
                    "sid": 113,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We define the ALIGN function to encode the Direct Correspondence Assumption (DCA) from Hwa et al. (2005): has now been lexicalized and is working in the space of target language sentences.",
                    "sid": 114,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Next, the algorithm iterates over the sentences in the parallel corpus.",
                    "sid": 115,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It parses the English sentence with an English parser (line 8, again a lexicalized parser).",
                    "sid": 116,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It then uses the current target language parameter vector to create a k-best parse list for the target sentence (line 9).",
                    "sid": 117,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From this list, it selects the parse whose dependencies align most closely with the English parse via the pre-specified alignment (line 10, also see below for the definition of the ALIGN function).",
                    "sid": 118,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It then uses this selected parse as a proxy to the gold standard parse to update the parameters (line 11).",
                    "sid": 119,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The intuition is simple.",
                    "sid": 120,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The parser starts with non-random accuracies by emulating the direct transfer model and slowly tries to induce better parameters by selecting parses from its k-best list The notation (i, j) E y indicates that a dependency from head i to modifier j is in tree y.",
                    "sid": 121,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The ALIGN function rewards aligned head-modifier pairs and penalizes unaligned pairs when a possible alignment exists.",
                    "sid": 122,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all other cases it is agnostic, i.e., when one or both of the modifier or head are not aligned.",
                    "sid": 123,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 3 shows an example of aligned EnglishGreek sentences, the English parse and a potential Greek parse.",
                    "sid": 124,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this case the ALIGN function returns a value of 2.",
                    "sid": 125,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is because there are three aligned dependencies: took\u2192book, book\u2192the and from\u2192John.",
                    "sid": 126,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These add 3 to the score.",
                    "sid": 127,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There is one incorrectly aligned dependency: the preposition mistakenly modifies the noun on the Greek side.",
                    "sid": 128,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This subtracts 1.",
                    "sid": 129,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, there are two dependencies that do not align: the subject on the English side and a determiner to a proper noun on the Greek side.",
                    "sid": 130,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These do not effect the result.",
                    "sid": 131,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The learning algorithm in Figure 2 is an instance of augmented-loss training (Hall et al., 2011) which is closely related to the constraint driven learning algorithms of Chang et al. (2007).",
                    "sid": 132,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In that work, external constraints on output structures are used to help guide the learner to good parameter regions.",
                    "sid": 133,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our model, we use constraints drawn from parallel data exactly in the same manner.",
                    "sid": 134,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since posterior regularization is closely related to constraint driven learning, this makes our algorithm also similar to the parser projection approach of Ganchev et al. (2009).",
                    "sid": 135,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are a couple of differences.",
                    "sid": 136,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we bias our model towards the direct transfer model, which is already quite powerful.",
                    "sid": 137,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, our alignment constraints are used to select parses from a k-best list, whereas in posterior regularization they are used as soft constraints on full model expectations during training.",
                    "sid": 138,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The latter is beneficial as the use of k-best lists does not limit the class of parsers to those whose parameters and search space decompose neatly with the DCA loss function.",
                    "sid": 139,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An empirical comparison to Ganchev et al. (2009) is given in Section 5.",
                    "sid": 140,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Results are given in Table 1 under the column enproj.",
                    "sid": 141,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all experiments we train the seed-stage perceptron for 5 iterations (J = 5) and we use one hundred times as much parallel data as seed stage non-parallel data (m = 100n).",
                    "sid": 142,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The seed-stage nonparallel data is the training portion of each treebank, stripped of all dependency annotations.",
                    "sid": 143,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After training the projected parser we average the parameters of the model (Collins, 2002).",
                    "sid": 144,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The parsers evaluated using predicted part-of-speech tags use the predicted tags at both training and testing time and are thus free of any target language specific resources.",
                    "sid": 145,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When compared with the direct transfer model (en-dir. in Table 1), we can see that there is an improvement for every single language, reducing relative error by 8% on average (57.0% to 60.4%) and up to 18% for Dutch (60.8 to 67.8%).",
                    "sid": 146,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One could wonder whether the true power of the projection model comes from the re-lexicalization step \u2013 lines 3-6 of the algorithm.",
                    "sid": 147,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, if just this step is run, then the average UAS only increases from 57.0% to 57.4%, showing that most of the improvement comes from the projection stage.",
                    "sid": 148,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that the results in Table 1 indicate that parsers using predicted part-of-speech tags are only slightly worse than the parsers using gold tags (about 2-3% absolute), showing that these methods are robust to tagging errors.",
                    "sid": 149,
                    "ssid": 75,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 multi-source transfer",
            "number": "4",
            "sents": [
                {
                    "text": "The previous section focused on transferring an English parser to a new target language.",
                    "sid": 150,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, there are over 20 treebanks available for a variety of language groups including Indo-European, Altaic (including Japanese), Semitic, and Sino-Tibetan.",
                    "sid": 151,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Many of these are even in standardized formats (Buchholz and Marsi, 2006; Nivre et al., 2007).",
                    "sid": 152,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Past studies have shown that for both part-of-speech tagging and grammar induction, learning with multiple comparable languages leads to improvements (Cohen and Smith, 2009; Snyder et al., 2009; BergKirkpatrick and Klein, 2010).",
                    "sid": 153,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section we examine whether this is also true for parser transfer.",
                    "sid": 154,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 shows the matrix of source-target language UAS for all nine languages we consider (the original eight target languages plus English).",
                    "sid": 155,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can see that there is a wide range from 33.3% to 74.7%.",
                    "sid": 156,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There is also a wide range of values depending on the source training data and/or target testing data, e.g., Portuguese as a source tends to parse target languages much better than Danish, and is also more amenable as a target testing language.",
                    "sid": 157,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some of these variations are expected, e.g., the Romance languages (Spanish, Italian and Portuguese) tend to transfer well to one another.",
                    "sid": 158,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, some are unexpected, e.g., Greek being the best source language for Dutch, as well as German being one of the worst.",
                    "sid": 159,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is almost certainly due to different annotation schemes across treebanks.",
                    "sid": 160,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Overall, Table 2 does indicate that there are possible gains in accuracy through the inclusion of additional languages.",
                    "sid": 161,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to take advantage of treebanks in multiple languages, our multi-source system simply concatenates the training data from all non-target languages.",
                    "sid": 162,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In other words, the multi-source direct transfer parser for Danish will be trained by first concatenating the training corpora of the remaining eight languages, training a delexicalized parser on this data and then directly using this parser to analyze the Danish test data.",
                    "sid": 163,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the multi-source projected parser, the procedure is identical to that in Section 3.2 except that we use the multi-source direct transfer model to seed the algorithm instead of the English-only direct transfer model.",
                    "sid": 164,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For these experiments we still only use English-target parallel data because that is the format of the readily available data in the Europarl corpus.",
                    "sid": 165,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 3 presents four sets of results.",
                    "sid": 166,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first (best-source) is the direct transfer results for the oracle single-best source language per target language.",
                    "sid": 167,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second (avg-source) is the mean UAS over all source languages per target language.",
                    "sid": 168,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The third (multi-dir.) is the multi-source direct transfer system.",
                    "sid": 169,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The fourth and final result set (multi-proj.) is the multi-source projected system.",
                    "sid": 170,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The resulting parsers are typically much more accurate than the English direct transfer system (Table 1).",
                    "sid": 171,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On average, the multi-source direct transfer system reduces errors by 10% relative over the English-only direct transfer system.",
                    "sid": 172,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These improvements are not consistent.",
                    "sid": 173,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For Greek and Dutch we see significant losses relative to the English-only system.",
                    "sid": 174,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An inspection of Table 2 shows that for these two languages English is a particularly good source training language.",
                    "sid": 175,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the multi-source projected system the results are mixed.",
                    "sid": 176,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some languages see basically no change relative the multi-source direct transfer model, while some languages see modest to significant increases.",
                    "sid": 177,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But again, there is an overall trend to better models.",
                    "sid": 178,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, starting with an English-only direct transfer parser with 57.0% UAS on average, by adding parallel corpora and multiple source languages we finish with parser having 63.8% UAS on average, which is a relative reduction in error of roughly 16% and more than doubles the performance of a DMV model (Table 1).",
                    "sid": 179,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Interestingly, the multi-source systems provide, on average, accuracies near that of the single-best source language and significantly better than the average source UAS.",
                    "sid": 180,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, even this simple method of multi-source transfer already provides strong performance gains.",
                    "sid": 181,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We expect that more principled techniques will lead to further improvements.",
                    "sid": 182,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, recent work by S\u00f8gaard (2011) explores data set sub-sampling methods.",
                    "sid": 183,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unlike our work, S\u00f8gaard found that simply concatenating all the data led to degradation in performance.",
                    "sid": 184,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Cohen et al. (2011) explores the idea learning language specific mixture coefficients for models trained independently on the target language treebanks.",
                    "sid": 185,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, their results show that this method often did not significantly outperform uniform mixing.",
                    "sid": 186,
                    "ssid": 37,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5 comparison",
            "number": "5",
            "sents": [
                {
                    "text": "Comparing unsupervised and parser projection systems is difficult as many publications use nonoverlapping sets of languages or different evaluation criteria.",
                    "sid": 187,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We compare to the following three systems that do not augment the treebanks and report results for some of the languages that we considered: Naseem et al. (2010), in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model.",
                    "sid": 188,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to their original results, we also report results using the same part-of-speech tagset as the systems described in this paper (USR\u2020).",
                    "sid": 189,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is useful for two reasons.",
                    "sid": 190,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, it makes the comparison more direct.",
                    "sid": 191,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, we can generate USR results for all eight languages and not just for the languages that they report.",
                    "sid": 192,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 4 gives results comparing the models presented in this work to those three systems.",
                    "sid": 193,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this comparison we use sentences of length 10 or less after punctuation has been removed in order to be consistent with reported results.",
                    "sid": 194,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The overall trends carry over from the full treebank setting to this reduced sentence length setup: the projected models outperform the direct transfer models and multisource transfer gives higher accuracy than transferring only from English.",
                    "sid": 195,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most previous work has assumed gold part-of-speech tags, but as the code for USR is publicly available we were able to train it using the same projected part-of-speech tags used in our models.",
                    "sid": 196,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These results are also given in Table 4 under USR\u2020.",
                    "sid": 197,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Again, we can see that the multisource systems (both direct and projected) significantly outperform the unsupervised models.",
                    "sid": 198,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is not surprising that a parser transferred from annotated resources does significantly better than unsupervised systems since it has much more information from which to learn.",
                    "sid": 199,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The PR system of Ganchev et al. (2009) is similar to ours as it also projects syntax across parallel corpora.",
                    "sid": 200,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For Spanish we can see that the multi-source direct transfer parser is better (75.1% versus 70.6%), and this is also true for the multi-source projected parser (73.2%).",
                    "sid": 201,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ganchev et al. also report results for Bulgarian.",
                    "sid": 202,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We trained a multi-source direct transfer parser for Bulgarian which obtained a score of 72.8% versus 67.8% for the PR system.",
                    "sid": 203,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If we only use English as a source language, as in Ganchev et al., the English direct transfer model achieves 66.1% on Bulgarian and 69.3% on Spanish versus 67.8% and 70.6% for PR.",
                    "sid": 204,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this setting the English projected model gets 72.0% on Spanish.",
                    "sid": 205,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, under identical conditions the direct transfer model obtains accuracies comparable to PR.6 Another projection based system is that of Smith and Eisner (2009), who report results for German (68.5%) and Spanish (64.8%) on sentences of length 15 and less inclusive of punctuation.",
                    "sid": 206,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Smith and Eisner use custom splits of the data and modify a subset of the dependencies.",
                    "sid": 207,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The multi-source projected parser obtains 71.9% for German and 67.8% for Spanish on this setup.7 If we cherry-pick the source language the results can improve, e.g., for Spanish we can obtain 71.7% and 70.8% by directly transferring parsers form Italian or Portuguese respectively.",
                    "sid": 208,
                    "ssid": 22,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "6 discussion",
            "number": "6",
            "sents": [
                {
                    "text": "One fundamental point the above experiments illustrate is that even for languages for which no resources exist, simple methods for transferring parsers work remarkably well.",
                    "sid": 209,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, if one can transfer part-of-speech tags, then a large part of transferring unlabeled dependencies has been solved.",
                    "sid": 210,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This observation should lead to a new baseline in unsupervised and projected grammar induction \u2013 the UAS of a delexicalized English parser.",
                    "sid": 211,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Of course, our experiments focus strictly on IndoEuropean languages.",
                    "sid": 212,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Preliminary experiments for Arabic (ar), Chinese (zh), and Japanese (ja) suggest similar direct transfer methods are applicable.",
                    "sid": 213,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, on the CoNLL test sets, a DMV model obtains UAS of 28.7/41.8/34.6% for ar/zh/ja respectively, whereas an English direct transfer parser obtains 32.1/53.8/32.2% and a multi-source direct transfer parser obtains 39.9/41.7/43.3%.",
                    "sid": 214,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this setting only Indo-European languages are used as source data.",
                    "sid": 215,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, even across language groups direct transfer is a reasonable baseline.",
                    "sid": 216,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, this is not necessary as treebanks are available for a number of language groups, e.g., Indo-European, Altaic, Semitic, and Sino-Tibetan.",
                    "sid": 217,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second fundamental observation is that when available, multiple sources should be used.",
                    "sid": 218,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Even through naive multi-source methods (concatenating data), it is possible to build a system that has comparable accuracy to the single-best source for all languages.",
                    "sid": 219,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This advantage does not come simply from having more data.",
                    "sid": 220,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In fact, if we randomly sampled from the multi-source data until the training set size was equivalent to the size of the English data, then the results still hold (and in fact go up slightly for some languages).",
                    "sid": 221,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This suggests that even better transfer models can be produced by separately weighting each of the sources depending on the target language \u2013 either weighting by hand, if we know the language group of the target language, or automatically, if we do not.",
                    "sid": 222,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As previously mentioned, the latter has been explored in both S\u00f8gaard (2011) and Cohen et al. (2011).",
                    "sid": 223,
                    "ssid": 15,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "7 conclusions",
            "number": "7",
            "sents": [
                {
                    "text": "We presented a simple, yet effective approach for projecting parsers from languages with labeled training data to languages without any labeled training data.",
                    "sid": 224,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Central to our approach is the idea of delexicalizing the models, which combined with a standardized part-of-speech tagset allows us to directly transfer models between languages.",
                    "sid": 225,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then use a constraint driven learning algorithm to adapt the transferred parsers to the respective target language, obtaining an additional 16% error reduction on average in a multi-source setting.",
                    "sid": 226,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our final parsers achieve state-of-the-art accuracies on eight Indo-European languages, significantly outperforming previous unsupervised and projected systems.",
                    "sid": 227,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Acknowledgements: We would like to thank Kuzman Ganchev, Valentin Spitkovsky and Dipanjan Das for numerous discussions on this topic and comments on earlier drafts of this paper.",
                    "sid": 228,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We would also like to thank Shay Cohen, Dipanjan Das, Noah Smith and Anders S\u00f8gaard for sharing early drafts of their recent related work.",
                    "sid": 229,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}