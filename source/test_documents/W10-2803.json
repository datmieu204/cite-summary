{
    "ID": "W10-2803",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "What Is Word Meaning Really? (And How Can Distributional Models Help Us Describe It?)",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we argue in favor of reconsidering models for word meaning, using as a basis results from cognitive science on human concept representation.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More specifically, we argue for a more flexible representation of word meaning than the assignment of a single best-fitting dictionary sense to each occurrence: Either use dictionary senses, but view them as having fuzzy boundaries, and assume that an occurrence can activate multiple senses to different degrees.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Or move away from dictionary senses completely, and only model similarities between individual word usages.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We argue that distributional models provide a flexible framework for experimenting with alternative models of word meanings, and discuss example models.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Word sense disambiguation (WSD) is one of the oldest problems in computational linguistics (Weaver, 1949) and still remains challenging today.",
                    "sid": 5,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "State-of-the-art performance on WSD for WordNet senses is at only around 70-80% accuracy (Edmonds and Cotton, 2001; Mihalcea et al., 2004).",
                    "sid": 6,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The use of coarse-grained sense groups (Palmer et al., 2007) has led to considerable advances in WSD performance, with accuracies of around 90% (Pradhan et al., 2007).",
                    "sid": 7,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But this figure averages over lemmas, and the problem remains that while WSD works well for some lemmas, others continue to be tough.",
                    "sid": 8,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In WSD, polysemy is typically modeled through a list of dictionary senses thought to be mutually disjoint, such that each occurrence of a word is characterized through one best-fitting dictionary sense.",
                    "sid": 9,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Accordingly, WSD is typically framed as a classification task.",
                    "sid": 10,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Interestingly, the task of assigning a single best word sense is very hard for human annotators, not just machines (Kilgarriff and Rosenzweig, 2000).",
                    "sid": 11,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper we advocate the exploration of alternative computational models of word meaning.",
                    "sid": 12,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After all, one possible reason for the continuing difficulty of (manual as well as automatic) word sense assignment is that the prevailing model might be suboptimal.",
                    "sid": 13,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We explore three main hypotheses.",
                    "sid": 14,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first builds on research on the human concept representation that has shown that concepts in the human mind do not work like sets with clear-cut boundaries; they show graded membership, and there are typical members as well as borderline cases (Rosch, 1975; Hampton, 2007).",
                    "sid": 15,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Accordingly, (A) we will suggest that word meaning may be better modeled using a graded notion of sense membership than through concepts with hard boundaries.",
                    "sid": 16,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, even if senses have soft boundaries, the question remains of whether they are disjoint.",
                    "sid": 17,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(B) We will argue in favor of a framework where multiple senses may apply to a single occurrence, to different degrees.",
                    "sid": 18,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This can be viewed as a dynamical grouping of senses for each occurrence, in contrast to static sense groups as in Palmer et al. (2007).",
                    "sid": 19,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first two hypotheses still rely on an existing sense list.",
                    "sid": 20,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, there is no universal agreement across dictionaries and across tasks on the number of senses that words have (Hanks, 2000).",
                    "sid": 21,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Kilgarriff (1997) even argues that general, task-independent word senses do not exist.",
                    "sid": 22,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(C) By focusing on individual occurrences (usages) of a lemma and their degree of similarity, we can model word meaning without recourse to dictionary senses.",
                    "sid": 23,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we are going to argue in favor of the use of vector space as a basis for alternative models of word meaning.",
                    "sid": 24,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Vector space models have been used widely to model word sense (Lund and Burgess, 1996; Deerwester et al., 1990; Landauer and Dumais, 1997; Sahlgren and Karlgren, 2005; Pad\u00b4o and Lapata, 2007), their central property being that proximity in space can be used to predict semantic similarity.",
                    "sid": 25,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By viewing word occurrences as points in vector space, we can model word meaning without recourse to senses.",
                    "sid": 26,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An additional advantage of vector space models is that they are also widely used in human concept representation models, yielding many modeling ideas that can be exploited for computational models.",
                    "sid": 27,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Section 2 we review the evidence that word sense is a tough phenomenon to model, and we lay out findings that support hypotheses (A)-(C).",
                    "sid": 28,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Section 4 considers distributional models that represent word meaning without recourse to dictionary senses, following (C).",
                    "sid": 29,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Section 5 we discuss possibilities for embedding dictionary senses in vector space in a way that respects points (A) and (B).",
                    "sid": 30,
                    "ssid": 26,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "2 computational and cognitive models of word meaning",
            "number": "2",
            "sents": [
                {
                    "text": "In this section, we review the problems of (manual and automatic) sense assignment, and we discuss discusses cognitive models of concept representation and polysemy, following the three hypotheses laid out in the introduction.",
                    "sid": 31,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Word sense assignment.",
                    "sid": 32,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In computational linguistics, the problem of polysemy is typically phrased as one of choosing one best-fitting sense for the given occurrence out of a dictionarydefined sense list.",
                    "sid": 33,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, this is a hard task both for humans and for machines.",
                    "sid": 34,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With WordNet (Fellbaum, 1998), the electronic lexicon resource that is currently most widely used in computational linguistics, inter-annotator agreement (ITA) lies in the range of 67% to 78% (Landes et al., 1998; Snyder and Palmer, 2004; Mihalcea et al., 2004), and state-of-the-art WSD systems achieve accuracy scores of 73% to 77% (Edmonds and Cotton, 2001; Mihalcea et al., 2004).",
                    "sid": 35,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This problem is not specific to WordNet: Analyses with the HECTOR dictionary led to similar numbers (Kilgarriff and Rosenzweig, 2000).",
                    "sid": 36,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sense granularity has been suggested as a reason for the difficulty of the task (Palmer et al., 2007).",
                    "sid": 37,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "And in fact, the use of more coarse-grained senses leads to greatly ITA as well as WSD accuracy, with about a 10% improvement for either measure (Palmer et al., 2007; Pradhan et al., 2007).",
                    "sid": 38,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In OntoNotes (Hovy et al., 2006), an ITA of 90% is used as the criterion for the construction of coarsegrained sense distinctions.",
                    "sid": 39,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, intriguingly, for some high-frequency lemmas such as leave this ITA threshold is not reached even after multiple re-partitionings of the semantic space (Chen and Palmer, 2009) \u2013 indicating that the meaning of these words may not be separable into senses distinct enough for consistent annotation.",
                    "sid": 40,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A recent analysis of factors influencing ITA differences between lemmas (Passonneau et al., 2010) found three main factors: sense concreteness, specificity of the context in which a target word occurs, and similarity between senses.",
                    "sid": 41,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is interesting to note that only one of those factors, the third, can be addressed through a change of dictionary.",
                    "sid": 42,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al., 2004), or to work directly with paraphrases (McCarthy and Navigli, 2009).",
                    "sid": 43,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(A) Graded sense membership.",
                    "sid": 44,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Research on the human concept representation (Murphy, 2002; Hampton, 2007) shows that categories in the human mind are not simply sets with clear-cut boundaries.",
                    "sid": 45,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some items are perceived as more typical than others (Rosch, 1975; Rosch and Mervis, 1975).",
                    "sid": 46,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Also, some items are clear members, others are rated as borderline (Hampton, 1979).",
                    "sid": 47,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On borderline items, people are more likely to change their mind about category membership (McCloskey and Glucksberg, 1978).",
                    "sid": 48,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, these results concern mental concepts, which raises the question of the relation between mental concepts and word senses.",
                    "sid": 49,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This relation is discussed in most depth by Murphy (1991; 2002), who argues that while not every human concept is associated with a word, word meanings show many of the same phenomena as concepts in general; word meaning is \u201cmade up of pieces of conceptual structure\u201d.",
                    "sid": 50,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In cognitive linguistics there has been much work on word meaning based on models with graded membership and typically effects (Coleman and Kay, 1981; Lakoff, 1987; Cruse, 1986; Taylor, 1989). we asked three human annotators to judge the applicability of WordNet senses on a graded scale of 1 (completely different) to 5 (identical) and giving a rating for each sense rather than picking one.",
                    "sid": 51,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 1 shows an example sentence with annotator ratings for the senses of the target argument.",
                    "sid": 52,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this sentence, the annotators agree that senses 2 and 3 are highly applicable, but there also individual differences in the perceived meaning: Only annotator 2 views sense 1 as applying to a high degree.",
                    "sid": 53,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In an annotation setting with graded judgments, it does not make sense to measure exact agreement on judgments.",
                    "sid": 54,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We instead evaluated ITA using Spearman\u2019s rho, a nonparametric correlation test, finding highly significant correlations (p \u00ab 0.001) between each pair of annotators, as well as highly significant correlations with the results of a previous, traditional word sense annotation of the same dataset.",
                    "sid": 55,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The annotators made use of the complete scale (1-5), often opting for intermediate values of sense applicability.",
                    "sid": 56,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, we tested whether there were groups of senses that always got the same ratings on any given sentence (which would mean that the annotators implicitly used more coarse-grained senses).",
                    "sid": 57,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "What we found instead is that the annotators seemed to have mixed and matched senses for the individual occurrences in a dynamic fashion.",
                    "sid": 58,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(C) Describing word meaning without dictionary senses.",
                    "sid": 59,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In lexicography, Kilgarriff (1997) and Hanks (2000) cast doubt on the existence of task-independent, distinct senses.",
                    "sid": 60,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In cognitive science, Kintsch (2007) calls word meaning \u201cfluid and flexible\u201d.",
                    "sid": 61,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "And some researchers in lexical semantics have suggested that word meanings lie on a continuum between clear cut cases of ambiguity on the one hand, and on the other hand vagueness where clear cut boundaries do not hold (Tuggy, 1993).",
                    "sid": 62,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are some psychological studies on whether different senses of a polysemous word are represented separately in the mind or whether there is some joint representation.",
                    "sid": 63,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, so far the evidence is inconclusive and varies strongly with the experimental setting.",
                    "sid": 64,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some studies found evidence for a separate representation (Klein and Murphy, 2001; Pylkkanen et al., 2006).",
                    "sid": 65,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Brown (2008) finds a linear change in semantic similarity effects with sense distance, which could possibly point to a continuous representation of word meaning without clear sense boundaries.",
                    "sid": 66,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But while there is no definitive answer yet on the question of the mental representation of polysemy, a computational model that does not rely on distinct senses has the advantage of making fewer assumptions.",
                    "sid": 67,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It also avoids the tough lexicographic problem mentioned above, of deciding on a best set of senses for a given domain.",
                    "sid": 68,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the recent USim annotation study (Erk et al., 2009), we tested whether human annotators could reliably and consistently provide word meaning judgments without the use of dictionary senses.",
                    "sid": 69,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Three annotators rated the similarity of pairs of occurrences (usages) of a common target word, again on a scale of 1-5.",
                    "sid": 70,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1 shows an example, with the corresponding annotator judgments.",
                    "sid": 71,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results on this task were encouraging: Again using correlation to measure ITA, we found a highly significant correlation (p \u00ab 0.001) between the judgments of each pair of annotators.",
                    "sid": 72,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, there was a strong correlation on judgments given with and without the use of dictionary senses (USim versus WSsim) for the same data.",
                    "sid": 73,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3 Vector space models of word meaning in isolation This section gives a brief overview of the use of vector spaces to model concepts and word meaning in cognition and computational linguistics.",
                    "sid": 74,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In two of the current main theories of concept representation, feature vectors play a prominent role.",
                    "sid": 75,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Prototype theory (Hampton, 1979; Smith and Medin, 1981) models degree of category membership through similarity to a single prototype.",
                    "sid": 76,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Exemplar models (Medin and Schaffer, 1978; Nosofsky, 1992; Nosofsky and Palmeri, 1997) represent a concept as a collection of all previously seen exemplars and compute degree of category membership as similarity to stored exemplars.",
                    "sid": 77,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both prototypes and exemplars are typically represented as feature vectors.",
                    "sid": 78,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Many models represent a concept as a region rather than a point in space, often characterized by a feature vector plus a separate dimension weight vector (Smith et al., 1988; Hampton, 1991; G\u00a8ardenfors, 2004).",
                    "sid": 79,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The features are individually meaningful and interpretable and include sensory and motor features as well as function and taxonomic features.",
                    "sid": 80,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are several datasets with features elicited from human subjects (McRae et al., 2005; Vigliocco et al., 2004).",
                    "sid": 81,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In computational linguistics, distributional models represent the meaning of a word as a vector in a high-dimensional space whose dimensions characterize the contexts in which the word typically occurs (Lund and Burgess, 1996; Landauer and Dumais, 1997; Sahlgren and Karlgren, 2005; Pad\u00b4o and Lapata, 2007).",
                    "sid": 82,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the simplest case, the dimensions are context words, and the values are co-occurrence counts.",
                    "sid": 83,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast to spaces used in cognitive science, the dimensions in distributional models are typically not interpretable (though see Almuhareb and Poesio (2005), Baroni et al. (2010)).",
                    "sid": 84,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A central property of distributional models is that proximity in vector space is a predictor of semantic similarity.",
                    "sid": 85,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These models have been used successfully in NLP (Deerwester et al., 1990; Manning et al., 2008), as well as in psychology (Landauer and Dumais, 1997; Lowe and McDonald, 2000; McDonald and Ramscar, 2001).",
                    "sid": 86,
                    "ssid": 56,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 vector space models of word meaning in context",
            "number": "3",
            "sents": [
                {
                    "text": "If we want to represent word meaning through individual usages and their similarity only, without the use of dictionary senses (along hypothesis (C)), distributional models are an obvious choice, if we can just represent each individual usage as a point in space.",
                    "sid": 87,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, vector space models have mostly been used to represent the meaning of a word in isolation: The vector for a word is computed by summing over all its corpus occurrences, thereby summing over all its meanings.",
                    "sid": 88,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are a few vector space models of meaning in context, though they differ in what it is that they model.",
                    "sid": 89,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One group of models computes a single vector for a whole sentence, encoding both the words and the syntactic structure (Smolensky, 1990; B. Coecke and Clark, 2010).",
                    "sid": 90,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this case, the dimensionality of the vectors varies with the syntactic complexity of the sentence in question.",
                    "sid": 91,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A second group also computes a single vector for a whole expression, but the vector for a larger expression is a combination of the word vectors for the words occurring in the expression (Landauer and Dumais, 1997; Mitchell and Lapata, 2008).",
                    "sid": 92,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Syntactic structure is not encoded.",
                    "sid": 93,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The resulting vector, of the same dimensionality as the word vectors, is then a combination of the contexts in which the words of the sentence occur.",
                    "sid": 94,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A third group of approaches derives a separate vector for each word in a given sentence (Erk and Pad\u00b4o, 2008; Thater et al., 2009; Erk and Pad\u00b4o, 2010).",
                    "sid": 95,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While an approach of the second type would derive a single, joint vector for, say, the expression catch a ball, an approach from the third group would derive two vectors, one for the word catch in the context of ball, and one for the word ball in the context of catch.",
                    "sid": 96,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this third group, the dimensionality of a vector for a word in context is the same as for a word in isolation.",
                    "sid": 97,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we focus on the third type of approaches.",
                    "sid": 98,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our aim is to study alternatives to dictionary senses for characterizing word meaning.",
                    "sid": 99,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "So we need a meaning characterization for each individual word in a given sentence context, rather than a single vector for a larger expression.",
                    "sid": 100,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can also classify distributional approaches to word meaning in context into prototype- and exemplar-based approaches.",
                    "sid": 101,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Prototype-based approaches first compute a (prototype) vector for each word in isolation, then modify this vector according to the context in a given occurrence (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Erk and Pad\u00b4o, 2008; Thater et al., 2009).",
                    "sid": 102,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Typical methods for combining prototype vectors are addition, component-wise multiplication (introduced by Mitchell and Lapata (2008)), and component-wise minimum.",
                    "sid": 103,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then there are multiple prototype approaches that statically cluster synonyms or occurrences to induce word senses(Sch\u00a8utze, 1998; Pantel and Lin, 2002; Reisinger and Mooney, 2010).",
                    "sid": 104,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Exemplar-based approaches represent a word in isolation as a collection of its occurrences or paraphrases, then select only the contextually appropriate exemplars for a given occurrence context (Kintsch, 2001; Erk and Pad\u00b4o, 2010).",
                    "sid": 105,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper we focus on the first and third group of approaches, as they do not rely on knowledge of how many word senses (clusters) there should be.",
                    "sid": 106,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A structured vector space model for word meaning in context.",
                    "sid": 107,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Erk and Pad\u00b4o (2008), we proposed the structured vector space model (SVS), which relies solely on syntactic context for computing a context-specific vector.",
                    "sid": 108,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is a prototypebased model, , and called structured because it explicitly represents argument structure, using multiple vectors to represent each word.",
                    "sid": 109,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 2 (left) illustrates the representation.",
                    "sid": 110,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A word, for example catch, has one vector describing the meaning of the word itself, the lexical vector ~catch.",
                    "sid": 111,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is a vector for the word in isolation, as is usual for prototype-based models.",
                    "sid": 112,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, the representation for catch contains further vectors describing the selectional preferences for each argument position.",
                    "sid": 113,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The obj preference vector of catch is computed from the lexical vectors of all words that have been observed as direct objects of catch in some syntactically parsed corpus.",
                    "sid": 114,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the example in Figure 2, we have observed the direct objects cold, baseball, and drift.",
                    "sid": 115,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the simplest case, the obj preference vector of catch is then computed as the (weighted) sum of the three vectors cold, baseball and drift.",
                    "sid": 116,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Likewise, ball is represented by one vector for ball itself, one for ball\u2019s preferences for its modifiers (mod), one vector for the verbs of which it is a subject (subj\u22121), and one for the verbs of which is an object (obj\u22121).",
                    "sid": 117,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The vector for catch in a given context, say in the context catch ball, is then computed as illustrated on the right side of Figure 2: The lexical vector ~catch is combined with the obj\u22121 vector of \ufffd ball, modifying the vector catch in the direction of verbs that typically take ball as an object.",
                    "sid": 118,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the vector combination, any of the usual operations can be used: addition, component-wise multiplication, or minimum.",
                    "sid": 119,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Likewise, the lexical vector \ufffd ball is combined with the obj preference vector of catch to compute the meaning of ball in the context catch ball.",
                    "sid": 120,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The standard evaluation for vector models of meaning in context is to predict paraphrase appropriateness.",
                    "sid": 121,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Paraphrases always apply to a word meaning, not a word.",
                    "sid": 122,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, contract is an appropriate paraphrase for catch in the context John caught the flu, but it is not an appropriate paraphrase in the context John caught a butterfly.",
                    "sid": 123,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A vector space model can predict paraphrase appropriateness as the similarity (measured, for example, using Cosine) of the context-specific vector of catch with the lexical vector of contract: The more similar the vectors, the higher the predicted appropriateness of the paraphrase.",
                    "sid": 124,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluated SVS on two datasets.",
                    "sid": 125,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first is a tightly controlled psycholinguistic dataset of subject/verb pairs with paraphrases for the verbs only (Mitchell and Lapata, 2008).",
                    "sid": 126,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The other is the Lexical Substitution dataset, which has annotator-generated paraphrases for target words in a larger sentential context and which is thus closer to typical NLP application scenarios (McCarthy and Navigli, 2009).",
                    "sid": 127,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "SVS showed comparable performance to the model by Mitchell and Lapata (2008) on the former dataset, and outperformed the Mitchell and Lapata model on the latter.",
                    "sid": 128,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One obvious extension is to use all available syntactic context, instead of focusing on a single syntactic neighbor.",
                    "sid": 129,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We found no improvement on SVS in a straightforward extension to additional syntactic context items (Erk and Pad\u00b4o, 2009).",
                    "sid": 130,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, Thater et al. (2009) did achieve better performance with a different model that used all syntactic context.",
                    "sid": 131,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Taking larger context into account in an exemplar-based model.",
                    "sid": 132,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But even if we take the complete local syntactic context into account, we are missing some evidence, in particular non-local information.",
                    "sid": 133,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The word ball is interpreted differently in sentences (1a) and (1b) 1 even though its predicate ran has more or less the same meaning in both sentences.",
                    "sid": 134,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "What is different is the subject of ran, player versus debutante, which is not a direct syntactic neighbor of the ambiguous word ball.",
                    "sid": 135,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Even though we are not using dictionary senses, the types of evidence that should be useful for computing occurrence-specific vectors should be the same as for traditional WSD; and one of the main type of features used there is bag-of-words context.",
                    "sid": 136,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In (Erk and Pad\u00b4o, 2010), we proposed an exemplar-based model of word meaning in context that relied on bag-of-words context information from the whole sentence, but did not use syntactic information.",
                    "sid": 137,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The model assumes that each target lemma is represented by a set of exemplars, where an exemplar is a sentence in which the target lemma occurs.",
                    "sid": 138,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Polysemy is then modeled by activating (selecting) relevant exemplars of a target lemma in a given occurrence s.2 Both the exemplars and the occurrence s are modeled as vectors.",
                    "sid": 139,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We simply use first-order vectors that reflect the number of times each word occurs in a given sentence.",
                    "sid": 140,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The activated exemplars are then simply the ones whose vectors are most similar to the vector of s. The results that we achieved with the exemplar-based model on the Lexical Substitution dataset were considerably better than those achieved with any of the syntax-based approaches (Erk and Pad\u00b4o, 2008; Erk and Pad\u00b4o, 2009; Thater et al., 2009).",
                    "sid": 141,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While prototype models compute a vector by first summing over all observed occurrences and then having to suppress dimensions that are not contextually appropriate, exemplar models only take contextually appropriate exemplars into account in the first place, which is conceptually simpler and thus more attractive.",
                    "sid": 142,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But there are still many open questions, in particular the best combination of bag-of-words context and syntactic context as evidence for computing occurrencespecific vector representations.",
                    "sid": 143,
                    "ssid": 57,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5 the role of dictionary senses",
            "number": "4",
            "sents": [
                {
                    "text": "Word meaning models that rely only on individual word usages and their similarities are more flexible than dictionary-based models and make less assumptions.",
                    "sid": 144,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the other hand, dictionaries offer not just sense lists but also a wealth of information that can be used for inferences.",
                    "sid": 145,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "WordNet (Fellbaum, 1998) has relations between words and between synsets, most importantly synonymy and hyponymy.",
                    "sid": 146,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "VerbNet (Kipper et al., 2000) specifies semantic properties of a predicate\u2019s arguments, as well as relations between the arguments.",
                    "sid": 147,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section we discuss approaches for embedding dictionary senses in a distributional model in a way that supports hypotheses (A) and (B) (graded sense membership, and description of an occurrence through multiple senses) and that supports testing the applicability of dictionary-based inference rules.",
                    "sid": 148,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Mapping dictionary senses to points in vector space.",
                    "sid": 149,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Dictionary senses can be mapped to points in vector space very straightforwardly if we have sense-annotated corpus data.",
                    "sid": 150,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In that case, we can compute a (prototype) vector for a sense from all corpus occurrences annotated with that sense.",
                    "sid": 151,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used this simple model (Erk and McCarthy, 2009) to predict the graded sense applicability judgments from the WSsim dataset.",
                    "sid": 152,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(See Section 2 for more information on this dataset.)",
                    "sid": 153,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The predictions of the vector space model significantly correlate with annotator judgments.",
                    "sid": 154,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In comparison with an approach that uses the confidence levels of a standard WSD model as predictions, the vector space model shows higher recall but lower precision \u2013 for definitions of precision and recall that are adapted to the graded case.",
                    "sid": 155,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Another way of putting the findings is to say that the WSD confidence levels tend to under-estimate sense applicability, while the vector space model tends to over-estimate it.",
                    "sid": 156,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Attachment sites for inference rules.",
                    "sid": 157,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As discussed above, vector space models for word meaning in context are typically evaluated on paraphrase applicability tasks (Mitchell and Lapata, 2008; Erk and Pad\u00b4o, 2008; Erk and Pad\u00b4o, 2009; Thater et al., 2009).",
                    "sid": 158,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They predict the applicability of a paraphrase like (2) based on the similarity between a context-specific vector for the lemma (here, catch) and a context-independent vector for the paraphrase.",
                    "sid": 159,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(in this case, contract).",
                    "sid": 160,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Another way of looking at this is to consider the inference rule (2) to be attached to a point in space, namely the vector for contract, and to trigger the inference rule for an occurrence of catch if it is close enough to the attachment site.",
                    "sid": 161,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If we know the WordNet sense of contract for which rule (2) holds \u2013 it happens to be sense 4 \u2013, we can attach the rule to a vector for sense 4 of contract, rather than a vector computed from all occurrences of the lemma.",
                    "sid": 162,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that when we use dictionaries as a source for inference rules, for example by creating an inference rule like (2) for each two words that share a synset and for each direct hyponym/hypernym pair, we do know the WordNet sense to which each inference rule attaches.",
                    "sid": 163,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Mapping dictionary senses to regions in vector space.",
                    "sid": 164,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Erk (2009) we expand on the idea of tying inference rules to attachment sites by representing a word sense not as a point but as a region in vector space.",
                    "sid": 165,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The extent of the regions is estimated through the use of both positive exemplars (occurrences of the word sense in question), and negative exemplars (occurrences of other words).",
                    "sid": 166,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The computational models we use are inspired by cognitive models of concept representation that represent concepts as regions (Smith et al., 1988; Hampton, 1991), in particular adopting Shepard\u2019s law (Shepard, 1987), which states that perceived similarity to an exemplar decreases exponentially with distance from its vector.",
                    "sid": 167,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the longer term, the goal for the association of inference rules with attachment sites is to obtain a principled framework for reasoning with partially applicable inference rules in vector space.",
                    "sid": 168,
                    "ssid": 25,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "6 conclusion and outlook",
            "number": "5",
            "sents": [
                {
                    "text": "In this paper, we have argued that it may be time to consider alternative computational models of word meaning, given that word sense disambiguation, after all this time, is still a tough problem for humans as well as machines.",
                    "sid": 169,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have followed three hypotheses.",
                    "sid": 170,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first two involve dictionary senses, suggesting that (A) senses may best be viewed as applying to a certain degree, rather than in a binary fashion, and (B) that it may make sense to describe an occurrence through multiple senses as a default rather than an exception.",
                    "sid": 171,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The third hypothesis then departs from dictionary senses, suggesting (C) focusing on individual word usages and their similarities instead.",
                    "sid": 172,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have argued that distributional models are a good match for word meaning models following hypotheses (A)(C): They can represent individual word usages as points in vector space, and they can also represent dictionary senses in a way that allows for graded membership and overlapping senses, and we have discussed some existing models, both prototypebased and exemplar-based.",
                    "sid": 173,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One big question is, of course, about the usability of these alternative models of word meaning in NLP applications.",
                    "sid": 174,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Will they do better than dictionary-based models?",
                    "sid": 175,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The current evaluations, testing paraphrase applicability in context, are a step in the right direction, but more task-oriented evaluation schemes have to follow.",
                    "sid": 176,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have argued that it makes sense to look to cognitive models of mental concept representation.",
                    "sid": 177,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They are often based on feature vectors, and there are many interesting ideas in these models that have not yet been used (much) in computational models of word meaning.",
                    "sid": 178,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One of the most exciting ones, perhaps, is that cognitive models often have interpretable dimensions.",
                    "sid": 179,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While dimensions of distributional models are usually not individually interpretable, there are some first models (Almuhareb and Poesio, 2005; Baroni et al., 2010) that use patterns to extract meaningful dimensions from corpus data.",
                    "sid": 180,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This offers many new perspectives: For which tasks can we improve performance by selecting dimensions that are meaningful specifically for that task (as in Mitchell et al. (2008))?",
                    "sid": 181,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Can interpretable dimensions be used for inferences?",
                    "sid": 182,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "And, when we are computing vector space representations for word meaning in context, is it possible to select meaningful dimensions that are appropriate for a given context?",
                    "sid": 183,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Acknowledgements.",
                    "sid": 184,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This work was supported in part by National Science Foundation grant IIS0845925, and by a Morris Memorial Grant from the New York Community Trust.",
                    "sid": 185,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "K. Erk, D. McCarthy, and N. Gaylord.",
                    "sid": 186,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2009.",
                    "sid": 187,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Investigations on word senses and word usages.",
                    "sid": 188,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Proceedings of ACL, Singapore.",
                    "sid": 189,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Katrin Erk.",
                    "sid": 190,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2009.",
                    "sid": 191,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Representing words as regions in vector space.",
                    "sid": 192,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Proceedings of CoNLL.",
                    "sid": 193,
                    "ssid": 25,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}