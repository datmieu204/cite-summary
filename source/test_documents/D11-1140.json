{
    "ID": "D11-1140",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Lexical Generalization in CCG Grammar Induction for Semantic Parsing",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Such lexicons can be inefficient when words appear repeatedly with closely related lexical content.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we introduce factored lexicons, which both model word meaning model systematic variation in word usage.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Semantic parsers automatically recover representations of meaning from natural language sentences.",
                    "sid": 7,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recent work has focused on learning such parsers directly from corpora made up of sentences paired with logical meaning representations (Kate et al., 2005; Kate and Mooney, 2006; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2005, 2007; Lu et al., 2008; Kwiatkowski et al., 2010).",
                    "sid": 8,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, in a flight booking domain we might have access to training examples such as: and the goal is to learn a grammar that can map new, unseen, sentences onto their corresponding meanings, or logical forms.",
                    "sid": 9,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One approach to this problem has developed algorithms for leaning probabilistic CCG grammars (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010).",
                    "sid": 10,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These grammars are well-suited to the task of semantic parsing, as they closely link syntax and semantics.",
                    "sid": 11,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They can be used to model a wide range of complex linguistic phenomena and are strongly lexicalized, storing all language-specific grammatical information directly with the words in the lexicon.",
                    "sid": 12,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, a typical learned lexicon might include entries such as: Although lexicalization of this kind is useful for learning, as we will see, these grammars can also suffer from sparsity in the training data, since closely related entries must be repeatedly learned for all members of a certain class of words.",
                    "sid": 13,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the list above shows a selection of lexical items that would have to be learned separately.",
                    "sid": 14,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this list, the word \u201cflight\u201d is paired with the predicate flight in three separate lexical items which are required for different syntactic contexts.",
                    "sid": 15,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Item (1) has the standard N category for entries of this are required to construct the final meaning representype, item (2) allows the use of the word \u201cflight\u201d tations are not explicitly labeled in the training data. with that-less relative clauses such as \u201cflight depart- Instead, we model them with hidden variables and ing Boston\u201d, and item (3) is useful for phrases with develop an online learning approach that simultaneunconventional word order such as \u201cfrom Boston ously estimates the parameters of a log-linear parsflight to New York\u201d.",
                    "sid": 16,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Representing these three lexi- ing model, while inducing the factored lexicon. cal items separately is inefficient, since each word of We evaluate the approach on the benchmark Atis this class (such as \u201cfare\u201d) will require three similarly and GeoQuery domains.",
                    "sid": 17,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is a challenging setup, structured lexical entries differing only in predicate since the GeoQuery data has complex meaning repname.",
                    "sid": 18,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There may also be systemtatic semantic vari- resentations and sentences in multiple languages, ation between entries for a certain class of words. while the Atis data contains spontaneous, unedited For example, in (6) \u201cBoston\u201d is paired with the con- text that can be difficult to analyze with a formal stant bos that represents its meaning.",
                    "sid": 19,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, item grammar representation.",
                    "sid": 20,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our approach achieves at (7) also adds the predicate from to the logical form. or near state-of-the-art recall across all conditions, This might be used to analyse somewhat elliptical, despite having no English or domain-specific inforunedited sentences such as \u201cShow me flights Boston mation built in.",
                    "sid": 21,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We believe that ours is the only systo New York,\u201d which can be challenging for seman- tem of sufficient generality to run with this degree of tic parsers (Zettlemoyer and Collins, 2007). success on all of these datasets.",
                    "sid": 22,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This paper builds upon the insight that a large pro- 2 Related work portion of the variation between lexical items for There has been significant previous work on learna given class of words is systematic.",
                    "sid": 23,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore it ing semantic parsers from training sentences lashould be represented once and applied to a small set belled with logical form meaning representations. of basic lexical units.",
                    "sid": 24,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 We develop a factored lex- We extend a line of research that has addressed icon that captures this insight by distinguishing lex- this problem by developing CCG grammar inducemes, which pair words with logical constants, from tion techniques.",
                    "sid": 25,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Zettlemoyer and Collins (2005, lexical templates, which map lexemes to full lexical 2007) presented approaches that use hand generitems.",
                    "sid": 26,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As we will see, this can lead to a significantly ated, English-language specific rules to generate lexmore compact lexicon that can be learned from less ical items from logical forms as well as English data.",
                    "sid": 27,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each word or phrase will be associated with a specific type-shifting rules and relaxations of the few lexemes that can be combined with a shared set CCG combinators to model spontaneous, unedited of general templates. sentences.",
                    "sid": 28,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Zettlemoyer and Collins (2009) extends We develop an approach to learning factored, this work to the case of learning in context depenprobabilistic CCG grammars for semantic pars- dent environments.",
                    "sid": 29,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Kwiatkowski et al. (2010) deing.",
                    "sid": 30,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Following previous work (Kwiatkowski et al., scribed an approach for language-independent learn2010), we make use of a higher-order unification ing that replaces the hand-specified templates with learning scheme that defines a space of CCG gram- a higher-order-unification-based lexical induction mars consistent with the (sentence, logical form) method, but their approach does not scale well to training pairs.",
                    "sid": 31,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, instead of constructing challenging, unedited sentences.",
                    "sid": 32,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The learning apfully specified lexical items for the learned grammar, proach we develop for inducing factored lexicons is we automatically generate sets of lexemes and lexi- also language independent, but scales well to these cal templates to model each example.",
                    "sid": 33,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is a dif- challenging sentences. ficult learning problem, since the CCG analyses that There have been a number of other approaches for learning semantic parsers, including ones based on machine translation techniques (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing models (Miller et al., 1996; Ge and Mooney, 2006; Lu et al., 2008), in1A related tactic is commonly used in wide-coverage CCG parsers derived from treebanks, such as work by Hockenmaier and Steedman (2002) and Clark and Curran (2007).",
                    "sid": 34,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These parsers make extensive use of category-changing unary rules, to avoid data sparsity for systematically related categories (such as those related by type-raising).",
                    "sid": 35,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will automatically learn to represent these types of generalizations in the factored lexicon.",
                    "sid": 36,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1513 ductive logic programming algorithms (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000), probabilistic automata (He and Young, 2005, 2006), and ideas from string kernels and support vector machines (Kate and Mooney, 2006; Nguyen et al., 2006).",
                    "sid": 37,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More recent work has focused on training semantic parsers without supervision in the form of logical-form annotations.",
                    "sid": 38,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Clarke et al. (2010) and Liang et al.",
                    "sid": 39,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2011) replace semantic annotations in the training set with target answers which are more easily available.",
                    "sid": 40,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Goldwasser et al. (2011) present work on unsupervised learning of logical form structure.",
                    "sid": 41,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, all of these systems require significantly more domain and language specific initialization than the approach presented here.",
                    "sid": 42,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other work has learnt semantic analyses from text in the context of interactions in computational environments (Branavan et al. (2010), Vogel and Jurafsky (2010)); text grounded in partial observations of a world state (Liang et al., 2009); and from raw text alone (Poon and Domingos, 2009, 2010).",
                    "sid": 43,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There is also related work that uses the CCG grammar formalism.",
                    "sid": 44,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Clark and Curran (2003) present a method for learning the parameters of a log-linear CCG parsing model from fully annotated normal\u2013form parse trees.",
                    "sid": 45,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Watkinson and Manandhar (1999) describe an unsupervised approach for learning syntactic CCG lexicons.",
                    "sid": 46,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Bos et al. (2004) present an algorithm for building semantic representations from CCG parses but requires fully\u2013specified CCG derivations in the training data.",
                    "sid": 47,
                    "ssid": 41,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 overview of the approach",
            "number": "2",
            "sents": [
                {
                    "text": "Here we give a formal definition of the problem and an overview of the learning approach.",
                    "sid": 48,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Problem We will learn a semantic parser that takes a sentences x and returns a logical form z representing its underlying meaning.",
                    "sid": 49,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We assume we have input data {(xi,zi)|i = 1...n} containing sentences xi and logical forms zi, for example xi =\u201cShow me flights to Boston\u201d and zi = \u03bbx. flight(x)nto(x,bos).",
                    "sid": 50,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Model We will represent the parser as a factored, probabilistic CCG (PCCG) grammar.",
                    "sid": 51,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A traditional CCG lexical item would fully specify the syntax and semantics for a word (reviewed in Section 4).",
                    "sid": 52,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, Boston \ufffd- NP : bos represents the entry for the word \u201cBoston\u201d with syntactic category NP and meaning represented by the constant bos.",
                    "sid": 53,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Where a lexicon would usually list lexical items such as this, we instead use a factored lexicon (L,T) containing: We will make central use of this factored representation to provide a more compact representation of the lexicon that can be learned efficiently.",
                    "sid": 54,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The factored PCCG will also contain a parameter vector, \u03b8, that defines a log-linear distribution over the possible parses y, conditioned on the sentence x.",
                    "sid": 55,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Learning Our approach for learning factored PCCGs extends the work of Kwiatkowski et al. (2010), as reviewed in Section 7.",
                    "sid": 56,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Specifically, we modify the lexical learning, to produce lexemes and templates, as well as the feature space of the model, but reuse the existing parameter estimation techniques and overall learning cycle, as described in Section 7.",
                    "sid": 57,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We present the complete approach in three parts by describing the factored representation of the lexicon (Section 5), techniques for proposing potential new lexemes and templates (Section 6), and finally a complete learning algorithm (Section 7).",
                    "sid": 58,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the next section first reviews the required background on semantic parsing with CCG.",
                    "sid": 59,
                    "ssid": 12,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 background",
            "number": "3",
            "sents": [
                {
                    "text": "We represent the meanings of sentences, words and phrases with logical expressions that can contain constants, quantifiers, logical connectors and lambda abstractions.",
                    "sid": 60,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We construct the meanings of sentences from the meanings of words and phrases using lambda-calculus operations.",
                    "sid": 61,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use a version of the typed lambda calculus (Carpenter, 1997), in which the basic types include e, for entities; t, for truth values; and i for numbers.",
                    "sid": 62,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also have function types that are assigned to lambda expressions.",
                    "sid": 63,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The expression \u03bbx.flight(x) takes an entity and returns a truth value, and has the function type he,ti.",
                    "sid": 64,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CCG (Steedman, 1996, 2000) is a linguistic formalism that tightly couples syntax and semantics, and can be used to model a wide range of language phenomena.",
                    "sid": 65,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A traditional CCG grammar includes a lexicon \u039b with entries like the following: where each lexical item w`X : h has words w, a syntactic category X, and a logical form h. For the first example, these are \u201cflights,\u201d N, and \u03bbx. flight(x).",
                    "sid": 66,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we introduce a new way of representing lexical items as (lexeme, template) pairs, as described in section 5.",
                    "sid": 67,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CCG syntactic categories may be atomic (such as S or NP) or complex (such as (N\\N)/NP) where the slash combinators encode word order information.",
                    "sid": 68,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CCG uses a small set of combinatory rules to build syntactic parses and semantic representations concurrently.",
                    "sid": 69,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Two example combinatory rules are forward (>) and backward (<) application: These rules apply to build syntactic and semantic derivations under the control of the word order information encoded in the slash directions of the lexical entries.",
                    "sid": 70,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, given the lexicon above, the phrase \u201cflights to Boston\u201d can be parsed to produce: flights to Boston These rules allow a relaxed notion of constituency which helps limit the number of distinct CCG lexical items required.",
                    "sid": 71,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To the standard forward and backward slashes of CCG we also add a vertical slash for which the direction of application is underspecified.",
                    "sid": 72,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We shall see examples of this in Section 10.",
                    "sid": 73,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Due to ambiguity in both the CCG lexicon and the order in which combinators are applied, there will be many parses for each sentence.",
                    "sid": 74,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We discriminate between competing parses using a log-linear model which has a feature vector \u03c6 and a parameter vector \u03b8.",
                    "sid": 75,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The probability of a parse y that returns logical form z, given a sentence x is defined as: Section 8 fully defines the set of features used in the system presented.",
                    "sid": 76,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The most important of these control the generation of lexical items from (lexeme, template) pairs.",
                    "sid": 77,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each (lexeme, template) pair used in a parse fires three features as we will see in more detail later.",
                    "sid": 78,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The parsing, or inference, problem done at test time requires us to find the most likely logical form z given a sentence x, assuming the parameters \u03b8 and lexicon \u039b are known: f (x) = argmax p(z|x;\u03b8,\u039b) (2) z where the probability of the logical form is found by summing over all parses that produce it: where each step in the parse is labeled with the combinatory rule (\u2212 > or \u2212 <) that was used.",
                    "sid": 79,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CCG also includes combinatory rules of forward (> B) and backward (< B) composition: In this approach the distribution over parse trees y is modeled as a hidden variable.",
                    "sid": 80,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The sum over parses in Eq.",
                    "sid": 81,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3 can be calculated efficiently using the inside-outside algorithm with a CKY-style parsing algorithm.",
                    "sid": 82,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To estimate the parameters themselves, we use stochastic gradient updates (LeCun et al., 1998).",
                    "sid": 83,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a set of n sentence-meaning pairs {(xi,zi) : i = 1...n}, we update the parameters \u03b8 iteratively, for each example i, by following the local gradient of the conditional log-likelihood objective < Oi = logP(zi|xi;\u03b8,\u039b).",
                    "sid": 84,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The local gradient of the individual parameter \u03b8j associated with feature \u03c6j and training instance (xi,zi) is given by: As with Eq.",
                    "sid": 85,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3, all of the expectations in Eq.",
                    "sid": 86,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4 are calculated through the use of the inside-outside algorithm on a pruned parse chart.",
                    "sid": 87,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For a sentence of length m, each parse chart span is pruned using 2 a beam width proportional to m3 , to allow larger beams for shorter sentences.",
                    "sid": 88,
                    "ssid": 29,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5 factored lexicons",
            "number": "4",
            "sents": [
                {
                    "text": "A factored lexicon includes a set L of lexemes and a set T of lexical templates.",
                    "sid": 89,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section, we formally define these sets, and describe how they are used to build CCG parses.",
                    "sid": 90,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will use a set of lexical items from our running example to discuss the details of how the following lexical items: A lexeme (w,~c) pairs a word sequence w with an ordered list of logical constants c~ = [c1 ...cm].",
                    "sid": 91,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, item (1) and (2) above would come from a single lexeme (flight,[flight]).",
                    "sid": 92,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similar lexemes would be represented for other predicates, for example (fare,[cost]).",
                    "sid": 93,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Lexemes also can contain multiple constants, for example (cheapest,[argmin,cost]), which we will see more examples of later.",
                    "sid": 94,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A lexical template takes a lexeme and produces a lexical item.",
                    "sid": 95,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Templates have the general form where h~v is a logical expression that contains variables from the list ~v.",
                    "sid": 96,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Applying this template to the input lexeme (w,~c) gives the full lexical item w ` X :h where the variable \u03c9 has been replaced with the wordspan w and the logical form h has been created by replacing each of the variables in~v with the counterpart constant from ~c.",
                    "sid": 97,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the lexical item (6) above would be constructed from the lexeme (Boston,[bos]) using the template \u03bb(\u03c9,~v).",
                    "sid": 98,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "[\u03c9 ` NP:v1].",
                    "sid": 99,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Items (1) and (2) would both be constructed from the single lexeme (flight,[flight]) with the two different templates \u03bb(\u03c9,~v).",
                    "sid": 100,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "[\u03c9 ` N : \u03bbx.v1(x)] and \u03bb(\u03c9,~v).",
                    "sid": 101,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "[\u03c9 `N/(S|NP): \u03bb f\u03bbx.v1(x) \u2227 f (x)] In general, there can by many different (lexeme, template) pairs that produce the same lexical item.",
                    "sid": 102,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, lexical item (7) in our running example above can be constructed from the lexemes (Boston,[bos]) and (Boston,[from,bos]), given appropriate templates.",
                    "sid": 103,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To model this ambiguity, we include the selection of a (lexeme, template) pair as a decision to be made while constructing a CCG parse tree.",
                    "sid": 104,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given the lexical item produced by the chosen lexeme and template, parsing continues with the traditional combinators, as reviewed in Section 4.2.",
                    "sid": 105,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This direct integration allows for features that signal which lexemes and templates have been used while also allowing for well defined marginal probabilities, by summing over all ways of deriving a specific lexical item.",
                    "sid": 106,
                    "ssid": 18,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "6 learning factored lexicons",
            "number": "5",
            "sents": [
                {
                    "text": "To induce factored lexicons, we will make use of two procedures, presented in this section, that factor lexical items into lexemes and templates.",
                    "sid": 107,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Section 7 will describe how this factoring operation is integrated into the complete learning algorithm.",
                    "sid": 108,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a lexical item l of the form w `X : h with words w, a syntactic category X, and a logical form h, we define the maximal factoring to be the unique (lexeme, template) pair that can be used to reconstruct l and includes all of the constants of h in the lexeme (listed in a fixed order based on an ordered traversal of h).",
                    "sid": 109,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the maximal factoring for the lexical item Boston ` NP : bos is the pair we saw before: (Boston,[bos]) and \u03bb(\u03c9,~v).",
                    "sid": 110,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "[\u03c9 ` NP : v1].",
                    "sid": 111,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similarly, the lexical item Boston ` N\\N : \u03bb f .\u03bbx. f (x) \u2227 from(x,bos) would be factored to produce (Boston,[from,bos]) and \u03bb(\u03c9,~v).",
                    "sid": 112,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "[\u03c9 ` N\\N :\u03bb f.\u03bbx.f(x)\u2227v1(x,v2)].",
                    "sid": 113,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As we will see in Section 7, this notion of factor\u2202Oi \u2202\u03b8j ing can be directly incorporated into existing algorithms that learn CCG lexicons.",
                    "sid": 114,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When the original algorithm would have added an entry l to the lexicon, we can instead compute the factoring of l and add the corresponding lexeme and template to the factored lexicon.",
                    "sid": 115,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Maximal factorings, as just described, provide for significant lexical generalization but do not handle all of the cases needed to learn effectively.",
                    "sid": 116,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, the maximal split for the item Boston ` N\\N : \u03bb f.\u03bbx. f (x) \u2227 from(x,bos) would introduce the lexeme (Boston,[from,bos]), which is suboptimal since each possible city would need a lexeme of this type, with the additional from constant included.",
                    "sid": 117,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead, we would ideally like to learn the lexeme (Boston,[bos]) and have a template that introduces the from constant.",
                    "sid": 118,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This would model the desired generalization with a single lexeme per city.",
                    "sid": 119,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to permit the introduction of extra constants into lexical items, we allow the creation of templates that contain logical constants through partial factorings.",
                    "sid": 120,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, the template below can introduce the predicate from \u03bb(\u03c9,v).",
                    "sid": 121,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "[\u03c9 `N\\N :\u03bb f.\u03bbx.f(x)\u2227 from(x,v1)] The use of templates to introduce extra semantic constants into a lexical item is similar to, but more general than, the English-specific type-shifting rules used in Zettlemoyer and Collins (2007), which were introduced to model spontaneous, unedited text.",
                    "sid": 122,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They are useful, as we will see, in learning to recover semantic content that is implied, but not explicitly stated, such as our original motivating phrase \u201cflights Boston to New York.\u201d To propose templates which introduce semantic content, during learning, we build on the intuition that we need to recover from missing words, such as in the example above.",
                    "sid": 123,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this scenario, there should also be other sentences that actually include the word, in our example this would be something like \u201cflights from Boston.\u201d We will also assume that we have learned a good factored lexicon for the complete example that could produce the parse: flights from Boston Given analyses of this form, we introduce new templates that will allow us to recover from missing words, for example if \u201cfrom\u201d was dropped.",
                    "sid": 124,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We identify commonly occurring nodes in the best parse trees found during training, in this case the nonterminal spanning \u201cfrom Boston,\u201d and introduce templates that can produce the nonterminal, even if one of the words is missing.",
                    "sid": 125,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here, this approach would introduce the desired template \u03bb(\u03c9,v).",
                    "sid": 126,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "[\u03c9 ` N\\N : \u03bb f .\u03bbx. f (x) \u2227 from(x,v1)] for mapping the lexeme (Boston,[bos]) directly to the intermediate structure.",
                    "sid": 127,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Not all templates introduced this way will model valid generalizations.",
                    "sid": 128,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, we will incorporate them into a learning algorithm with indicator features that can be weighted to control their use.",
                    "sid": 129,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The next section presents the complete approach.",
                    "sid": 130,
                    "ssid": 24,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "7 learning factored pccgs",
            "number": "6",
            "sents": [
                {
                    "text": "Our Factored Unification Based Learning (FUBL) method extends the UBL algorithm (Kwiatkowski et al., 2010) to induce factored lexicons, while also simultanously estimating the parameters of a loglinear CCG parsing model.",
                    "sid": 131,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section, we first review the NEW-LEX lexical induction procedure from UBL, and then present the FUBL algorithm.",
                    "sid": 132,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "NEW-LEX generates lexical items by splitting and merging nodes in the best parse tree of each training example.",
                    "sid": 133,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each parse node has a CCG category X : h and a sequence of words w that it spans.",
                    "sid": 134,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will present an overview of the approach using the running example with the phrase w =\u201cin Boston\u201d and the category X : h = S\\NP : \u03bbx.loc(x,bos), which is of the type commonly seen during learning.",
                    "sid": 135,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The splitting procedure is a two step process that first splits the logical form h, then splits the CCG syntactic category X and finally splits the string w. The first step enumerates all possible splits of the logical form h into a pair of new expressions (f,g) that can be used to reconstruct h by either function application (h = f (g)) or composition (h = \u03bbx. f (g(x))).",
                    "sid": 136,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, one possible split is: (f = \u03bby.\u03bbx.loc(x,y) , g = bos) which corresponds to the function application case.",
                    "sid": 137,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The next two steps enumerate all ways of splitting the syntactic category X and words w to introduce two new lexical items which can be recombined with CCG combinators (application or composition) to recreate the original parse node X : h spanning w. In our example, one possibility would be: which could be recombined with the forward application combinator from Section 4.2.",
                    "sid": 138,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To assign categories while splitting, the grammar used by NEW-LEX only uses two atomic syntactic categories S and NP.",
                    "sid": 139,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This allows NEW-LEX to make use of a direct mapping from semantic type to syntactic category when proposing syntactic categories.",
                    "sid": 140,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this schema, the standard syntactic category N is replaced by the category S|NP which matches the type he,ti and uses the vertical slash introduced in Section 4.2.",
                    "sid": 141,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will see categories such as this in the evaluation.",
                    "sid": 142,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1 shows the FUBL learning algorithm.",
                    "sid": 143,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We assume training data {(xi,zi) : i = 1...n} where each example is a sentence xi paired with a logical form zi.",
                    "sid": 144,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The algorithm induces a factored PCCG, including the lexemes L, templates T, and parameters \u03b8.",
                    "sid": 145,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The algorithm is online, repeatedly performing both lexical expansion (Step 1) and a parameter update (Step 2) for each training example.",
                    "sid": 146,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The overall approach is closely related to the UBL algorithm (Kwiatkowski et al., 2010), but includes extensions for updating the factored lexicon, as motivated in Section 6.",
                    "sid": 147,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Initialization The model is initialized with a factored lexicon as follows.",
                    "sid": 148,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "MAX-FAC is a function that takes a lexical item l and returns the maximal factoring of it, that is the unique, maximal (lexeme, template) pair that can be combined to construct l, as described in Section 6.1.",
                    "sid": 149,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We apply MAX-FAC to each of the training examples (xi,zi), creating a single way of producing the desired meaning zi from a Inputs: Training set {(xi,zi) : i = 1...n} where each example is a sentence xi paired with a logical form zi.",
                    "sid": 150,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Set of entity name lexemes Le.",
                    "sid": 151,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Number of iterations J.",
                    "sid": 152,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Learning rate parameter \u03b10 and cooling rate parameter c. Empty lexeme set L. Empty template set T. Definitions: NEW-LEX(y) returns a set of new lexical items from a parse y as described in Section 7.1.",
                    "sid": 153,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "MAX-FAC(l) generates a (lexeme, template) pair from a lexical item l. PART-FAC(y) generates a set of templates from parse y.",
                    "sid": 154,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both of these are described in Section 7.2.",
                    "sid": 155,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The distributions p(y|x,z;\u03b8,(L,T)) and p(y,z|x;\u03b8,(L,T)) are defined by the log-linear model described in Section 4.3.",
                    "sid": 156,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Initialization: lexeme containing all of the words in xi.",
                    "sid": 157,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The lexemes and templates created in this way provide the initial factored lexicon.",
                    "sid": 158,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Step 1 The first step of the learning algorithm in Figure 1 adds lexemes and templates to the factored model given by performing manipulations on the highest scoring correct parse y\u2217 of the current training example (xi,zi).",
                    "sid": 159,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First the NEW-LEX procedure is run on y\u2217 as described in Section 6.1 to generate new lexical items.",
                    "sid": 160,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then use the function MAX-FAC to create the maximal factorings of each of these new lexical items as described in Section 6 and these are added to the factored representation of the lexicon.",
                    "sid": 161,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "New templates can also be introduced through partial factorings of internal parse nodes as described in Section 6.2.",
                    "sid": 162,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These templates are generated by using the function PART-FAC to abstract over the wordspan and a subset of the constants contained in the internal parse nodes of y\u2217.",
                    "sid": 163,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This step allows for templates that introduce new semantic content to model elliptical language, as described in Section 6.2.",
                    "sid": 164,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Step 2 The second step does a stochastic gradient descent update on the parameters \u03b8 used in the parsing model.",
                    "sid": 165,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This update is described in Section 4.3 Discussion The FUBL algorithm makes use of a direct online approach, where lexemes and templates are introduced in place while analyzing specific sentences.",
                    "sid": 166,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In general, this will overgeneralize; not all ways of combining lexemes and templates will produce high quality lexical items.",
                    "sid": 167,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the overall approach includes features, presented in Section 8, that can be used to learn which ones are best in practice.",
                    "sid": 168,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The complete algorithm iterates between adding new lexical content and updating the parameters of the parsing model with each procedure guiding the other.",
                    "sid": 169,
                    "ssid": 39,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "8 experimental setup",
            "number": "7",
            "sents": [
                {
                    "text": "Data Sets We evaluate on two benchmark semantic parsing datasets: GeoQuery, which is made up of natural language queries to a database of geographical information; and Atis, which contains natural language queries to a flight booking system.",
                    "sid": 170,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Geo880 dataset has 880 (English-sentence, logicalform) pairs split into a training set of 600 pairs and a test set of 280.",
                    "sid": 171,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Geo250 data is a subset of the Geo880 sentences that have been translated into Japanese, Spanish and Turkish as well as the original English.",
                    "sid": 172,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We follow the standard evaluation procedure for Geo250, using 10-fold cross validation experiments with the same splits of the data as Wong and Mooney (2007).",
                    "sid": 173,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Atis dataset contains 5410 (sentence, logical-form) pairs split into a 4480 example training set, a 480 example development set and a 450 example test set.",
                    "sid": 174,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Evaluation Metrics We report exact match Recall (percentage of sentences for which the correct logical-form was returned), Precision (percentage of returned logical-forms that are correct) and F1 (harmonic mean of Precision and Recall).",
                    "sid": 175,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For Atis we also report partial match Recall (percentage of correct literals returned), Precision (percentage of returned literals that are correct) and F1, computed as described by Zettlemoyer and Collins (2007).",
                    "sid": 176,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Features We introduce two types of features to discriminate between parses: lexical features and logical-form features.",
                    "sid": 177,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Lexical features fire on the lexemes and templates used to build the lexical items used in a parse.",
                    "sid": 178,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each (lexeme,template) pair used to create a lexical item we have indicator features \u03c6l for the lexeme used, \u03c6t for the template used, and \u03c6(l,t) for the pair that was used.",
                    "sid": 179,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We assign the features on lexical templates a weight of 0.1 to prevent them from swamping the far less frequent but equally informative lexeme features.",
                    "sid": 180,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Logical-form features are computed on the lambda-calculus expression z returned at the root of the parse.",
                    "sid": 181,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each time a predicate p in z takes an argument a with type Ty(a) in position i, it triggers two binary indicator features: \u03c6(p,a,i) for the predicate-argument relation; and \u03c6(p,Ty(a),i) for the predicate argument-type relation.",
                    "sid": 182,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Boolean operator features look at predicates that occurr together in conjunctions and disjunctions.",
                    "sid": 183,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each variable vi that fills argument slot i in two conjoined predicates p1 and p2 we introduce a binary indicator feature \u03c6conj(i,p1,p2).",
                    "sid": 184,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We introduce similar features \u03c6disj(i,p1,p2) for variables vi that are shared by predicates in a disjunction.",
                    "sid": 185,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Initialization The weights for lexeme features are initialized according to coocurrance statistics between words and logical constants.",
                    "sid": 186,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These are estimated with the Giza++ (Och and Ney, 2003) implementation of IBM Model 1.",
                    "sid": 187,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The initial weights for templates are set by adding \u22120.1 for each slash in the syntactic category and \u22122 if the template contains logical constants.",
                    "sid": 188,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Features on lexeme-template pairs and all parse features are initialized to zero.",
                    "sid": 189,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Systems We compare performance to all recentlypublished, directly-comparable results.",
                    "sid": 190,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For GeoQuery, this includes the ZC05, ZC07 (Zettlemoyer and Collins, 2005, 2007), \u03bb-WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al., 2010) systems and DCS (Liang et al., 2011).",
                    "sid": 191,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For Atis, we report results from HY06 (He and Young, 2006), ZC07, and UBL.",
                    "sid": 192,
                    "ssid": 23,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "9 results",
            "number": "8",
            "sents": [
                {
                    "text": "Tables 1-4 present the results on the Atis and Geoquery domains.",
                    "sid": 193,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In all cases, FUBL achieves at or near state-of-the-art recall (overall number of correct parses) when compared to directly comparable systems and it significantly outperforms UBL on Atis.",
                    "sid": 194,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On Geo880 the only higher recall is achieved by DCS with prototypes - which uses significant English-specific resources, including manually specified lexical content, but does not require training sentences annotated with logical-forms.",
                    "sid": 195,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On Geo250, FUBL achieves the highest recall across languages.",
                    "sid": 196,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each individual result should be interpreted with care, as a single percentage point corresponds to 2-3 sentences, but the overall trend is encouraging.",
                    "sid": 197,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the Atis development set, FUBL outperforms ZC07 by 7.5% of recall but on the Atis test set FUBL lags ZC07 by 2%.",
                    "sid": 198,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The reasons for this discrepancy are not clear, however, it is possible that the syntactic constructions found in the Atis test set do not exhibit the same degree of variation as those seen in the development set.",
                    "sid": 199,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This would negate the need for the very general lexicon learnt by FUBL.",
                    "sid": 200,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Across the evaluations, despite achieving high recall, FUBL achieves significantly lower precision than ZC07 and \u03bb-WASP.",
                    "sid": 201,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This illustrates the tradeoff from having a very general model of proposing lexical structure.",
                    "sid": 202,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With the ability to skip unseen words, FUBL returns a parse for all of the Atis test sentences, since the factored lexicons we are learning can produce a very large number of lexical items.",
                    "sid": 203,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These parses are, however, not always correct.",
                    "sid": 204,
                    "ssid": 12,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "10 analysis",
            "number": "9",
            "sents": [
                {
                    "text": "The Atis results in Tables 1 and 2 highlight the advantages of factored lexicons.",
                    "sid": 205,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "FUBL outperforms the UBL baseline by 16 and 11 points respectively in exact-match recall.",
                    "sid": 206,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Without making any modification to the CCG grammars or parsing combinators, we are able to induce a lexicon that is general enough model the natural occurring variations in the data, for example due to sloppy, unedited sentences.",
                    "sid": 207,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 2 shows a parse returned by FUBL for a sentence on which UBL failed.",
                    "sid": 208,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While the word \u201ccheapest\u201d is seen 208 times in the training data, in only a handful of these instances is it seen in the middle of an utterance.",
                    "sid": 209,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this reason, UBL never proposes the lexical item, cheapest \ufffd- NP\\(S|NP)/(S|NP) : \u03bb f\u03bbg.argmin(\u03bbx.f(x)ng(x),\u03bby.cost(y)), which is used to parse the sentence in Figure 2.",
                    "sid": 210,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast, FUBL uses a lexeme learned from the same word in different contexts, along with a template learnt from similar words in a similar context, to learn to perform the desired analysis.",
                    "sid": 211,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As well as providing a new way to search the lexicon during training, the factored lexicon provides a way of proposing new, unseen, lexical items at test time.",
                    "sid": 212,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We find that new, non-NP, lexical items are used in 6% of the development set parses.",
                    "sid": 213,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Interestingly, the addition of templates that introduce semantic content (as described in Section 6.2) account for only 1.2% of recall on the Atis development set.",
                    "sid": 214,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is suprising as elliptical constructions are found in a much larger proportion of the sentences than this.",
                    "sid": 215,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In practice, FUBL learns to model many elliptical constructions with lexemes and templates introduced through maximal factorings.",
                    "sid": 216,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the lexeme (to,[from,to]) can be used with the correct lexical template to deal with our motivating example \u201cflights Boston to New York\u201d.",
                    "sid": 217,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Templates that introduce content are therefore only used in truly novel elliptical constructions for which an alternative analysis could not be learned.",
                    "sid": 218,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 5 shows a selection of lexemes and templates learned for Atis.",
                    "sid": 219,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Examples 2 and 3 show that morphological variants of the same word must still be stored in separate lexemes.",
                    "sid": 220,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, as these lexemes now share templates, the total number of lexical variants that must be learned is reduced.",
                    "sid": 221,
                    "ssid": 17,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "11 discussion",
            "number": "10",
            "sents": [
                {
                    "text": "We argued that factored CCG lexicons, which include both lexemes and lexical templates, provide a compact representation of lexical knowledge that can have advantages for learning.",
                    "sid": 222,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also described a complete approach for inducing factored, probabilistic CCGs for semantic parsing, and demonstrated strong performance across a wider range of benchmark datasets that any previous approach.",
                    "sid": 223,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the future, it will also be important to explore morphological models, to better model variation within the existing lexemes.",
                    "sid": 224,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The factored lexical representation also has significant potential for lexical transfer learning, where we would need to learn new lexemes for each target application, but much of the information in the templates could, potentially, be ported across domains.",
                    "sid": 225,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgements",
            "number": "11",
            "sents": [
                {
                    "text": "The work was supported in part by EU ERC Advanced Fellowship 249520 GRAMPLUS, and an ESPRC PhD studentship.",
                    "sid": 226,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We would like to thank Yoav Artzi for helpful discussions.",
                    "sid": 227,
                    "ssid": 2,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}