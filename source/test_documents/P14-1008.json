{
    "ID": "P14-1008",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Logical Inference on Dependency-based Compositional Semantics",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Dependency-based Compositional Semantics (DCS) is a framework of natural language semantics with easy-to-process structures as well as strict semantics.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we equip the DCS framework logical inference, by defining abdenotations an abstraction of the computing process of denotations in original DCS.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An inference engine is built to achieve inference on abstract denotations.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Experiments on FraCaS and PASCAL RTE datasets show promising results.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al., 2011).",
                    "sid": 6,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is expressive enough to represent complex natural language queries on a relational database, yet simple enough to be latently learned from question-answer pairs.",
                    "sid": 7,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we equip DCS with logical inference, which, in one point of view, is \u201cthe best way of testing an NLP system\u2019s semantic capacity\u201d (Cooper et al., 1996).",
                    "sid": 8,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It should be noted that, however, a framework primarily designed for question answering is not readily suited for logical inference.",
                    "sid": 9,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Because, answers returned by a query depend on the specific database, but implication is independent of any databases.",
                    "sid": 10,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, answers to the question \u201cWhat books are read by students?\u201d, should always be a subset of answers to \u201cWhat books are ever read by anyone?\u201d, no matter how we store the data of students and how many records of books are there in our database.",
                    "sid": 11,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, our first step is to fix a notation which abstracts the calculation process of DCS trees, so as to clarify its meaning without the aid of any existing database.",
                    "sid": 12,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The idea is to borrow a minimal set of operators from relational algebra (Codd, 1970), which is already able to formulate the calculation in DCS and define abstract denotation, which is an abstraction of the computation of denotations guided by DCS trees.",
                    "sid": 13,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Meanings of sentences then can be represented by primary relations among abstract denotations.",
                    "sid": 14,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This formulation keeps the simpleness and computability of DCS trees mostly unaffected; for example, our semantic calculation for DCS trees is parallel to the denotation computation in original DCS.",
                    "sid": 15,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An inference engine is built to handle inference on abstract denotations.",
                    "sid": 16,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, to compensate the lack of background knowledge in practical inference, we combine our framework with the idea of tree transformation (Bar-Haim et al., 2007), to propose a way of generating knowledge in logical representation from entailment rules (Szpektor et al., 2007), which are by now typically considered as syntactic rewriting rules.",
                    "sid": 17,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We test our system on FraCaS (Cooper et al., 1996) and PASCAL RTE datasets (Dagan et al., 2006).",
                    "sid": 18,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The experiments show: (i) a competitive performance on FraCaS dataset; (ii) a big impact of our automatically generated on-the-fly knowledge in achieving high recall for a logicbased RTE system; and (iii) a result that outperforms state-of-the-art RTE system on RTE5 data.",
                    "sid": 19,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our whole system is publicly released and can be downloaded from http://kmcs.nii.ac. jp/tianran/tifmo/.",
                    "sid": 20,
                    "ssid": 15,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "2 the idea",
            "number": "2",
            "sents": [
                {
                    "text": "In this section we describe the idea of representing natural language semantics by DCS trees, and achieving inference by computing logical relations among the corresponding abstract denotations.",
                    "sid": 21,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al., 2011) (Figure 1).",
                    "sid": 22,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the sentence \u201cstudents read books\u201d, imagine a database consists of three tables, namely, a set of students, a set of books, and a set of \u201creading\u201d events (Table 1).",
                    "sid": 23,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The DCS tree in Figure 1 is interpreted as a command for querying these tables, obtaining \u201creading\u201d entries whose \u201cSUBJ\u201d field is student and whose \u201cOBJ\u201d field is book.",
                    "sid": 24,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The result is a set {John reads Ulysses, ...1, which is called a denotation.",
                    "sid": 25,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "DCS trees can be extended to represent linguistic phenomena such as quantification and coreference, with additional markers introducing additional operations on tables.",
                    "sid": 26,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 2 shows an example with a quantifier \u201cevery\u201d, which is marked as \u201cC\u201d on the edge (love)OBJ-ARG(dog) and interpreted as a division operator qOBJ \u2282 (\u00a72.2).",
                    "sid": 27,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Optimistically, we believe DCS can provide a framework of semantic representation with sufficiently wide coverage for real-world texts.",
                    "sid": 28,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The strict semantics of DCS trees brings us the idea of applying DCS to logical inference.",
                    "sid": 29,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is not trivial, however, because DCS works under the assumption that databases are explicitly available.",
                    "sid": 30,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Obviously this is unrealistic for logical inference on unrestricted texts, because we cannot prepare a database for everything in the world.",
                    "sid": 31,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This fact fairly restricts the applicable tasks of DCS.",
                    "sid": 32,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our solution is to redefine DCS trees without the aid of any databases, by considering each node of a DCS tree as a content word in a sentence (but may no longer be a table in a specific database), while each edge represents semantic relations between two words.",
                    "sid": 33,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The labels on both ends of an edge, such as SUBJ (subject) and OBJ (object), are considered as semantic roles of the corresponding words1.",
                    "sid": 34,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To formulate the database querying process defined by a DCS tree, we provide formal semantics to DCS trees by employing relational algebra (Codd, 1970) for representing the query.",
                    "sid": 35,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As described below, we represent meanings of sentences with abstract denotations, and logical relations among sentences are computed as relations among their abstract denotations.",
                    "sid": 36,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this way, we can perform inference over formulas of relational algebra, without computing database entries explicitly.",
                    "sid": 37,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Abstract denotations are formulas constructed from a minimal set of relational algebra (Codd, 1970) operators, which is already able to formulate the database queries defined by DCS trees.",
                    "sid": 38,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the semantics of \u201cstudents read books\u201d is given by the abstract denotation: Fl = read n (studentSUBJ x bookOBJ), where read, student and book denote sets represented by these words respectively, and wr represents the set w considered as the domain of the semantic role r (e.g. bookOBJ is the set of books considered as objects).",
                    "sid": 39,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The operators n and x represent intersection and Cartesian product respectively, both borrowed from relational algebra.",
                    "sid": 40,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is not hard to see the abstract denotation denotes the intersection of the \u201creading\u201d set (as illustrated by the \u201cread\u201d table in Table 1) with the product of \u201cstudent\u201d set and \u201cbook\u201d set, which results in the same denotation as computed by the DCS tree in Figure 1, i.e.",
                    "sid": 41,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "{John reads Ulysses, ...1.",
                    "sid": 42,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the point is that Fl itself is an algebraic formula that does not depend on any concrete databases.",
                    "sid": 43,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Formally, we introduce the following constants: example phrase abstract denotation / statement compound noun petfish pet n fish modification nice day day n (WARG x niceMOD) temporal relation boys study at night study n (boySUBJ x nightTIME) In addition we introduce following functions: An abstract denotation is then defined as finite applications of functions on either constants or other abstract denotations.",
                    "sid": 44,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As the semantics of DCS trees is formulated by abstract denotations, the meanings of declarative sentences are represented by statements on abstract denotations.",
                    "sid": 45,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Statements are declarations of some relations among abstract denotations, for which we consider the following set relations: Non-emptiness A =\ufffd 0: the set A is not empty.",
                    "sid": 46,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Subsumption A C B: set A is subsumed by B.3 Roughly speaking, the relations correspond to the logical concepts satisfiability and entailment.",
                    "sid": 47,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Abstract denotations and statements are convenient for representing semantics of various types of expressions and linguistic knowledge.",
                    "sid": 48,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some examples are shown in Table 2.4 Based on abstract denotations, we briefly describe our process to apply DCS to textual inference.",
                    "sid": 49,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To obtain DCS trees from natural language, we use Stanford CoreNLP5 for dependency parsing (Socher et al., 2013), and convert Stanford dependencies to DCS trees by pattern matching on POS tags and dependency labels.6 Currently we use the following semantic roles: ARG, SUBJ, OBJ, IOBJ, TIME and MOD.",
                    "sid": 50,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The semantic role MOD is used for any restrictive modifiers.",
                    "sid": 51,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Determiners such as \u201call\u201d, \u201cevery\u201d and \u201ceach\u201d trigger quantifiers, as shown in Figure 2.",
                    "sid": 52,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A DCS tree T = (N, \u00a3) is defined as a rooted tree, where each node a E N is labeled with a content word w(a) and each edge (a, a0) E \u00a3 C N x N is labeled with a pair of semantic roles (r, r0)7.",
                    "sid": 53,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here a is the node nearer to the root.",
                    "sid": 54,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, for each edge (a, a0) we can optionally assign a quantification marker.",
                    "sid": 55,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Abstract denotation of a DCS tree can be calculated in a bottom-up manner.",
                    "sid": 56,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the abstract denotation of H in Figure 2 is calculated from the leaf node Mary, and then: Formally, suppose the root a of a DCS tree T has children 7-1, ... , 7-n, and edges (a, 7-1), ... , (a, 7-n) labeled by (r1, r01), ... , (rn, r0n), respectively.",
                    "sid": 57,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The abstract denotation of T is defined as: where T\u03c4i is the subtree of T rooted at \u03c4i, and R\u03c3 is the set of possible semantic roles for content word w(\u03c3) (e.g.",
                    "sid": 58,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Rlove = {SUBJ, OBJ}), and WR\u03c3\\ri is the product of W which has dimension R\u03c3 \\ ri (e.g.",
                    "sid": 59,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "W{SUBJ,OBJ}\\SUBJ = WOBJ).",
                    "sid": 60,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When universal quantifiers are involved, we need to add division operators to the formula.",
                    "sid": 61,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If (\u03c3, \u03c4i) is assigned by a quantification marker \u201cC\u201d8, then the abstract denotation is9 where T0 is the same tree as T except that the edge (\u03c3, \u03c4i) is removed.",
                    "sid": 62,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the abstract denotation of the first sentence of T in Figure 2 (Mary loves every dog) is calculated from F2 (Mary loves) as After the abstract denotation [T ] is calculated, the statement representing the meaning of the sentence is defined as [T ] =\ufffd 0.",
                    "sid": 63,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the statement of \u201cstudents read books\u201d is read n (studentSUBJ x bookOBJ) =\ufffd 0, and the statement of \u201cMary loves every dog\u201d is equivalent to dog C \u03c0OBJ(F2).10 Since meanings of sentences are represented by statements on abstract denotations, logical inference among sentences is reduced to deriving new relations among abstract denotations.",
                    "sid": 64,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is done by applying axioms to known statements, and approximately 30 axioms are implemented (Table 3).",
                    "sid": 65,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These are algebraic properties of abstract denotations, among which we choose a set of axioms that can be handled efficiently and enable most common types of inference seen in natural language.",
                    "sid": 66,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the example in Figure 2, by constructing the following abstract denotations: Tom has a dog: we can use the lexical knowledge dog C animal, the statements of T (i.e. dog C \u03c0OBJ(F2) and F6 =\ufffd 0), and the axioms in Table 3,11 to prove the statement of H (i.e.",
                    "sid": 67,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "F4 =\ufffd 0) (Figure 3).",
                    "sid": 68,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We built an inference engine to perform logical inference on abstract denotations as above.",
                    "sid": 69,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this logical system, we treat abstract denotations as terms and statements as atomic sentences, which are far more easier to handle than first order predicate logic (FOL) formulas.",
                    "sid": 70,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, all implemented axioms are horn clauses, hence we can employ forward-chaining, which is very efficient.",
                    "sid": 71,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Further extensions of our framework are made to deal with additional linguistic phenomena, as briefly explained below.",
                    "sid": 72,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Negation To deal with negation in our forwardchaining inference engine, we introduce one more relation on abstract denotations, namely disjointness A 11 B, meaning that A and B are disjoint sets.",
                    "sid": 73,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using disjointness we implemented two types of negations: (i) atomic negation, for each content word w we allow negation w\u00af of that word, characterized by the property w 11 \u00afw; and (ii) root negation, for a DCS tree T and its denotation [T ], the negation of T is represented by T 11 T, meaning that T = 0 in its effect.",
                    "sid": 74,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Selection Selection operators in relational algebra select a subset from a set to satisfy some specific properties.",
                    "sid": 75,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This can be employed to represent linguistic phenomena such as downward monotonicity and generalized quantifiers.",
                    "sid": 76,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the current system, we implement (i) superlatives, e.g. shighest(mountain n (WARG x AsiaMOD)) (the highest mountain in Asia) and (ii) numerics, e.g. stwo(pet n fish) (two pet fish), where sf is a selection marker.",
                    "sid": 77,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Selection operators are implemented as markers assigned to abstract denotations, with specially designed axioms.",
                    "sid": 78,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example superlatives satisfy the following property: A C B & shighest(B) C A S shighest(B) = shighest(A).",
                    "sid": 79,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "New rules can be added if necessary.",
                    "sid": 80,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Coreference We use Stanford CoreNLP to resolve coreferences (Raghunathan et al., 2010), whereas coreference is implemented as a special type of selection.",
                    "sid": 81,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If a node Q in a DCS tree T belongs to a mention cluster m, we take the abstract denotation [T\u03c3] and make a selection sm([T\u03c3]), which is regarded as the abstract denotation of that mention.",
                    "sid": 82,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then all selections of the same mention cluster are declared to be equal.",
                    "sid": 83,
                    "ssid": 63,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 generating on-the-fly knowledge",
            "number": "3",
            "sents": [
                {
                    "text": "Recognizing textual entailment (RTE) is the task of determining whether a given textual statement H can be inferred by a text passage T. For this, our primary textual inference system operates as: However, this method does not work for realworld datasets such as PASCAL RTE (Dagan et al., 2006), because of the knowledge bottleneck: it is often the case that the lack of sufficient linguistic knowledge causes failure of inference, thus the system outputs \u201cno entailment\u201d for almost all pairs (Bos and Markert, 2005).",
                    "sid": 84,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The transparent syntax-to-semantics interface of DCS enables us to back off to NLP techniques during inference for catching up the lack of knowledge.",
                    "sid": 85,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We extract fragments of DCS trees as paraphrase candidates, translate them back to linguistic expressions, and apply distributional similarity to judge their validity.",
                    "sid": 86,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this way, our framework combines distributional and logical semantics, which is also the main subject of Lewis and Steedman (2013) and Beltagy et al. (2013).",
                    "sid": 87,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As follows, our full system (Figure 4) additionally invokes linguistic knowledge on-the-fly: On-the-fly knowledge is generated by aligning paths in DCS trees.",
                    "sid": 88,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A path is considered as joining two germs in a DCS tree, where a germ is defined as a specific semantic role of a node.",
                    "sid": 89,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, Figure 5 shows DCS trees of the following sentences (a simplified pair from RTE2-dev): T: Tropical storm Debby is blamed for deaths.",
                    "sid": 90,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "H: A storm has caused loss of life.",
                    "sid": 91,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The germ OBJ(blame) and germ ARG(death) in DCS tree of T are joined by the underscored path.",
                    "sid": 92,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Two paths are aligned if the joined germs are aligned, and we impose constraints on aligned germs to inhibit meaningless alignments, as described below.",
                    "sid": 93,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Two germs are aligned if they are both at leaf nodes (e.g.",
                    "sid": 94,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ARG(death) in T and ARG(life) in H, Figure 5), or they already have part of their meanings in common, by some logical clues.",
                    "sid": 95,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To formulate this properly, we define the abstract denotation of a germ, which, intuitively, represents the meaning of the germ in the specific sentence.",
                    "sid": 96,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The abstract denotation of a germ is defined in a top-down manner: for the root node P of a DCS tree T, we define its denotation [P]T as the denotation of the entire tree [T ]; for a non-root node T and its parent node Q, let the edge (Q, T) be labeled by semantic roles (r, r'), then define Now for a germ r(Q), the denotation is defined as the projection of the denotation of node Q onto the specific semantic role r: [r(Q)]T = 7rr([Q]T ).",
                    "sid": 97,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the abstract denotation of germ ARG(book) in Figure 1 is defined as 7rARG(book n 7rOBJ(readn(studentSUBJ xbookOBJ))), meaning \u201cbooks read by students\u201d.",
                    "sid": 98,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similarly, denotation of germ OBJ(blame) in T of Figure 5 indicates the object of \u201cblame\u201d as in the sentence \u201cTropical storm Debby is blamed for death\u201d, which is a tropical storm, is Debby, etc.",
                    "sid": 99,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al., 2011) of that variable.",
                    "sid": 100,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The logical clue to align germs is: if there exists an abstract denotation, other than W, that is a superset of both abstract denotations of two germs, then the two germs can be aligned.",
                    "sid": 101,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A simple example is that ARG(storm) in T can be aligned to ARG(storm) in H, because their denotations have a common superset other than W, namely 7rARG(storm).",
                    "sid": 102,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Amore complicated example is that OBJ(blame) and SUBJ(cause) can be aligned, because inference can induce [OBJ(blame)]T = [ARG(Debby)]T = [ARG(storm)]T, as well as [SUBJ(cause)]H = [ARG(storm)]H, so they also have the common superset 7rARG(storm).",
                    "sid": 103,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, for example, logical clues can avoid aligning ARG(storm) to ARG(loss), which is obviously meaningless.",
                    "sid": 104,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Aligned paths are evaluated by a similarity score, for which we use distributional similarity of the words that appear in the paths (\u00a74.1).",
                    "sid": 105,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Only path alignments with high similarity scores can be accepted.",
                    "sid": 106,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Also, we only accept paths of length < 5, to prevent too long paths to be aligned.",
                    "sid": 107,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Accepted aligned paths are converted into statements, which are used as new knowledge.",
                    "sid": 108,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The conversion is done by first performing a DCS tree transformation according to the aligned paths, and then declare a subsumption relation between the denotations of aligned germs.",
                    "sid": 109,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, to apply the aligned path pair generated in Figure 5, we use it to transform T into a new tree T\u2019 (Figure 6), and then the aligned germs, OBJ(blame) in T and SUBJ(cause) in T\u2019, will generate the on-the-fly knowledge: [OBJ(blame)]T C [SUBJ(cause)]T\u2019.",
                    "sid": 110,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similar to the tree transformation based approach to RTE (Bar-Haim et al., 2007), this process can also utilize lexical-syntactic entailment rules (Szpektor et al., 2007).",
                    "sid": 111,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, since the on-the-fly knowledge is generated by transformed pairs of DCS trees, all contexts are preserved: in Figure 6, though the tree transformation can be seen as generated from the entailment rule \u201cX is blamed for death \u2014* X causes loss of life\u201d, the generated on-the-fly knowledge, as shown above the trees, only fires with the additional condition that X is a tropical storm and is Debby.",
                    "sid": 112,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hence, the process can also be used to generate knowledge from context sensitive rules (Melamud et al., 2013), which are known to have higher quality (Pantel et al., 2007; Clark and Harrison, 2009).",
                    "sid": 113,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, it should be noted that using on-thefly knowledge in logical inference is not a trivial task.",
                    "sid": 114,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the FOL formula of the rule \u201cX is blamed for death \u2014* X causes loss of life\u201d is: which is not a horn clause.",
                    "sid": 115,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The FOL formula for the context-preserved rule in Figure 6 is even more involved.",
                    "sid": 116,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Still, it can be efficiently treated by our inference engine because as a statement, the formula QOBJ(blame)]T C QSUBJ(cause)]T\u2019 is an atomic sentence, more than a horn clause.",
                    "sid": 117,
                    "ssid": 34,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 experiments",
            "number": "4",
            "sents": [
                {
                    "text": "In this section, we evaluate our system on FraCaS (\u00a74.2) and PASCAL RTE datasets (\u00a74.3).",
                    "sid": 118,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The lexical knowledge we use are synonyms, hypernyms and antonyms extracted from WordNet12.",
                    "sid": 119,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also add axioms on named entities, stopwords, numerics and superlatives.",
                    "sid": 120,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, named entities are singletons, so we add axioms such as bx;(x C Tom & x =\ufffd0) \u2014* Tom C x.",
                    "sid": 121,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To calculate the similarity scores of path alignments, we use the sum of word vectors of the words from each path, and calculate the cosine similarity.",
                    "sid": 122,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the similarity score of the path alignment \u201cOBJ(blame)TOBJ-ARG(death) \ufffd SUBJ(cause)OBJ-ARG(loss)MOD-ARG(life)\u201d is calculated as the cosine similarity of vectors blame+death and cause+loss+life.",
                    "sid": 123,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other structures in the paths, such as semantic roles, are ignored in the calculation.",
                    "sid": 124,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The word vectors we use are from Mikolov et al. (2013)13 (Mikolov13), and additional results are also shown using Turian et al.",
                    "sid": 125,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2010)14 (Turian10).",
                    "sid": 126,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The threshold for accepted path alignments is set to 0.4, based on preexperiments on RTE development sets.",
                    "sid": 127,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The FraCaS test suite contains 346 inference problems divided into 9 sections, each focused on a category of semantic phenomena.",
                    "sid": 128,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the data by MacCartney and Manning (2007), and experiment on the first section, Quantifiers, following Lewis and Steedman (2013).",
                    "sid": 129,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This section has 44 single premise and 30 multi premise problems.",
                    "sid": 130,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most of the problems do not require lexical knowledge, so we use our primary textual inference system without on-the-fly knowledge nor WordNet, to test the performance of the DCS framework as formal semantics.",
                    "sid": 131,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To obtain the three-valued output (i.e. yes, no, and unknown), we output \u201cyes\u201d if H is proven, or try to prove the negation of H if H is not proven.",
                    "sid": 132,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To negate H, we use the root negation as described in \u00a72.5.",
                    "sid": 133,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the negation of H is proven, we output \u201cno\u201d, otherwise we output \u201cunknown\u201d.",
                    "sid": 134,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The result is shown in Table 4.",
                    "sid": 135,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since our system uses an off-the-shelf dependency parser, and semantic representations are obtained from simple rule-based conversion from dependency trees, there will be only one (right or wrong) interpretation in face of ambiguous sentences.",
                    "sid": 136,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Still, our system outperforms Lewis and Steedman (2013)\u2019s probabilistic CCG-parser.",
                    "sid": 137,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Compared to MacCartney and Manning (2007) and MacCartney and Manning (2008), our system does not need a pretrained alignment model, and it improves by making multi-sentence inferences.",
                    "sid": 138,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To sum up, the result shows that DCS is good at handling universal quantifiers and negations.",
                    "sid": 139,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most errors are due to wrongly generated DCS trees (e.g. wrongly assigned semantic roles) or unimplemented quantifier triggers (e.g.",
                    "sid": 140,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u201cneither\u201d) or generalized quantifiers (e.g.",
                    "sid": 141,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u201cat least a few\u201d).",
                    "sid": 142,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These could be addressed by future work.",
                    "sid": 143,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On PASCAL RTE datasets, strict logical inference is known to have very low recall (Bos and Markert, 2005), so on-the-fly knowledge is crucial in this setting.",
                    "sid": 144,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We test the effect of on-the-fly knowledge on RTE2, RTE3, RTE4 and RTE5 datasets, and compare our system with other approaches.",
                    "sid": 145,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Results on test data are shown in Table 5.",
                    "sid": 146,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When only primary knowledge is used in inference (the first row), recalls are actually very low; After we activate the on-the-fly knowledge, recalls jump to over 50%, with a moderate fall of precision.",
                    "sid": 147,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a result, accuracies significantly increase.",
                    "sid": 148,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A comparison between our system and other RTE systems is shown in Table 6.",
                    "sid": 149,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Bos06 (Bos and Markert, 2006) is a hybrid system combining deep features from a theorem prover and a model builder, together with shallow features such as lexical overlap and text length.",
                    "sid": 150,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "MacCartney08 (MacCartney and Manning, 2008) uses natural logic to calculate inference relations between two superficially aligned sentences.",
                    "sid": 151,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Clark08 (Clark and Harrison, 2008) is a logic-based system utilizing various resources including WordNet and DIRT paraphrases (Lin and Pantel, 2001), and is tolerant to partially unproven H sentences in some degree.",
                    "sid": 152,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All of the three systems pursue a logical approach, while combining various techniques to achieve robustness.",
                    "sid": 153,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The result shows that our system has comparable performance.",
                    "sid": 154,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the other hand, Wang10 (Wang and Manning, 2010) learns a treeedit model from training data, and captures entailment relation by tree edit distance.",
                    "sid": 155,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Stern11 (Stern and Dagan, 2011) and Stern12 (Stern et al., 2012) extend this framework to utilize entailment rules as tree transformations.",
                    "sid": 156,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These are more tailored systems using machine learning with many handcrafted features.",
                    "sid": 157,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Still, our unsupervised system outperforms the state-of-the-art on RTE5 dataset.",
                    "sid": 158,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Summing up test data from RTE2 to RTE5, Figure 7 shows the proportion of all proven pairs and their precision.",
                    "sid": 159,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Less than 5% pairs can be proven primarily, with a precision of 77%.",
                    "sid": 160,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Over 40% pairs can be proven by one piece of on-the-fly knowledge, yet pairs do exist in which more than 2 pieces are necessary.",
                    "sid": 161,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The precisions of 1 and 2 pieces on-the-fly knowledge application are over 60%, which is fairly high, given our rough estimation of the similarity score.",
                    "sid": 162,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a comparison, Dinu and Wang (2009) studied the proportion of proven pairs and precision by applying DIRT rules to tree skeletons in RTE2 and RTE3 data.",
                    "sid": 163,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The proportion is 8% with precision 65% on RTE2, and proportion 6% with precision 72% on RTE3.",
                    "sid": 164,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Applied by our logical system, the noisy on-the-fly knowledge can achieve a precision comparable to higher quality resources such as DIRT.",
                    "sid": 165,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A major type of error is caused by the ignorance of semantic roles in calculation of similarity scores.",
                    "sid": 166,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, though \u201cItaly beats Kazakhstan\u201d is not primarily proven from \u201cItaly is defeated by Kazakhstan\u201d, our system does produce the path alignment \u201cSUBJ(beat)OBJ \u2248 OBJ(defeat)SUBJ\u201d with a high similarity score.",
                    "sid": 167,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The impact of such errors depends on the data making methodology, though.",
                    "sid": 168,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It lowers precisions in RTE2 and RTE3 data, particularly in \u201cIE\u201d subtask (where precisions drop under 0.5).",
                    "sid": 169,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the other hand, it occurs less often in \u201cIR\u201d subtask.",
                    "sid": 170,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, to see if we \u201cget lucky\u201d on RTE5 data in the choice of word vectors and thresholds, we change the thresholds from 0.1 to 0.7 and draw the precision-recall curve, using two types of word vectors, Mikolov13 and Turian10.",
                    "sid": 171,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As shown in Figure 8, though the precision drops for Turian10, both curves show the pattern that our system keeps gaining recall while maintaining precision to a certain level.",
                    "sid": 172,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Not too much \u201cmagic\u201d in Mikolov13 actually: for over 80% pairs, every node in DCS tree of H can be covered by a path of length \u2264 5 that has a corresponding path of length \u2264 5 in T with a similarity score > 0.4.",
                    "sid": 173,
                    "ssid": 56,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}