{
    "ID": "P11-2067",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Clause Restructuring For SMT Not Absolutely Helpful",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are a number of systems that use a syntax-based reordering step prior to phrasebased statistical MT.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An early work proposing this idea showed improved translation performance, but subsequent work has had mixed results.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Speculations as to cause have suggested the parser, the data, or other factors.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We systematically investigate possible factors to give an initial answer to the question: Under what conditions does this use of syntax help PSMT?",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Phrase-based statistical machine translation (PSMT) translates documents from one human language to another by dividing text into contiguous sequences of words (phrases), translating each, and finally reordering them according to a distortion model.",
                    "sid": 5,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The PSMT distortion model typically does not consider linguistic information, and as such encounters difficulty in language pairs that require specific long-distance reorderings, such as German\u2013English.",
                    "sid": 6,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Collins et al. (2005) address this problem by reordering German sentences to more closely parallel English word order, prior to translation by a PSMT system.",
                    "sid": 7,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They find that this reordering-aspreprocessing approach results in a significant improvement in translation performance over the baseline.",
                    "sid": 8,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, there have been several other systems using the reordering-as-preprocessing approach, and they have met with mixed success.",
                    "sid": 9,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We systematically explore possible explanations for these contradictory results, and conclude that, while reordering is helpful for some sentences, potential improvement can be eroded by many aspects of the PSMT system, independent of the reordering.",
                    "sid": 10,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "2 prior work",
            "number": "2",
            "sents": [
                {
                    "text": "Reordering-as-preprocessing systems typically involve three steps.",
                    "sid": 11,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, the input sentence is parsed.",
                    "sid": 12,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, the parse is used to permute the words according to some reordering rules, which may be automatically or manually determined.",
                    "sid": 13,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, a phrase-based SMT system is trained and tested using the reordered sentences as input, in place of the original sentences.",
                    "sid": 14,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Many such systems exist, with results being mixed; we review several here.",
                    "sid": 15,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Xia and McCord (2004) (English-to-French translation, using automatically-extracted reordering rules) train on the Canadian Hansard.",
                    "sid": 16,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On a Hansard test set, an improvement over the baseline was only seen if the translation system\u2019s phrase table was restricted to phrases of length at most four.",
                    "sid": 17,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On a news test set, the reordered system performed significantly better than the baseline regardless of the maximum length of phrases.",
                    "sid": 18,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, this improvement was only apparent with monotonic decoding; when using a distortion model, the difference disappeared.",
                    "sid": 19,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Xia and McCord attribute the drop-off in performance on the Hansard set to similarity of training and test data.",
                    "sid": 20,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Collins et al. (2005) (German-to-English) use six hand-crafted reordering rules targeting the placement of verbs, subjects, particles and negation.",
                    "sid": 21,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They train and evaluate their system on Europarl text and obtain a BLEU score (Papineni et al., 2002) of 26.8, with the baseline PSMT system achieving 25.2.",
                    "sid": 22,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A human evaluation confirms that reordered translations are generally (but not universally) better.",
                    "sid": 23,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On Web text, Xu et al. (2009) report significant improvements applying one set of hand-crafted rules to translation from English to each of five SOV languages: Korean, Japanese, Hindi, Urdu and Turkish.",
                    "sid": 24,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Training on news text, Wang et al. (2007) (Chinese-to-English, hand-crafted rules) report a significant improvement over the baseline system on the NIST 2006 test set, using a distance-based distortion model.",
                    "sid": 25,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similar results are mentioned in passing for a lexicalised distortion model.",
                    "sid": 26,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Also on news text, Habash (2007) (automaticallyextracted rules, Arabic-to-English) reports a very large improvement when phrases are limited to length 1 and translation is monotonic.",
                    "sid": 27,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, allowing phrases up to 7 words in length or using a distance-based distortion model causes the difference in performance to disappear.",
                    "sid": 28,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Habash attributes this to parser and alignment performance.",
                    "sid": 29,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "He also includes oracle experiments, in which each system outperforms the other on 40\u201350% of sentences, suggesting that reordering is useful for many sentences.",
                    "sid": 30,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Zwarts and Dras (2007) implement six rules for Dutch-to-English translation, analogous to those of Collins et al. (2005), as part of an exploration of dependency distance in syntax-augmented PSMT.",
                    "sid": 31,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Considering only their baseline and reordered systems, the improvement is from 20.7 to only 20.8; they attribute their poor result to the parser used.",
                    "sid": 32,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Howlett and Dras (2010) reimplement the Collins et al. (2005) system for use in lattice-based translation.",
                    "sid": 33,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to their main system, they give results for the baseline and reordered systems, training and testing on Europarl and news text.",
                    "sid": 34,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In strong contrast to the results of Collins et al. (2005), Howlett and Dras (2010) report 20.04 for the reordered system, below the baseline at 20.77.",
                    "sid": 35,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They explain their lower absolute scores as a consequence of the different test set, but do not explore the reversal in conclusion.",
                    "sid": 36,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Like Habash (2007), Howlett and Dras (2010) include oracle experiments which demonstrate that the reordering is useful for some sentences.",
                    "sid": 37,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we focus on the Collins et al. (2005) and Howlett and Dras (2010) systems (hereafter CKK and HD), as they are the most similar but have perhaps the most divergent results.",
                    "sid": 38,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Possible explanations for the difference are differences in the reordering process, from either parser performance or implementation of the rules, and differences in the translation process, including PSMT system setup and data used.",
                    "sid": 39,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We examine parser performance in \u00a73 and the remaining possibilities in \u00a74\u20135.",
                    "sid": 40,
                    "ssid": 30,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 parser performance",
            "number": "3",
            "sents": [
                {
                    "text": "We first compare the performance of the two parsers used.",
                    "sid": 41,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CKK uses the Dubey and Keller (2003) parser, which is trained on the Negra corpus (Skut et al., 1997).",
                    "sid": 42,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "HD instead uses the Berkeley parser (Petrov et al., 2006), trained on Negra\u2019s successor, the larger Tiger corpus (Brants et al., 2002).",
                    "sid": 43,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Refer to Table 1 for precision and recall for each model.",
                    "sid": 44,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that the CKK reordering requires not just category labels (e.g.",
                    "sid": 45,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "NP) but also function labels (e.g.",
                    "sid": 46,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "SB for subject); parser performance typically goes down when these are learnt, due to sparsity.",
                    "sid": 47,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All models in Table 1 include function labels.",
                    "sid": 48,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Dubey and Keller (2003) train and test on the Negra corpus, with 18,602 sentences for training, 1,000 development and 1,000 test, removing sentences longer than 40 words.",
                    "sid": 49,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Petrov and Klein (2008) train and test the Berkeley parser on part of the Tiger corpus, with 20,894 sentences for training and 2,611 sentences for each of development and test, all at most 40 words long.",
                    "sid": 50,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The parsing model used by HD is trained on the full Tiger corpus, unrestricted for length, with 38,020 sentences for training and 2,000 sentences for development.",
                    "sid": 51,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The figures reported in Table 1 are the model\u2019s performance on this development set.",
                    "sid": 52,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With twice as much data, the increase in performance is unsurprising.",
                    "sid": 53,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From these figures, we conclude that sheer parser grunt is unlikely to be responsible for the discrepancy between CKK and HD.",
                    "sid": 54,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is possible that parser output differs qualitatively in some important way; parser figures alone do not reveal this.",
                    "sid": 55,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here, we reuse the HD parsing model, plus five additional models trained by the same method.",
                    "sid": 56,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first is trained on the same data, lowercased; the next two use only 19,000 training sentences (for one model, lowercased); the fourth uses 9,500 sentences; the fifth only 3,800 sentences.",
                    "sid": 57,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The 50% data models are closer to the amount of data available to CKK, and the 25% and 10% models are to investigate the effects of further reduced parser quality.",
                    "sid": 58,
                    "ssid": 18,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 experiments",
            "number": "4",
            "sents": [
                {
                    "text": "We conduct a number of experiments with the HD system to attempt to replicate the CKK and HD findings.",
                    "sid": 59,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All parts of the system are available online.1 Each experiment is paired: the reordered system reuses the recasing and language models of its corresponding baseline system, to eliminate one source of possible variation.",
                    "sid": 60,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Training the parser with less data affects only the reordered systems; for experiments using these models, the corresponding baselines (and thus the shared models) are not retrained.",
                    "sid": 61,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each system pair, we also run the HD oracle.",
                    "sid": 62,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CKK uses the PSMT system Pharaoh (Koehn et al., 2003), whereas HD uses its successor Moses (Koehn et al., 2007).",
                    "sid": 63,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In itself, this should not cause a dramatic difference in performance, as the two systems perform similarly (Hoang and Koehn, 2008).",
                    "sid": 64,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, there are a number of other differences between the two systems.",
                    "sid": 65,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Koehn et al. (2003) (and thus presumably CKK) use an unlexicalised distortion model, whereas HD uses a lexicalised model.",
                    "sid": 66,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CKK does not include a tuning (minimum error rate training) phase, unlike HD.",
                    "sid": 67,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, HD uses a 5gram language model.",
                    "sid": 68,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The CKK language model is unspecified; we assume a 3-gram model would be more likely for the time.",
                    "sid": 69,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We explore combinations of all these choices.",
                    "sid": 70,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A likely cause of the results difference between HD and CKK is the data used.",
                    "sid": 71,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CKK used Europarl for training and test, while HD used Europarl and news for training, with news for tuning and test.",
                    "sid": 72,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our first experiment attempts to replicate CKK as closely as possible, using the CKK training and test data.",
                    "sid": 73,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This data came already tokenized and lowercased; we thus skip tokenisation in preprocessing, use the lowercased parsing models, and skip tokenisation and casing steps in the PSMT system.",
                    "sid": 74,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We try both the full data and 50% data parsing models.",
                    "sid": 75,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our next experiments use untokenised and cased text from the Workshop on Statistical Machine Translation.",
                    "sid": 76,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To remain close to CKK, we use data from the 2009 Workshop,2 which provided Europarl sets for both training and development.",
                    "sid": 77,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use europarl-v4 for training, test2007 for tuning, and test2008 for testing.",
                    "sid": 78,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also run the 3-gram systems of this set with each of the reduced parser models.",
                    "sid": 79,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our final experiments start to bridge the gap to HD.",
                    "sid": 80,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We still train on europarl-v4 (diverging from HD), but substitute one or both of the tuning and test sets with those of HD: news-test2008 and newstest2009 from the 2010 Workshop.3 For the language model, HD uses both Europarl and news text.",
                    "sid": 81,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To remain close to CKK, we train our language models only on the Europarl training data, and thus use considerably less data than HD here.",
                    "sid": 82,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All systems are evaluated using case-insensitive BLEU (Papineni et al., 2002).",
                    "sid": 83,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "HD used the NIST BLEU scorer, which requires SGML format.",
                    "sid": 84,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The CKK data is plain text, so instead we report scores from the Moses multi-reference BLEU script (multibleu), using one reference translation.",
                    "sid": 85,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Comparing the scripts, we found that the NIST scores are always lower than multi-bleu\u2019s on test2008, but higher on newstest2009, with differences at most 0.23.",
                    "sid": 86,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This partially indicates the noise level in the scores.",
                    "sid": 87,
                    "ssid": 29,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5 results",
            "number": "5",
            "sents": [
                {
                    "text": "Results for the first experiments, closely replicating CKK, are given in Table 3.",
                    "sid": 88,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results are very similar to the those CKK reported (baseline 25.2, reordered 26.8).",
                    "sid": 89,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus the HD reimplementation is indeed close to the original CKK system.",
                    "sid": 90,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Any qualitative differences in parser output not revealed by \u00a73, in the implementation of the rules, or in the PSMT system, are thus producing only a small effect.",
                    "sid": 91,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Results for the remaining experiments are given in Tables 4 and 5, which give results on the test2008 and newstest2009 test sets respectively, and Table 6, which gives results on the test2008 test set using the reduced parsing models.",
                    "sid": 92,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We see that the choice of data can have a profound effect, nullifying or even reversing the overall result, even when the reordering system remains the same.",
                    "sid": 93,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Genre differences are an obvious possibility, but we have demonstrated only a dependence on data set.",
                    "sid": 94,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The other factors tested\u2014language model order, lexicalisation of the distortion model, and use of a tuning phase\u2014can all affect the overall performance gain of the reordered system, but less distinctly.",
                    "sid": 95,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Reducing the quality of the parsing model (by training on less data) also has a negative effect, but the drop must be substantial before it outweighs other factors.",
                    "sid": 96,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In all cases, the oracle outperforms both baseline and reordered systems by a large margin.",
                    "sid": 97,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Its selections show that, in changing test sets, the balance shifts from one system to the other, but both still contribute strongly.",
                    "sid": 98,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This shows that improvements are possible across the board if it is possible to correctly choose which sentences will benefit from reordering.",
                    "sid": 99,
                    "ssid": 12,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "6 conclusion",
            "number": "6",
            "sents": [
                {
                    "text": "Collins et al. (2005) reported that a reorderingas-preprocessing approach improved overall performance in German-to-English translation.",
                    "sid": 100,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The reimplementation of this system by Howlett and Dras (2010) came to the opposite conclusion.",
                    "sid": 101,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have systematically varied several aspects of the Howlett and Dras (2010) system and reproduced results close to both papers, plus a full range in between.",
                    "sid": 102,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our results show that choices in the PSMT system can completely erode potential gains of the reordering preprocessing step, with the largest effect due to simple choice of data.",
                    "sid": 103,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have shown that a lack of overall improvement using reordering-aspreprocessing need not be due to the usual suspects, language pair and reordering process.",
                    "sid": 104,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Significantly, our oracle experiments show that in all cases the reordering system does produce better translations for some sentences.",
                    "sid": 105,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We conclude that effort is best directed at determining for which sentences the improvement will appear.",
                    "sid": 106,
                    "ssid": 7,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgements",
            "number": "7",
            "sents": [
                {
                    "text": "Our thanks to Michael Collins for providing the data used in Collins et al. (2005), and to members of the Centre for Language Technology and the anonymous reviewers for their helpful comments.",
                    "sid": 107,
                    "ssid": 1,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}