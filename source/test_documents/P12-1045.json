{
    "ID": "P12-1045",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Fast Online Lexicon Learning for Grounded Language Acquisition",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Learning a semantic lexicon is often an important first step in building a system that learns to interpret the meaning of natural language.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is especially important in language grounding where the training data usually consist of language paired with an ambiguous perceptual context.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recent work by Chen and Mooney (2011) introduced a lexicon learning method that deals with ambiguous relational data by taking intersections of graphs.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While the algorithm produced good lexicons for the task of learning to interpret navigation instructions, it only works in batch settings and does not scale well to large datasets.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper we introduce a new online algorithm that is an order of magnitude faster and surpasses the stateof-the-art results.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We show that by changing the grammar of the formal meaning representation language and training on additional data collected from Amazon\u2019s Mechanical Turk we can further improve the results.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also include experimental results on a Chinese translation of the training data to demonstrate the generality of our approach.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Learning to understand the semantics of human languages has been one of the ultimate goals of natural language processing (NLP).",
                    "sid": 8,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Traditional learning approaches have relied on access to parallel corpora of natural language sentences paired with their meanings (Mooney, 2007; Zettlemoyer and Collins, 2007; Lu et al., 2008; Kwiatkowski et al., 2010).",
                    "sid": 9,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, constructing such semantic annotations can be difficult and time-consuming.",
                    "sid": 10,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More recently, there has been work on learning from ambiguous supervision where a set of potential sentence meanings are given, only one (or a small subset) of which are correct (Chen and Mooney, 2008; Liang et al., 2009; Bordes et al., 2010; Chen and Mooney, 2011).",
                    "sid": 11,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given the training data, the system needs to infer the correcting meaning for each training sentence.",
                    "sid": 12,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Building a lexicon of the formal meaning representations of words and phrases, either implicitly or explicitly, is usually an important step in inferring the meanings of entire sentences.",
                    "sid": 13,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, Chen and Mooney (2011) first learned a lexicon to help them resolve ambiguous supervision of relational data in which the number of choices is exponential.",
                    "sid": 14,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They represent the perceptual context as a graph and allow each sentence in the training data to align to any connected subgraph.",
                    "sid": 15,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Their lexicon learning algorithm finds the common connected subgraph that occurs with a word by taking intersections of the graphs that represent the different contexts in which the word appears.",
                    "sid": 16,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While the algorithm produced a good lexicon for their application of learning to interpret navigation instructions, it only works in batch settings and does not scale well to large datasets.",
                    "sid": 17,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper we introduce a novel online algorithm that is an order of magnitude faster and also produces better results on their navigation task.",
                    "sid": 18,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to the new lexicon learning algorithm, we also look at modifying the meaning representation grammar (MRG) for their formal semantic language.",
                    "sid": 19,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By using a MRG that correlates better to the structure of natural language, we further improve the performance on the navigation task.",
                    "sid": 20,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since our algorithm can scale to larger datasets, we present results on collecting and training on additional data from Amazon\u2019s Mechanical Turk.",
                    "sid": 21,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we show the generality of our approach by demonstrating our system\u2019s ability to learn from a Chinese translation of the training data.",
                    "sid": 22,
                    "ssid": 15,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "2 background",
            "number": "2",
            "sents": [
                {
                    "text": "A common way to learn a lexicon across many different contexts is to find the common parts of the formal representations associated with different occurrences of the same words or phrases (Siskind, 1996).",
                    "sid": 23,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For graphical representations, this involves finding the common subgraph between multiple graphs (Thompson and Mooney, 2003; Chen and Mooney, 2011).",
                    "sid": 24,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section we review the lexicon learning algorithm introduced by Chen and Mooney (2011) as well as the overall task they designed to test semantic understanding of navigation instructions.",
                    "sid": 25,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The goal of the navigation task is to build a system that can understand free-form natural-language instructions and follow them to move to the intended destination (MacMahon et al., 2006; Shimizu and Haas, 2009; Matuszek et al., 2010; Kollar et al., 2010; Vogel and Jurafsky, 2010; Chen and Mooney, 2011).",
                    "sid": 26,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Chen and Mooney (2011) defined a learning task in which the only supervision the system receives is in the form of observing how humans behave when following sample navigation instructions in a virtual world.",
                    "sid": 27,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Formally, the system is given training data in the form: {(e1, a1, w1), (e2, a2, w2),... , (en, an, wn)}, where ei is a written natural language instruction, ai is an observed action sequence, and wi is a description of the virtual world.",
                    "sid": 28,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The goal is then to build a system that can produce the correct aj given a previously unseen (ej, wj) pair.",
                    "sid": 29,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the observed actions ai only consists of low-level actions (e.g. turn left, turn right, walk forward) and not high-level concepts (e.g. turn your back against the wall and walk to the couch), Chen and Mooney first use a set of rules to automatically construct the space of reasonable plans using the action trace and knowledge about the world.",
                    "sid": 30,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The space is represented compactly using a graph as shown in Given that these landmarks plans contain a lot of extraneous details, Chen and Mooney learn a lexicon and use it to identify and remove the irrelevant parts of the plans.",
                    "sid": 31,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They use a greedy method to remove nodes from the graphs that are not associated with any of the words in the instructions.",
                    "sid": 32,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The remaining refined landmarks plans are then treated as supervised training data for a semantic-parser learner, KRISP (Kate and Mooney, 2006).",
                    "sid": 33,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Once a semantic parser is trained, it can be used at test time to transform novel instructions into formal navigation plans which are then carried out by a virtual robot (MacMahon et al., 2006).",
                    "sid": 34,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The central component of the system is the lexicon learning process which associates words and short phrases (n-grams) to their meanings (connected graphs).",
                    "sid": 35,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To learn the meaning of an n-gram w, Chen and Mooney first collect all navigation plans g that co-occur with w. This forms the initial candidate meaning set for w. They then repeatedly take the intersections between the candidate meanings to generate additional candidate meanings.",
                    "sid": 36,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They use the term intersection to mean a maximal common subgraph (i.e. it is not a subgraph of any other common subgraphs).",
                    "sid": 37,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In general, there are multiple possible intersections between two graphs.",
                    "sid": 38,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this case, they bias toward finding large connected components by greedily removing the largest common connected subgraph from both graphs until the two graphs have no overlapping nodes.",
                    "sid": 39,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The output of the intersection process consists of all the removed subgraphs.",
                    "sid": 40,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An example of the intersection operation is shown in Figure 1.",
                    "sid": 41,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Once the list of candidate meanings are generated, they are ranked by the following scoring metric for an n-gram w and a graph g: Intuitively, the score measures how much more likely a graph g appears when w is present compared to when it is not.",
                    "sid": 42,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The probabilities are estimated by counting how many examples contain the word w or graph g, ignoring multiple occurrences in a single example.",
                    "sid": 43,
                    "ssid": 21,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 online lexicon learning algorithm",
            "number": "3",
            "sents": [
                {
                    "text": "While the algorithm presented by Chen and Mooney (2011) produced good lexicons, it only works in batch settings and does not scale well to large datasets.",
                    "sid": 44,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The bottleneck of their algorithm is the intersection process which is time-consuming to perform.",
                    "sid": 45,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, their algorithm requires taking many intersections between many different graphs.",
                    "sid": 46,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Even though they use beam-search to limit the size of the candidate set, if the initial candidate meaning set for a n-gram is large, it can take a long time to take just one pass through the list of all candidates.",
                    "sid": 47,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, reducing the beam size could also hurt the quality of the lexicon learned.",
                    "sid": 48,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section, we present another lexicon learning algorithm that is much faster and works in an online setting.",
                    "sid": 49,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The main insight is that most words or short phrases correspond to small graphs.",
                    "sid": 50,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, we concentrate our attention on only candidate meanings that are less than a certain size.",
                    "sid": 51,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using this constraint, we generate all the potential small connected subgraphs for each navigation plan in the training examples and discard the original graph.",
                    "sid": 52,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pseudocode for the new algorithm, Subgraph Generation Online Lexicon Learning (SGOLL) algorithm, is shown in Algorithm 1.",
                    "sid": 53,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As we encounter each new training example which consists of a written navigation instruction",
                    "sid": 54,
                    "ssid": 11,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "algorithm 1 subgraph generation online lexicon learning (sgoll)",
            "number": "4",
            "sents": [
                {
                    "text": "input A sequence of navigation instructions and the corresponding navigation plans (e1, p1), ... , (en, pn) and the corresponding navigation plan, we first segment the instruction into word tokens and construct n-grams from them.",
                    "sid": 55,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From the corresponding navigation plan, we find all connected subgraphs of size less than or equal to m. We then update the cooccurrence counts between all the n-grams w and all the connected subgraphs g. We also update the counts of how many examples we have encountered so far and counts of the n-grams w and subgraphs g. At any given time, we can compute a lexicon using these various counts.",
                    "sid": 56,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Specifically, for each n-gram w, we look at all the subgraphs g that cooccurred with it, and compute a score for the pair (w, g).",
                    "sid": 57,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the score is higher than the threshold t, we add the entry (w, g) to the lexicon.",
                    "sid": 58,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the same scoring function as Chen and Mooney, which can be computed efficiently using the counts we keep.",
                    "sid": 59,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast to Chen and Mooney\u2019s algorithm though, we add the constraint of minimum support by not creating lexical entries for any n-gram w that appeared in less than minSup training examples.",
                    "sid": 60,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is to prevent rarely seen n-grams from receiving high scores in our lexicon simply due to their sparsity.",
                    "sid": 61,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unless otherwise specified, we compute lexical entries for up to 4-grams with threshold t = 0.4, maximum subgraph size m = 3, and minimum support minSup = 10.",
                    "sid": 62,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It should be noted that SGOLL can also become computationally intractable if the sizes of the navigations plans are large or if we set the maximum subgraph size m to a large number.",
                    "sid": 63,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, the memory requirement can be quite high if there are many different subgraphs g associated with each ngram w. To deal with such scalability issues, we could use beam-search and only keep the top k candidates associated with each w. Another important step is to define canonical orderings of the nodes in the graphs.",
                    "sid": 64,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This allows us to determine if two graphs are identical in constant time and also lets us use a hash table to quickly update the co-occurrence and subgraph counts.",
                    "sid": 65,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, even given a large number of subgraphs for each training example, each subgraph can be processed very quickly.",
                    "sid": 66,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, this algorithm readily lends itself to being parallelized.",
                    "sid": 67,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each processor would get a fraction of the training data and compute the counts individually.",
                    "sid": 68,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then the counts can be merged together at the end to produce the final lexicon.",
                    "sid": 69,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to introducing a new lexicon learning algorithm, we also made another modification to the original system proposed by Chen and Mooney (2011).",
                    "sid": 70,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To train a semantic parser using KRISP (Kate and Mooney, 2006), they had to supply a MRG, a context-free grammar, for their formal navigation plan language.",
                    "sid": 71,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "KRISP learns string-kernel classifiers that maps natural language substrings to MRG production rules.",
                    "sid": 72,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consequently, it is important that the production rules in the MRG mirror the structure of natural language (Kate, 2008).",
                    "sid": 73,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The original MRG used by Chen and Mooney is a compact grammar that contains many recursive rules that can be used to generate an infinite number of actions or arguments.",
                    "sid": 74,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While these rules are quite expressive, they often do not correspond well to any words or phrases in natural language.",
                    "sid": 75,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To alleviate this problem, we designed another MRG by expanding out many of the rules.",
                    "sid": 76,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the original MRG contained the following production rules for generating an infinite number of travel actions from the root symbol S. We expand out the production rules as shown below to map S directly to specific travel actions so they correspond better to patterns such as \u201cgo forward\u201d or \u201cwalk N steps\u201d.",
                    "sid": 77,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While this process of expanding the production rules resulted in many more rules, these expanded rules usually correspond better with words or phrases in natural language.",
                    "sid": 78,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We still retain some of the recursive rules to ensure that the formal language remains as expressive as before.",
                    "sid": 79,
                    "ssid": 25,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 collecting additional data with mechanical turk",
            "number": "5",
            "sents": [
                {
                    "text": "One of the motivations for studying ambiguous supervision is the potential ease of acquiring large amounts of training data.",
                    "sid": 80,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Without requiring semantic annotations, a human only has to demonstrate how language is used in context which is generally simple to do.",
                    "sid": 81,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We validate this claim by collecting additional training data for the navigation domain using Mechanical Turk (Snow et al., 2008).",
                    "sid": 82,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are two types of data we are interested in collecting: natural language navigation instructions and follower data.",
                    "sid": 83,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, we created two tasks on Mechanical Turk.",
                    "sid": 84,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first one asks the workers to supply instructions for a randomly generated sequence of actions.",
                    "sid": 85,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second one asks the workers to try to follow a given navigation instruction in our virtual environment.",
                    "sid": 86,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The latter task is used to generate the corresponding action sequences for instructions collected from the first task.",
                    "sid": 87,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To facilitate the data collection, we first recreated the 3D environments used to collect the original data (MacMahon et al., 2006).",
                    "sid": 88,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We built a Java application that allows the user to freely navigate the three virtual worlds constructed by MacMahon et al. (2006) using the discrete controls of turning left, turning right, and moving forward one step.",
                    "sid": 89,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The follower task is fairly straightforward using our application.",
                    "sid": 90,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The worker is given a navigation instruction and placed at the starting location.",
                    "sid": 91,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They are asked to follow the navigation instruction as best as they could using the three discrete controls.",
                    "sid": 92,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They could also skip the problem if they did not understand the instruction or if the instruction did not describe a viable route.",
                    "sid": 93,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each Human Intelligence Task (HIT), we asked the worker to complete 5 follower problems.",
                    "sid": 94,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We paid them $0.05 for each HIT, or 1 cent per follower problem.",
                    "sid": 95,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The instructions used for the follower problems were mainly collected from our Mechanical Turk instructor task with some of the instructions coming from data collected by MacMahon (2007) that was not used by Chen and Mooney (2011).",
                    "sid": 96,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The instructor task is slightly more involved because we ask the workers to provide new navigation instructions.",
                    "sid": 97,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The worker is shown a 3D simulation of a randomly generated action sequence between length 1 to 4 and asked to write short, free-form instructions that would lead someone to perform those actions.",
                    "sid": 98,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since this task requires more time to complete, each HIT consists of only 3 instructor problems.",
                    "sid": 99,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, we pay the workers $0.10 for each HIT, or about 3 cents for each instruction they write.",
                    "sid": 100,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To encourage quality contributions, we use a tiered payment structure (Chen and Dolan, 2011) that rewards the good workers.",
                    "sid": 101,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Workers who have been identified to consistently provide good instructions were allowed to do higher-paying version of the same HITs that paid $0.15 instead of $0.10.",
                    "sid": 102,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Over a 2-month period we accepted 2,884 follower HITs and 810 instructor HITs from 653 workers.",
                    "sid": 103,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This corresponds to over 14,000 follower traces and 2,400 instructions with most of them consisting of single sentences.",
                    "sid": 104,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instructions with multiple sentences, we merged all the sentences together and treated it as a single sentence.",
                    "sid": 105,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The total cost of the data collection was $277.92.",
                    "sid": 106,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While there were 2,400 instructions, we filtered them to make sure they were of reasonable quality.",
                    "sid": 107,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we discarded any instructions that did not have at least 5 follower traces.",
                    "sid": 108,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then we looked at all the follower traces and discarded any instruction that did not have majority agreement on what the correct path is.",
                    "sid": 109,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using our strict filter, we were left with slightly over 1,000 instructions.",
                    "sid": 110,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Statistics about the new corpus and the one used by Chen and Mooney can be seen in Table 1.",
                    "sid": 111,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Overall, the new corpus has a slightly smaller vocabulary, and each instruction is slightly shorter both in terms of the number of words and the number of actions.",
                    "sid": 112,
                    "ssid": 33,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5 experiments",
            "number": "6",
            "sents": [
                {
                    "text": "We evaluate our new lexicon learning algorithm as well as the other modifications to the navigation system using the same three tasks as Chen and Mooney (2011).",
                    "sid": 113,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first task is disambiguating the training data by inferring the correct navigation plans associated with each training sentence.",
                    "sid": 114,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second task is evaluating the performance of the semantic parsers trained on the disambiguated data.",
                    "sid": 115,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We measure the performance of both of these tasks by comparing to gold-standard data using the same partial correctness metric used by Chen and Mooney which gives credit to a parse for producing the correct action type and additional credit if the arguments were also correct.",
                    "sid": 116,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, the third task is to complete the end-to-end navigation task.",
                    "sid": 117,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are two versions of this task, the complete task uses the original instructions which are several sentences long and the other version uses instructions that have been manually split into single sentences.",
                    "sid": 118,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Task completion is measured by the percentage of trials in which the system reached the correct destination (and orientation in the single-sentence version).",
                    "sid": 119,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We follow the same evaluation scheme as Chen and Mooney and perform leave-one-map-out experiments.",
                    "sid": 120,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the first task, we build a lexicon using ambiguous training data from two maps, and then use the lexicon to produce the best disambiguated semantic meanings for those same data.",
                    "sid": 121,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the second and third tasks, we train a semantic parser on the automatically disambiguated data, and test on sentences from the third, unseen map.",
                    "sid": 122,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all comparisons to the Chen and Mooney results, we use the performance of their refined landmarks plans system which performed the best overall.",
                    "sid": 123,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, it provides the most direct comparison to our approach since both use a lexicon to refine the landmarks plans.",
                    "sid": 124,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other than the modifications discussed, we use the same components as their system including using KRISP to train the semantic parsers and using the execution module from MacMahon et al. (2006) to carry out the navigation plans.",
                    "sid": 125,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we examine the quality of the refined navigation plans produced using SGOLL\u2019s lexicon.",
                    "sid": 126,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The precision, recall, and F1 (harmonic mean of precision and recall) of these plans are shown in Table 2.",
                    "sid": 127,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Compared to Chen and Mooney, the plans produced by SGOLL has higher precision and lower recall.",
                    "sid": 128,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is mainly due to the additional minimum support constraint we added which discards many noisy lexical entries from infrequently seen n-grams.",
                    "sid": 129,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Next we look at the performance of the semantic parsers trained on the inferred navigation plans.",
                    "sid": 130,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results are shown in Table 3.",
                    "sid": 131,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here SGOLL performs almost the same as Chen and Mooney, with slightly better precision.",
                    "sid": 132,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also look at the effect of changing the MRG.",
                    "sid": 133,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using the new MRG for KRISP to train the semantic parser produced slightly lower precision but higher recall, with similar overall F1 score.",
                    "sid": 134,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we evaluate the system on the end-to-end navigation task.",
                    "sid": 135,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to SGOLL and SGOLL with the new MRG, we also look at augmenting each of the training splits with the data we collected using Mechanical Turk.",
                    "sid": 136,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Completion rates for both the single-sentence tasks and the complete tasks are shown in Table 4.",
                    "sid": 137,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here we see the benefit of each of our modifications.",
                    "sid": 138,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "SGOLL outperforms Chen and Mooney\u2019s system on both versions of the navigation task.",
                    "sid": 139,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using the new MRG to train the semantic parsers further improved performance on both tasks.",
                    "sid": 140,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, augmenting the training data with additional instructions and follower traces collected from Mechanical Turk produced the best results.",
                    "sid": 141,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Having established the superior performance of our new system compared to Chen and Mooney\u2019s, we next look at the computational efficiency of SGOLL.",
                    "sid": 142,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The average time (in seconds) it takes for each algorithm to build a lexicon is shown in Table 5.",
                    "sid": 143,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All the results are obtained running the algorithms on Dell PowerEdge 1950 servers with 2x Xeon X5440 (quad-core) 2.83GHz processors and 32GB of RAM.",
                    "sid": 144,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here SGOLL has a decidedly large advantage over the lexicon learning algorithm from Chen and Mooney, requiring an order of magnitude less time to run.",
                    "sid": 145,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Even after incorporating the new Mechanical Turk data into the training set, SGOLL still takes much less time to build a lexicon.",
                    "sid": 146,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This shows how inefficient it is to perform graph intersection operations and how our online algorithm can more realistically scale to large datasets.",
                    "sid": 147,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to evaluating the system on English data, we also translated the corpus used by Chen and Mooney into Mandarin Chinese.1 To run our system, we first segmented the sentences using the Stanford Chinese Word Segmenter (Chang et al., 2008).",
                    "sid": 148,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluated using the same three tasks as before.",
                    "sid": 149,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This resulted in a precision, recall, and F1 of 87.07, 71.67, and 78.61, respectively for the inferred plans.",
                    "sid": 150,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The trained semantic parser\u2019s precision, recall, and F1 were 88.87, 58.76, and 70.74, respectively.",
                    "sid": 151,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, the system completed 58.70% of the single-sentence task and 20.13% of the complete task.",
                    "sid": 152,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All of these numbers are very similar to the English results, showing the generality of the system in its ability to learn other languages.",
                    "sid": 153,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have introduced a novel, online lexicon learning algorithm that is much faster than the one proposed by Chen and Mooney and also performs better on the navigation tasks they devised.",
                    "sid": 154,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Having a computationally efficient algorithm is critical for building systems that learn from ambiguous supervision.",
                    "sid": 155,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Compared to systems that train on supervised semantic annotations, a system that only receives weak, ambiguous training data is expected to have to train on much larger datasets to achieve similar performance.",
                    "sid": 156,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consequently, such system must be able to scale well in order to keep the learning process tractable.",
                    "sid": 157,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Not only is SGOLL much faster in building a lexicon, it can also be easily parallelized.",
                    "sid": 158,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, the online nature of SGOLL allows the lexicon to be continually updated while the system is in use.",
                    "sid": 159,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A deployed navigation system can gather new instructions from the user and receive feedback about whether it is performing the correct actions.",
                    "sid": 160,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As new training examples are collected, we can update the corresponding n-gram and subgraph counts without rebuilding the entire lexicon.",
                    "sid": 161,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One thing to note though is that while SGOLL makes the lexicon learning step much faster and scalable, another bottleneck in the overall system is training the semantic parser.",
                    "sid": 162,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Existing semanticparser learners such as KRISP were not designed to scale to very large datasets and have trouble training on more than a few thousand examples.",
                    "sid": 163,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, designing new scalable algorithms for learning semantic parsers is critical to scaling the entire system.",
                    "sid": 164,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have performed a pilot data collection of new training examples using Mechanical Turk.",
                    "sid": 165,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Even though the instructions were collected from very different sources (paid human subjects from a university for the original data versus workers recruited over the Internet), we showed that adding the new data into the training set improved the system\u2019s performance on interpreting instructions from the original corpus.",
                    "sid": 166,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It verified that we are indeed collecting useful information and that non-experts are fully capable of training the system by demonstrating how to use natural language in relevant contexts.",
                    "sid": 167,
                    "ssid": 55,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "6 related work",
            "number": "7",
            "sents": [
                {
                    "text": "The earliest work on cross-situational word learning was by Siskind (1996) who developed a rule-based system to solve the referential ambiguity problem.",
                    "sid": 168,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, it did not handle noise and was tested only on artificial data.",
                    "sid": 169,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More recently, Fazly et al. (2010) proposed a probabilistic incremental model that can learn online similar to our algorithm and was tested on transcriptions of child-directed speech.",
                    "sid": 170,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, they generated the semantic representations from the text itself rather than from the environment.",
                    "sid": 171,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, the referential ambiguity was introduced artificially by including the correct semantic representation of the neighboring sentence.",
                    "sid": 172,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our work falls into the larger framework of learning the semantics of language from weak supervision.",
                    "sid": 173,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This problem can be seen as an alignment problem where each sentence in the training data needs to be aligned to one or more records that represent its meaning.",
                    "sid": 174,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Chen and Mooney (2008) previously introduced another task that aligns sportscasting commentaries to events in a simulated soccer game.",
                    "sid": 175,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using an EM-like retraining method, they alternated between building a semantic parser and estimating the most likely alignment.",
                    "sid": 176,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Liang et al. (2009) developed an unsupervised approach using a generative model to solve the alignment problem.",
                    "sid": 177,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They demonstrated improved results on matching sentences and events on the sportscasting task and also introduced a new task of aligning weather forecasts to weather information.",
                    "sid": 178,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Kim and Mooney (2010) further improved the generative alignment model by incorporating the full semantic parsing model from Lu et al. (2008).",
                    "sid": 179,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This resulted in a joint generative model that outperformed all previous results.",
                    "sid": 180,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to treating the ambiguous supervision problem as an alignment problem, there have been other approaches such as treating it as a ranking problem (Bordes et al., 2010), or a PCFG learning problem (Borschinger et al., 2011).",
                    "sid": 181,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Parallel to the work of learning from ambiguous supervision, other recent work has also looked at training semantic parsers from supervision other than logical-form annotations.",
                    "sid": 182,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Clarke et al. (2010) and Liang et al.",
                    "sid": 183,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers.",
                    "sid": 184,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Artzi and Zettlemoyer (2011) use conversation logs between a computer system and a human user to learn to interpret the human utterances.",
                    "sid": 185,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, Goldwasser et al. (2011) presented an unsupervised approach of learning a semantic parser by using an EM-like retraining loop.",
                    "sid": 186,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They use confidence estimation as a proxy for the model\u2019s prediction quality, preferring models that have high confidence about their parses.",
                    "sid": 187,
                    "ssid": 20,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "7 conclusion",
            "number": "8",
            "sents": [
                {
                    "text": "Learning the semantics of language from the perceptual context in which it is uttered is a useful approach because only minimal human supervision is required.",
                    "sid": 188,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper we presented a novel online algorithm for building a lexicon from ambiguously supervised relational data.",
                    "sid": 189,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast to the previous approach that computed common subgraphs between different contexts in which an n-gram appeared, we instead focus on small, connected subgraphs and introduce an algorithm, SGOLL, that is an order of magnitude faster.",
                    "sid": 190,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to being more scalable, SGOLL also performed better on the task of interpreting navigation instructions.",
                    "sid": 191,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, we showed that changing the MRG and collecting additional training data from Mechanical Turk further improve the performance of the overall navigation system.",
                    "sid": 192,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we demonstrated the generality of the system by using it to learn Chinese navigation instructions and achieved similar results.",
                    "sid": 193,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments",
            "number": "9",
            "sents": [
                {
                    "text": "The research in this paper was supported by the National Science Foundation (NSF) under the grants IIS-0712097 and IIS-1016312.",
                    "sid": 194,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We thank Lu Guo for performing the translation of the corpus into Mandarin Chinese.",
                    "sid": 195,
                    "ssid": 2,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}