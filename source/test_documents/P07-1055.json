{
    "ID": "P07-1055",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Structured Models for Fine-to-Coarse Sentiment Analysis",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Experiments show that this method can significantly reduce classification error relative to models trained in isolation.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Extracting sentiment from text is a challenging problem with applications throughout Natural Language Processing and Information Retrieval.",
                    "sid": 5,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previous work on sentiment analysis has covered a wide range of tasks, including polarity classification (Pang et al., 2002; Turney, 2002), opinion extraction (Pang and Lee, 2004), and opinion source assignment (Choi et al., 2005; Choi et al., 2006).",
                    "sid": 6,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, these systems have tackled the problem at different levels of granularity, from the document level (Pang et al., 2002), sentence level (Pang and Lee, 2004; Mao and Lebanon, 2006), phrase level (Turney, 2002; Choi et al., 2005), as well as the speaker level in debates (Thomas et al., 2006).",
                    "sid": 7,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The ability to classify sentiment on multiple levels is important since different applications have different needs.",
                    "sid": 8,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, a summarization system for product reviews might require polarity classification at the sentence or phrase level; a question answering system would most likely require the sentiment of paragraphs; and a system that determines which articles from an online news source are editorial in nature would require a document level analysis.",
                    "sid": 9,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This work focuses on models that jointly classify sentiment on multiple levels of granularity.",
                    "sid": 10,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consider the following example, This is the first Mp3 player that I have used ...",
                    "sid": 11,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "I thought it sounded great ... After only a few weeks, it started having trouble with the earphone connection ...",
                    "sid": 12,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "I won\u2019t be buying another.",
                    "sid": 13,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This excerpt expresses an overall negative opinion of the product being reviewed.",
                    "sid": 14,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, not all parts of the review are negative.",
                    "sid": 15,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first sentence merely provides some context on the reviewer\u2019s experience with such devices and the second sentence indicates that, at least in one regard, the product performed well.",
                    "sid": 16,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We call the problem of identifying the sentiment of the document and of all its subcomponents, whether at the paragraph, sentence, phrase or word level, fine-to-coarse sentiment analysis.",
                    "sid": 17,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The simplest approach to fine-to-coarse sentiment analysis would be to create a separate system for each level of granularity.",
                    "sid": 18,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are, however, obvious advantages to building a single model that classifies each level in tandem.",
                    "sid": 19,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consider the sentence, for a piece of fitness equipment, where hard essen- labeling or chunking, but have also been applied to tially means good workout.",
                    "sid": 20,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this domain, hard\u2019s parsing (Taskar et al., 2004; McDonald et al., 2005), sentiment can only be determined in context (i.e., machine translation (Liang et al., 2006) and summahard to assemble versus a hard workout).",
                    "sid": 21,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the clas- rization (Daum\u00b4e III et al., 2006). sifier knew the overall sentiment of a document, then Structured models have previously been used for disambiguating such cases would be easier. sentiment analysis.",
                    "sid": 22,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Choi et al. (2005, 2006) use Conversely, document level analysis can benefit CRFs to learn a global sequence model to classify from finer level classification by taking advantage and assign sources to opinions.",
                    "sid": 23,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Mao and Lebanon of common discourse cues, such as the last sentence (2006) used a sequential CRF regression model to being a reliable indicator for overall sentiment in re- measure polarity on the sentence level in order to views.",
                    "sid": 24,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, during training, the model will determine the sentiment flow of authors in reviews. not need to modify its parameters to explain phe- Here we show that fine-to-coarse models of sentinomena like the typically positive word great ap- ment can often be reduced to the sequential case. pearing in a negative text (as is the case above).",
                    "sid": 25,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Cascaded models for fine-to-coarse sentiment model can also avoid overfitting to features derived analysis were studied by Pang and Lee (2004).",
                    "sid": 26,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In from neutral or objective sentences.",
                    "sid": 27,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In fact, it has al- that work an initial model classified each sentence ready been established that sentence level classifica- as being subjective or objective using a global mintion can improve document level analysis (Pang and cut inference algorithm that considered local labelLee, 2004).",
                    "sid": 28,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This line of reasoning suggests that a ing consistencies.",
                    "sid": 29,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The top subjective sentences are cascaded approach would also be insufficient.",
                    "sid": 30,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Valu- then input into a standard document level polarity able information is passed in both directions, which classifier with improved results.",
                    "sid": 31,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The current work means any model of fine-to-coarse analysis should differs from that in Pang and Lee through the use of account for this. a single joint structured model for both sentence and In Section 2 we describe a simple structured document level analysis. model that jointly learns and infers sentiment on dif- Many problems in natural language processing ferent levels of granularity.",
                    "sid": 32,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, we reduce can be improved by learning and/or predicting multhe problem of joint sentence and document level tiple outputs jointly.",
                    "sid": 33,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This includes parsing and relaanalysis to a sequential classification problem us- tion extraction (Miller et al., 2000), entity labeling ing constrained Viterbi inference.",
                    "sid": 34,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Extensions to the and relation extraction (Roth and Yih, 2004), and model that move beyond just two-levels of analysis part-of-speech tagging and chunking (Sutton et al., are also presented.",
                    "sid": 35,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Section 3 an empirical eval- 2004).",
                    "sid": 36,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One interesting work on sentiment analysis uation of the model is given that shows significant is that of Popescu and Etzioni (2005) which attempts gains in accuracy over both single level classifiers to classify the sentiment of phrases with respect to and cascaded systems. possible product features.",
                    "sid": 37,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To do this an iterative al1.1 Related Work gorithm is used that attempts to globally maximize The models in this work fall into the broad class of the classification of all phrases while satisfying local global structured models, which are typically trained consistency constraints. with structured learning algorithms.",
                    "sid": 38,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hidden Markov 2 Structured Model models (Rabiner, 1989) are one of the earliest struc- In this section we present a structured model for tured learning algorithms, which have recently been fine-to-coarse sentiment analysis.",
                    "sid": 39,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We start by examfollowed by discriminative learning approaches such ining the simple case with two-levels of granularity as conditional random fields (CRFs) (Lafferty et al., \u2013 the sentence and document \u2013 and show that the 2001; Sutton and McCallum, 2006), the structured problem can be reduced to sequential classification perceptron (Collins, 2002) and its large-margin vari- with constrained inference.",
                    "sid": 40,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then discuss the feaants (Taskar et al., 2003; Tsochantaridis et al., 2004; ture space and give an algorithm for learning the paMcDonald et al., 2005; Daum\u00b4e III et al., 2006). rameters based on large-margin structured learning.",
                    "sid": 41,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These algorithms are usually applied to sequential 433 Extensions to the model are also examined.",
                    "sid": 42,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let Y(d) be a discrete set of sentiment labels at the document level and Y(s) be a discrete set of sentiment labels at the sentence level.",
                    "sid": 43,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As input a system is given a document containing sentences s = s1, ... , sn and must produce sentiment labels for the document, yd E Y(d), and each individual sentence, ys = ys1, ... , ysn, where ysi E Y(s) V 1 G i G n. Define y = (yd, ys) = (yd, ys1, .",
                    "sid": 44,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "..,ysn) as the joint labeling of the document and sentences.",
                    "sid": 45,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, in Pang and Lee (2004), yd would be the polarity of the document and ysi would indicate whether sentence si is subjective or objective.",
                    "sid": 46,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The models presented here are compatible with arbitrary sets of discrete output labels.",
                    "sid": 47,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1 presents a model for jointly classifying the sentiment of both the sentences and the document.",
                    "sid": 48,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this undirected graphical model, the label of each sentence is dependent on the labels of its neighbouring sentences plus the label of the document.",
                    "sid": 49,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The label of the document is dependent on the label of every sentence.",
                    "sid": 50,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that the edges between the input (each sentence) and the output labels are not solid, indicating that they are given as input and are not being modeled.",
                    "sid": 51,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The fact that the sentiment of sentences is dependent not only on the local sentiment of other sentences, but also the global document sentiment \u2013 and vice versa \u2013 allows the model to directly capture the importance of classification decisions across levels in fine-tocoarse sentiment analysis.",
                    "sid": 52,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The local dependencies between sentiment labels on sentences is similar to the work of Pang and Lee (2004) where soft local consistency constraints were created between every sentence in a document and inference was solved using a min-cut algorithm.",
                    "sid": 53,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, jointly modeling the document label and allowing for non-binary labels complicates min-cut style solutions as inference becomes intractable.",
                    "sid": 54,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Learning and inference in undirected graphical models is a well studied problem in machine learning and NLP.",
                    "sid": 55,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, CRFs define the probability over the labels conditioned on the input using the property that the joint probability distribution over the labels factors over clique potentials in undirected graphical models (Lafferty et al., 2001).",
                    "sid": 56,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this work we will use structured linear classifiers (Collins, 2002).",
                    "sid": 57,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We denote the score of a labeling y for an input s as score(y, s) and define this score as the sum of scores over each clique, where each clique score is a linear combination of features and their weights, score(yd, ysi\ufffd1, ysi , s) = w \u00b7 f(yd, ysi\ufffd1, ysi , s) (1) and f is a high dimensional feature representation of the clique and w a corresponding weight vector.",
                    "sid": 58,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that s is included in each score since it is given as input and can always be conditioned on.",
                    "sid": 59,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In general, inference in undirected graphical models is intractable.",
                    "sid": 60,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, for the common case of sequences (a.k.a. linear-chain models) the Viterbi algorithm can be used (Rabiner, 1989; Lafferty et al., 2001).",
                    "sid": 61,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Fortunately there is a simple technique that reduces inference in the above model to sequence classification with a constrained version of Viterbi.",
                    "sid": 62,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The inference problem is to find the highest scoring labeling y for an input s, i.e., arg max score(y, s) Y If the document label yd is fixed, then inference in the model from Figure 1 reduces to the sequential case.",
                    "sid": 63,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is because the search space is only over the sentence labels ysi , whose graphical structure forms a chain.",
                    "sid": 64,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus the problem of finding the The argmax in line 3 can be solved using Viterbi\u2019s algorithm since yd is fixed. highest scoring sentiment labels for all sentences, given a particular document label yd, can be solved efficiently using Viterbi\u2019s algorithm.",
                    "sid": 65,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The general inference problem can then be solved by iterating over each possible yd, finding y8 maximizing score((yd, y8), s) and keeping the single best y = (yd, y8).",
                    "sid": 66,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This algorithm is outlined in Figure 2 and has a runtime of O(|Y(d)||Y(s)|2n), due to running Viterbi |Y(d) |times over a label space of size |Y(s)|.",
                    "sid": 67,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The algorithm can be extended to produce exact k-best lists.",
                    "sid": 68,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is achieved by using k-best Viterbi techniques to return the k-best global labelings for each document label in line 3.",
                    "sid": 69,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Merging these sets will produce the final k-best list.",
                    "sid": 70,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is possible to view the inference algorithm in Figure 2 as a constrained Viterbi search since it is equivalent to flattening the model in Figure 1 to a sequential model with sentence labels from the set Y(s) x Y(d).",
                    "sid": 71,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The resulting Viterbi search would then need to be constrained to ensure consistent solutions, i.e., the label assignments agree on the document label over all sentences.",
                    "sid": 72,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If viewed this way, it is also possible to run a constrained forwardbackward algorithm and learn the parameters for CRFs as well.",
                    "sid": 73,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section we define the feature representation for each clique, f(yd, ysi_1, ysi , s).",
                    "sid": 74,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Assume that each sentence si is represented by a set of binary predicates P(si).",
                    "sid": 75,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This set can contain any predicate over the input s, but for the present purposes it will include all the unigram, bigram and trigrams in the sentence si conjoined with their part-of-speech (obtained from an automatic classifier).",
                    "sid": 76,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Back-offs of each predicate are also included where one or more word is discarded.",
                    "sid": 77,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, if P(si) contains the predicate a:DT great:JJ product:NN, then it would also have the predicates a:DT great:JJ *:NN, a:DT *:JJ product:NN, *:DT great:JJ product:NN, a:DT *:JJ *:NN, etc.",
                    "sid": 78,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each predicate, p, is then conjoined with the label information to construct a binary feature.",
                    "sid": 79,
                    "ssid": 75,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, if the sentence label set is Y(s) = {subj, obj} and the document set is Y(d) = {pos, neg}, then the system might contain the following feature, { 1 if p E P(si) and y!",
                    "sid": 80,
                    "ssid": 76,
                    "kind_of_tag": "s"
                },
                {
                    "text": "= obj and ysi = subj and yd = neg 0 otherwise Where f(j) is the jth dimension of the feature space.",
                    "sid": 81,
                    "ssid": 77,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each feature, a set of back-off features are included that only consider the document label yd, the current sentence label ysi , the current sentence and document label ysi and yd, and the current and previous sentence labels ysi and ysi\ufffd1.",
                    "sid": 82,
                    "ssid": 78,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that through these back-off features the joint models feature set will subsume the feature set of any individual level model.",
                    "sid": 83,
                    "ssid": 79,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Only features observed in the training data were considered.",
                    "sid": 84,
                    "ssid": 80,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Depending on the data set, the dimension of the feature vector f ranged from 350K to 500K.",
                    "sid": 85,
                    "ssid": 81,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Though the feature vectors can be sparse, the feature weights will be learned using large-margin techniques that are well known to be robust to large and sparse feature representations.",
                    "sid": 86,
                    "ssid": 82,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let Y = Y(d) x Y(s)n be the set of all valid sentence-document labelings for an input s. The weights, w, are set using the MIRA learning algorithm, which is an inference based online largemargin learning technique (Crammer and Singer, 2003; McDonald et al., 2005).",
                    "sid": 87,
                    "ssid": 83,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An advantage of this algorithm is that it relies only on inference to learn the weight vector (see Section 2.1.1).",
                    "sid": 88,
                    "ssid": 84,
                    "kind_of_tag": "s"
                },
                {
                    "text": "MIRA has been shown to provide state-of-the-art accuracy for many language processing tasks including parsing, chunking and entity extraction (McDonald, 2006).",
                    "sid": 89,
                    "ssid": 85,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The basic algorithm is outlined in Figure 3.",
                    "sid": 90,
                    "ssid": 86,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The algorithm works by considering a single training instance during each iteration.",
                    "sid": 91,
                    "ssid": 87,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The weight vector w is updated in line 4 through a quadratic programming problem.",
                    "sid": 92,
                    "ssid": 88,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This update modifies the weight vector so a margin proportional to the loss.",
                    "sid": 93,
                    "ssid": 89,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The constraint set C can be chosen arbitrarily, but it is usually taken to be the k labelings that have the highest score under the old weight vector w(z) (McDonald et al., 2005).",
                    "sid": 94,
                    "ssid": 90,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this manner, the learning algorithm can update its parameters relative to those labelings closest to the decision boundary.",
                    "sid": 95,
                    "ssid": 91,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Of all the weight vectors that satisfy these constraints, MIRA chooses the one that is as close as possible to the previous weight vector in order to retain information about previous updates.",
                    "sid": 96,
                    "ssid": 92,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The loss function L(y, y') is a positive real valued function and is equal to zero when y = y'.",
                    "sid": 97,
                    "ssid": 93,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This function is task specific and is usually the hamming loss for sequence classification problems (Taskar et al., 2003).",
                    "sid": 98,
                    "ssid": 94,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Experiments with different loss functions for the joint sentence-document model on a development data set indicated that the hamming loss over sentence labels multiplied by the 0-1 loss over document labels worked best.",
                    "sid": 99,
                    "ssid": 95,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An important modification that was made to the learning algorithm deals with how the k constraints are chosen for the optimization.",
                    "sid": 100,
                    "ssid": 96,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Typically these constraints are the k highest scoring labelings under the current weight vector.",
                    "sid": 101,
                    "ssid": 97,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, early experiments showed that the model quickly learned to discard any labeling with an incorrect document label for the instances in the training set.",
                    "sid": 102,
                    "ssid": 98,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a result, the constraints were dominated by labelings that only differed over sentence labels.",
                    "sid": 103,
                    "ssid": 99,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This did not allow the algorithm adequate opportunity to set parameters relative to incorrect document labeling decisions.",
                    "sid": 104,
                    "ssid": 100,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To combat this, k was divided by the number of document labels, to get a new value k'.",
                    "sid": 105,
                    "ssid": 101,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each document label, the k' highest scoring labelings were To this point, we have focused solely on a model for two-level fine-to-coarse sentiment analysis not only for simplicity, but because the experiments in Section 3 deal exclusively with this scenario.",
                    "sid": 106,
                    "ssid": 102,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section, we briefly discuss possible extensions for more complex situations.",
                    "sid": 107,
                    "ssid": 103,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, longer documents might benefit from an analysis on the paragraph level as well as the sentence and document levels.",
                    "sid": 108,
                    "ssid": 104,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One possible model for this case is given in Figure 4, which essentially inserts an additional layer between the sentence and document level from the original model.",
                    "sid": 109,
                    "ssid": 105,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sentence level analysis is dependent on neighbouring sentences as well as the paragraph level analysis, and the paragraph analysis is dependent on each of the sentences within it, the neighbouring paragraphs, and the document level analysis.",
                    "sid": 110,
                    "ssid": 106,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This can be extended to an arbitrary level of fine-to-coarse sentiment analysis by simply inserting new layers in this fashion to create more complex hierarchical models.",
                    "sid": 111,
                    "ssid": 107,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The advantage of using hierarchical models of this form is that they are nested, which keeps inference tractable.",
                    "sid": 112,
                    "ssid": 108,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Observe that each pair of adjacent levels in the model is equivalent to the original model from Figure 1.",
                    "sid": 113,
                    "ssid": 109,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a result, the scores of the every label at each node in the graph can be calculated with a straight-forward bottom-up dynamic programming algorithm.",
                    "sid": 114,
                    "ssid": 110,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Details are omitted Three baseline systems were created, for space reasons.",
                    "sid": 115,
                    "ssid": 111,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other models are possible where dependencies occur across non-neighbouring levels, e.g., by inserting edges between the sentence level nodes and the document level node.",
                    "sid": 116,
                    "ssid": 112,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the general case, inference is exponential in the size of each clique.",
                    "sid": 117,
                    "ssid": 113,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both the models in Figure 1 and Figure 4 have maximum clique sizes of three.",
                    "sid": 118,
                    "ssid": 114,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 experiments",
            "number": "2",
            "sents": [
                {
                    "text": "To test the model we compiled a corpus of 600 online product reviews from three domains: car seats for children, fitness equipment, and Mp3 players.",
                    "sid": 119,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Of the original 600 reviews that were gathered, we discarded duplicate reviews, reviews with insufficient text, and spam.",
                    "sid": 120,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All reviews were labeled by online customers as having a positive or negative polarity on the document level, i.e., Y(d) _ {pos, neg}.",
                    "sid": 121,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each review was then split into sentences and every sentence annotated by a single annotator as either being positive, negative or neutral, i.e., Y( s) _ {pos, neg, neu}.",
                    "sid": 122,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Data statistics for the corpus are given in Table 1.",
                    "sid": 123,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All sentences were annotated based on their context within the document.",
                    "sid": 124,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sentences were annotated as neutral if they conveyed no sentiment or had indeterminate sentiment from their context.",
                    "sid": 125,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Many neutral sentences pertain to the circumstances under which the product was purchased.",
                    "sid": 126,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A common class of sentences were those containing product features.",
                    "sid": 127,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These sentences were annotated as having positive or negative polarity if the context supported it.",
                    "sid": 128,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This could include punctuation such as exclamation points, smiley/frowny faces, question marks, etc.",
                    "sid": 129,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The supporting evidence could also come from another sentence, e.g., \u201cI love it.",
                    "sid": 130,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It has 64Mb of memory and comes with a set of earphones\u201d. ure 1 without the top level document node.",
                    "sid": 131,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This baseline will help to gage the empirical gains of the different components of the joint structured model on sentence level classification.",
                    "sid": 132,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The model described in Section 2 will be called Joint-Structured.",
                    "sid": 133,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All models use the same basic predicate space: unigram, bigram, trigram conjoined with part-of-speech, plus back-offs of these (see Section 2.1.2 for more).",
                    "sid": 134,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, due to the structure of the model and its label space, the feature space of each might be different, e.g., the document classifier will only conjoin predicates with the document label to create the feature set.",
                    "sid": 135,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All models are trained using the MIRA learning algorithm.",
                    "sid": 136,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Results for each model are given in the first four rows of Table 2.",
                    "sid": 137,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These results were gathered using 10-fold cross validation with one fold for development and the other nine folds for evaluation.",
                    "sid": 138,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This table shows that classifying sentences in isolation from one another is inferior to accounting for a more global context.",
                    "sid": 139,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A significant increase in performance can be obtained when labeling decisions between sentences are modeled (Sentence-Structured).",
                    "sid": 140,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More interestingly, even further gains can be had when document level decisions are modeled (JointStructured).",
                    "sid": 141,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In many cases, these improvements are highly statistically significant.",
                    "sid": 142,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the document level, performance can also be improved by incorporating sentence level decisions \u2013 though these improvements are not consistent.",
                    "sid": 143,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This inconsistency may be a result of the model overfitting on the small set of training data.",
                    "sid": 144,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We suspect this because the document level error rate is a slight improvement in performance suggesting on the Mp3 training set converges to zero much that an iterative approach might be beneficial.",
                    "sid": 145,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That more rapidly for the Joint-Structured model than the is, a system could start by classifying documents, Document-Classifier.",
                    "sid": 146,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This suggests that the Joint- use the document information to classify sentences, Structured model might be relying too much on use the sentence information to classify documents, the sentence level sentiment features \u2013 in order to and repeat until convergence.",
                    "sid": 147,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, experiments minimize its error rate \u2013 instead of distributing the showed that this did not improve accuracy over a sinweights across all features more evenly. gle iteration and often hurt performance.",
                    "sid": 148,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One interesting application of sentence level sen- Improvements from the cascaded models are far timent analysis is summarizing product reviews on less consistent than those given from the joint strucretail websites like Amazon.com or review aggrega- ture model.",
                    "sid": 149,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is because decisions in the castors like Yelp.com.",
                    "sid": 150,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this setting the correct polar- caded system are passed to the next layer as the ity of a document is often known, but we wish to \u201cgold\u201d standard at test time, which results in errors label sentiment on the sentence or phrase level to from the first classifier propagating to errors in the aid in generating a cohesive and informative sum- second.",
                    "sid": 151,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This could be improved by passing a lattice mary.",
                    "sid": 152,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The joint model can be used to classify sen- of possibilities from the first classifier to the second tences in this setting by constraining inference to the with corresponding confidences.",
                    "sid": 153,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, solutions known fixed document label for a review.",
                    "sid": 154,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If this is such as these are really just approximations of the done, then sentiment accuracy on the sentence level joint structured model that was presented here. increases substantially from 62.6% to 70.3%.",
                    "sid": 155,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4 Future Work Finally we should note that experiments using One important extension to this work is to augment CRFs to train the structured models and logistic re- the models for partially labeled data.",
                    "sid": 156,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is realistic gression to train the local models yielded similar re- to imagine a training set where many examples do sults to those in Table 2. not have every level of sentiment annotated.",
                    "sid": 157,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For 3.2.1 Cascaded Models example, there are thousands of online product reAnother approach to fine-to-coarse sentiment views with labeled document sentiment, but a much analysis is to use a cascaded system.",
                    "sid": 158,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In such a sys- smaller amount where sentences are also labeled. tem, a sentence level classifier might first be run Work on learning with hidden variables can be used on the data, and then the results input into a docu- for both CRFs (Quattoni et al., 2004) and for inment level classifier \u2013 or vice-versa.'",
                    "sid": 159,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Two cascaded ference based learning algorithms like those used in systems were built.",
                    "sid": 160,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first uses the Sentence- this work (Liang et al., 2006).",
                    "sid": 161,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Structured classifier to classify all the sentences Another area of future work is to empirically infrom a review, then passes this information to the vestigate the use of these models on longer docudocument classifier as input.",
                    "sid": 162,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, for ev- ments that require more levels of sentiment analery predicate in the original document classifier, an ysis than product reviews.",
                    "sid": 163,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, the relaadditional predicate that specifies the polarity of the tive position of a phrase to a contrastive discourse sentence in which this predicate occurred was cre- connective or a cue phrase like \u201cin conclusion\u201d or ated.",
                    "sid": 164,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second cascaded system uses the docu- \u201cto summarize\u201d may lead to improved performance ment classifier to determine the global polarity, then since higher level classifications can learn to weigh passes this information as input into the Sentence- information passed from these lower level compoStructured model, constructing predicates in a simi- nents more heavily. lar manner.",
                    "sid": 165,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 Discussion The results for these two systems can be seen in In this paper we have investigated the use of a global the last two rows of Table 2.",
                    "sid": 166,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In both cases there structured model that learns to predict sentiment on different levels of granularity for a text.",
                    "sid": 167,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We de'Alternatively, decisions from the sentence classifier can guide which input is seen by the document level classifier (Pang and Lee, 2004).",
                    "sid": 168,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "438 scribed a simple model for sentence-document analysis and showed that inference in it is tractable.",
                    "sid": 169,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Experiments show that this model obtains higher accuracy than classifiers trained in isolation as well as cascaded systems that pass information from one level to another at test time.",
                    "sid": 170,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, extensions to the sentence-document model were discussed and it was argued that a nested hierarchical structure would be beneficial since it would allow for efficient inference algorithms.",
                    "sid": 171,
                    "ssid": 53,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}