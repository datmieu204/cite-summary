{
    "ID": "P14-2110",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Learning Polylingual Topic Models from Code-Switched Social Media Documents",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Code-switched documents are in social media, providing evidence polylingual topic models to infer aligned topics across languages.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We Code-Switched LDA (csLDA), which infers language specific topic distributions based on code-switched documents to facilitate multi-lingual corpus analysis.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We experiment on two code-switching corpora (English-Spanish Twitter data and English-Chinese Weibo data) and show that csLDA improves perplexity over LDA, and learns semantically coherent aligned topics as judged by human annotators.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Topic models (Blei et al., 2003) have become standard tools for analyzing document collections, and topic analyses are quite common for social media (Paul and Dredze, 2011; Zhao et al., 2011; Hong and Davison, 2010; Ramage et al., 2010; Eisenstein et al., 2010).",
                    "sid": 4,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Their popularity owes in part to their data driven nature, allowing them to adapt to new corpora and languages.",
                    "sid": 5,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In social media especially, there is a large diversity in terms of both the topic and language, necessitating the modeling of multiple languages simultaneously.",
                    "sid": 6,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A good candidate for multi-lingual topic analyses are polylingual topic models (Mimno et al., 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic.",
                    "sid": 7,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Polylingual topic models enable cross language analysis by grouping documents by topic regardless of language.",
                    "sid": 8,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Training of polylingual topic models requires parallel or comparable corpora: document tuples from multiple languages that discuss the same topic.",
                    "sid": 9,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While additional non-aligned documents can be folded in during training, the \u201cglue\u201d documents are required to aid in the alignment across languages.",
                    "sid": 10,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the ever changing vocabulary and topics of social media (Eisenstein, 2013) make finding suitable comparable corpora difficult.",
                    "sid": 11,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Standard techniques \u2013 such as relying on machine translation parallel corpora or comparable documents extracted from Wikipedia in different languages \u2013 fail to capture the specific terminology of social media.",
                    "sid": 12,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Alternate methods that rely on bilingual lexicons (Jagarlamudi and Daum\u00b4e, 2010) similarly fail to adapt to shifting vocabularies.",
                    "sid": 13,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The result: an inability to train polylingual models on social media.",
                    "sid": 14,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we offer a solution: utilize codeswitched social media to discover correlations across languages.",
                    "sid": 15,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Social media is filled with examples of code-switching, where users switch between two or more languages, both in a conversation and even a single message (Ling et al., 2013).",
                    "sid": 16,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This mixture of languages in the same context suggests alignments between words across languages through the common topics discussed in the context.",
                    "sid": 17,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We learn from code-switched social media by extending the polylingual topic model framework to infer the language of each token and then automatically processing the learned topics to identify aligned topics.",
                    "sid": 18,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our model improves both in terms of perplexity and a human evaluation, and we provide some example analyses of social media that rely on our learned topics.",
                    "sid": 19,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 674\u2013679, Baltimore, Maryland, USA, June 23-25 2014. c\ufffd2014 Association for Computational Linguistics",
                    "sid": 20,
                    "ssid": 17,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "2 code-switching",
            "number": "2",
            "sents": [
                {
                    "text": "Code-switched documents has received considerable attention in the NLP community.",
                    "sid": 21,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982).",
                    "sid": 22,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Code-switching specifically in social media has also received some recent attention.",
                    "sid": 23,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors.",
                    "sid": 24,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ling et al. (2013) mined translation spans for Chinese and English in codeswitched documents to improve a translation system, relying on an existing translation model to aid in the identification and extraction task.",
                    "sid": 25,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast to this work, we take an unsupervised approach, relying only on readily available document level language ID systems to utilize code-switched data.",
                    "sid": 26,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additionally, our focus is not on individual messages, rather we aim to train a model that can be used to analyze entire corpora.",
                    "sid": 27,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this work we consider two types of codeswitched documents: single messages and conversations, and two language pairs: Chinese-English and Spanish-English.",
                    "sid": 28,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1 shows an example of a code-switched Spanish-English conversation, in which three users discuss Mexico\u2019s football team advancing to the Gold medal game in the 2012 Summer Olympics.",
                    "sid": 29,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this conversation, some tweets are code-switched and some are in a single language.",
                    "sid": 30,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By collecting the entire conversation into a single document we provide the topic model with additional content.",
                    "sid": 31,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An example of a Chinese-English code-switched messages is given by Ling et al. (2013): watup Kenny Mayne!!",
                    "sid": 32,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "- Kenny Mayne \u6700\u8fd1\u8fd9\u4e48\u6837\u554a!!",
                    "sid": 33,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here a user switches between languages in a single message.",
                    "sid": 34,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We empirically evaluate our model on both conversations and messages.",
                    "sid": 35,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the model presentation we will refer to both as \u201cdocuments.\u201d",
                    "sid": 36,
                    "ssid": 16,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 cslda",
            "number": "3",
            "sents": [
                {
                    "text": "To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al. (2009): add a token specific language variable, and a process for identifying aligned topics.",
                    "sid": 37,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, polylingual topic models require parallel or comparable corpora in which each document has an assigned language.",
                    "sid": 38,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the case of code-switched social media data, we require apertoken language variable.",
                    "sid": 39,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, while document level language identification (LID) systems are common place, very few languages have pertoken LID systems (King and Abney, 2013; Lignos and Marcus, 2013).",
                    "sid": 40,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To address the lack of available LID systems, we add a per-token latent language variable to the polylingual topic model.",
                    "sid": 41,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For documents that are not code-switched, we observe these variables to be the output of a document level LID system.",
                    "sid": 42,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the case of code-switched documents, these variables are inferred during model inference.",
                    "sid": 43,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, polylingual topic models assume the aligned topics are from parallel or comparable corpora, which implicitly assumes that a topics popularity is balanced across languages.",
                    "sid": 44,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Topics that show up in one language necessarily show up in another.",
                    "sid": 45,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, in the case of social media, we can make no such assumption.",
                    "sid": 46,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The topics discussed are influenced by users, time, and location, all factors intertwined with choice of language.",
                    "sid": 47,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, English speakers will more likely discuss Olympic basketball while Spanish speakers football.",
                    "sid": 48,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There may be little or no documents on a given topic in one language, while they are plentiful in another.",
                    "sid": 49,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this case, a polylingual topic model, which necessarily infers a topicspecific word distribution for each topic in each language, would learn two unrelated word distributions in two languages for a single topic.",
                    "sid": 50,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, naively using the produced topics as \u201caligned\u201d across languages is ill-advised.",
                    "sid": 51,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our solution is to automatically identify aligned polylingual topics after learning by examining a topic\u2019s distribution across code-switched documents.",
                    "sid": 52,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our metric relies on distributional properties of an inferred topic across the entire collection.",
                    "sid": 53,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To summarize, based on the model of Mimno et al. (2009) we will learn: The first two goals are achieved by incorporating new hidden variables in the traditional polylingual topic model.",
                    "sid": 54,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The third goal requires an automated post-processing step.",
                    "sid": 55,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We call the resulting model Code-Switched LDA (csLDA).",
                    "sid": 56,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The generative process is as follows: For monolingual documents, we fix li to the LID tag for all tokens.",
                    "sid": 57,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additionally, we use a single background distribution for each language to capture stopwords; a control variable \u03c0, which follows a Dirichlet distribution with prior parameterized by \u03b4, is introduced to decide the choice between background words and topic words following (Chemudugunta et al., 2006)1.",
                    "sid": 58,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use asymmetric Dirichlet priors (Wallach et al., 2009), and let the optimization process learn the hyperparameters.",
                    "sid": 59,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The graphical model is shown in Figure 2.",
                    "sid": 60,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Inference for csLDA follows directly from LDA.",
                    "sid": 61,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A Gibbs sampler learns the word distributions \u03c6lz for each language and topic.",
                    "sid": 62,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use a block Gibbs sampler to jointly sample topic and language variables for each token.",
                    "sid": 63,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As is customary, we collapse out \u03c6, \u03b8 and \u03c8.",
                    "sid": 64,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The sampling posterior is: where (nl,z )\u2212i is the number of times the type for wi \u2212i is the number of tokens assigned to topic z in document d (excluding current word wi), ol,d\u2212i is the number of tokens assigned to language l in document d (excluding current word wi), and these variables with superscripts or subscripts omitted are totals across all values for the variable.",
                    "sid": 65,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "W is the number of words in the corpus.",
                    "sid": 66,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All counts omit words assigned to the background.",
                    "sid": 67,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During sampling, words are first assigned to the background/topic distribution and then topic and language are sampled for nonbackground words.",
                    "sid": 68,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We optimize the hyperparameters \u03b1, \u03b2, \u03b3 and \u03b4 by interleaving sampling iterations with a NewtonRaphson update to obtain the MLE estimate for the hyperparameters.",
                    "sid": 69,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Taking \u03b1 as an example, one step of the Newton-Raphson update is: \u2202\u03b1 where H is the Hessian matrix and \u2202L \u2202\u03b1 is the gradient of the likelihood function with respect to the optimizing hyperparameter.",
                    "sid": 70,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We interleave 200 sampling iterations with one Newton-Raphson update.",
                    "sid": 71,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We next identify learned topics (a set of related word-distributions) that truly represent an aligned topic across languages, as opposed to an unrelated set of distributions for which there is no supporting alignment evidence in the corpus.",
                    "sid": 72,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We begin by measuring how often each topic occurs in codeswitched documents.",
                    "sid": 73,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If a topic never occurs in a code-switched document, then there can be no evidence to support alignment across languages.",
                    "sid": 74,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the topics that appear at least once in a codeswitched document, we estimate their probability in the code-switched documents by a MAP estimate of \u03b8.",
                    "sid": 75,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Topics appearing in at least one codeswitched document with probability greater than a threshold p are selected as candidates for true cross-language topics.",
                    "sid": 76,
                    "ssid": 40,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 data",
            "number": "4",
            "sents": [
                {
                    "text": "We used two datasets: a Sina Weibo ChineseEnglish corpus (Ling et al., 2013) and a SpanishEnglish Twitter corpus.",
                    "sid": 77,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Weibo Ling et al. (2013) extracted over 1m Chinese-English parallel segments from Sina Weibo, which are code-switched messages.",
                    "sid": 78,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We randomly sampled 29,705 code-switched messages along with 42,116 Chinese and 42,116 English messages from the the same time frame.",
                    "sid": 79,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used these data for training.",
                    "sid": 80,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then sampled an additional 2475 code-switched messages, 4221 English and 4211 Chinese messages as test data.",
                    "sid": 81,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Olympics We collected tweets from July 27, 2012 to August 12, 2012, and identified 302,775 tweets about the Olympics based on related hashtags and keywords (e.g. olympics, #london2012, etc.)",
                    "sid": 82,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We identified code-switched tweets using the Chromium Language Detector2.",
                    "sid": 83,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This system provides the top three possible languages for a given document with confidence scores; we identify a tweet as code-switched if two predicted languages each have confidence greater than 33%.",
                    "sid": 84,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then used the tagger of Lignos and Marcus (2013) to obtain token level LID tags, and only tweets with tokens in both Spanish and English are used as code-switched tweets.",
                    "sid": 85,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In total we identified 822 Spanish-English code-switched tweets.",
                    "sid": 86,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We further expanded the mined tweets to full conversations, yielding 1055 Spanish-English codeswitched documents (including both tweets and conversations), along with 4007 English and 4421 Spanish tweets composes our data set.",
                    "sid": 87,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We reserve 10% of the data for testing.",
                    "sid": 88,
                    "ssid": 12,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5 experiments",
            "number": "5",
            "sents": [
                {
                    "text": "We evaluated csLDA on the two datasets and evaluated each model using perplexity on held out data and human judgements.",
                    "sid": 89,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While our goal is to learn polylingual topics, we cannot compare to previous polylingual models since they require comparable data, which we lack.",
                    "sid": 90,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead, we constructed a baseline from LDA run on the entire dataset (no language information.)",
                    "sid": 91,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each model, we measured the document completion perplexity (RosenZvi et al., 2004) on the held out data.",
                    "sid": 92,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We experimented with different numbers of topics (T).",
                    "sid": 93,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since csLDA duplicates topic distributions (T xL) we used twice as many topics for LDA.",
                    "sid": 94,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 3 shows test perplexity for varying T and perplexity for the best setting of csLDA (T =60) and LDA (T=120).",
                    "sid": 95,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The table lists both monolingual and code-switched test data; csLDA improves over LDA in almost every case, and across all values of T .",
                    "sid": 96,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The background distribution (-bg) has mixed results for LDA, whereas for csLDA it shows consistent improvement.",
                    "sid": 97,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 4 shows some csLDA topics.",
                    "sid": 98,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While there are some mistakes, overall the topics are coherent and aligned.",
                    "sid": 99,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the available per-token LID system (Lignos and Marcus, 2013) for Spanish/English to justify csLDA\u2019s ability to infer the hidden language variables.",
                    "sid": 100,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We ran csLDA-bg with lz set to the value provided by the LID system for codeswitched documents (csLDA-bg with LID), which gives csLDA high quality LID labels.",
                    "sid": 101,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While we see gains for the code-switched data, overall the results for csLDA-bg and csLDA-bg with LID are similar, suggesting that the model can operate effectively even without a supervised per-token LID system.",
                    "sid": 102,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluate topic alignment quality through a human judgements (Chang et al., 2009).",
                    "sid": 103,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each aligned topic, we show an annotator the 20 most frequent words from the foreign language topic (Chinese or Spanish) with the 20 most frequent words from the aligned English topic and two random English topics.",
                    "sid": 104,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The annotators are asked to select the most related English topic among the three; the one with the most votes is considered the aligned topic.",
                    "sid": 105,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We count how often the model\u2019s alignments agree.",
                    "sid": 106,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "LDA may learn comparable topics in different languages but gives no explicit alignments.",
                    "sid": 107,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We create alignments by classifying each LDA topic by language using the KL-divergence between the topic\u2019s words distribution and a word distribution for the English/foreign language inferred from the monolingual documents.",
                    "sid": 108,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Language is assigned to a topic by taking the minimum KL.",
                    "sid": 109,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For Weibo data, this was not effective since the vocabularies of each language are highly unbalanced.",
                    "sid": 110,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead, we manually labeled the topics by language.",
                    "sid": 111,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then pair topics across languages using the cosine similarity of their co-occurrence statistics in codeswitched documents.",
                    "sid": 112,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Topic pairs with similarity above t are considered aligned topics.",
                    "sid": 113,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also used a threshold p (\u00a73.2) to select aligned topics in csLDA.",
                    "sid": 114,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To ensure a fair comparison, we select the same number of aligned topics for LDA and csLDA.3.",
                    "sid": 115,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used the best performing setting: csLDA T=60, LDA T=120, which produced 12 alignments from Olympics and 28 from Weibo.",
                    "sid": 116,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using Mechanical Turk we collected multiple judgements per alignment.",
                    "sid": 117,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For Spanish, we removed workers who disagreed with the majority more than 50% of the time (83 deletions), leaving 6.5 annotations for each alignment (85.47% inter-annotator agreement.)",
                    "sid": 118,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For Chinese, since quality of general Chinese turkers is low (Pavlick et al., 2014) we invited specific workers and obtained 9.3 annotations per alignment (78.72% inter-annotator agreement.)",
                    "sid": 119,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For Olympics, LDA alignments matched the judgements 25% of the time, while csLDA matched 50% of the time.",
                    "sid": 120,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While csLDA found 12 alignments and LDA 29, the 12 topics evaluated from both models show that csLDA\u2019s alignments are higher quality.",
                    "sid": 121,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the Weibo data, LDA matched judgements 71.4%, while csLDA matched 75%.",
                    "sid": 122,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both obtained high 3We used thresholds p = 0.2 and t = 0.0001.",
                    "sid": 123,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We limited the model with more alignments to match the one with less. quality alignments \u2013 likely due both to the fact that the code-switched data is curated to find translations and we hand labeled topic language \u2013 but csLDA found many more alignments: 60 as compared to 28.",
                    "sid": 124,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These results confirm our automated results: csLDA finds higher quality topics that span both languages.",
                    "sid": 125,
                    "ssid": 37,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}