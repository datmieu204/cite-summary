{
    "ID": "W12-3160",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Optimization Strategies for Online Large-Margin Learning in Machine Translation",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The introduction of large-margin based dis criminative methods for optimizing statistical machine translation systems in recent years has allowed exploration into many new types of features for the translation process.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By removing the limitation on the number of parameters which can be optimized, these methods have allowed integrating millions of sparse features.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, these methods have not yet met with wide-spread adoption.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thismay be partly due to the perceived complex ity of implementation, and partly due to the lack of standard methodology for applying these methods to MT. This papers aims to shedlight on large-margin learning for MT, explic itly presenting the simple passive-aggressivealgorithm which underlies many previous ap proaches, with direct application to MT, andempirically comparing several widespread op timization strategies.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Statistical machine translation (SMT) systems rep resent knowledge sources in the form of features, and rely on parameters, or weights, on each feature, to score alternative translations.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As in all statistical models, these parameters need to be learned from the data.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In recent years, there has been a growing trend of moving away from discriminative trainingusing batch log-linear optimization, with Minimum Error Rate Training (MERT) (Och, 2003) being theprinciple method, to online linear optimization (Chi ang et al, 2008; Watanabe et al, 2007; Arun and Koehn, 2007).",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The major motivation for this has been that while MERT is able to efficiently optimizea small number of parameters directly toward an ex ternal evaluation metric, such as BLEU (Papineni et al., 2002), it has been shown that its performance can be erratic, and it is unable to scale to a large set of features (Foster and Kuhn, 2009; Hopkins and May, 2011).",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, it is designed for batch learning, which may be prohibitive or undesirable in certain scenarios, for instance if we have a large tuning set.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One or both of these limitations have led to recent introduction of alternative optimizationstrategies, such as minimum-risk (Smith and Eisner, 2006), PRO (Hopkins and May, 2011), Structured SVM (Cherry and Foster, 2012), and RAM PION (Gimpel and Smith, 2012), which are batchlearners, and online large-margin structured learn ing (Chiang et al, 2009; Watanabe et al, 2007; Watanabe, 2012).A popular method of large-margin optimiza tion is the margin-infused relaxed algorithm (MIRA) (Crammer et al, 2006), which has been shown to perform well for machine translation, as well as other structured prediction tasks, such asparsing.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(McDonald et al, 2005).",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is an at tractive method because we have a simple analytical solution for the optimization problem at each step,which reduces to dual coordinate descent when us ing 1-best MIRA.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is also quite easy to implement, as will be shown below.Despite the proven success of MIRA-based largemargin optimization for both small and large num bers of features, these methods have not yieldedwide adoption in the community.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Part of the rea son for this is a perception that these methods are complicated to implement, which has been cited as motivation for other work (Hopkins and May, 2011;Gimpel and Smith, 2012).",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, there is a di 480 vergence between the standard application of these methods in machine learning, and our application in machine translation (Gimpel and Smith, 2012), where in machine learning there are usually clearcorrect outputs and no latent structures.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a con sequence of the above, there is a lack of standard practices for large-margin learning for MT, which has resulted in numerous different implementations of MIRA-based optimizers, which further add to the confusion.This paper aims to shed light on practical concerns with online large margin training.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Specif ically, our contribution is first, to present the MIRA passive-aggressive update, which underliesall MIRA-based training, with an eye to applica tion in MT. Then, we empirically compare several widespread as well as novel optimization strategiesfor large-margin training on Czech-to-English (csen) and French-to-English (fr-en) translation.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ana lyzing the findings, we recommend an optimizationstrategy which should ensure convergence and sta bility.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "large-margin learning. ",
            "number": "2",
            "sents": [
                {
                    "text": "2.1 Description.",
                    "sid": 19,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "MIRA is an online large-margin learner, and belongs to a class of passive-aggressive (PA) algo rithms (Crammer et al, 2006).",
                    "sid": 20,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although the exactprocedure it employs is different from other subgradient optimizers, in essence it is performing a sub gradient descent step, where the step size is adjusted based on each example.",
                    "sid": 21,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The underlying objective of MIRA is the same as that of the margin rescaledStructural SVM (Tsochantaridis et al, 2004; Martins et al, 2010), where we want to predict the cor rect output over the incorrect one by a margin at leastas large as the cost incurred by predicting the in correct output.",
                    "sid": 22,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the norm constraint fromSVM is replaced with a proximity constraint, indi cating we want to update our parameters, but keepthem as close as possible to the previous parameter estimates.",
                    "sid": 23,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the original formulation for sepa rable classification (Crammer and Singer, 2003), ifno constraints are violated, no update occurs.",
                    "sid": 24,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "How ever, when there is a loss, the algorithm updates the parameters to satisfy the constraints.",
                    "sid": 25,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To allow for noise in the data, i.e. nonseparable instances, a slack variable ?i is introduced for each example, and we optimize a soft-margin.",
                    "sid": 26,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The usual presentation of MIRA is then given as: wt+1 = argmin w 1 2 ||w ?wt||2 + C?i s.t. w>f(xi, yi)?w>f(xi, y?)",
                    "sid": 27,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "cost(yi, y?)?",
                    "sid": 28,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "?i (1) where f(xi, yi) is a vector of feature functions1, w is a vector of corresponding parameters, y?",
                    "sid": 29,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Y(xi), where Y(xi) is the space of possible translations weare able to produce from x,2 and cost(yi, ?) is com puted using an external measure of quality, such as BLEU.",
                    "sid": 30,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The underlying structured hinge loss objective function can be rewritten as: `h = ?w >f(xi, yi)+ max y??Y(xi) ( w>f(xi, y?)",
                    "sid": 31,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "+ cost(yi, y?)",
                    "sid": 32,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": ") (2) 2.2 Hypothesis Selection.",
                    "sid": 33,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our training corpus T = (xi, yi) T i=1 for selecting the parameters w that optimize this objective consists of input sentences xi in the source language paired withreference translations yi in the target language.",
                    "sid": 34,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Notice that `h depends on computing the margin between y?",
                    "sid": 35,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Y(xi) and the correct output, yi.",
                    "sid": 36,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "How ever, there is no guarantee that yi ? Y(xi) sinceour decoder is often incapable of producing the ref erence translation yi.",
                    "sid": 37,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since we need to have some notion of the correct output in order to compute its feature vector for the margin, in practice we revert to using surrogate references in place of yi.",
                    "sid": 38,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These are often referred to as oracles, y+, which are selected from the hypothesis space Y(xi) of the decoder.",
                    "sid": 39,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We are also faced with the problem of how best to select the most appropriate y?",
                    "sid": 40,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "to shy away from, which we will refer to as y?.",
                    "sid": 41,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since optimization will proceed by setting parameters to increase the score of y+, and decrease the score of y?, the selection of these two hypotheses is crucial to success.",
                    "sid": 42,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The range of possibilities is presented in Eq.",
                    "sid": 43,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3 below.",
                    "sid": 44,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1More appropriately, since we only observe translations yi, which may have many possible derivations dj , we model the derivations as a latent variable, and our feature functions are actually computed over derivation and translation pairs f(xi, yi, dj).",
                    "sid": 45,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We omit dj for clarity.",
                    "sid": 46,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2The entire hypergraph in hierarchical translation or lattice in phrase based translation.",
                    "sid": 47,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "481 `r = ? max y+?Y(xi) ( ?+w>f(xi, y+)?",
                    "sid": 48,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "?+cost(yi, y+) ) + max y??Y(xi) ( ??w>f(xi, y?)",
                    "sid": 49,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "+ ??cost(yi, y?)",
                    "sid": 50,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": ") (3) Although this formulation has commonly beenreferred to as the hinge loss in previous litera ture, Gimpel and Smith (2012) have recently pointed out that we are in fact optimizing losses that are closer to different variants of the structured ramp loss.",
                    "sid": 51,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The difference in definition between the two is subtle, in that for the ramp loss, yi is replaced withy+.",
                    "sid": 52,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each setting of ??",
                    "sid": 53,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "and ??",
                    "sid": 54,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "corresponds to opti mizing a different loss function.",
                    "sid": 55,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Several definitions of `r have been explored in the literature, and we discuss them below with corresponding settings of ??",
                    "sid": 56,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "and ??.",
                    "sid": 57,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In selecting y+, we vary the settings of ?+ and ?+.",
                    "sid": 58,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Assuming our cost function is based on BLEU, in setting ?+ ? 1 and ?+ ? 0, if Y(xi) is taken to be the entire space of possible translations, we are selecting the hypothesis with the highest BLEUoverall.",
                    "sid": 59,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB).",
                    "sid": 60,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If we ap proximate the search space by restricting Y(xi) to a k-best list, we have the local-update (Liang etal., 2006), where we select the highest BLEU candidate from those hypotheses that the model considers good (LU).",
                    "sid": 61,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With increasing k-best size, the max BLEU and local-update strategies begin to converge.Setting both ?+ ? 1 and ?+ ? 1, we obtain the cost-diminished hypothesis, which consid ers both the model and the cost, and corresponds tothe ?hope?",
                    "sid": 62,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "hypothesis in Chiang et al (2008) (M C).",
                    "sid": 63,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This can be computed over the entire space of hypotheses or a k-best list.",
                    "sid": 64,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In a sense, this is the intuition that local-updating is after, but expressed more directly.The alternatives for selecting y?",
                    "sid": 65,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "are quite sim ilar.",
                    "sid": 66,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Setting ??",
                    "sid": 67,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 and ??",
                    "sid": 68,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0, we select the hypothesis with the highest cost (MC).",
                    "sid": 69,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Setting??",
                    "sid": 70,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 and ??",
                    "sid": 71,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1, we have the highest scoring hypothesis according to the model, which cor responds to prediction-based selection (Crammer etal., 2006) (PB).",
                    "sid": 72,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Setting both to 1, we have the cost augmented hypothesis, which is referred to as the?fear?",
                    "sid": 73,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Chiang et al, 2008), and max-loss (Crammer et al, 2006) (M+C).",
                    "sid": 74,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This hypothesis is consid ered the most dangerous because it has a high model score along with a high cost.",
                    "sid": 75,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Considering the settings for both parts of Eq.",
                    "sid": 76,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3, ?+, ?+ and ??, ??, assigning all ??",
                    "sid": 77,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "and ??",
                    "sid": 78,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "to 1corresponds to the most commonly used loss func tion in MT (Gimpel and Smith, 2012; Chiang et al., 2009).",
                    "sid": 79,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is the ?hope?/?fear?",
                    "sid": 80,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "pairing, wherewe use the cost-diminished hypothesis y+ and cost augmented hypothesis y?.",
                    "sid": 81,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other loss functions have also been explored, such as ??",
                    "sid": 82,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1, ?+ ? 1,??",
                    "sid": 83,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 (Liang et al, 2006), and something ap proximating ??",
                    "sid": 84,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1, ?+ ? 0, ??",
                    "sid": 85,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 (Cherry and Foster, 2012), which is closer to the usual loss used for max-margin in machine learing.",
                    "sid": 86,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To our best knowledge, other loss functions explored below are novel to this work.",
                    "sid": 87,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since our external metric, BLEU, is a gain, we can think of the first term in Eq.",
                    "sid": 88,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3 as the model score plus the BLEU score, and the second term as the model minus the BLEU score.",
                    "sid": 89,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, with all ??",
                    "sid": 90,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "and ??",
                    "sid": 91,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "set to 1, we want y+ to be the hypothesis with ahigh model score, as well as being close to the refer ence translation, as indicated by a high BLEU score.",
                    "sid": 92,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While for y?, we want a high model score, but it should be far away from the reference, as indicated by a low BLEU score.",
                    "sid": 93,
                    "ssid": 75,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The motivation for choosing y?",
                    "sid": 94,
                    "ssid": 76,
                    "kind_of_tag": "s"
                },
                {
                    "text": "in this fashion is grounded in the fact that sincewe are penalized by this term in the ramp loss ob jective, we should try to optimize on it directly.",
                    "sid": 95,
                    "ssid": 77,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In practice, we can compute the cost for both terms as (1-BLEU(y,yi)), or use that as the cost of the first term, and after selecting y+, compute the cost of y?",
                    "sid": 96,
                    "ssid": 78,
                    "kind_of_tag": "s"
                },
                {
                    "text": "by taking the difference between BLEU(y+,yi) and BLEU(y,yi).",
                    "sid": 97,
                    "ssid": 79,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The ramp loss objectives are non-convex, and by separately computing the max for both y+ and y?, we are theoretically prohibited from online learning since we are no longer guaranteed to be optimizing the desired loss.",
                    "sid": 98,
                    "ssid": 80,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is one motivation for the batchlearner, RAMPION (Gimpel and Smith, 2012).",
                    "sid": 99,
                    "ssid": 81,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, as with many non-convex optimization problems in NLP, such as those involving latent variables, in practice online learning in this setting be haves quite well.",
                    "sid": 100,
                    "ssid": 82,
                    "kind_of_tag": "s"
                },
                {
                    "text": "482 2.3 Parameter Update.",
                    "sid": 101,
                    "ssid": 83,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The major practical concern with these methods for SMT is that oftentimes the implementation aspect is unclear, a problem which is further exacerbated by the apparent difficulty of implementation.",
                    "sid": 102,
                    "ssid": 84,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thisis further compounded with a lack of standard practices; both theoretical, such as the objective to optimize, and practical, such as efficient parallelization.",
                    "sid": 103,
                    "ssid": 85,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The former is a result of the disconnect be tween the standard machine learning setting, whichposits reachable references and lack of latent vari ables, and our own application.",
                    "sid": 104,
                    "ssid": 86,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The latter is an active engineering problem.",
                    "sid": 105,
                    "ssid": 87,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both of these aspects have been receiving recent attention (McAllester et al., 2010; Mcallester and Keshet, 2011; Gimpel and Smith, 2012; McDonald et al, 2010), and although certain questions remain as to the exact loss being optimized, we now have a better understanding ofthe theoretical underpinnings of this method of opti mization.",
                    "sid": 106,
                    "ssid": 88,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first adaptations of MIRA-based learning forstructured prediction in NLP utilized a set of k con straints, either for y+, y?, or both.",
                    "sid": 107,
                    "ssid": 89,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This complicated the optimization by creating a QP problem with a set of linear constraints which needed to be solved witheither Hildreth?s algorithm or SMO style optimization, thereby precluding the possibility of a simple analytical solution.",
                    "sid": 108,
                    "ssid": 90,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Later, Chiang (2012) introduced a cutting-plane algorithm, like that of Structural SVM?s (Tsochantaridis et al, 2004), which op timizes on a small set of active constraints.",
                    "sid": 109,
                    "ssid": 91,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While these methods of dealing with structured prediction may perform better empirically, they come with a higher computational cost.",
                    "sid": 110,
                    "ssid": 92,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Crammer et al (2006) shows that satisfying the single most violated margin constraint, commonly referred toas 1-best MIRA, is amenable to a simple analyt ical solution for the optimization problem at eachstep.",
                    "sid": 111,
                    "ssid": 93,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, the 1-best MIRA update is conceptually and practically much simpler, while retaining most of the optimization power of the more ad vanced methods.",
                    "sid": 112,
                    "ssid": 94,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, this is the method we present below.Since the MIRA optimization problem is an in stance of a general structured problem with an `2norm, the update at each step reduces to dual co ordinate descent (Smith, 2011).",
                    "sid": 113,
                    "ssid": 95,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our soft-margin Algorithm 1 MIRA Training Require: : Training set T = (xi, yi) T i=1, w, C 1: for j ? 1 to N do 2: for i?",
                    "sid": 114,
                    "ssid": 96,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 to T do 3: Y(xi)?Decode(xi,w) 4: y+ ? FindOracle(Y(xi)) 5: y?",
                    "sid": 115,
                    "ssid": 97,
                    "kind_of_tag": "s"
                },
                {
                    "text": "FindPrediction(Y(xi)) 6: margin?",
                    "sid": 116,
                    "ssid": 98,
                    "kind_of_tag": "s"
                },
                {
                    "text": "w>f(xi, y?)?w>f(xi, y+) 7: cost?",
                    "sid": 117,
                    "ssid": 99,
                    "kind_of_tag": "s"
                },
                {
                    "text": "BLEU(yi, y+)?",
                    "sid": 118,
                    "ssid": 100,
                    "kind_of_tag": "s"
                },
                {
                    "text": "BLEU(yi, y?)",
                    "sid": 119,
                    "ssid": 101,
                    "kind_of_tag": "s"
                },
                {
                    "text": "8: loss = margin + cost 9: if loss > 0 then 10: ? ?",
                    "sid": 120,
                    "ssid": 102,
                    "kind_of_tag": "s"
                },
                {
                    "text": "min ( C, loss ?f(xi,y+)?f(xi,y?)?",
                    "sid": 121,
                    "ssid": 103,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 ) 11: w?",
                    "sid": 122,
                    "ssid": 104,
                    "kind_of_tag": "s"
                },
                {
                    "text": "w+ ?",
                    "sid": 123,
                    "ssid": 105,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(f(xi, y+)?",
                    "sid": 124,
                    "ssid": 106,
                    "kind_of_tag": "s"
                },
                {
                    "text": "f(xi, y?)) 12: end if 13: end for 14: end for 15: return w Algorithm 2 FindOracle Require: : Y(xi) 1: if ?+=0 and ?+=1 then 2: y+ ? argmaxy?Y(xi)?cost(yi, y) 3: else if ?+ = ?+ = 1 then 4: y+ ? argmaxy?Y(xi)w >f(xi, y) ? cost(yi, y) 5: end if 6: return y+setting, this is analogous to the PA-I update of Cram mer et al (2006).",
                    "sid": 125,
                    "ssid": 107,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In fact, this update remains largely intact as the inner core within k-best constraint or cutting plane optimization.",
                    "sid": 126,
                    "ssid": 108,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Algorithm 1 presents the entire training regime necessary for 1-best MIRA training of a machine translation system.",
                    "sid": 127,
                    "ssid": 109,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As can be seen, the parameter update at step 11 depends on the difference between the features of y+ and y?, where?",
                    "sid": 128,
                    "ssid": 110,
                    "kind_of_tag": "s"
                },
                {
                    "text": "is the step size, which is controlled by the regularization parameter C; indicating how far we are will ing to move at each step.",
                    "sid": 129,
                    "ssid": 111,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Y(xi) may be a k-best list or the entire space of hypotheses.33For a more in depth examination and derivation of large margin learning in MT, see (Chiang, 2012).",
                    "sid": 130,
                    "ssid": 112,
                    "kind_of_tag": "s"
                },
                {
                    "text": "483 Algorithm 3 FindPrediction Require: : Y(xi) 1: if ??=0 and ??=1 then 2: y?",
                    "sid": 131,
                    "ssid": 113,
                    "kind_of_tag": "s"
                },
                {
                    "text": "argmaxy?Y(xi) cost(yi, y) 3: else if ??=1 and ??=0 then 4: y?",
                    "sid": 132,
                    "ssid": 114,
                    "kind_of_tag": "s"
                },
                {
                    "text": "argmaxy?Y(xi)w >f(xi, y) 5: else if ??",
                    "sid": 133,
                    "ssid": 115,
                    "kind_of_tag": "s"
                },
                {
                    "text": "= 1 then 6: y?",
                    "sid": 134,
                    "ssid": 116,
                    "kind_of_tag": "s"
                },
                {
                    "text": "argmaxy?Y(xi)w >f(xi, y) + cost(yi, y) 7: end if 8: return y?",
                    "sid": 135,
                    "ssid": 117,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "experiments. ",
            "number": "3",
            "sents": [
                {
                    "text": "3.1 Setup.",
                    "sid": 136,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To empirically analyze which loss, and thereby which strategy, for selecting y+ and y?",
                    "sid": 137,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "is most appropriate for machine translation, we conducted a series of experiments on Czech-to-English and French-to-English translation.",
                    "sid": 138,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The parallel corpora are taken from the WMT2012 shared translation task, and consist of Europarl data along with the News Commentary corpus.",
                    "sid": 139,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All data were tokenized and lowercased, then filtered for length and aligned using the GIZA++ implementation of IBM Model4 (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the growdiag-final-and method (Koehn et al, 2003).",
                    "sid": 140,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Gram mars were extracted from the resulting parallel textand used in our hierarchical phrase-based system using cdec (Dyer et al, 2010) as the decoder.",
                    "sid": 141,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We con structed a 5-gram language model from the provided English News monolingual training data as well as the English side of the parallel corpus using the SRIlanguage modeling toolkit with modified Kneser Ney smoothing (Chen and Goodman, 1996).",
                    "sid": 142,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This was used to create a KenLM (Heafield, 2011).",
                    "sid": 143,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As the tuning set for both language pairs, we usedthe 2051 sentences in news-test2008 (NT08), and re port results on the 2525 sentences of news-test2009 (NT09) and 2489 of news-test2010 (NT10).",
                    "sid": 144,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Corpus Sentences Tokens en * cs-en 764K 20.5M 17.5M fr-en 2M 57M 63M Table 1: Corpus statistics pair 1 500 50k 100k cs-en 17.9 24.9 29.4 29.7 fr-en 20.25 29.9 33.8 34.1 Table 2: Oracle score for model 1-best (baseline) and for k-best of size 500, 50k, and 100k on NT08We approximate cost-augmented decoding by ob taining a k-best list with k=500 unique best from ourdecoder at each iteration, and selecting the respective hypotheses for optimization from it.",
                    "sid": 145,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To approx imate max-BLEU decoding using a k-best list, we set k=50k unique best hypotheses.4 As can be seen inTable 2, we found this size was sufficient for our pur poses as increasing size led to small improvements in oracle BLEU score.",
                    "sid": 146,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "C is set to 0.01.For comparison with MERT, we create a baseline model which uses a small standard set of fea tures found in translation systems: language modelprobability, phrase translation probabilities, lexical weighting probabilities, and source word, pass through, and word penalties.",
                    "sid": 147,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While BLEU is usually calculated at the corpuslevel, we need to approximate the metric at the sentence level.",
                    "sid": 148,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this, we mostly follow previous approaches, where in the first iteration through the corpus we use a smoothed sentence level BLEU approximation, similar to Lin and Och (2004), and in sub sequently iterations, the BLEU score is calculated in the context of the previous set of 1-best translations of the entire tuning set.",
                    "sid": 149,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To make parameter estimation more efficient,some form of parallelization is preferred.",
                    "sid": 150,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While earlier versions of MIRA training had complex parallelization procedures which necessitated passing information between learners, performing iterative pa rameter mixing (McDonald et al, 2010) has been shown to be just as effective (Chiang, 2012).",
                    "sid": 151,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use a simple implementation of this regime, where we divide the tuning set into n shards and distribute them amongst n learners, along with the parametervectorw.",
                    "sid": 152,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each learner decodes and updates parame 4We are able to theoretically extract more constraints from a large list, in the spirit of k-constraints or a cutting plane, but Chiang (2012) showed that cutting plane performance is approximately 0.2-0.4 BLEU better than a single constraint, so although there is a trade off between the simplicity of a single constraint and performance, it is not substantial.",
                    "sid": 153,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "484 cs-en NT09 NT10 LU M-C LU M-C PB 16.4 18.3 17 19.3 MC 18.5 16 19.1 17.5 M+C 17.8 18.7 18.4 19.6Table 3: Results with different strategies on cs-en transla tion.",
                    "sid": 154,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "MERT baseline is 18.4 for NT09 and 19.7 for NT10ters on its shard of the tuning set, and once all learners are finished, these n parameter vectors are aver aged to form the initial parameter vector for the next iteration.",
                    "sid": 155,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our experiments, n=20.",
                    "sid": 156,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2 Results.",
                    "sid": 157,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results of using different optimization strategies for cs-en and fr-en are presented in Tables 3 and 4below.",
                    "sid": 158,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all experiments, all settings are kept exactly the same, with the only variation being the se lection of the oracle y+ and prediction y?.",
                    "sid": 159,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The firstcolumn in each table indicates the method for selecting the prediction, y?.",
                    "sid": 160,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "PB indicates prediction based, MC is the hypothesis with the highest cost, and M+C is cost-augmented selection.",
                    "sid": 161,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Analogously,the headings across the table indicate oracle selec tion strategies, with LU indicating local updating, and M-C being cost-diminished selection.",
                    "sid": 162,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From the cs-en results in Table 3, we can see that two settings fair the best: LU oracle selection paired with MC prediction selection (LU/MC), and M-Coracle selection paired with M+C prediction selec tion (M?C).",
                    "sid": 163,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On both sets, (M?C) performs better, but the results are comparable.",
                    "sid": 164,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pairing M-C with PB is also a viable strategy, while no other pairing is successful for LU.",
                    "sid": 165,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When comparing with MERT, note that we use a hypergraph based MERT (Kumar et al, 2009), while the MIRA updates are computed from a k-best list.",
                    "sid": 166,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For max-BLEU oracle selection paired with MC, the performance decreases substantially, to 15.4 and 16.6 BLEU on NT09 and NT10, respectively.",
                    "sid": 167,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using.",
                    "sid": 168,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "the augmented k-best list did not significantly affect performance for M-C oracle selection.",
                    "sid": 169,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For fr-en, we see much the same behavior as in cs-en.",
                    "sid": 170,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, here LU/MC slightly outperforms M?C. From both tasks, we can see that LU is moresensitive to prediction selection, and can only op fr-en NT09 NT10 LU M-C LU M-C PB 20.5 23.1 22.2 25 MC 23.9 23 25.8 24.8 M+C 22.2 23.6 24 25.4Table 4: Results with different strategies on fr-en transla tion.",
                    "sid": 171,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "MERT baseline is 24.2 for NT09 and 26 for NT10 timize effectively when paired with MC.",
                    "sid": 172,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "M-C on the other hand, is more forgiving, and can make progress with PB and MC, albeit not as effectively as with M+C. 3.3 Large Feature Set.",
                    "sid": 173,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since one of the primary motivations for large margin learning is the ability to effectively handle large quantities of features, we further evaluate theability of the strategies by introducing a large num ber of sparse features into our model.",
                    "sid": 174,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We introducesparse binary indicator features of the form com monly found in MT research (Chiang et al, 2009; Watanabe et al, 2007).",
                    "sid": 175,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Specifically, we introduce two types of features based on word alignment fromhierarchical phrase pairs and a target bigram fea ture.",
                    "sid": 176,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first type, a word pair feature, fires for every word pair (ei, fj) observed in the phrase pair.",
                    "sid": 177,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second, insertion features, account for spurious words on the target side of a phrase pair by firing forunaligned target words, associating them with ev ery source word, i.e.",
                    "sid": 178,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(ei, fj), (ei, fj+1), etc..",
                    "sid": 179,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thetarget bigram feature fires for every pair of consec utive words on the target side (ei, ei+1).",
                    "sid": 180,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In all, weintroduce 650k features for cs-en, and 1.1M for fr en.",
                    "sid": 181,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Taking the two best performing strategies from the baseline model, LU/MC and M?C, we comparetheir performance with the larger feature set in Ta ble 5.Although integrating these features does not sig nificantly alter the performance on either task, ourpurpose was to establish once again that the large margin learning framework is capable of effectively optimizing parameters for a large number of sparse features in the MT setting.",
                    "sid": 182,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "485 0.07 0.12 0.17 0.22 0.27 0.32 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 BLE U Iteration Figure 1: Comparison of performance on development set for cs-en when using LU/MC and M?C selection.",
                    "sid": 183,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0.07 0.12 0.17 0.22 0.27 0.32 0.37 0.42 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 BLE U Iteration Figure 2: Comparison of performance on development set for fr-en when using LU/MC and M?C selection.",
                    "sid": 184,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "fr-en cs-en NT09 NT10 NT09 NT10 LU/MC 23.9 25.7 18.5 19.6 M?C 23.8 25.4 18.6 19.6 Table 5: Results on cs-en and fr-en with extended feature set.",
                    "sid": 185,
                    "ssid": 50,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "discussion. ",
            "number": "4",
            "sents": [
                {
                    "text": "Although the performance of the two strategies iscompetitive on the evaluation sets, this does not re lay the entire story.",
                    "sid": 186,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For a more complete view of the differences between optimization strategies, we turn to Figures 1-6.",
                    "sid": 187,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1 and 2 present thecomparison of performance on the NT08 development set for cs-en and fr-en, respectively, when using LU/MC to select the oracle and prediction ver sus M?C selection.",
                    "sid": 188,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "M?C is indicated with a solid black line, while LU/MC is a dotted red line.",
                    "sid": 189,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The corpus-level oracle and prediction BLEU scores at each iteration are indicated with error bars around each point, using solid lines for M?C and dotted lines for LU/MC.",
                    "sid": 190,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As can be seen in Figure 1, while optimizing with M?C is stable and smooth, where we converge on our optimum after several iterations, optimizing with LU/MC is highly unstable.",
                    "sid": 191,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is at least in part due to the wide range in BLEU scores for the oracle and prediction, which are in the range of 10 BLEU points higher or lower than the current model best.",
                    "sid": 192,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the contrary, the range of BLEU scores for the M?C optimizer is on the order of 2 BLEU points, leading to more gradual changes.",
                    "sid": 193,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We see a similar, albeit slightly less pronounced behavior on fr-en in Figure 2.",
                    "sid": 194,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "M?C optimization is once again smooth, and converges quickly, with a small range for the oracle and prediction scores around the model best.",
                    "sid": 195,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "LU/MC remains unstable, oscillating up to 2 BLEU points between iterations.",
                    "sid": 196,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figures 3-6 compare the different optimization strategies further.",
                    "sid": 197,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Figures 3 and 5, we use M-Cas the oracle, and show performance on the develop ment set while using the three prediction selection strategies, M+C with a solid blue line, PB with a dotted green line, and MC with a dashed red line.",
                    "sid": 198,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Error bars indicate the oracle and prediction BLEU scores for each pairing as before.",
                    "sid": 199,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In all three cases, the oracle BLEU score is in about the same range,as expected, since all are using the same oracle se lection strategy.",
                    "sid": 200,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can immediately observe that PB has no error bars going down, indicating that the PB method for selecting the prediction keeps pace with the model best at each iteration.",
                    "sid": 201,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the other hand, MC selection also stands out, since it is the only one with a large drop in prediction BLEU score.",
                    "sid": 202,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Crucially, all learners are stable, and move toward convergence smoothly, which serves to validate our earlier observation that M-C oracle selection can bepaired with any prediction selection strategy and op timize effectively.",
                    "sid": 203,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In both cs-en and fr-en, we can observe that M?C performs the best.",
                    "sid": 204,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Figures 4 and 6, we use LU as the oracle, andshow performance using the three prediction selec tion strategies, with each line representing the same strategy as described above.",
                    "sid": 205,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The major difference, which is immediately evident, is that the optimizers are highly unstable.",
                    "sid": 206,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The only pairing which showssome stability is LU/MC, with both the other predic 486 0.05 0.07 0.09 0.11 0.13 0.15 0.17 0.19 0.21 0.23 0.25 1 2 3 4 5 6 7 8 9 10 BLE U Iteration Figure 3: Comparison of performance on development set for cs-en of the three prediction selection strategies when using M-C selection as oracle.",
                    "sid": 207,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0.05 0.1 0.15 0.2 0.25 0.3 0.35 1 2 3 4 5 6 7 8 9 10 BLE U Iteration Figure 4: Comparison of performance on development set for cs-en of the three prediction selection strategies when using LU selection as oracle.",
                    "sid": 208,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0.05 0.1 0.15 0.2 0.25 0.3 1 2 3 4 5 6 7 8 9 10 BLE U Iteration Figure 5: Comparison of performance on development set for fr-en of the three prediction selection strategies when using M-C selection as oracle.",
                    "sid": 209,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 1 2 3 4 5 6 7 8 9 10 BLE U Iteration Figure 6: Comparison of performance on development set for fr-en of the three prediction selection strategies when using LU selection as oracle.",
                    "sid": 210,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "tion selection methods, PB and M+C significantly underperforming it.Given that the translation performance of optimiz ing the loss functions represented by LU/MC and M?C selection is comparable on the evaluation sets for fr-en and cs-en, it may be premature to make a general recommendation for one over the other.",
                    "sid": 211,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, taking the unstable nature of LU/MC intoaccount, the extent of which may depend on the tun ing set, as well as other factors which need to befurther examined, the current more prudent alterna tive is selecting the oracle and prediction pair based on M?C.",
                    "sid": 212,
                    "ssid": 27,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusion. ",
            "number": "5",
            "sents": [
                {
                    "text": "In this paper, we strove to elucidate aspects of large margin structured learning with concrete application to the MT setting.",
                    "sid": 213,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Towards this goal, we presented the MIRA passive-aggressive algorithm, which can be used directly to effectively tune a statistical MT system with millions of parameters, in the hope that some confusion surrounding MIRA-based methods may be cleared, and more MT researchers can adoptit for their own use.",
                    "sid": 214,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then used the presented al gorithm to empirically compare several widespreadloss functions and strategies for selecting hypothe ses for optimization.",
                    "sid": 215,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We showed that although thereare two competing strategies with comparable per formance, one is an unstable learner, and before weunderstand more regarding the nature of the insta bility, the preferred alternative is to use M?C as the hypothesis pair in optimization.",
                    "sid": 216,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Acknowledgments We would like to thank the anonymous reviewers for their comments.",
                    "sid": 217,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The author is supported by the Department of Defense through the NationalDefense Science and Engineering Graduate Fellow 487ship.",
                    "sid": 218,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Any opinions, findings, conclusions, or rec ommendations expressed are the author?s and do not necessarily reflect those of the sponsors.",
                    "sid": 219,
                    "ssid": 7,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}