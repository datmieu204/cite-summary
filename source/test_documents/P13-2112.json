{
    "ID": "P13-2112",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Simpler unsupervised POS tagging with bilingual projections",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1: Overall accuracy, accuracy on known tokens, accuracy on unknown tokens, and proportion of known tokens for Italian (left) and Dutch (right).",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 shows results for our seed model, self training and revision, and the results reported by Das and Petrov.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Self training and revision improve the accuracy for every language over the seed model, and gives an average improvement of roughly two percentage points.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The average accuracy of self training and revision is on par with that reported by Das and Petrov.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On individual languages, self training and revision and the method of Das and Petrov are split \u2014 each performs better on half of the cases.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Interestingly, our method achieves higher accuracies on Germanic languages \u2014 the family of our source language, English \u2014 while Das and Petrov perform better on Romance languages.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This might be because our model relies on alignments, which might be more accurate for more-related languages, whereas Das and Petrov additionally rely on label propagation.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Compared to Das and Petrov, our model performs poorest on Italian, in terms of percentage point difference in accuracy.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1 (left panel) shows accuracy, accuracy on known words, accuracy on unknown words, and proportion of known tokens for each iteration of our model for Italian; iteration 0 is the seed model, and iteration 31 is the final model.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our model performs poorly on unknown words as indicated by the low accuracy on unknown words, and high accuracy on known words compared to the overall accuracy.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The poor performance on unknown words is expected because we do not use any language-specific rules to handle this case.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, on average for the final model, approximately 10% of the test data tokens are unknown.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One way to improve the performance of our tagger might be to reduce the proportion of unknown words by using a larger training corpus, as Das and Petrov did.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We examine the impact of self-training and revision over training iterations.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We find that for all languages, accuracy rises quickly in the first 5\u20136 iterations, and then subsequently improves only slightly.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We exemplify this in Figure 1 (right panel) for Dutch.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Findings are similar for other languages.)",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although accuracy does not increase much in later iterations, they may still have some benefit as the vocabulary size continues to grow.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6 Conclusion We have proposed a method for unsupervised POS tagging that performs on par with the current stateof-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM). complexity of our algorithm is to that of Das and Petrov 637 where the size of training We our code are available for In future work we intend to consider using a larger training corpus to reduce the proportion of unknown tokens and improve accuracy.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given the improvements of our model over that of Das and Petrov on languages from the same family as our source language, and the observation of Snyder et al. (2008) that a better tagger can be learned from a more-closely related language, we also plan to consider strategies for selecting an appropriate source language for a given target language.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using our final model with unsupervised HMM methods might improve the final performance too, i.e. use our final model as the initial state for HMM, then experiment with different inference algorithms such as Expectation Maximization (EM), Variational Bayers (VB) or Gibbs Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In many cases, GS outperformed other methods, thus we would like to try GS first for our model.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7 Acknowledgements This work is funded by Erasmus Mundus European Masters Program in Language and Communication Technologies (EM-LCT) and by the Czech Science Foundation (grant no.",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "P103/12/G084).",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We would like to thank Prokopis Prokopidis for providing us the Greek Treebank and Antonia Marti for the Spanish CoNLL 06 dataset.",
                    "sid": 25,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we thank Siva Reddy and Spandana Gella for many discussions and suggestions.",
                    "sid": 26,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "References Thorsten Brants.",
                    "sid": 27,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2000.",
                    "sid": 28,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "TnT: A statistical part-oftagger.",
                    "sid": 29,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In of the sixth conference on Applied natural language processing pages 224\u2013231.",
                    "sid": 30,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Seattle, Washington, USA.",
                    "sid": 31,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Dipanjan Das and Slav Petrov.",
                    "sid": 32,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2011.",
                    "sid": 33,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unsupervised part-of-speech tagging with bilingual projections.",
                    "sid": 34,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In of re-implemented label propagation from Das and Petrov (2011).",
                    "sid": 35,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It took over a day to complete this step on an eight core Intel Xeon 3.16GHz CPU with 32 Gb Ram, but only 15 minutes for our model. in fact have tried EM, but it did not help.",
                    "sid": 36,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The overall performance dropped slightly.",
                    "sid": 37,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This might be because selftraining with revision already found the local maximal point. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language - Volume 1 (ACL pages 600\u2013609.",
                    "sid": 38,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Portland, Oregon, USA.",
                    "sid": 39,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pascal Denis and Benoit Sagot.",
                    "sid": 40,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2009.",
                    "sid": 41,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art POS tagging with less effort.",
                    "sid": 42,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In of the 23rd Pacific Asia Conference on Language, Information",
                    "sid": 43,
                    "ssid": 43,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "2 related work",
            "number": "1",
            "sents": [
                {
                    "text": "There is a wealth of prior research on building unsupervised POS taggers.",
                    "sid": 44,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some approaches have exploited similarities between typologically similar languages (e.g., Czech and Russian, or Telugu and Kannada) to estimate the transition probabilities for an HMM tagger for one language based on a corpus for another language (e.g., Hana et al., 2004; Feldman et al., 2006; Reddy and Sharoff, 2011).",
                    "sid": 45,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other approaches have simultaneously tagged two languages based on alignments in a parallel corpus (e.g., Snyder et al., 2008).",
                    "sid": 46,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A number of studies have used tag projection to copy tag information from a resource-rich to a resource-poor language, based on word alignments in a parallel corpus.",
                    "sid": 47,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After alignment, the resource-rich language is tagged, and tags are projected from the source language to the target language based on the alignment (e.g., Yarowsky and Ngai, 2001; Das and Petrov, 2011).",
                    "sid": 48,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language.",
                    "sid": 49,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Graph-based label propagation was used to automatically produce more labelled training data.",
                    "sid": 50,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, a graph was constructed in which each vertex corresponds to a unique trigram, and edge weights represent the syntactic similarity between vertices.",
                    "sid": 51,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Labels were then propagated by optimizing a convex function to favor the same tags for closely related nodes while keeping a uniform tag distribution for unrelated nodes.",
                    "sid": 52,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A tag dictionary was then extracted from the automatically labelled data, and this was used to constrain a feature-based HMM tagger.",
                    "sid": 53,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The method we propose here is simpler to that of Das and Petrov in that it does not require convex optimization for label propagation or a feature based HMM, yet it achieves comparable results.",
                    "sid": 54,
                    "ssid": 11,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 tagset",
            "number": "2",
            "sents": [
                {
                    "text": "Our tagger exploits the idea of projecting tag information from a resource-rich to resource-poor language.",
                    "sid": 55,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To facilitate this mapping, we adopt Petrov et al.\u2019s (2012) twelve universal tags: NOUN, VERB, ADJ, ADV, PRON (pronouns), DET (determiners and articles), ADP (prepositions and postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), \u201c.\u201d (punctuation), and X (all other categories, e.g., foreign words, abbreviations).",
                    "sid": 56,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These twelve basic tags are common across taggers for most languages.",
                    "sid": 57,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Adopting a universal tagset avoids the need to map between a variety of different, languagespecific tagsets.",
                    "sid": 58,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, it makes it possible to apply unsupervised tagging methods to languages for which no tagset is available, such as Telugu and Vietnamese.",
                    "sid": 59,
                    "ssid": 5,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 a simpler unsupervised pos tagger",
            "number": "3",
            "sents": [
                {
                    "text": "Here we describe our proposed tagger.",
                    "sid": 60,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The key idea is to maximize the amount of information gleaned from the source language, while limiting the amount of noise.",
                    "sid": 61,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We describe the seed model and then explain how it is successively refined through self-training and revision.",
                    "sid": 62,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first step is to construct a seed tagger from directly-projected labels.",
                    "sid": 63,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a parallel corpus for a source and target language, Algorithm 1 provides a method for building an unsupervised tagger for the target language.",
                    "sid": 64,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In typical applications, the source language would be a better-resourced language having a tagger, while the target language would be lesser-resourced, lacking a tagger and large amounts of manually POS-labelled data.",
                    "sid": 65,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Algorithm 1 Build seed model We eliminate many-to-one alignments (Step 2).",
                    "sid": 66,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Keeping these would give more POS-tagged tokens for the target side, but also introduce noise.",
                    "sid": 67,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, suppose English and French were the source and target language, respectively.",
                    "sid": 68,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this case alignments such as English laws (NNS) to French les (DT) lois (NNS) would be expected (Yarowsky and Ngai, 2001).",
                    "sid": 69,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, in Step 3, where tags are projected from the source to target language, this would incorrectly tag French les as NN.",
                    "sid": 70,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We build a French tagger based on English\u2013 French data from the Europarl Corpus (Koehn, 2005).",
                    "sid": 71,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also compare the accuracy and coverage of the tags obtained through direct projection using the French Melt POS tagger (Denis and Sagot, 2009).",
                    "sid": 72,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 1 confirms that the one-to-one alignments indeed give higher accuracy but lower coverage than the many-to-one alignments.",
                    "sid": 73,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "At this stage of the model we hypothesize that highconfidence tags are important, and hence eliminate the many-to-one alignments.",
                    "sid": 74,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Step 4, in an effort to again obtain higher quality target language tags from direct projection, we eliminate all but the top n sentences based on their alignment scores, as provided by the aligner via IBM model 3.",
                    "sid": 75,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We heuristically set this cutoff to 60k to balance the accuracy and size of the seed model.1 Returning to our preliminary English\u2013 French experiments in Table 1, this process gives improvements in both accuracy and coverage.2 The number of parameters for the emission probability is |V  |\u00d7 |T  |where V is the vocabulary and T is the tag set.",
                    "sid": 76,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The transition probability, on the other hand, has only |T|3 parameters for the trigram model we use.",
                    "sid": 77,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Because of this difference in number of parameters, in step 5, we use different strategies to estimate the emission and transition probabilities.",
                    "sid": 78,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The emission probability is estimated from all 60k selected sentences.",
                    "sid": 79,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, for the transition probability, which has less parameters, we again focus on \u201cbetter\u201d sentences, by estimating this probability from only those sentences that have (1) token coverage > 90% (based on direct projection of tags from the source language), and (2) length > 4 tokens.",
                    "sid": 80,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These criteria aim to identify longer, mostly-tagged sentences, which we hypothesize are particularly useful as training data.",
                    "sid": 81,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the case of our preliminary English\u2013French experiments, roughly 62% of the 60k selected sentences meet these criteria and are used to estimate the transition probability.",
                    "sid": 82,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For unaligned words, we simply assign a random POS and very low probability, which does not substantially affect transition probability estimates.",
                    "sid": 83,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Step 6 we build a tagger by feeding the estimated emission and transition probabilities into the TNT tagger (Brants, 2000), an implementation of a trigram HMM tagger.",
                    "sid": 84,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For self training and revision, we use the seed model, along with the large number of target language sentences available that have been partially tagged through direct projection, in order to build a more accurate tagger.",
                    "sid": 85,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Algorithm 2 describes this process of self training and revision, and assumes that the parallel source\u2013target corpus has been word aligned, with many-to-one alignments removed, and that the sentences are sorted by alignment score.",
                    "sid": 86,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast to Algorithm 1, all sentences are used, not just the 60k sentences with the highest alignment scores.",
                    "sid": 87,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We believe that sentence alignment score might correspond to difficulty to tag.",
                    "sid": 88,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By sorting the sentences by alignment score, sentences which are more difficult to tag are tagged using a more mature model.",
                    "sid": 89,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Following Algorithm 1, we divide sentences into blocks of 60k.",
                    "sid": 90,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In step 3 the tagged block is revised by comparing the tags from the tagger with those obtained through direct projection.",
                    "sid": 91,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Suppose source Algorithm 2 Self training and revision language word wsi is aligned with target language word wtj with probability p(wtj|wsi), Tis is the tag for wsi using the tagger available for the source language, and Tjt is the tag for wtj using the tagger learned for the target language.",
                    "sid": 92,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If p(wt j|ws i) > 5, where 5 is a threshold which we heuristically set to 0.7, we replace Tjt by Tis.",
                    "sid": 93,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model (McClosky et al., 2006).",
                    "sid": 94,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To avoid this, we remove the tag of any token that the model is uncertain of, i.e., if p(wtj|wsi) < 5 and Tjt =\ufffd Tis then Tjt = Null.",
                    "sid": 95,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "So, on the target side, aligned words have a tag from direct projection or no tag, and unaligned words have a tag assigned by our model.",
                    "sid": 96,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Step 4 estimates the emission and transition probabilities as in Algorithm 1.",
                    "sid": 97,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Step 5, emission probabilities for lexical items in the previous model, but missing from the current model, are added to the current model.",
                    "sid": 98,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Later models therefore take advantage of information from earlier models, and have wider coverage.",
                    "sid": 99,
                    "ssid": 40,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5 experimental results",
            "number": "4",
            "sents": [
                {
                    "text": "Using parallel data from Europarl (Koehn, 2005) we apply our method to build taggers for the same eight target languages as Das and Petrov (2011) \u2014 Danish, Dutch, German, Greek, Italian, Portuguese, Spanish and Swedish \u2014 with English as the source language.",
                    "sid": 100,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our training data (Europarl) is a subset of the training data of Das and Petrov (who also used the ODS United Nations dataset which we were unable to obtain).",
                    "sid": 101,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The evaluation metric and test data are the same as that used by Das and Petrov.",
                    "sid": 102,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our results are comparable to theirs, although our system is penalized by having less training data.",
                    "sid": 103,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We tag the source language with the Stanford POS tagger (Toutanova et al., 2003).",
                    "sid": 104,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 shows results for our seed model, self training and revision, and the results reported by Das and Petrov.",
                    "sid": 105,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Self training and revision improve the accuracy for every language over the seed model, and gives an average improvement of roughly two percentage points.",
                    "sid": 106,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The average accuracy of self training and revision is on par with that reported by Das and Petrov.",
                    "sid": 107,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On individual languages, self training and revision and the method of Das and Petrov are split \u2014 each performs better on half of the cases.",
                    "sid": 108,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Interestingly, our method achieves higher accuracies on Germanic languages \u2014 the family of our source language, English \u2014 while Das and Petrov perform better on Romance languages.",
                    "sid": 109,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This might be because our model relies on alignments, which might be more accurate for more-related languages, whereas Das and Petrov additionally rely on label propagation.",
                    "sid": 110,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Compared to Das and Petrov, our model performs poorest on Italian, in terms of percentage point difference in accuracy.",
                    "sid": 111,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1 (left panel) shows accuracy, accuracy on known words, accuracy on unknown words, and proportion of known tokens for each iteration of our model for Italian; iteration 0 is the seed model, and iteration 31 is the final model.",
                    "sid": 112,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our model performs poorly on unknown words as indicated by the low accuracy on unknown words, and high accuracy on known words compared to the overall accuracy.",
                    "sid": 113,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The poor performance on unknown words is expected because we do not use any language-specific rules to handle this case.",
                    "sid": 114,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, on average for the final model, approximately 10% of the test data tokens are unknown.",
                    "sid": 115,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One way to improve the performance of our tagger might be to reduce the proportion of unknown words by using a larger training corpus, as Das and Petrov did.",
                    "sid": 116,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We examine the impact of self-training and revision over training iterations.",
                    "sid": 117,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We find that for all languages, accuracy rises quickly in the first 5\u20136 iterations, and then subsequently improves only slightly.",
                    "sid": 118,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We exemplify this in Figure 1 (right panel) for Dutch.",
                    "sid": 119,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Findings are similar for other languages.)",
                    "sid": 120,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although accuracy does not increase much in later iterations, they may still have some benefit as the vocabulary size continues to grow.",
                    "sid": 121,
                    "ssid": 22,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "6 conclusion",
            "number": "5",
            "sents": [
                {
                    "text": "We have proposed a method for unsupervised POS tagging that performs on par with the current stateof-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM).",
                    "sid": 122,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The complexity of our algorithm is O(nlogn) compared to O(n2) for that of Das and Petrov (2011) where n is the size of training data.3 We made our code are available for download.4 In future work we intend to consider using a larger training corpus to reduce the proportion of unknown tokens and improve accuracy.",
                    "sid": 123,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given the improvements of our model over that of Das and Petrov on languages from the same family as our source language, and the observation of Snyder et al. (2008) that a better tagger can be learned from a more-closely related language, we also plan to consider strategies for selecting an appropriate source language for a given target language.",
                    "sid": 124,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using our final model with unsupervised HMM methods might improve the final performance too, i.e. use our final model as the initial state for HMM, then experiment with different inference algorithms such as Expectation Maximization (EM), Variational Bayers (VB) or Gibbs sampling (GS).5 Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging.",
                    "sid": 125,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In many cases, GS outperformed other methods, thus we would like to try GS first for our model.",
                    "sid": 126,
                    "ssid": 5,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "7 acknowledgements",
            "number": "6",
            "sents": [
                {
                    "text": "This work is funded by Erasmus Mundus European Masters Program in Language and Communication Technologies (EM-LCT) and by the Czech Science Foundation (grant no.",
                    "sid": 127,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "P103/12/G084).",
                    "sid": 128,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We would like to thank Prokopis Prokopidis for providing us the Greek Treebank and Antonia Marti for the Spanish CoNLL 06 dataset.",
                    "sid": 129,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we thank Siva Reddy and Spandana Gella for many discussions and suggestions.",
                    "sid": 130,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}