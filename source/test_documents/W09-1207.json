{
    "ID": "W09-1207",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Multilingual Dependency-based Syntactic and Semantic Parsing",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "dependency parser.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In EMNLP/CoNLL- Chang and Chih-Jen Lin, 2001. a for support vector Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, and Sheng Li.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2008.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A cascaded syntactic and semantic dependency parsing system.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Jason Eisner.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2000.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Bilexical grammars and their cubicparsing algorithms.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In in Probabilistic",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 system architecture",
            "number": "1",
            "sents": [
                {
                    "text": "Our CoNLL 2009 Shared Task (Haji\u02c7c et al., 2009): multilingual syntactic and semantic dependencies system includes three cascaded components: syntactic parsing, predicate classification, and semantic role labeling.",
                    "sid": 9,
                    "ssid": 1,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "2 syntactic dependency parsing",
            "number": "2",
            "sents": [
                {
                    "text": "We extend our CoNLL 2008 graph-based model (Che et al., 2008) in four ways: The model of (Che et al., 2008) decided one label for each arc before decoding according to unigram features, which caused lower labeled attachment score (LAS).",
                    "sid": 10,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the other hand, keeping all possible labels for each arc made the decoding inefficient.",
                    "sid": 11,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, in the system of this year, we adopt approximate techniques to compromise, as shown in the following formulas. flbl arc, and the third parameter indicates the direction.",
                    "sid": 12,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "L denotes the whole label set.",
                    "sid": 13,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then we re-rank the labels by combining the bigram features, and choose K2-best labels.",
                    "sid": 14,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During decoding, we only use the K2 labels chosen for each arc (K2 \u00bf K1 < |L|).",
                    "sid": 15,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Following the Eisner (2000) algorithm, we use spans as the basic unit.",
                    "sid": 16,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A span is defined as a substring of the input sentence whose sub-tree is already produced.",
                    "sid": 17,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Only the start or end words of a span can link with other spans.",
                    "sid": 18,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this way, the algorithm parses the left and the right dependence of a word independently, and combines them in the later stage.",
                    "sid": 19,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We follow McDonald (2006)\u2019s implementation of first-order Eisner parsing algorithm by modifying its scoring method to incorporate high-order features.",
                    "sid": 20,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our extended algorithm is shown in Algorithm 1.",
                    "sid": 21,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are four different span-combining operations.",
                    "sid": 22,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here we explain two of them that correspond to right-arc (s < t), as shown in Figure 1 and 2.",
                    "sid": 23,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We follow the way of (McDonald, 2006) and (Carreras, 2007) to represent spans.",
                    "sid": 24,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The other two operations corresponding to left-arc are similar. an incomplete span.",
                    "sid": 25,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A complete span means that only the head word can link with other words further, noted as \u201c\u2014*\u201d or \u201c+\u2014\u201d.",
                    "sid": 26,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An incomplete span indicates that both the start and end words of the span will link with other spans in the future, noted as \u201c--;\u201d or \u201cF--\u201d.",
                    "sid": 27,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this operation, we combine two smaller spans, sps\u2192r and spr+1\u2190t, into sps99Kt with adding arcs\u2192t.",
                    "sid": 28,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As shown in the following formulas, the score of sps99Kt is composed of three parts: the score of sps\u2192r, the score of spr+1\u2190t, and the score of adding arcs\u2192t.",
                    "sid": 29,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The score of arcs\u2192t is determined by four different feature sets: unigram features, bigram features, sibling features and left grandchildren features (or inside grandchildren features, meaning that the grandchildren lie between s and t).",
                    "sid": 30,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that the sibling features are only related to the nearest sibling node of t, which is denoted as sck here.",
                    "sid": 31,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "And the inside grandchildren features are related to all the children of t. This is different from the models used by Carreras (2007) and Johansson and Nugues (2008).",
                    "sid": 32,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They only used the left-most child of t, which is tck, here.",
                    "sid": 33,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Figure 2 we combine sps99Kr and spr\u2192t into sps\u2192t, which explains line 10 in Algorithm 1.",
                    "sid": 34,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The score of sps\u2192t also includes three parts, as shown in the following formulas.",
                    "sid": 35,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although there is no new arc added in this operation, the third part is necessary because it reflects the right (or called outside) grandchildren information of arcs\u2192r.",
                    "sid": 36,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As shown above, features used in our model can be decomposed into four parts: unigram features, bigram features, sibling features, and grandchildren features.",
                    "sid": 37,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each part can be seen as two different sets: arc-related and label-related features, except sibling features, because we do not consider labels when using sibling features.",
                    "sid": 38,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Arc-related features can be understood as back-off of label-related features.",
                    "sid": 39,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Actually, label-related features are gained by simply attaching the label to the arc-features.",
                    "sid": 40,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The unigram and bigram features used in our model are similar to those of (Che et al., 2008), except that we use bigram label-related features.",
                    "sid": 41,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The sibling features we use are similar to those of (McDonald, 2006), and the grandchildren features are similar to those of (Carreras, 2007).",
                    "sid": 42,
                    "ssid": 33,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 predicate classification",
            "number": "3",
            "sents": [
                {
                    "text": "The predicate classification is regarded as a supervised word sense disambiguation (WSD) task here.",
                    "sid": 43,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The task is divided into four steps:",
                    "sid": 44,
                    "ssid": 2,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 semantic role labeling",
            "number": "4",
            "sents": [
                {
                    "text": "The semantic role labeling (SRL) can be divided into two separate stages: semantic role classification (SRC) and post inference (PI).",
                    "sid": 45,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During the SRC stage, a Maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a word in the sentence to be each semantic role.",
                    "sid": 46,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We add a virtual role \u201cNULL\u201d (presenting none of roles is assigned) to the roles set, so we do not need semantic role identification stage anymore.",
                    "sid": 47,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For a predicate of each language, two classifiers (one for noun predicates, and the other for verb predicates) predict probabilities of each word in a sentence to be each semantic role (including virtual role \u201cNULL\u201d).",
                    "sid": 48,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The features used in this stage are listed in Table 4.",
                    "sid": 49,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The probability of each word to be a semantic role for a predicate is given by the SRC stage.",
                    "sid": 50,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results generated by selecting the roles with the largest probabilities, however, do not satisfy some constrains.",
                    "sid": 51,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As we did in the last year\u2019s system (Che et al., 2008), we use the ILP (Integer Linear Programming) (Punyakanok et al., 2004) to get the global optimization, which is satisfied with three constrains: C1: Each word should be labeled with one and only one label (including the virtual label \u201cNULL\u201d).",
                    "sid": 52,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "C2: Roles with a small probability should never be labeled (except for the virtual role \u201cNULL\u201d).",
                    "sid": 53,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The threshold we use in our system is 0.3.",
                    "sid": 54,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "C3: Statistics show that some roles (except for the virtual role \u201cNULL\u201d) usually appear once for a predicate.",
                    "sid": 55,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We impose a no-duplicate-roles constraint with a no-duplicate-roles list, which is constructed according to the times of semantic roles\u2019 duplication for each single predicate.",
                    "sid": 56,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 1 shows the no-duplicate-roles for different languages.",
                    "sid": 57,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our maximum entropy classifier is implemented with Maximum Entropy Modeling Toolkit1.",
                    "sid": 58,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The classifier parameters are tuned with the development data for different languages respectively. lp solve 5.52 is chosen as our ILP problem solver.",
                    "sid": 59,
                    "ssid": 15,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5 experiments",
            "number": "5",
            "sents": [
                {
                    "text": "We participate in the CoNLL 2009 shared task with all 7 languages: Catalan (Taul\u00b4e et al., 2008), Chinese (Palmer and Xue, 2009), Czech (Haji\u02c7c et al., 2006), English (Surdeanu et al., 2008), German (Burchardt et al., 2006), Japanese (Kawahara et al., 2002), and Spanish (Taul\u00b4e et al., 2008).",
                    "sid": 60,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Besides the closed challenge, we also submitted the open challenge results.",
                    "sid": 61,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our open challenge strategy is very simple.",
                    "sid": 62,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We add the SRL development data of each language into their training data.",
                    "sid": 63,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The purpose is to examine the effect of the additional data, especially for out-of-domain (ood) data.",
                    "sid": 64,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Three machines (with 2.5GHz Xeon CPU and 16G memory) were used to train our models.",
                    "sid": 65,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During the peak time, Amazon\u2019s EC2 (Elastic Compute Cloud)3 was used, too.",
                    "sid": 66,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our system requires 15G memory at most and the longest training time is about 36 hours.",
                    "sid": 67,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During training the predicate classification (PC) and the semantic role labeling (SRL) models, golden syntactic dependency parsing results are used.",
                    "sid": 68,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previous experiments show that the PC and SRL test results based on golden parse trees are slightly worse than that based on cross trained parse trees.",
                    "sid": 69,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is, however, a pity that we have no enough time and machines to do cross training for so many languages.",
                    "sid": 70,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to examine the performance of the ILP based post inference (PI) for different languages, we adopt a simple PI strategy as baseline, which selects the most likely label (including the virtual label \u201cNULL\u201d) except for those duplicate non-virtual labels with lower probabilities (lower than 0.5).",
                    "sid": 71,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 shows their performance on development data.",
                    "sid": 72,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can see that the ILP based post inference can improve the precision but decrease the recall.",
                    "sid": 73,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Except for Czech, almost all languages are improved.",
                    "sid": 74,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Among them, English benefits most.",
                    "sid": 75,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The final system results are shown in Table 3.",
                    "sid": 76,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Comparing with our CoNLL 2008 (Che et al., 2008) syntactic parsing results on English4, we can see that our new high-order model improves about 1%.",
                    "sid": 77,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the open challenge, because we did not modify the syntactic training data, its results are the same as the closed ones.",
                    "sid": 78,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can, therefore, examine the effect of the additional training data on SRL.",
                    "sid": 79,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can see that along with the development data are added into the training data, the performance on the indomain test data is increased.",
                    "sid": 80,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, it is interesting that the additional data is harmful to the ood test.",
                    "sid": 81,
                    "ssid": 22,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "6 conclusion and future work",
            "number": "6",
            "sents": [
                {
                    "text": "Our CoNLL 2009 Shared Task system is composed of three cascaded components.",
                    "sid": 82,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The pseudoprojective high-order syntactic dependency model outperforms our CoNLL 2008 model (in English).",
                    "sid": 83,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The additional in-domain (devel) SRL data can help the in-domain test.",
                    "sid": 84,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, it is harmful to the ood test.",
                    "sid": 85,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our final system achieves promising results.",
                    "sid": 86,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the future, we will study how to solve the domain adaptive problem and how to do joint learning between syntactic and semantic parsing.",
                    "sid": 87,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments",
            "number": "7",
            "sents": [
                {
                    "text": "This work was supported by National Natural Science Foundation of China (NSFC) via grant 60803093, 60675034, and the \u201c863\u201d National HighTech Research and Development of China via grant 2008AA01Z144.",
                    "sid": 88,
                    "ssid": 1,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}