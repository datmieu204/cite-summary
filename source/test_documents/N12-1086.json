{
    "ID": "N12-1086",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Graph-Based Lexicon Expansion with Sparsity-Inducing Penalties",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We present novel methods to construct compact natural language lexicons within a graph based semi-supervised learning framework, an attractive platform suited for propagating soft labels onto new natural language types from seed data.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To achieve compactness, we induce sparse measures at graph vertices by incorporating sparsity-inducing penalties in Gaussian and entropic pairwise Markovnetworks constructed from labeled and unla beled data.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sparse measures are desirable forhigh-dimensional multi-class learning problems such as the induction of labels on natu ral language types, which typically associate with only a few labels.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Compared to standard graph-based learning methods, for two lexicon expansion problems, our approach producessignificantly smaller lexicons and obtains bet ter predictive performance.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Semi-supervised learning (SSL) is attractive for thelearning of complex phenomena, for example, linguistic structure, where data annotation is expen sive.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Natural language processing applications havebenefited from various SSL techniques, such as dis tributional word representations (Huang and Yates, 2009; Turian et al, 2010; Dhillon et al, 2011), self-training (McClosky et al, 2006), and entropy regularization (Jiao et al, 2006; Smith and Eisner, 2007).",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we focus on semi-supervised learning that uses a graph constructed from labeled and unlabeled data.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This framework, graph-based SSL?see Bengio et al (2006) and Zhu (2008) for introductory material on this topic?has been widelyused and has been shown to perform better than sev eral other semi-supervised algorithms on benchmark datasets (Chapelle et al, 2006, ch.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "21).",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The methodconstructs a graph where a small portion of ver tices correspond to labeled instances, and the rest are unlabeled.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pairs of vertices are connected by weighted edges denoting the similarity between thepair.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Traditionally, Markov random walks (Szummer and Jaakkola, 2001; Baluja et al, 2008) or op timization of a loss function based on smoothness properties of the graph (Corduneanu and Jaakkola, 2003; Zhu et al, 2003; Subramanya and Bilmes, 2008, inter alia) are performed to propagate labels from the labeled vertices to the unlabeled ones.In this work, we are interested in multi-class generalizations of graph-propagation algorithms suitable for NLP applications, where each graph ver tex can assume one or more out of many possible labels (Talukdar and Crammer, 2009; Subramanyaand Bilmes, 2008, 2009).",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For us, graph vertices cor respond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recently, this setup has been used to learn soft labels on natural language types (say,word n-grams or syntactically disambiguated pred icates) from seed data, resulting in large but noisylexicons, which are used to constrain structured pre diction models.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning ofPOS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow seman tic parsing for unknown predicates (Das and Smith,2011).",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, none of the above captured the empirical fact that only a few categories typically asso ciate with a given type (vertex).",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Take the case of POS tagging: Subramanya et al (2010) construct agraph over trigram types as vertices, with 45 pos sible tags for the middle word of a trigram as the 677 label set for each vertex.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is empirically observed that contextualized word types can assume very few (most often, one) POS tags.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, along with graph smoothness terms, they apply a penalty that encourages distributions to be close to uniform, the premise being that it would maximize the entropy of the distribution for a vertex that is far away ordisconnected from a labeled vertex.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To prefer maxi mum entropy solutions in low confidence regions ofgraphs, a similar entropic penalty is applied by Sub ramanya and Bilmes (2008, 2009).In this paper, we make two major algorithmic con tributions.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we relax the assumption made by most previous work (Zhu and Ghahramani, 2002; Baluja et al, 2008; Subramanya and Bilmes, 2008; Subramanya and Bilmes, 2009; Subramanya et al, 2010; Das and Petrov, 2011; Das and Smith, 2011)that the `1 norm of the masses assigned to the la bels for a given vertex must be 1.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In other words,in our framework, the label distribution at each ver tex is unnormalized?the only constraint we put onthe vertices?",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "vectors is that they must be nonnega tive.1 This relaxation simplifies optimization: since only a nonnegativity constraint for each label?s mass at each vertex needs to be imposed, we can apply a generic quasi-Newton method (Zhu et al, 1997).Second, we replace the penalties that prefer max imum entropy, used in prior work, with penalties that aim to identify sparse unnormalized measures at each graph vertex.",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We achieve this by penalizing the graph propagation objective with the `1 norm or the mixed `1,2 norm (Kowalski and Torre?sani, 2009) of the measures at each vertex, aiming for global and vertex-level sparsity, respectively.",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Importantly, the proposed graph objective functions are convex, so we avoid degenerate solutions and local minima.",
                    "sid": 25,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We present experiments on two natural language lexicon expansion problems in a semi-supervised setting: (i) inducing distributions of POS tags over n-gram types in the Wall Street Journal section of the Penn Treebank corpus (Marcus et al, 1993) and (ii) inducing distributions of semantic frames(Fillmore, 1982) over predicates unseen in anno 1Moreover, we also assume the edge weights in a givengraph are unconstrained, consistent with prior work on graphbased SSL (Das and Petrov, 2011; Das and Smith, 2011; Subramanya and Bilmes, 2008; Subramanya and Bilmes, 2009; Sub ramanya et al, 2010; Zhu and Ghahramani, 2002).",
                    "sid": 26,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "tated data.",
                    "sid": 27,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our methods produce sparse measures at graph vertices resulting in compact lexicons, andalso result in better performance with respect to la bel propagation using Gaussian penalties (Zhu andGhahramani, 2002) and entropic measure propagation (Subramanya and Bilmes, 2009), two state-of the-art graph propagation algorithms.",
                    "sid": 28,
                    "ssid": 28,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "model. ",
            "number": "2",
            "sents": [
                {
                    "text": "2.1 Graph-Based SSL as MAP Inference.",
                    "sid": 29,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let Dl = {(xj , rj)}lj=1 denote l annotated data types;2 xj?s empirical label distribution is rj . Let the unlabeled data types be denoted by Du = {xi}mi=l+1.",
                    "sid": 30,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Usually, l  m. Thus, the entire dataset can be called D , Dl ? Du.",
                    "sid": 31,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Traditionally, thegraph-based SSL problem has been set up as fol lows.",
                    "sid": 32,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let G = (V,E) correspond to an undirected graph with vertices V and edges E. G is constructedby transforming each data type xi ? D to a ver tex; thus V = {1, 2, . . .",
                    "sid": 33,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": ",m}, and E ? V ? V . Let Vl (Vu) denote the labeled (unlabeled) vertices.",
                    "sid": 34,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, we assume a symmetric weight matrixW that defines the similarity between a pair of verticesi, k ? V . We first define a component of this ma trix as wij , [W]ik = sim(xi,xk).",
                    "sid": 35,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also fix wii = 0 and set wik = wki = 0 if k 6?",
                    "sid": 36,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "N (i) and i 6?",
                    "sid": 37,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "N (k), where N (j) denotes the K-nearest neighbors of vertex j, to reduce the density of the graph.",
                    "sid": 38,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We next define an unnormalized measure qi for every vertex i ? V . As mentioned before, we have rj , a probability distribution estimated from annotated data for a labeled vertex j ? Vl.",
                    "sid": 39,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "qi and rj are |Y |-dimensional measures, where Y is thepossible set of labels; while rj lies within the |Y |dimensional probability simplex,3 qi are unnormal ized with each component qi(y) ? 0.",
                    "sid": 40,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For most NLP problems, rj are expected to be sparse, with very few components active, the rest being zero.",
                    "sid": 41,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Graph-based SSL aims at finding the best q ={qi : 1 ? i ? m} given the empirical distribu tions rj , and the weight matrix W, which provides 2As explained in more detail in ?4, these types are entities like n-grams or individual predicates, not tokens in running text.",
                    "sid": 42,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3Note that our framework does not necessitate that rj be anormalized probability distribution; we could have unnormal ized rj to allow strongly evident types appearing in more data to have larger influence than types that appear infrequently.",
                    "sid": 43,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We leave this extension to future work.",
                    "sid": 44,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "678 the geometry of all the vertices.",
                    "sid": 45,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We visualize this problem using a pairwise Markov network (MN).",
                    "sid": 46,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For every vertex (including labeled ones) i ? V , wecreate a variable Xi.",
                    "sid": 47,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additionally, for labeled ver tices j ? Vl, we create variables X?j . All variables in the MN are defined to be vector-valued; specifically, variables Xi, ?i ? V , take value qi, and variablesX?j corresponding to the labeled vertices in G are ob served with values rj . An example factor graph forthis MN, with only four vertices, is shown in Fig ure 1.",
                    "sid": 48,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the figure, the variables indexed by 1 and 4 correspond to labeled vertices.",
                    "sid": 49,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Factor ?j with scope {Xj , X?j} encourages qj to be close to rj . For every edge i ? k ? E, factor ?i?k encourages similarity between qi and qk, making use of the weight matrix W (i.e., when wik is larger, the two measures aremore strongly encouraged to be close).",
                    "sid": 50,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These fac tors are white squares with solid boundaries in thefigure.",
                    "sid": 51,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we define unary factors on all vari ablesXi, i ? V , named ?i(Xi), that can incorporateprior information.",
                    "sid": 52,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Figure 1, these factors are rep resented by white squares with dashed boundaries.According to the factor graph, the joint probabil ity for all the measures qi, ?i ? V that we want to induce, is defined as: P (X; ?) = 1 Z l?",
                    "sid": 53,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "j=1 ?j(Xj , X?j) ? ?",
                    "sid": 54,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "i?k?E ?i?k(Xi, Xk) ? m? i=1 ?i(Xi) where ? is the set of all factors in the factor graph,and Z is a partition function that normalizes the fac tor products for a given configuration of q. Since the graph-based SSL problem aims at finding the best q, we optimize lnP (X; ?); equivalently, arg max q s.t. q?0 l?",
                    "sid": 55,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "j=1 ln?j(Xj , X?j) + ? i?k?E ln?i?k(Xi, Xk) + m?",
                    "sid": 56,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "i=1 ln?i(Xi) (1) The above denotes an optimization problem withonly non-negativity constraints.",
                    "sid": 57,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It equates to maximum a posteriori (MAP) inference; hence, the par tition function Z can be ignored.",
                    "sid": 58,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We next discuss the nature of the three different factors in Eq.",
                    "sid": 59,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.",
                    "sid": 60,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.2 Log-Factors as Penalties.",
                    "sid": 61,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The nature of the three types of factors in Eq.",
                    "sid": 62,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1governs the behavior of a graph-based SSL algo rithm.",
                    "sid": 63,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hence, the equation specifies a family of X1 X4 X3 X2 Figure 1: An example factor graph for the graph-based SSL problem.",
                    "sid": 64,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "See text for the significance of the shaded and dotted factors, and the shaded variables.",
                    "sid": 65,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "graph-based methods that generalize prior research.",
                    "sid": 66,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We desire the following properties to be satisfied in the factors: (i) convexity of Eq.",
                    "sid": 67,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1, (ii) amenability to scalable optimization algorithms, and (iii) sparse solutions as expected in natural language lexicons.",
                    "sid": 68,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pairwise factors: In our work, for the pairwise factors ?j(Xj , X?j) and ?i?k(Xi, Xk), we examine two functions that penalize inconsistencies between neighboring vertices: the squared `2 norm and the Jensen-Shannon (JS) divergence (Burbea and Rao,1982; Lin, 1991), which is a symmetrized gener alization of the Kullback-Leibler (KL) divergence (Kullback and Leibler, 1951; Cover and Thomas, 1991).",
                    "sid": 69,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These two divergences are symmetric.",
                    "sid": 70,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both are inspired by previous work; however, the use ofthe JS divergence is a novel extension to Subra manya and Bilmes (2008).",
                    "sid": 71,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Specifically, the factors are: ln?j(Xj , X?j) = ??(qj , rj) (2) ln?i?k(Xi, Xk) = ?2 ? ?",
                    "sid": 72,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "wik ? ?(qi, qk) (3)where ? is a hyperparameter whose choice we dis cuss in ?4.",
                    "sid": 73,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The function ?(u, v) for two vectors u and v is defined in two ways: ?(u, v) Gaussian = ?u? v?22 (4) ?(u, v) Entropic = 12 ? y?Y ( u(y) ? ln 2 ? u(y)u(y) + v(y) + v(y) ? ln 2 ? v(y)u(y) + v(y) ) (5) We call the version of ?(u, v) that uses the squared `2 distance (Eq.",
                    "sid": 74,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4) Gaussian, as it represents the idea of label propagation via Gaussian fields proposed by Zhu et al (2003).",
                    "sid": 75,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A minor difference lies in the fact that we include variables Xj , j ? Vl for labeled 679vertices too, and allow them to change, but penal ize them if they go too far away from the observed labeled distributions rj . The other ?(u, v) shown in Eq.",
                    "sid": 76,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 uses the generalized JS-divergence defined interms of the generalized KL-divergence for unnor malized measures (O?Sullivan, 1998).4Eq.",
                    "sid": 77,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 improves prior work by replacing the asym metric KL-divergence used to bring the distributionsat labeled vertices close to the corresponding observed distributions, as well as replacing the KL based graph smoothness term with the symmetric JS-divergence (Subramanya and Bilmes, 2008, see first two terms in Eq.",
                    "sid": 78,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1).",
                    "sid": 79,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Empirical evidence showsthat entropic divergences help in multiclass prob lems where a vertex can assume multiple labels, and may perform better than objectives with quadratic penalties (Subramanya and Bilmes, 2008, 2009).",
                    "sid": 80,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A major departure from prior work is the use of unnormalized measures in Eq.",
                    "sid": 81,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4-5, which simplifies optimization even with the complex JS-divergence in the objective function (see ?3), and, we will see, produces comparable and often better results than baselines using normalized distributions (see ?4).",
                    "sid": 82,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unary factors: The unary factors in our factorgraph ?i(Xi) can incorporate prior information specific to a particular vertex xi embodied by the vari able Xi.",
                    "sid": 83,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Herein, we examine three straightforward penalties, which can be thought of as penalties that encourage either uniformity or sparsity: Uniform squared `2: ln?i(Xi) = ??",
                    "sid": 84,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "?qi ? 1|Y | ? ?",
                    "sid": 85,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 2 (6) Sparse `1: ln?i(Xi) = ??",
                    "sid": 86,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "?qi?1 (7) Sparse `1,2: ln?i(Xi) = ??",
                    "sid": 87,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "?qi?21 (8)where ? is a hyperparameter whose choice we discuss in ?4.",
                    "sid": 88,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The penalty expressed in Eq.",
                    "sid": 89,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6 penalizes qi if it is far away from the uniform distribu tion.",
                    "sid": 90,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This penalty has been used previously (Das and Petrov, 2011; Das and Smith, 2011; Subramanya et al., 2010), and is similar to the maximum entropy penalty of Subramanya and Bilmes (2008, 2009).The intuition behind its use is that for low confi dence or disconnected regions, one would prefer to have a uniform measure on a graph vertex.",
                    "sid": 91,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thepenalties in equations 7?8, on the other hand, en courage sparsity in the measure qi; these are related 4The generalized KL divergence is defined asDKL(u?v) = ? y ( u(y) ln u(y)v(y) ? u(y) + v(y) ) . to regularizers for generalized linear models: thelasso (Tibshirani, 1996) and the elitist lasso (Kowal ski and Torre?sani, 2009).",
                    "sid": 92,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The former encourages global sparsity, the latter sparsity per vertex.5 For each vertex, the `1,2 penalty takes the form: ?qi?21 = ? ?",
                    "sid": 93,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "y?Y |qi(y)| ? ?",
                    "sid": 94,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 (9) The `1 norm encourages its argument to be sparse, while the usual observed effect of an `2 norm is a dense vector without many extreme values.",
                    "sid": 95,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The `1,2 penalty is the squared `2 norm of the `1 norms ofevery qi, hence it promotes sparsity within each ver tex, but we observe density over the vertices that are selected.Talukdar (2010) enforced label sparsity for information extraction by discarding poorly scored la bels during graph propagation updates, but did notuse a principled mechanism to arrive at sparse mea sures at graph vertices.",
                    "sid": 96,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unlike the uniform penalty (Eq.",
                    "sid": 97,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6), sparsity corresponds to the idea of entropy minimization (Grandvalet and Bengio, 2004).",
                    "sid": 98,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since we use unnormalized measures at each variable Xi, for low confidence graph regions or disconnectedvertices, sparse penalties will result in all zero components in qi, which conveys that the graph prop agation algorithm is not confident on any potential label, a condition that is perfectly acceptable.Model variants: We compare six objective func tions: we combine factor representations from eachof Eqs.",
                    "sid": 99,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4?5 with those from each of Eqs.",
                    "sid": 100,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6?8, replac ing them in the generic graph objective function of Eq.",
                    "sid": 101,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.",
                    "sid": 102,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The nature of these six models is succinctly summarized in Table 1.",
                    "sid": 103,
                    "ssid": 75,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each model, we findthe best set of measures q that maximize the corre sponding graph objective functions, such that q ? 0.",
                    "sid": 104,
                    "ssid": 76,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that in each of the graph objectives, we havetwo hyperparameters ? and ? that control the influence of the second and the third terms of Eq.",
                    "sid": 105,
                    "ssid": 77,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 re 5One could additionally consider a non-sparse penalty based on the squared `2 norm with zero mean: ln?i(Xi) = ??",
                    "sid": 106,
                    "ssid": 78,
                    "kind_of_tag": "s"
                },
                {
                    "text": "?qi?",
                    "sid": 107,
                    "ssid": 79,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 2.",
                    "sid": 108,
                    "ssid": 80,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We experimented with this unary penalty (along with the.",
                    "sid": 109,
                    "ssid": 81,
                    "kind_of_tag": "s"
                },
                {
                    "text": "pairwise Gaussian penalty for binary factors) for the semantic frame lexicon expansion problem, and found that it performs exactly on par with the squared `2 penalty with uniform mean.",
                    "sid": 110,
                    "ssid": 82,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To limit the number of non-sparse graph objectives, we omit detailed discussion of experiments with this unary penalty.",
                    "sid": 111,
                    "ssid": 83,
                    "kind_of_tag": "s"
                },
                {
                    "text": "680 abbrev.",
                    "sid": 112,
                    "ssid": 84,
                    "kind_of_tag": "s"
                },
                {
                    "text": "factors pairwise unary UGF-`2 Gaussian Uniform squared `2 UGF-`1 Gaussian Sparse `1 UGF-`1,2 Gaussian Sparse `1,2 UJSF-`2 Entropic Uniform squared `2 UJSF-`1 Entropic Sparse `1 UJSF-`1,2 Entropic Sparse `1,2 Table 1: Six variants of graph objective functions novel to this work.",
                    "sid": 113,
                    "ssid": 85,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These variants combine the pairwise factorrepresentations from Eqs.",
                    "sid": 114,
                    "ssid": 86,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4?5 with unary factor representations from each of Eqs.",
                    "sid": 115,
                    "ssid": 87,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6?8 (which either encour age uniform or sparse measures), to be used in the graph objective function expressed in Eq.",
                    "sid": 116,
                    "ssid": 88,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.",
                    "sid": 117,
                    "ssid": 89,
                    "kind_of_tag": "s"
                },
                {
                    "text": "spectively.",
                    "sid": 118,
                    "ssid": 90,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We discuss how these hyperparameters are chosen in ?4.",
                    "sid": 119,
                    "ssid": 91,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Baseline Models: We compare the performance of the six graph objectives of Table 1 with two strong baselines that have been used in previouswork.",
                    "sid": 120,
                    "ssid": 92,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These two models use the following two ob jective functions, and find q s.t. q ? 0 and ?i ? V, ?y?Y qi(y) = 1.",
                    "sid": 121,
                    "ssid": 93,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first is a normalized Gaus sian field with a squared uniform `2 penalty as the unary factor (NGF-`2): arg min q, s.t. q?0, ?i?V,?qi?1=1 l?",
                    "sid": 122,
                    "ssid": 94,
                    "kind_of_tag": "s"
                },
                {
                    "text": "j=1 ?qj ? rj?22 + m?",
                    "sid": 123,
                    "ssid": 95,
                    "kind_of_tag": "s"
                },
                {
                    "text": "i=1 ? ??",
                    "sid": 124,
                    "ssid": 96,
                    "kind_of_tag": "s"
                },
                {
                    "text": "k?N (i) wik ?qi ? qk?22 + ? ?",
                    "sid": 125,
                    "ssid": 97,
                    "kind_of_tag": "s"
                },
                {
                    "text": "?qi ? 1|Y | ? ?",
                    "sid": 126,
                    "ssid": 98,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 2 ? ?(10) The second is a normalized KL field with an entropy penalty as the unary factor (NKLF-ME): arg min q, s.t. q?0, ?i?V,?qi?1=1 l?",
                    "sid": 127,
                    "ssid": 99,
                    "kind_of_tag": "s"
                },
                {
                    "text": "j=1 DKL(rj ? qj)+ m?",
                    "sid": 128,
                    "ssid": 100,
                    "kind_of_tag": "s"
                },
                {
                    "text": "i=1 ? ??",
                    "sid": 129,
                    "ssid": 101,
                    "kind_of_tag": "s"
                },
                {
                    "text": "k?N (i) wikDKL(qi ? qk)?",
                    "sid": 130,
                    "ssid": 102,
                    "kind_of_tag": "s"
                },
                {
                    "text": "?H(qi) ? ?(11)whereH(qi) denotes the Shannon entropy of the dis tribution qi.",
                    "sid": 131,
                    "ssid": 103,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both these objectives are constrainedby the fact that every qi must be within the |Y | dimensional probability simplex.",
                    "sid": 132,
                    "ssid": 104,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The objective function in 10 has been used previously (Das and Smith, 2011; Subramanya et al, 2010) and serves as a generalization of Zhu et al (2003).",
                    "sid": 133,
                    "ssid": 105,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The entropic objective function in 11, originally called measurepropagation, performed better at multiclass prob lems when compared to graph objectives using the quadratic criterion (Subramanya and Bilmes, 2008).",
                    "sid": 134,
                    "ssid": 106,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "optimization. ",
            "number": "3",
            "sents": [
                {
                    "text": "The six variants of Eq.",
                    "sid": 135,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 in Table 1 are convex in q. This is because the `1, squared `2 and the`1,2 penalties are convex.",
                    "sid": 136,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, the generalized JS-divergence term, which is a sum of two KL divergence terms, is convex (Cover and Thomas,1991).",
                    "sid": 137,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since we choose ?, ? and wik to be non negative, these terms?",
                    "sid": 138,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "sums are also convex.",
                    "sid": 139,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thegraph objectives of the two baselines noted in ex pressions 10?11 are also convex because negative entropy in expression 11 is convex, and rest of the penalties are the same as our six objectives.",
                    "sid": 140,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our work, to optimize the objectives of Table 1, we use a generic quasi-Newton gradient-based optimizer thatcan handle bound-inequality constraints, called L BFGS-B (Zhu et al, 1997).",
                    "sid": 141,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Partial derivatives of the graph objectives are computed with respect to each parameter ?i, y, qi(y) of q and passed on tothe optimizer which updates them such that the ob jective function of Eq.",
                    "sid": 142,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 is maximized.",
                    "sid": 143,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that since the `1 and `1,2 penalties are non-differentiable at 0, special techniques are usually used to compute updates for unconstrained parameters (Andrew and Gao, 2007).",
                    "sid": 144,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, since q ? 0, their absolutevalue can be assumed to be right-continuous, mak ing the function differentiable.",
                    "sid": 145,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, ? ?qi(y) ?qi?1 = 1 ? ?qi(y) ?qi?21 = 2 ? ?qi?1 (We omit the form of the derivatives of the other penalties for space.)",
                    "sid": 146,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are several advantages to taking this route towards optimization.",
                    "sid": 147,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The `2 and the JS-divergence penalties for the pairwise termscan be replaced with more interesting convex di vergences if required, and still optimization will bestraightforward.",
                    "sid": 148,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, the nonnegative con straints make optimization with sparsity inducing penalties easy.",
                    "sid": 149,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, computing the objectivefunction and the partial derivatives is easily paral lelizable on MPI (Gropp et al, 1994) or MapReduce(Dean and Ghemawat, 2008) architectures, by divid ing up the computation across graph vertices.",
                    "sid": 150,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In comparison, constrained problems such as theone in Eq.",
                    "sid": 151,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "11 require a specialized alternating mini 681 mization technique (Subramanya and Bilmes, 2008, 2009), that performs two passes through the graph vertices during one iteration of updates, introducesan auxiliary set of probability distributions (thus, increasing memory requirements) and another hyper parameter ? that is used to transform the weightmatrix W to be suitable for the alternating minimization procedure.",
                    "sid": 152,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To optimize the baseline objectives, we borrow the gradient-free iterative up dates described by Subramanya and Bilmes (2009) and Subramanya et al (2010).",
                    "sid": 153,
                    "ssid": 19,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "experiments. ",
            "number": "4",
            "sents": [
                {
                    "text": "In this section, we compare the six graph objective functions in Table 1 with the two baseline objectives on two lexicon expansion tasks.",
                    "sid": 154,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.1 POS Lexicon Expansion.",
                    "sid": 155,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We expand a POS lexicon for word types with a context word on each side, using distributional similar ity in an unlabeled corpus and few labeled trigrams.",
                    "sid": 156,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Data and task: We constructed a graph over wordtrigram types as vertices, using co-occurrence statistics.",
                    "sid": 157,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Following Das and Petrov (2011) and Sub ramanya et al (2010), a similarity score between two trigram types was computed by measuring thecosine similarity between their empirical senten tial context statistics.",
                    "sid": 158,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This similarity score resulted in the symmetric weight matrix W, defining edge weights between pairs of graph vertices.",
                    "sid": 159,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Detailsof the similarity computation are given in those papers.",
                    "sid": 160,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "W is thresholded so that only the K near est neighbors for each vertex have similarity greater than zero, giving a sparse graph.",
                    "sid": 161,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We set K = 8 as itresulted in the sparsest graph which was fully con nected.6 For this task, Y is the set of 45 POS tags defined in the Penn Treebank (Marcus et al, 1993), and the measure qi for vertex i (for trigram type xi) corresponds to the set of tags that can be associatedwith the middle word of xi.",
                    "sid": 162,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The trigram representation, as in earlier work, helps reduce the ambi guity of POS tags for the middle word, and helps in graph construction.",
                    "sid": 163,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The 690,705-vertex graph was constructed over all trigram types appearing in6Our proposed methods can deal with graphs containing disconnected components perfectly well.",
                    "sid": 164,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Runtime is asymptoti cally linear in K for all objectives considered here.Sections 00?21 (union of the training and develop ment sets used for POS tagging experiments in prior work) of the WSJ section of the Penn Treebank, but co-occurrence statistics for graph construction were gathered from a million sentences drawn from the English Gigaword corpus (Graff, 2003).",
                    "sid": 165,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given the graph G with m vertices, we assume that the tag distributions r for l labeled vertices are also provided.",
                    "sid": 166,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our goal is to find the best set of measures q over the 45 tags for all vertices in the graph.",
                    "sid": 167,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Prior work used a similar lexicon for POSdomain adaptation and POS induction for resource poor languages (Das and Petrov, 2011; Subramanya et al, 2010); such applications of a POS lexicon areout of scope here; we consider only the lexicon ex pansion problem and do an intrinsic evaluation at a type-level to compare the different graph objectives.",
                    "sid": 168,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Experimental details: To evaluate, we randomlychose 6,000 out of the 690,705 types for devel opment.",
                    "sid": 169,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From the remaining types, we randomly chose 588,705 vertices for testing.",
                    "sid": 170,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This left us with96,000 types from which we created sets of differ ent sizes containing 3,000, 6,000, 12,000, 24,000,48,000 and 96,000 labeled types, creating 6 increas ingly easy transduction settings.",
                    "sid": 171,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The developmentand the test types were kept constant for direct per formance comparison across the six settings and oureight models.",
                    "sid": 172,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After running inference, the mea sure qi at vertex i was normalized to 1.",
                    "sid": 173,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Next, for all thresholds ranging from 0 to 1, with steps of 0.001, we measured the average POS tag precision and recall on the development data ? this gave us the area under the precision-recall curve (prAUC),which is often used to measure performance on re trieval tasks.",
                    "sid": 174,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a transduction setting and the final q for an objective, hyperparameters ? and ? were tuned on the development set by performing a grid search, targeting prAUC.7 We ran 100 rounds7For the objectives using the uniform `2 and the maxi mum entropy penalties, namely UGF-`2, UJSF-`2, NGF-`2 and NKLF-ME, we chose ? from {0, 10?6, 10?4, 0.1}.",
                    "sid": 175,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the rest of the models using sparsity inducing penalties, we chose ? from {10?6, 10?4, 0.1}.",
                    "sid": 176,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This suggests that for the formertype of objectives, we allowed a zero unary penalty if that set ting resulted in the best development performance, while for the latter type of models, we enforced a positive unary penalty.",
                    "sid": 177,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In fact, ? = 0 was chosen in several cases for the objectives with uniform penalties indicating that uniformity hurts performance.",
                    "sid": 178,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We chose ? from {0.1, 0.5, 1.0}.",
                    "sid": 179,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "682 |Dl|: 3K 6K 12K 24K 48K 96K NGF-`2 0.208 0.219 0.272 0.335 0.430 0.544 NKLF-ME 0.223 0.227 0.276 0.338 0.411 0.506 UGF-`2 0.223 0.257 0.314 0.406 0.483 0.564 UGF-`1 0.223 0.257 0.309 0.406 0.483 0.556 UGF-`1,2 0.223 0.256 0.313 0.403 0.478 0.557 UJSF-`2 0.271 0.250 0.310 0.364 0.409 0.481 UJSF-`1 0.227 0.257 0.317 0.369 0.410 0.481 UJSF-`1,2 0.227 0.258 0.309 0.369 0.409 0.479 Table 2: Area under the precision recall curve for the two baseline objectives and our methods for POS tag lexicon induction.",
                    "sid": 180,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is a measure of how well the type lexicon (for some types unlabeled during training) is recovered by each method.",
                    "sid": 181,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The test set contains 588,705 types.",
                    "sid": 182,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "of iterative updates for all 8 graph objectives.",
                    "sid": 183,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Type-level evaluation: To measure the quality ofthe lexicons, we perform type level evaluation us ing area under the precision-recall curve (prAUC).",
                    "sid": 184,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The same measure (on development data) was used to tune the two hyperparameters.",
                    "sid": 185,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 shows the results measured on 588,705 test vertices (the same test set was used for all the transduction settings).The general pattern we observe is that our unnor malized approaches almost always perform better than the normalized baselines.",
                    "sid": 186,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(The exception isthe 3,000 labeled example case, where most unnor malized models are on par with the better baseline.)In scenarios with fewer labeled types, pairwise en tropic penalties perform better than Gaussian ones, and the pattern reverses as more labeled types come available.",
                    "sid": 187,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This trend is the same when we compareonly the two baselines.",
                    "sid": 188,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In four out of the six trans duction settings, one of the sparsity-inducing graph objectives achieves the best performance in terms ofprAUC, which is encouraging given that they gener ally produce smaller models than the baselines.",
                    "sid": 189,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Overall, though, using sparsity-inducing unaryfactors seems to have a weak negative effect on performance.",
                    "sid": 190,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Their practical advantage, however is apparent when we consider the size of the model.",
                    "sid": 191,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Af ter the induction of the set of measures q for all transduction settings and all graph objectives, wenoticed that our numerical optimizer (LBFGS-B) of ten assigns extremely small positive values ratherthan zero.",
                    "sid": 192,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This problem can be attributed to sev eral artifacts, including our limit of 100 iterations of optimization.",
                    "sid": 193,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hence, we use a global threshold of 10?6, and treat any real value below this threshold 0M 5M 11M 16M 21M 27M 32M 3k 6k 12k 24k 48k 96k UGF-?1 UGF-?1,2 UJSF-?2 UJSF-?1 UJSF-?1,2 Figure 2: The number of non-zero components in q forfive graph objective functions proposed in this work, plot ted against various numbers of labeled datapoints.",
                    "sid": 194,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that NGF-`2, NKLF-ME and UGF-`2 produce non-zero components for virtually all q, and are therefore not shown (the dotted line marks the maximally non-sparse solution, with 31,081,725 components).",
                    "sid": 195,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All of these five objectives result in sparsity.",
                    "sid": 196,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On average, the objectives employing entropic pairwise penalties with sparse unary penalties UJSF-`1 and UJSF-`1,2 produce very sparse lexicons.",
                    "sid": 197,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although UGF-`2 produces no sparsity at all, its entropic counterpart UJSF-`2 produces considerablesparsity, which we attribute to JS-divergence as a pair wise penalty.",
                    "sid": 198,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "to be zero.",
                    "sid": 199,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 2 shows the number of non-zero components in q (or, the lexicon size) for the graph objectives that achieve sparsity (baselines NGF-`2 and NKLF-ME, plus our UGF-`2 are not expectedto, and do not, achieve sparsity; surprisingly UJSF`2 does and is shown).",
                    "sid": 200,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Even though the hyperpa rameters ? and ? in the graph objective functionswere not tuned towards sparsity, we see that sparsityinducing factors are able to achieve far more com pact lexicons.",
                    "sid": 201,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011).",
                    "sid": 202,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.2 Expansion of a Semantic Frame Lexicon.",
                    "sid": 203,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In a second set of experiments, we follow Das and Smith (2011, DS11 henceforth) in expanding a lexicon that associates lexical predicates (targets) with semantic frames (abstract events or scenarios that a predicate evokes when used in a sentential context) as labels.",
                    "sid": 204,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More concretely, each vertex in the graph corresponds to a lemmatized word type with its coarse part of speech, and the labels are frames from the FrameNet lexicon (Fillmore et al, 2003).",
                    "sid": 205,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Graph construction leverages distributional 683 UNKNOWN ALL PREDICATES PREDICATES lexicon exact partial exact partial sizeSupervised 23.08 46.62 82.97 90.51 ?NGF-`2 39.86 62.35 83.51 91.02 128,960 NKLF-ME 36.36 60.07 83.40 90.95 128,960 UGF-`2 37.76 60.81 83.44 90.97 128,960 UGF-`1 39.86 62.85 83.51 91.04 122,799 UGF-`1,2 39.86 62.85 83.51 91.04 128,732 UJSF-`2 40.56 62.81 83.53 91.04 128,232 UJSF-`1 39.16 62.43 83.49 91.02 128,771 UJSF-`1,2 42.67 65.29 83.60 91.12 45,544 Table 3: Exact and partial frame identification accuracywith lexicon size (non-zero frame components).",
                    "sid": 206,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The ?un known predicates?",
                    "sid": 207,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "section of the test data contains 144 targets, while the entire test set contains 4,458 targets.Bold indicates best results.",
                    "sid": 208,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The UJSF-`1,2 model pro duces statistically significant results (p  0.001) for all metrics with respect to the supervised baseline used in DS11.",
                    "sid": 209,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For both the unknown targets as well as the whole test set.",
                    "sid": 210,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, it is weakly significant (p  0.1) compared to the NGF-`2 model for the unseen portion of the test set, when partial frame matching is used.",
                    "sid": 211,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For rest of the settings, the two are statistically indistinguishable.",
                    "sid": 212,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "indicates the best results in DS11.",
                    "sid": 213,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "similarity as well as linguistic annotations.",
                    "sid": 214,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Data: We borrow the graph-based SSL process ofDS11 in its entirety.",
                    "sid": 215,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The constructed graph contains 64,480 vertices, each corresponding to a tar get, out of which 9,263 were drawn from the labeled data.",
                    "sid": 216,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The possible set of labels Y is the set of 877frames defined in FrameNet; the measure qi corre sponds to the set of frames that a target can evoke.",
                    "sid": 217,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The targets drawn from FrameNet annotated data (l = 9,263) have frame distributions ri with which the graph objectives are seeded.8 Evaluation: The evaluation metric used for this task is frame disambiguation accuracy on a blind test set containing marked targets in free text.",
                    "sid": 218,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A sectionof this test set contained 144 targets, previously un seen in annotated FrameNet data; this section is ofinterest to us and we present separate accuracy results on it.",
                    "sid": 219,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given the measure qi over frames induced using graph-based SSL for target i, we trun cate it to keep at most the top M frames that get the highest mass under qi, only retaining those with non-zero values.",
                    "sid": 220,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If all components of qi are zero,we remove target i from the lexicon, which is often the case in the sparsity-inducing graph objectives.",
                    "sid": 221,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If a target is unseen in annotated data, a separate probabilistic model (which serves as a supervised baseline like in DS11, row 1 in Table 3) dis ambiguates among the M filtered frames observing the sentential context of the target instance.",
                    "sid": 222,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thiscan be thought of as combining type- and token level information for inference.",
                    "sid": 223,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the target waspreviously seen, it is disambiguated using the su 8We refer the reader to DS11 for the details of the graphconstruction method, the FrameNet dataset used, example se mantic frames, and an excerpt of the graph over targets.pervised baseline.",
                    "sid": 224,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The test set and the probabilis tic model are identical to the ones in DS11.",
                    "sid": 225,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We fixed K, the number of nearest neighbors for each vertex, to be 10.",
                    "sid": 226,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each graph objective, ?, ? and M were chosen by five-fold cross-validation.",
                    "sid": 227,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thecross-validation sets were the same as the ones de scribed in ?6.3 of DS11.9Results and discussion: Table 3 shows frame iden tification accuracy, both using exact match as wellas partial match that assigns partial credit when a re lated frame is predicted (Baker et al, 2007).",
                    "sid": 228,
                    "ssid": 75,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The final column presents lexicon size in terms of the set of truncated frame distributions (filtered according to the top M frames in qi) for all the targets in a graph.",
                    "sid": 229,
                    "ssid": 76,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All the graph-based models are better than the supervised baseline; for our objectives usingpairwise Gaussian fields with sparse unary penal ties, the accuracies are equal or better with respect to NGF-`2; however, the lexicon sizes are reduced by a few hundred to a few thousand entries.",
                    "sid": 230,
                    "ssid": 77,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Massive reduction in lexicon sizes (as in the POS problem in ?4.1) is not visible for these objectives because we throw out most of the components of the entire set of distributions q and keep only at most the top M(which is automatically chosen to be 2 for all ob jectives) frames per target.",
                    "sid": 231,
                    "ssid": 78,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although a significant number of components in the whole distribution qin the sparse objectives get zero mass, the M components for a target tend to be non-zero for a major ity of the targets.",
                    "sid": 232,
                    "ssid": 79,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Better results are observed for theobjectives using entropic pairwise penalties; the ob 9We chose ? from {0.01, 0.1, 0.3, 0.5, 1.0}; ? was chosen from the same sets as the POS problem.",
                    "sid": 233,
                    "ssid": 80,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The graph construction hyperparameter ? described by DS11 was fixed to 0.2.",
                    "sid": 234,
                    "ssid": 81,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As in DS11, M was chosen from {2, 3, 5, 10}.",
                    "sid": 235,
                    "ssid": 82,
                    "kind_of_tag": "s"
                },
                {
                    "text": "684 (a) t = discrepancy.N t = contribution.N t = print.V t = mislead.V ?SIMILARITY ?GIVING ?TEXT CREATION EXPERIENCER OBJ NATURAL FEATURES MONEY SENDING ?PREVARICATION PREVARICATION COMMITMENT DISPERSAL MANIPULATE INTO DOING QUARRELING ASSISTANCE READING COMPLIANCE DUPLICATION EARNINGS AND LOSSES STATEMENT EVIDENCE t = abused.A t = maker.N t = inspire.V t = failed.A OFFENSES COMMERCE SCENARIO CAUSE TO START SUCCESS OR FAILURE KILLING ?MANUFACTURING EXPERIENCER OBJ ?SUCCESSFUL ACTION COMPLIANCE BUSINESSES ?SUBJECTIVE INFLUENCE UNATTRIBUTED INFORMATION DIFFERENTIATION BEHIND THE SCENES EVOKING PIRACY COMMITTING CRIME SUPPLY ATTEMPT SUASION WANT SUSPECT (b) t = discrepancy.N t = contribution.N t = print.V t = mislead.V ?SIMILARITY ?GIVING ?TEXT CREATION ?PREVARICATION NON-COMMUTATIVE STATEMENT COMMERCE PAY STATE OF ENTITY EXPERIENCER OBJ NATURAL FEATURES COMMITMENT DISPERSAL MANIPULATE INTO DOING ASSISTANCE CONTACTING REASSURING EARNINGS AND LOSSES READING EVIDENCE t = abused.A t = maker.N t = inspire.V t = failed.A ?MANUFACTURING CAUSE TO START ?SUCCESSFUL ACTION BUSINESSES ?SUBJECTIVE INFLUENCE SUCCESSFULLY COMMUNICATE MESSAGE COMMERCE SCENARIO OBJECTIVE INFLUENCE SUPPLY EXPERIENCER OBJ BEING ACTIVE SETTING FIRE Table 4: Top 5 frames (if there are ? 5 frames with mass greater than zero) according to the graph posterior qt(f) for (a) NGF-`2 and (b) UJSF-`1,2, given eight unseen predicates in annotated FrameNet data.",
                    "sid": 236,
                    "ssid": 83,
                    "kind_of_tag": "s"
                },
                {
                    "text": "marks the correct frame, according to the predicate instances in test data (each of these predicates appear only once in test data).",
                    "sid": 237,
                    "ssid": 84,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that UJSF-`1,2 ranks the correct frame higher than NGF-`2 for several predicates, and produces sparsity quite often; for the predicate abused.A, the correct frame is not listed by NGF-`2, while UJSF-`1,2 removes it altogether from the expanded lexicon, resulting in compactness.",
                    "sid": 238,
                    "ssid": 85,
                    "kind_of_tag": "s"
                },
                {
                    "text": "jective UJSF-`1,2 gives us the best absolute result by outperforming the baselines by strong margins, and also resulting in a tiny lexicon, less than half the size of the baseline lexicons.",
                    "sid": 239,
                    "ssid": 86,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The size can be attributed tothe removal of predicates for which all frame com ponents were zero (qi = 0).",
                    "sid": 240,
                    "ssid": 87,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 4 contrasts the induced frames for several unseen predicates for the NGF-`2 and the UJSF-`2 objectives; the latter often ranks the correct frame higher, and produces a small set of frames per predicate.",
                    "sid": 241,
                    "ssid": 88,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusion. ",
            "number": "5",
            "sents": [
                {
                    "text": "We have presented a family of graph-based SSL objective functions that incorporate penalties encour aging sparse measures at each graph vertex.",
                    "sid": 242,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ourmethods relax the oft-used assumption that the measures at each vertex form a normalized probabil ity distribution, making optimization and the use ofcomplex penalties easier than prior work.",
                    "sid": 243,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Optimiza tion is also easy when there are additional terms in a graph objective suited to a specific problem; ourgeneric optimizer would simply require the compu tation of new partial derivatives, unlike prior workthat required specialized techniques for a novel ob jective function.",
                    "sid": 244,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, experiments on two natural language lexicon learning problems show that our methods produce better performance with respect to state-of-the-art graph-based SSL methods, and also result in much smaller lexicons.",
                    "sid": 245,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Acknowledgments We thank Andre?",
                    "sid": 246,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Martins, Amar Subramanya, and Partha Talukdar for helpful discussion during the progress of thiswork and the three anonymous reviewers for their valuable feedback.",
                    "sid": 247,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This research was supported by Qatar Na tional Research Foundation grant NPRP 08-485-1-083, Google?s support of the Worldly Knowledge Project, andTeraGrid resources provided by the Pittsburgh Supercom puting Center under NSF grant number TG-DBS110003.",
                    "sid": 248,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "685",
                    "sid": 249,
                    "ssid": 8,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}