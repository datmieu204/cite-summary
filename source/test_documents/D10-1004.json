{
    "ID": "D10-1004",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Turbo Parsers: Dependency Parsing by Approximate Variational Inference",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009).",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Experiments show state-of-the-art performance for 14 languages.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Feature-rich discriminative models that break locality/independence assumptions can boost a parser\u2019s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010).",
                    "sid": 6,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms.",
                    "sid": 7,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009).",
                    "sid": 8,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While those two parsers are differently motivated, we show that both correspond to inference in a factor graph, and both optimize objective functions over local approximations of the marginal polytope.",
                    "sid": 9,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al. (2009).",
                    "sid": 10,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The success of both approaches parallels similar approximations in other fields, such as statistical image processing and error-correcting coding.",
                    "sid": 11,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Throughtout, we call these turbo parsers.1 Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints (\u00a72), which extends some combinatorial factors considered by Smith and Eisner (2008).",
                    "sid": 12,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After presenting a geometric view of the variational approximations underlying message-passing algorithms (\u00a73), and closing the gap between the two aforementioned parsers (\u00a74), we consider the problem of learning the model parameters (\u00a75).",
                    "sid": 13,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To this end, we propose an aggressive online algorithm that generalizes MIRA (Crammer et al., 2006) to arbitrary loss functions.",
                    "sid": 14,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We adopt a family of losses subsuming CRFs (Lafferty et al., 2001) and structured SVMs (Taskar et al., 2003; Tsochantaridis et al., 2004).",
                    "sid": 15,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we present a technique for including features not attested in the training data, allowing for richer models without substantial runtime costs.",
                    "sid": 16,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our experiments (\u00a76) show state-of-the-art performance on dependency parsing benchmarks.",
                    "sid": 17,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Denote by X a set of input objects from which we want to infer some hidden structure conveyed in an output set Y.",
                    "sid": 18,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each input x \u2208 X (e.g., a sentence) is associated with a set of candidate outputs Y(x) \u2286 Y (e.g., parse trees); we are interested in the case where Y(x) is a large structured set.",
                    "sid": 19,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Choices about the representation of elements of Y(x) play a major role in algorithm design.",
                    "sid": 20,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In many problems, the elements of Y(x) can be represented as discrete-valued vectors of the form y = hy1, ... , yIi, each yi taking values in a label set Yi.",
                    "sid": 21,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, in unlabeled dependency parsing, I is the number of candidate dependency arcs (quadratic in the sentence length), and each Yi = {0, 1}.",
                    "sid": 22,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Of course, the yi are highly interdependent.",
                    "sid": 23,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Factor Graphs.",
                    "sid": 24,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Probabilistic models like CRFs (Lafferty et al., 2001) assume a factorization of the conditional distribution of Y , where each C \u2286 {1, ... , I} is a factor, C is the set of factors, each yC \u00b0_ hyiiiEC denotes a partial output assignment, and each 'FC is a nonnegative potential function that depends on the output only via its restriction to C. A factor graph (Kschischang et al., 2001) is a convenient representation for the factorization in Eq.",
                    "sid": 25,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1: it is a bipartite graph Gx comprised of variable nodes {1, ... , I} and factor nodes C \u2208 C, with an edge connecting the ith variable node and a factor node C iff i \u2208 C. Hence, the factor graph Gx makes explicit the direct dependencies among the variables {y1, ... , yI}.",
                    "sid": 26,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Factor graphs have been used for several NLP tasks, such as dependency parsing, segmentation, and co-reference resolution (Sutton et al., 2007; Smith and Eisner, 2008; McCallum et al., 2009).",
                    "sid": 27,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hard and Soft Constraint Factors.",
                    "sid": 28,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It may be the case that valid outputs are a proper subset of Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 YI\u2014for example, in dependency parsing, the entries of the output vector y must jointly define a spanning tree.",
                    "sid": 29,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This requires hard constraint factors that rule out forbidden partial assignments by mapping them to zero potential values.",
                    "sid": 30,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "See Table 1 for an inventory of hard constraint factors used in this paper.",
                    "sid": 31,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Factors that are not of this special kind are called soft factors, and have strictly positive potentials.",
                    "sid": 32,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We thus have a partition C = Chard \u222a Csoft.",
                    "sid": 33,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We let the soft factor potentials take the form `pC(x,yC) \u00b0_ exp(\u03b8T\u03c6C(x,yC)), where \u03b8 \u2208 Rd is a vector of parameters (shared across factors) and \u03c6C(x, yC) is a local feature vector.",
                    "sid": 34,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The conditional distribution of Y (Eq.",
                    "sid": 35,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1) thus becomes log-linear: where Zx(\u03b8) \u00b0_ Ey,E%x) exp(\u03b8T\u03c6(x, y')) is the partition function, and the features decompose as: Dependency Parsing.",
                    "sid": 36,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig.",
                    "sid": 37,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1).",
                    "sid": 38,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The graph has O(n2) variable nodes (n is the sentence length), one per candidate arc a \u00b0_ hh, mi linking a head h and modifier m. Outputs are binary, with ya = 1 iff arc a belongs to the dependency tree.",
                    "sid": 39,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There is a hard factor TREE connected to all variables, that constrains the overall arc configurations to form a spanning tree.",
                    "sid": 40,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There is a unary soft factor per arc, whose log-potential reflects the score of that arc.",
                    "sid": 41,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are also O(n3) pairwise factors; their log-potentials reflect the scores of sibling and grandparent arcs.",
                    "sid": 42,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These factors create loops, thus calling for approximate inference.",
                    "sid": 43,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Without them, the model is arc-factored, and exact inference in it is well studied: finding the most probable parse tree takes O(n3) time with the ChuLiu-Edmonds algorithm (McDonald et al., 2005),2 and computing posterior marginals for all arcs takes O(n3) time via the matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007).",
                    "sid": 44,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Message-passing algorithms.",
                    "sid": 45,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In general factor graphs, both inference problems\u2014 obtaining the most probable output (the MAP) argmaxyE%x) Pre(y|x), and computing the marginals Pre(Yi = yi|x)\u2014can be addressed with the belief propagation (BP) algorithm (Pearl, 1988), which iteratively passes messages between variables and factors reflecting their local \u201cbeliefs.\u201d In sum-product BP, the messages take the form:3 In max-product BP, the summation in Eq.",
                    "sid": 46,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 is replaced by a maximization.",
                    "sid": 47,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Upon convergence, variable and factor beliefs are computed as: to the true marginals, and in the max-product case, maximizing each Ti(yi) yields the MAP output.",
                    "sid": 48,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In graphs with loops, BP is an approximate method, not guaranteed to converge, nicknamed loopy BP.",
                    "sid": 49,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We highlight a variational perspective of loopy BP in \u00a73; for now we consider algorithmic issues.",
                    "sid": 50,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that computing the factor-to-variable messages for each factor C (Eq.",
                    "sid": 51,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5) requires a summation/maximization over exponentially many configurations.",
                    "sid": 52,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Fortunately, for all the hard constraint factors in rows 3\u20135 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor)\u2014this extends results presented in Smith and Eisner (2008).4 In Table 1 we present closed-form expressions for the factor-to-variable message ratios mC\u2192i , MC\u2192i(1)/MC\u2192i(0) in terms of their variable-tofactor counterparts mi\u2192C , Mi\u2192C(1)/Mi\u2192C(0); these ratios are all that is necessary when the variables are binary.",
                    "sid": 53,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Detailed derivations are presented in an extended version of this paper (Martins et al., 2010b).",
                    "sid": 54,
                    "ssid": 49,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 variational representations",
            "number": "2",
            "sents": [
                {
                    "text": "Let Tx , {Pr\u03b8(.|x)  |\u03b8 \u2208 Rd} be the family of all distributions of the form in Eq.",
                    "sid": 55,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.",
                    "sid": 56,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We next present an alternative parametrization for the distributions in Tx in terms of factor marginals.",
                    "sid": 57,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will see that each distribution can be seen as a point in the socalled marginal polytope (Wainwright and Jordan, 2008); this will pave the way for the variational representations to be derived next.",
                    "sid": 58,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Parts and Output Indicators.",
                    "sid": 59,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A part is a pair hC, yCi, where C is a soft factor and yC a partial output assignment.",
                    "sid": 60,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We let 9Z = {hC, yCi  |C \u2208 Osofk, yC \u2208 Hi\u2208C \ufffdi} be the set of all parts.",
                    "sid": 61,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given an output y0 \u2208 \ufffd(x), a part hC, yCi is said to be active if it locally matches the output, i.e., if yC = y0C.",
                    "sid": 62,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Any output y0 \u2208 \ufffd(x) can be mapped to a |9Z|dimensional binary vector \u03c7(y0) indicating which parts are active, i.e., [\u03c7(y0)]hC,yCi = 1 if yC = y0C sum-product and max-product cases; these probabilities are induced by the messages in Eq.",
                    "sid": 63,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4: for an event A C REC Pr{YC E A} o Eyc H(yC E A) FLEC Mi\u2014C(yi). and 0 otherwise; \u03c7(y0) is called the output indicator vector.",
                    "sid": 64,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This mapping allows decoupling the feature vector in Eq.",
                    "sid": 65,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3 as the product of an input matrix and an output vector: where F(x) is a d-by-|9Z |matrix whose columns contain the part-local feature vectors \u03c6C(x, yC).",
                    "sid": 66,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Observe, however, that not every vector in {0,1}|T-| corresponds necessarily to a valid output in \ufffd(x).",
                    "sid": 67,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Marginal Polytope.",
                    "sid": 68,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moving to vector representations of outputs leads naturally to a geometric view of the problem.",
                    "sid": 69,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The marginal polytope is the convex hull5 of all the \u201cvalid\u201d output indicator vectors: Note that M(Sx) only depends on the factor graph Sx and the hard constraints (i.e., it is independent of the parameters \u03b8).",
                    "sid": 70,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The importance of the marginal polytope stems from two facts: (i) each vertex of M(Sx) corresponds to an output in \ufffd(x); (ii) each point in M(Sx) corresponds to a vector of marginal probabilities that is realizable by some distribution (not necessarily in Tx) that factors according to Sx.",
                    "sid": 71,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Variational Representations.",
                    "sid": 72,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We now describe formally how the points in M(Sx) are linked to the distributions in Tx.",
                    "sid": 73,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We extend the \u201ccanonical overcomplete parametrization\u201d case, studied by Wainwright and Jordan (2008), to our scenario (common in NLP), where arbitrary features are allowed and the parameters are tied (shared by all factors).",
                    "sid": 74,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let H(Pr\u03b8(.|x)) ,\u2212 Ey\u2208%x) Pr\u03b8(y|x) log Pr\u03b8(y|x) denote the entropy of Pr\u03b8(.|x), and E\u03b8[.] the expectation under Pr\u03b8(.|x).",
                    "sid": 75,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The component of \u00b5 \u2208 M(Sx) indexed by part hC, yCi is denoted \u00b5C(yC).",
                    "sid": 76,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A proof of this proposition can be found in Martins et al. (2010a).",
                    "sid": 77,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Fig.",
                    "sid": 78,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 provides an illustration of the dual parametrization implied by Prop.",
                    "sid": 79,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.",
                    "sid": 80,
                    "ssid": 26,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 approximate inference & turbo parsing",
            "number": "3",
            "sents": [
                {
                    "text": "We now show how the variational machinery just described relates to message-passing algorithms and provides a common framework for analyzing two recent dependency parsers.",
                    "sid": 81,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Later (\u00a75), Prop.",
                    "sid": 82,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 is used constructively for learning the model parameters.",
                    "sid": 83,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For general factor graphs with loops, the marginal polytope M(Gx) cannot be compactly specified and the entropy term H(\u00b5) lacks a closed form, rendering exact optimizations in Eqs.",
                    "sid": 84,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9\u201310 intractable.",
                    "sid": 85,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A popular approximate algorithm for marginal inference is sum-product loopy BP, which passes messages as described in \u00a72 and, upon convergence, computes beliefs via Eqs.",
                    "sid": 86,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6\u20137.",
                    "sid": 87,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Were loopy BP exact, these beliefs would be the true marginals and hence a point in the marginal polytope M(Gx).",
                    "sid": 88,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, this need not be the case, as elucidated by Yedidia et al. (2001) and others, who first analyzed loopy BP from a variational perspective.",
                    "sid": 89,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The following two approximations underlie loopy BP: Namely, it is characterized by L(Gx) , {\u03c4 E R|\ufffd |\ufffd |Eq.",
                    "sid": 90,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "11 holds Vi, yi E Yi, C E C}.",
                    "sid": 91,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The elements of L(Gx) are called pseudo-marginals.",
                    "sid": 92,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Clearly, the true marginals satisfy Eq.",
                    "sid": 93,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "11, and therefore M(Gx) C L(Gx).",
                    "sid": 94,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Any stationary point of sum-product BP is a local optimum of the variational problem in Eq.",
                    "sid": 95,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 with M(Gx) replaced by L(Gx) and H replaced by HBethe (Yedidia et al., 2001).",
                    "sid": 96,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note however that multiple optima may exist, since HBethe is not necessarily concave, and that BP may not converge.",
                    "sid": 97,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 1 shows closed form expressions for the local agreement constraints and entropies of some hard-constraint factors, obtained by invoking Eq.",
                    "sid": 98,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7 and observing that TC(yC) must be zero if configuration yC is forbidden.",
                    "sid": 99,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "See Martins et al. (2010b).",
                    "sid": 100,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We next present our main contribution: a formal connection between two recent approximate dependency parsers, which at first sight appear unrelated.",
                    "sid": 101,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig.",
                    "sid": 102,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1) in which they run loopy BP, and that (ii) Martins et al. (2009) approximate parsing as the solution of a linear program.",
                    "sid": 103,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here, we fill the blanks in the two approaches: we derive explicitly the variational problem addressed in (i) and we provide the underlying factor graph in (ii).",
                    "sid": 104,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This puts the two approaches side-by-side as approximate methods for marginal and MAP inference.",
                    "sid": 105,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since both rely on \u201clocal\u201d approximations (in the sense of Eq.",
                    "sid": 106,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "11) that ignore the loops in their graphical models, we dub them turbo parsers by analogy with error-correcting turbo decoders (see footnote 1).",
                    "sid": 107,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Turbo Parser #1: Sum-Product Loopy BP.",
                    "sid": 108,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The factor graph depicted in Fig.",
                    "sid": 109,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1\u2014call it Gx\u2014includes pairwise soft factors connecting sibling and grandparent arcs.6 We next characterize the local polytope L(Gx) and the Bethe approximation HBethe inherent in Smith and Eisner\u2019s loopy BP algorithm.",
                    "sid": 110,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let A be the set of candidate arcs, and P C_ A2 the set of pairs of arcs that have factors.",
                    "sid": 111,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let \u03c4 = (\u03c4A,\u03c4P) with \u03c4A = (Ta)aEA and \u03c4P = (Tab)(a,b)EP.",
                    "sid": 112,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since all variables are binary, we may write, for each a E A, Ta(1) = za and Ta(0) = 1 \u2212 za, where za is a variable constrained to [0, 1].",
                    "sid": 113,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let zA = (za)aEA; the local agreement constraints at the TREE factor (see Table 1) are written as zA E Ztree(x), where Ztree(x) is the arborescence polytope, i.e., the convex hull of all incidence vectors of dependency trees (Martins et al., 2009).",
                    "sid": 114,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is straightforward to write a contingency table and obtain the following local agreement constraints at the pairwise factors: Noting that all these pseudo-marginals are constrained to the unit interval, one can get rid of all variables Tab and write everything as inequalities which, along with zA E Ztree(x), define the local polytope L(Gx).",
                    "sid": 115,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As for the factor entropies, start by noting that the TREE-factor entropy Htree can be obtained in closed form by computing the marginals ZA and the partition function Zx(\u03b8) (via the matrix-tree theorem) and recalling the variational representation in Eq.",
                    "sid": 116,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9, yielding Htree = log Zx(\u03b8) \u2212 \u03b8TF(x)zA.",
                    "sid": 117,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some algebra allows writing the overall Bethe entropy approximation as: where we introduced the mutual information associated with each pairwise factor, Ia;b(za, zb, zab) = whose maximizer corresponds to the beliefs returned by the Smith and Eisner\u2019s loopy BP algorithm (if it converges).",
                    "sid": 118,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Turbo Parser #2: LP-Relaxed MAP.",
                    "sid": 119,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We now turn to the concise integer LP formulation of Martins et al. (2009).",
                    "sid": 120,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The formulation is exact but NPhard, and so an LP relaxation is made there by dropping the integer constraints.",
                    "sid": 121,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We next construct a factor graph G' and show that the LP relaxation corresponds to an optimization of the form in Eq.",
                    "sid": 122,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "10, with the marginal polytope M(G') replaced by L(G').",
                    "sid": 123,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "G' includes the following auxiliary variable nodes: path variables (pij)i=0,...,n,j=1,...,n, which indicate whether word j descends from i in the dependency tree, and flow variables (fka )aEA,k=1,...,n, which evaluate to 1 iff arc a \u201ccarries flow\u201d to k, i.e., iff there is a path from the root to k that passes through a.",
                    "sid": 124,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We need to seed these variables imposing i.e., any word descends from the root and from itself, and arcs leaving a word carry no flow to that word.",
                    "sid": 125,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This can be done with unary hard constraint factors.",
                    "sid": 126,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then replace the TREE factor in Fig.",
                    "sid": 127,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 by the factors shown in Fig.",
                    "sid": 128,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3: L(G0x) is thus defined by the constraints in Eq.",
                    "sid": 129,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "12 and 15\u201319.",
                    "sid": 130,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The approximate MAP problem, that replaces M(G0 x) by L(G0x) in Eq.",
                    "sid": 131,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "10, thus becomes: maxz,f,p \u03b8>F(x)z (20) s.t.",
                    "sid": 132,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Eqs.",
                    "sid": 133,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "12 and 15\u201319 are satisfied.",
                    "sid": 134,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is exactly the LP relaxation considered by Martins et al. (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features.7 They also considered a configuration with non-projectivity features\u2014which fire if an arc is non-projective.8 That configuration can also be obtained here if variables {nhh,mi} are added to indicate non-projective arcs and OR-WITHOUTPUT hard constraint factors are inserted to enforce nhh,mi = zhh,mi\u2227V min(h,m)<j<min(h,m) \u00acphj.",
                    "sid": 135,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Details are omitted for space.",
                    "sid": 136,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In sum, although the approaches of Smith and Eisner (2008) and Martins et al. (2009) look very different, in reality both are variational approximations emanating from Prop.",
                    "sid": 137,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1, respectively for marginal and MAP inference.",
                    "sid": 138,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, they operate on distinct factor graphs, respectively Figs.",
                    "sid": 139,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 and 3.9",
                    "sid": 140,
                    "ssid": 60,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5 online learning",
            "number": "4",
            "sents": [
                {
                    "text": "Our learning algorithm is presented in Alg.",
                    "sid": 141,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.",
                    "sid": 142,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is a generalized online learner that tackles E2-regularized empirical risk minimization of the form where each hxi, yii is a training example, A \u2265 0 is the regularization constant, and L(\u03b8; x, y) is a nonnegative convex loss.",
                    "sid": 143,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Examples include the logistic loss used in CRFs (\u2212 log Pro(y|x)) and the hinge loss of structured SVMs (maxy,\u2208%x) \u03b8>(\u03c6(x, y0)\u2212 \u03c6(x, y)) + E(y0, y) for some cost function E).",
                    "sid": 144,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These are both special cases of the family defined in Fig.",
                    "sid": 145,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4, which also includes the structured perceptron\u2019s loss (Q \u2192 \u221e, -y = 0) and the softmax-margin loss of Gimpel and Smith (2010; Q = -y = 1).",
                    "sid": 146,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Alg.",
                    "sid": 147,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 is closely related to stochastic or online gradient descent methods, but with the key advantage of not needing a learning rate hyperparameter.",
                    "sid": 148,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We sketch the derivation of Alg.",
                    "sid": 149,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1; full details can be found in Martins et al. (2010a).",
                    "sid": 150,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the tth round, one example hxt, yti is considered.",
                    "sid": 151,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We seek to solve 9Given what was just exposed, it seems appealing to try max-product loopy BP on the factor graph of Fig.",
                    "sid": 152,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1, or sumproduct loopy BP on the one in Fig.",
                    "sid": 153,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.",
                    "sid": 154,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both attempts present serious challenges: the former requires computing messages sent by the tree factor, which requires O(n2) calls to the Chu-LiuEdmonds algorithm and hence O(n5) time.",
                    "sid": 155,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "No obvious strategy seems to exist for simultaneous computation of all messages, unlike in the sum-product case.",
                    "sid": 156,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The latter is even more challenging, as standard sum-product loopy BP has serious issues in the factor graph of Fig.",
                    "sid": 157,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3; we construct in Martins et al. (2010b) a simple example with a very poor Bethe approximation.",
                    "sid": 158,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This might be fixed by using other variants of sum-product BP, e.g., ones in which the entropy approximation is concave. which trades off conservativeness (stay close to the most recent solution \u03b8t) and correctness (keep the loss small).",
                    "sid": 159,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Alg.",
                    "sid": 160,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1\u2019s lines 7\u20138 are the result of taking the first-order Taylor approximation of L around \u03b8t, which yields the lower bound L(\u03b8; xt, yt) \u2265 L(\u03b8t; xt, yt) + (\u03b8 \u2212 \u03b8t)>\u2207L(\u03b8t; xt, yt), and plugging that linear approximation into the constraint of Eq.",
                    "sid": 161,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "23, which gives a simple Euclidean projection problem (with slack) with a closed-form solution.",
                    "sid": 162,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The online updating requires evaluating the loss and computing its gradient.",
                    "sid": 163,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both quantities can be computed using the variational expression in Prop.",
                    "sid": 164,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1, for any loss L\u03b2,\u03b3(\u03b8; x, y) in Fig.",
                    "sid": 165,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.10 Our only assumption is that the cost function `(y0, y) can be written as a sum over factor-local costs; letting \u00b5 = \u03c7(y) and \u00b50 = \u03c7(y0), this implies `(y0, y) = p>\u00b50 + q for some p and q which are constant with respect to \u00b50.11 Under this assumption, L\u03b2,\u03b3(\u03b8; x, y) becomes expressible in terms of the log-partition function of a distribution whose log-potentials are set to \u03b2(F(x)>\u03b8 + \u03b3p).",
                    "sid": 166,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From Eq.",
                    "sid": 167,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 and after some algebra, we finally obtain L\u03b2,\u03b3(\u03b8;x,y) = Let \u00b5\ufffd be a maximizer in Eq.",
                    "sid": 168,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "24; from the second statement of Prop.",
                    "sid": 169,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 we obtain \u2207L\u03b2,\u03b3(\u03b8; x, y) = F(x)(\u00b5\u2212\u00b5).",
                    "sid": 170,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When the inference problem in Eq.",
                    "sid": 171,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "24 is intractable, approximate message-passing algorithms like loopy BP still allow us to obtain approximations of the loss L\u03b2,\u03b3 and its gradient.",
                    "sid": 172,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the hinge loss, we arrive precisely at the maxloss variant of 1-best MIRA (Crammer et al., 2006).",
                    "sid": 173,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the logistic loss, we arrive at a new online learning algorithm for CRFs that resembles stochastic gradient descent but with an automatic step size that follows from our variational representation.",
                    "sid": 174,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unsupported Features.",
                    "sid": 175,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As datasets grow, so do the sets of features, creating further computational challenges.",
                    "sid": 176,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Often only \u201csupported\u201d features\u2014those observed in the training data\u2014are included, and even those are commonly eliminated when their frequencies fall below a threshold.",
                    "sid": 177,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Important information may be lost as a result of these expedient choices.",
                    "sid": 178,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Formally, the supported feature set is Fsupp g Umi=1 supp \u03c6(xi, yi), where supp u \u00b0_ {j  |uj =6 0} denotes the support of vector u. Fsupp is a subset of the complete feature set, comprised of those features that occur in some candidate output, in Fcomp\\Fsupp are called unsupported.",
                    "sid": 179,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sha and Pereira (2003) have shown that training a CRF-based shallow parser with the complete feature set may improve performance (over the supported one), at the cost of 4.6 times more features.",
                    "sid": 180,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Dependency parsing has a much higher ratio (around 20 for bilexical word-word features, as estimated in the Penn Treebank), due to the quadratic or faster growth of the number of parts, of which only a few are active in a legal output.",
                    "sid": 181,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We propose a simple strategy for handling Fcomp efficiently, which can be applied for those losses in Fig.",
                    "sid": 182,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4 where \u03b2 = \u221e.",
                    "sid": 183,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(e.g., the structured SVM and perceptron).",
                    "sid": 184,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our procedure is the following: keep an active set F containing all features that have been instantiated in Alg.",
                    "sid": 185,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.",
                    "sid": 186,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "At each round, run lines 4\u20135 as usual, using only features in T. Since the other features have not been used before, they have a zero weight, hence can be ignored.",
                    "sid": 187,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When Q = oo, the variational problem in Eq.",
                    "sid": 188,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "24 consists of a MAP computation and the solution corresponds to one output Yt E \ufffd(xt).",
                    "sid": 189,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Only the parts that are active in Yt but not in yt, or vice-versa, will have features that might receive a nonzero update.",
                    "sid": 190,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Those parts are reexamined for new features and the active set T is updated accordingly.",
                    "sid": 191,
                    "ssid": 51,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "6 experiments",
            "number": "5",
            "sents": [
                {
                    "text": "We trained non-projective dependency parsers for 14 languages, using datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006) and two datasets for English: one from the CoNLL-2008 shared task (Surdeanu et al., 2008), which contains non-projective arcs, and another derived from the Penn Treebank applying the standard head rules of Yamada and Matsumoto (2003), in which all parse trees are projective.12 We implemented Alg.",
                    "sid": 192,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1, 12We used the provided train/test splits for all datasets.",
                    "sid": 193,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For English, we used the standard test partitions (section 23 of the Wall Street Journal).",
                    "sid": 194,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We did not exploit the fact that some datasets only contain projective trees and have unique roots. which handles any loss function Lo\ufffd,y.13 When 0 < oo, Turbo Parser #1 and the loopy BP algorithm of Smith and Eisner (2008) is used; otherwise, Turbo Parser #2 is used and the LP relaxation is solved with CPLEX.",
                    "sid": 195,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In both cases, we employed the same pruning strategy as Martins et al. (2009).",
                    "sid": 196,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Two different feature configurations were first tried: an arc-factored model and a model with second-order features (siblings and grandparents).",
                    "sid": 197,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used the same arc-factored features as McDonald et al. (2005) and second-order features that conjoin words and lemmas (at most two), parts-ofspeech tags, and (if available) morphological information; this was the same set of features as in Martins et al.",
                    "sid": 198,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2009).",
                    "sid": 199,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 shows the results obtained in both configurations, for CRF and SVM loss functions.",
                    "sid": 200,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While in the arc-factored case performance is similar, in second-order models there seems to be a consistent gain when the SVM loss is used.",
                    "sid": 201,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are two possible reasons: first, SVMs take the cost function into consideration; second, Turbo Parser #2 is less approximate than Turbo Parser #1, since only the marginal polytope is approximated (the entropy function is not involved).",
                    "sid": 202,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The loopy BP algorithm managed to converge for nearly all sentences (with message damping).",
                    "sid": 203,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The last three columns show the beneficial effect of unsupported features for the SVM case (with a more powerful model with non-projectivity features).",
                    "sid": 204,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For most languages, unsupported features convey helpful information, which can be used with little extra cost (on average, 2.5 times more features are instantiated).",
                    "sid": 205,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A combination of the techniques discussed here yields parsers that are in line with very strong competitors\u2014for example, the parser of Koo and Collins (2010), which is exact, third-order, and constrains the outputs to be projective, does not outperform ours on the projective English dataset.14 Finally, Table 3 shows results obtained for different settings of \u03b2 and \u03b3. Interestingly, we observe that higher scores are obtained for loss functions that are \u201cbetween\u201d SVMs and CRFs.",
                    "sid": 206,
                    "ssid": 15,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "7 related work",
            "number": "6",
            "sents": [
                {
                    "text": "There has been recent work studying efficient computation of messages in combinatorial factors: bipartite matchings (Duchi et al., 2007), projective and non-projective arborescences (Smith and Eisner, 2008), as well as high order factors with countbased potentials (Tarlow et al., 2010), among others.",
                    "sid": 207,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some of our combinatorial factors (OR, OR-WITHOUTPUT) and the analogous entropy computations were never considered, to the best of our knowledge.",
                    "sid": 208,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Prop.",
                    "sid": 209,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 appears in Wainwright and Jordan (2008) for canonical overcomplete models; we adapt it here for models with shared features.",
                    "sid": 210,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We rely on the variational interpretation of loopy BP, due to Yedidia et al. (2001), to derive the objective being optimized by Smith and Eisner\u2019s loopy BP parser.",
                    "sid": 211,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Independently of our work, Koo et al. (2010) recently proposed an efficient dual decomposition method to solve an LP problem similar (but not equal) to the one in Eq.",
                    "sid": 212,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "20,15 with excellent parsing performance.",
                    "sid": 213,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Their parser is also an instance of a turbo parser since it relies on a local approximation of a marginal polytope.",
                    "sid": 214,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While one can also use dual decomposition to address our MAP problem, the fact that our model does not decompose as nicely as the one in Koo et al. (2010) would likely result in slower convergence.",
                    "sid": 215,
                    "ssid": 9,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "8 conclusion",
            "number": "7",
            "sents": [
                {
                    "text": "We presented a unified view of two recent approximate dependency parsers, by stating their underlying factor graphs and by deriving the variational problems that they address.",
                    "sid": 216,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We introduced new hard constraint factors, along with formulae for their messages, local belief constraints, and entropies.",
                    "sid": 217,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We provided an aggressive online algorithm for training the models with a broad family of losses.",
                    "sid": 218,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are several possible directions for future work.",
                    "sid": 219,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recent progress in message-passing algorithms yield \u201cconvexified\u201d Bethe approximations that can be used for marginal inference (Wainwright et al., 2005), and provably convergent max-product variants that solve the relaxed LP (Globerson and Jaakkola, 2008).",
                    "sid": 220,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other parsing formalisms can be handled with the inventory of factors shown here\u2014 among them, phrase-structure parsing.",
                    "sid": 221,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments",
            "number": "8",
            "sents": [
                {
                    "text": "The authors would like to thank the reviewers for their comments, and Kevin Gimpel, David Smith, David Sontag, and Terry Koo for helpful discussions.",
                    "sid": 222,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A. M. was supported by a grant from FCT/ICTI through the CMUPortugal Program, and also by Priberam Inform\u00b4atica.",
                    "sid": 223,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "N. S. was supported in part by Qatar NRF NPRP-08-4851-083.",
                    "sid": 224,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "E. X. was supported by AFOSR FA9550010247, ONR N000140910758, NSF CAREER DBI-0546594, NSF IIS-0713379, and an Alfred P. Sloan Fellowship.",
                    "sid": 225,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "M. F. and P. A. were supported by the FET programme (EU FP7), under the SIMBAD project (contract 213250).",
                    "sid": 226,
                    "ssid": 5,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}