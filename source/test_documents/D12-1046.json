{
    "ID": "D12-1046",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Joint Chinese Word Segmentation, POS Tagging and Parsing",
                    "sid": 0,
                    "ssid": null,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previous work often used a pipeline method \u2013 Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "1 introduction",
            "number": "1",
            "sents": [
                {
                    "text": "For Asian languages such as Japanese and Chinese that do not contain explicitly marked word boundaries, word segmentation is an important first step for many subsequent language processing tasks, such as POS tagging, parsing, semantic role labeling, and various applications.",
                    "sid": 7,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previous studies for POS tagging and syntax parsing on these languages sometimes assume that gold standard word segmentation information is provided, which is not the real scenario.",
                    "sid": 8,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In a fully automatic system, a pipeline approach is often adopted, where raw sentences are first segmented into word sequences, then POS tagging and parsing are performed.",
                    "sid": 9,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This kind of approach suffers from error propagation.",
                    "sid": 10,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, word segmentation errors will result in tagging and parsing errors.",
                    "sid": 11,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additionally, early modules cannot use information from subsequent modules.",
                    "sid": 12,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Intuitively a joint model that performs the three tasks together should help the system make the best decisions.",
                    "sid": 13,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we propose a unified model for joint Chinese word segmentation, POS tagging, and parsing.",
                    "sid": 14,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Three sub-models are independently trained using the state-of-the-art methods.",
                    "sid": 15,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We do not use the joint inference algorithm for training because of the high complexity caused by the large amount of parameters.",
                    "sid": 16,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use linear chain Conditional Random Fields (CRFs) (Lafferty et al., 2001) to train the word segmentation model and POS tagging model, and averaged perceptron (Collins, 2002) to learn the parsing model.",
                    "sid": 17,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During decoding, parameters of each sub-model are scaled to represent its importance in the joint model.",
                    "sid": 18,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our decoding algorithm is an extension of CYK parsing.",
                    "sid": 19,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Initially, weights of all possible words together with their POS tags are calculated.",
                    "sid": 20,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When searching the parse tree, the word and POS tagging features are dynamically generated and the transition information of POS tagging is considered in the span merge operation.",
                    "sid": 21,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Experiments are conducted on Chinese Tree Bank (CTB) 5 dataset, which is widely used for Chinese word segmentation, POS tagging and parsing.",
                    "sid": 22,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We compare our proposed joint model with the pipeline system, both built using the state-of-the-art submodels.",
                    "sid": 23,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also propose an evaluation metric to calculate the bracket scores for parsing in the face of word segmentation errors.",
                    "sid": 24,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our experimental results show that the joint model significantly outperforms the pipeline method based on the state-of-the-art sub-models.",
                    "sid": 25,
                    "ssid": 19,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "2 related work",
            "number": "2",
            "sents": [
                {
                    "text": "There is very limited previous work on joint Chinese word segmentation, POS tagging, and parsing.",
                    "sid": 26,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al., 2010), cascaded linear model (Jiang et al., 2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), reranking (Jiang et al., 2008b).",
                    "sid": 27,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These joint models showed about 0.2 \u2212 1% F-score improvement over the pipeline method.",
                    "sid": 28,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recently, joint tagging and dependency parsing has been studied as well (Li et al., 2011; Lee et al., 2011).",
                    "sid": 29,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009).",
                    "sid": 30,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In (Jiang et al., 2009), additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing task using the pipeline framework.",
                    "sid": 31,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).",
                    "sid": 32,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew.",
                    "sid": 33,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Different from that work, we use a discriminative model, which benefits from large amounts of features and is easier to deal with unknown words.",
                    "sid": 34,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Another main difference is that, besides segmentation and parsing, we also incorporate the POS tagging model into the CYK parsing framework.",
                    "sid": 35,
                    "ssid": 10,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "3 methods",
            "number": "3",
            "sents": [
                {
                    "text": "For a given Chinese sentence, our task is to generate the word sequence, its POS tag sequence, and the parse tree (constituent parsing).",
                    "sid": 36,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A joint model is expected to make more optimal decisions than a pipeline approach; however, such a model will be too complex and it is difficult to estimate model parameters.",
                    "sid": 37,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore we do not perform joint inference for training.",
                    "sid": 38,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead, we develop three individual models independently during training and perform joint decoding using them.",
                    "sid": 39,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section, we first describe the three sub-models and then the joint decoding algorithm.",
                    "sid": 40,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Methods for Chinese word segmentation can be broadly categorized into character based and word based models.",
                    "sid": 41,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previous studies showed that character-based models are more effective to detect out-of-vocabulary words while word-based models are more accurate to predict in-vocabulary words (Zhang et al., 2006).",
                    "sid": 42,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here, we use order-0 semiMarkov model (Sarawagi and Cohen, 2004) to take advantages of both approaches.",
                    "sid": 43,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More specifically, given a sentence x = c1, c2, ... , cl (where cz is the ith Chinese character, l is the sentence length), the character-based model assigns each character with a word boundary tag.",
                    "sid": 44,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here we use the BCDIES tag set, which achieved the best official performance (Zhao and Kit, 2008): B, C, D, E denote the first, second, third, and last character of a multi-character word respectively, I denotes the other characters, and S denotes the single character word.",
                    "sid": 45,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the same characterbased feature templates as in the best official system, shown in Table 1 (1.1-1.3), including character unigram and bigram features, and transition features.",
                    "sid": 46,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Linear chain CRFs are used for training.",
                    "sid": 47,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Feature templates in the word-based model are shown in Table 1 (1.4-1.6), including word features, sub-word features, and character bigrams within words.",
                    "sid": 48,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The word feature is activated if a predicted word w is in the vocabulary (i.e., appears in training data).",
                    "sid": 49,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Subword(w) is the longest in-vocabulary word within w. To use word features, we adopt a Kbest reranking approach.",
                    "sid": 50,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The top K candidate segmentation results for each training sample are generated using the character-based model, and the gold segmentation is added if it is not in the candidate set.",
                    "sid": 51,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the Maximum Entropy (ME) model to learn the weights of word features such that the probability of the gold candidate is maximal.",
                    "sid": 52,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A problem arises when combining the two models and using it in joint segmentation and parsing, since the linear chain used in the character-based model is incompatible with CYK parsing model and the word-based model due to the transition information.",
                    "sid": 53,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, we slightly modify the linear chain CRFs by fixing the weights of transition features during training and testing.",
                    "sid": 54,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, weights of impossible transition features (e.g., B\u2014*B) are set to \u2212oc, and weights of the other transition features (e.g., E\u2014*B) are set to 0.",
                    "sid": 55,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this way, the transition feature could be neglected in testing for two reasons.",
                    "sid": 56,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, all illegal label assignments are prohibited in prediction, since their weights are \u2212oc; second, because weights of legal transition features are 0, they do not affect the prediction at all.",
                    "sid": 57,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the following, transition features are excluded.",
                    "sid": 58,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Now we can use order-0 semi Markov model as the hybrid model.",
                    "sid": 59,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We define the score of a word as the sum of the weights of all the features within the word.",
                    "sid": 60,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Formally, the score of a multi-character word where fCRF and fME are the feature vectors in the character and word based models respectively, and BCRF, BME are their corresponding weight vectors.",
                    "sid": 61,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For simplicity, we denote Bseg = BCRF\u00aeME, fseg = fCRF\u00aeME, where BCRF\u00aeME means the concatenation of BCRF and BME.",
                    "sid": 62,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Scores for single character words are defined similarly.",
                    "sid": 63,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These word scores will be used in the joint segmentation and parsing task Section 3.4.",
                    "sid": 64,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Though syntax parsing model can directly predict the POS tag itself, we choose not to use this, but use an independent POS tagger for two reasons.",
                    "sid": 65,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, there is a large amount of data with labeled POS tags but no syntax annotations, such as the People\u2019s Daily corpus and SIGHAN bakeoff corpora (Jin and Chen, 2008).",
                    "sid": 66,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Such data can only be used to train POS taggers, but not for training the parsing model.",
                    "sid": 67,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Often using a larger training set will result in a better POS tagger.",
                    "sid": 68,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, the state-of-the-art POS tagging systems are often trained by sequence labeling models, not parsing models.",
                    "sid": 69,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The POS tagging problem is to assign a POS tag t E T to each word in a sentence.",
                    "sid": 70,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also use linear chain CRFs for POS tagging.",
                    "sid": 71,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Feature templates shown in Table 2 are the same as those in (Qian et al., 2010), which have been shown effective on CTB corpus.",
                    "sid": 72,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Three feature sets are considered: (i) word level features, including surrounding word unigrams, bigrams, and word length; (ii) character level features, such as the first and last characters in the words; (iii) transition features.",
                    "sid": 73,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We choose discriminative models for parsing since it is easy to handle unknown words by simply adding character level features.",
                    "sid": 74,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008).",
                    "sid": 75,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this study, we use averaged perceptron algorithm for parameter estimation since it is easier to implement and has competitive performance.",
                    "sid": 76,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A Context Free Grammar (CFG) consists of (i) a set of terminals; (ii) a set of nonterminals {Nk}; (iii) a designated start symbol ROOT; and (iv) a set of rules, {r = Ni \u2014* (j}, where (j is a sequence of terminals and nonterminals.",
                    "sid": 77,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the parsing task, terminals are the words, and nonterminals are the POS tags and phrase types.",
                    "sid": 78,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, nonterminal is named state for short.",
                    "sid": 79,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A parse tree T of sentence x can be factorized into several one-level subtrees, each corresponding to a rule r. In practice, binarization of rules is necessary to obtain cubic parsing time.",
                    "sid": 80,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, the right hand side of each rule should contain no more than 2 states.",
                    "sid": 81,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used right branching binarization, as illustrated in Figure 1.",
                    "sid": 82,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We did not use parent annotation, since we found it degraded the performance in our experiments (shown in Section 4).",
                    "sid": 83,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used the same preprocessing step as (Harper and Huang, 2009), collapsing all the allowed nonterminal-yield unary chains to single unary rules.",
                    "sid": 84,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, all spans in the binarized trees contain no more than one unary rules.",
                    "sid": 85,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To facilitate decoding, we unify the form of spans so that each span contains exactly one unary rule.",
                    "sid": 86,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is done by adding identity unary rules (N \u2014* N ) to spans that have no unary rule.",
                    "sid": 87,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These identity unary rules will be removed in evaluation.",
                    "sid": 88,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hence, there are two states of a span: the top state N and the bottom state N that correspond to the left and right hand of the unary rule runary = N \u2014* N respectively, as shown in Figure 2.",
                    "sid": 89,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 3 lists the feature templates we use for parsing.",
                    "sid": 90,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are 4 feature sets: (i) bottom state features fbottom(i, j, x, Ni,j), which depend on the bottom states; (ii) top state features ftop(i, j, x, Ni,j); (iii) unary rule features funary(i, j, x, runary i,j ), which extract the transition information from bottom states to top states; (iv) binary rule features where Ni,k\u22121, Nk,r are the top states of the left and right children.",
                    "sid": 91,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The score function for a sentence x with parse tree T is defined as: where \u03b8bottom, \u03b8top, \u03b8unary, \u03b8binary are the weight vectors of the four feature sets.",
                    "sid": 92,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given the training corpus {(xi, \u02dcTi)}, the learning task is to estimate the weight vectors so that for each sentence xi, the gold standard tree \u02dcTi achieves the maximal score among all the possible trees.",
                    "sid": 93,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The perceptron algorithm is guaranteed to find the solution if it exists.",
                    "sid": 94,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The three models described above are separately trained to make parameter estimation feasible as well as optimize each individual component.",
                    "sid": 95,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In testbigram of word, POS tag.",
                    "sid": 96,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Xl+a/Xr_a denotes the first/last ath X in the span, while Xl_a/Xr+a denotes the ath X left/right to span.",
                    "sid": 97,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Xm is the first X of right child, and Xm_1 is the last X of the left child. len, lenl, lenr denote the length of the span, left child and right child respectively. wl is the length of word.",
                    "sid": 98,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ROOT/LEAF means the template can only generate the features for the root/initial span. ing, we perform joint decoding to combine information from the three models.",
                    "sid": 99,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Parameters of word segmentation (\u03b8seg), POS tagging (\u03b8pos), and parsing models (\u03b8parse = \u03b8bottom\u00aetop\u00ae unary\u00aebianry) are scaled by three positive hyper-parameters \u03b1, \u03b2, and \u03b3 respectively, which control their contribution in the joint model.",
                    "sid": 100,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If \u03b1 >> \u03b2 >> \u03b3, then the joint model is equivalent to a pipeline model, in which there is no feedback from downstream models to upstream ones.",
                    "sid": 101,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For well tuned hyper-parameters, we expect that segmentation and POS tagging results can be improved by parsing information.",
                    "sid": 102,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The hyperparameters are tuned on development data.",
                    "sid": 103,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the following sections, for simplicity we drop \u03b1, \u03b2, \u03b3, and just use \u03b8seg, \u03b8pos, \u03b8parse to represent the scaled parameters.",
                    "sid": 104,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The basic idea of our decoding algorithm is to extend the CYK parsing algorithm so that it can deal with transition features in POS tagging and segmentation scores in word segmentation.",
                    "sid": 105,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The joint decoding algorithm is shown in Algorithm 1.",
                    "sid": 106,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a sentence x = c1, ... , cl, Line 0 calculates the scores of all possible words in the sentence using Eq(1).",
                    "sid": 107,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are l(l + 1)/2 word candidates in total.",
                    "sid": 108,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Surrounding words are important features for POS tagging and parsing; however, they are unavailable because segmentation is incomplete before parsing.",
                    "sid": 109,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, we adopt pseudo surrounding features by simply fixing the context words as the single most likely ones.",
                    "sid": 110,
                    "ssid": 75,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a word candidate wi,j from ci to cj, its previous word s\u2032 is the rightmost one in the best word sequence of c1, ... , ci\u22121, which can be obtained by dynamic programming.",
                    "sid": 111,
                    "ssid": 76,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recursively, the second word left to wi,j is the previous word of s\u2032.",
                    "sid": 112,
                    "ssid": 77,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The next word of wi,j is defined similarly.",
                    "sid": 113,
                    "ssid": 78,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Line 1, we use bidirectional Viterbi decoding to obtain all the surrounding words.",
                    "sid": 114,
                    "ssid": 79,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the forward direction, the algorithm starts from the first character boundary to the last, and finds the best previous word for the ith character boundary bi.",
                    "sid": 115,
                    "ssid": 80,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the backward direction, the algorithm starts from right to left, and finds the best next word of each bi.",
                    "sid": 116,
                    "ssid": 81,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Line 2, for each word candidate, we can calculate the score of each POS tag using state features in the POS tagging model, since the context words are available now.",
                    "sid": 117,
                    "ssid": 82,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The score function of word wi,j with POS tag t is: In Line 3, POS tags of surrounding words can be obtained similarly using bidirectional decoding.",
                    "sid": 118,
                    "ssid": 83,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, for wiJ with POS tag t, we use Viterbi algorithm to search the optimal POS tags of its left and right words.",
                    "sid": 119,
                    "ssid": 84,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Lines 4-9, each word was initialized as a basic span.",
                    "sid": 120,
                    "ssid": 85,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A span structure in the joint model is a 6-tuple: 5(i, j, w, t, N, N), where i, j are the boundary indices, w, t are the word sequence and POS sequence within the span respectively, and N, N are the bottom and top states.",
                    "sid": 121,
                    "ssid": 86,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are two types of surrounding n-grams: one is inside the span, for example, the first word of a span, which can be obtained from w; the other is outside the span, for example, the previous word of a span, which is obtained from the pseudo context information.",
                    "sid": 122,
                    "ssid": 87,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The score of a basic span depends on its corresponding word and POS pair score, and the weights of the active state and unary features.",
                    "sid": 123,
                    "ssid": 88,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To avoid enumerating the combination of the bottom and top states, initialization for each span is divided into 2 steps.",
                    "sid": 124,
                    "ssid": 89,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the first step, the score of every bottom state is calculated using bottom state features, and only the B best states are maintained (see Line 6-7).",
                    "sid": 125,
                    "ssid": 90,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the second step, top state features and unary rule features are used to get the score of each top state (Line 9), and only the top B states are preserved.",
                    "sid": 126,
                    "ssid": 91,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similarly, there are two steps in the merge operation: S(i, j, w, t, N, N) = Sl(i, k, wl, tl, Nl, Nl) + Sr(k + 1, j, wr, tr, Nr, Nr).",
                    "sid": 127,
                    "ssid": 92,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The score of the bottom state N is calculated using binary features fbinary(x, i, j, k, w, t, N \u2014* Nr+Nr), bottom state features fbottom(x, i, j, w, t, N), and POS tag transition features that depend on the boundary POS tags of Sl and Sr. See Line 14 of Algorithm 1, where tlast l and tfirst r are the POS tags of the last word in the left child span and the first word in the right child span respectively.",
                    "sid": 128,
                    "ssid": 93,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a sentence of length l, the complexity for each line of Algorithm 1 is listed in Table 4, where |T  |is the size of POS tag set, M is the number of states, and B is the beam size.",
                    "sid": 129,
                    "ssid": 94,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "4 experiments",
            "number": "4",
            "sents": [
                {
                    "text": "For comparison with other systems, we use the CTB5 corpus, which has been studied for Chinese word segmentation, POS tagging and parsing.",
                    "sid": 130,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the standard train/develop/test split of the data.",
                    "sid": 131,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Details are shown in Table 5.",
                    "sid": 132,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For joint word segmentation and POS tagging, a word is correctly predicted if both the boundaries and the POS tag are correctly identified.",
                    "sid": 133,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For joint segmentation, POS tagging, and parsing task, when calculating the bracket scores using existing parseval tools, we need to consider possible word segmentation errors.",
                    "sid": 134,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To do this, we add the word boundary information in states \u2013 a bracket is correct only if its boundaries, label and word segmentation are all correct.",
                    "sid": 135,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One example is shown in Figure 3.",
                    "sid": 136,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Notice that identity unary rules are removed during evaluation.",
                    "sid": 137,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The basic spans are characters, not words, because the number of words in reference and prediction may be different.",
                    "sid": 138,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "POS tags are removed since they do not affect the bracket scores.",
                    "sid": 139,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the segmentation is perfect, then the bracket scores of the modified tree are exactly the same as the original tree.",
                    "sid": 140,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is similar to evaluating parsing performance on speech transcripts with automatic sentence segmentation (Roark et al., 2006).",
                    "sid": 141,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluate system performance on the individual tasks, as well as the joint tasks.'",
                    "sid": 142,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For word segmentation, three metrics are used for evaluation: precision (P), recall (R), and F-score (F) defined by 2PR/(P+R).",
                    "sid": 143,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Precision is the percentage of correct words in the system output.",
                    "sid": 144,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recall is the percentage of words in gold standard annotations that are correctly predicted.",
                    "sid": 145,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For parsing, we use the standard parseval evaluation metrics: bracketing precision, recall and F-score.",
                    "sid": 146,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "'Note that the joint task refers to automatic segmentation and tagging/parsing.",
                    "sid": 147,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It can be achieved using a pipeline system or our joint decoding method.",
                    "sid": 148,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We train three submodels using the gold features, that is, POS tagger is trained using the perfect segmentation, and parser is trained using perfect segmentation and POS tags.",
                    "sid": 149,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some studies reported that better performance may be achieved by training subsequent models using representative output of the preceding models (Che et al., 2009).",
                    "sid": 150,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hence for comparison we trained another parser using automatically generated POS tags obtained from 10-fold cross validation, but did not find significant difference between these two parsers when testing on the perfectly segmented development dataset.",
                    "sid": 151,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore we use the parser trained with perfect POS tags for the joint task.",
                    "sid": 152,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Three hyper-parameters, \u03b1, 0, and -y, are tuned on development data using a heuristic search.",
                    "sid": 153,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Parameters that achieved the best joint parsing result are selected.",
                    "sid": 154,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the search, we fixed -y = 1 and varied \u03b1, 0.",
                    "sid": 155,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we set 0 = 1, and enumerate \u03b1 = 4, 2, 1, 2, ... , and choose the best \u03b1*.",
                    "sid": 156,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then, we set\u03b1 = \u03b1* and vary 0 = 4, 2, 1, 2, ... , and select the best 0*.",
                    "sid": 157,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 6 lists the parameters we used for training the submodels, as well as the hyper-parameters for joint decoding.",
                    "sid": 158,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section we first show that our sub-models are better than or comparable to state-of-the-art systems, and then the joint model is superior to the pipeline approach.",
                    "sid": 159,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 7 shows word segmentation results using our word segmentation submodel, in comparison to a few state-of-the-art systems.",
                    "sid": 160,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our segmentor, we show results for two variants: one removes transition features as described in Section 3.1, the other uses CRFs to learn the weights of transition features.",
                    "sid": 161,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can see that our system is competitive with all the others except Sun\u2019s that used additional idiom resources.",
                    "sid": 162,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our two word segmentors have similar performance.",
                    "sid": 163,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the one without transition features can be naturally integrated into the joint system, we use it in the following joint tasks.",
                    "sid": 164,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the POS tagging only task that takes gold standard word segmentation as input, we have two systems.",
                    "sid": 165,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One uses the linear chain CRFs as described in Section 3.2, the other is obtained using the parser described in Section 3.3 \u2013 the parser generates POS tag hypotheses when POS tag features are not used.",
                    "sid": 166,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The POS tagging accuracy is 95.53% and 95.10% using these two methods respectively.",
                    "sid": 167,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The better performance from the former system may be because the local label dependency is more helpful for POS tagging than the long distance dependencies that might be noisy.",
                    "sid": 168,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This result also confirms our choice of using an independent POS tagger for the sub-model, rather than relying on a parser for POS tagging.",
                    "sid": 169,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, since there are no reported results for this setup, we demonstrate the competence of our POS tagger using the joint word segmentation and POS tagging task.",
                    "sid": 170,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 8 shows the performance of a few systems along with ours, all using the pipeline approach where automatic segmentation is followed by POS tagging.",
                    "sid": 171,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can see that our POS tagger is comparable to the others.",
                    "sid": 172,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For parsing, Table 9 presents the parsing result on gold standard segmented sentence.",
                    "sid": 173,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Notice that the result of (Harper and Huang, 2009; Zhang and Clark, 2011) are not directly comparable to ours, as they used a different data split.",
                    "sid": 174,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The best published system result on CTB5 is Petrov and Klein\u2019s, which used PCFG with latent Variables.",
                    "sid": 175,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our system performs better mainly because it benefits from a large amount of features.",
                    "sid": 176,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our parser, besides the model described in Section 3.3, we tried two variations: one does not use the automatic POS tag features, the other one is learned on the parent annotated training data.",
                    "sid": 177,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results in Table 9 show that there is a performance degradation when using parent annotation.",
                    "sid": 178,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This may be due to the introduction of a large number of states, resulting in sparse features.",
                    "sid": 179,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also notice that with the help of the POS tag information, even automatically generated, the parser gained 0.9% improvement in F-score.",
                    "sid": 180,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This demonstrates the advantage of using a better independent POS tagger and incorporating it in parsing.",
                    "sid": 181,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally Table 10 shows the results for the three tasks using our joint decoding method in comparison to the pipeline method.",
                    "sid": 182,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can see that the joint model outperforms the pipeline one.",
                    "sid": 183,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is mainly because of a better parsing module as well as joint decoding.",
                    "sid": 184,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the table we also include results of (Jiang et al., 2009), which is the only reported joint parsing result we found using the same data split on CTB5.",
                    "sid": 185,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They achieved 80.28% parsing F-score using automatic word segmentation.",
                    "sid": 186,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Their adapted system Jiang09+ leveraged additional corpus to improve Chinese word segmentation, resulting in an Fscore of 81.07%.",
                    "sid": 187,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our system has better performance than these.",
                    "sid": 188,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We compared the results from the pipeline and our joint decoding systems in order to understand the impact of the joint model on word segmentation and POS tagging.",
                    "sid": 189,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We notice that the joint model tend to generate more words than the pipeline model.",
                    "sid": 190,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, \u201c\u5df4\u5c14\u4e00\u884c\u201d is one word in the pipeline model, but correctly segmented as two words \u201c\u5df4 \u5c14/\u4e00\u884c\u201d in the joint model.",
                    "sid": 191,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This tendency of segmentation also makes it fail to recognize some long words, especially OOV words.",
                    "sid": 192,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, \u201c\u4e8b \u5b9e\u4e0a\u201d is segmented as \u201c\u4e8b\u5b9e/\u4e0a\u201d.",
                    "sid": 193,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the data set, we find that, the joint model corrected 10 missing boundaries over the pipeline method, and introduced 3 false positive segmentation errors.",
                    "sid": 194,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the analysis of POS tags, we only examined the words that are correctly segmented by both the pipeline and the joint models.",
                    "sid": 195,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 11 shows the increase and decrease of error patterns of the joint model over the pipeline POS tagger.",
                    "sid": 196,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An error pattern \u201cX -* Y\u201d means that the word whose true tag is \u2018X\u2019 is assigned a tag \u2018Y\u2019.",
                    "sid": 197,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All the patterns are ranked in descending order of the reduction/increase of the error number.",
                    "sid": 198,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can see that the joint model has a clear advantage in the disambiguation of {VV, NN} and {DEG, DEC}, which results in the overall improved performance.",
                    "sid": 199,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast, the joint method performs worse on ambiguous POS pairs such as {NN, NR}.",
                    "sid": 200,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This observation is similar to those reported by (Li et al., 2011; Hatori et al., 2011).",
                    "sid": 201,
                    "ssid": 72,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "5 conclusion",
            "number": "5",
            "sents": [
                {
                    "text": "In this paper, we proposed a new algorithm for joint Chinese word segmentation, POS tagging, and parsing.",
                    "sid": 202,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our algorithm is an extension of the CYK number of the corresponding pattern made by the pipeline tagging model.",
                    "sid": 203,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2193 and \u2191 mean the error number reduced or increased by the joint model. parsing method.",
                    "sid": 204,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The sub-models are independently trained for the three tasks to reduce model complexity and optimize individual sub-models.",
                    "sid": 205,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our experiments demonstrate the advantage of the joint models.",
                    "sid": 206,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the future work, we will compare this joint model to the pipeline approach that uses multiple candidates or soft decisions in the early modules.",
                    "sid": 207,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will also investigate methods for joint learning as well as ways to speed up the joint decoding algorithm.",
                    "sid": 208,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Acknowledgments The authors thank Zhongqiang Huang for his help with experiments.",
                    "sid": 209,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This work is partly supported by DARPA under Contract No.",
                    "sid": 210,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "HR0011-12-C-0016.",
                    "sid": 211,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.",
                    "sid": 212,
                    "ssid": 11,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}