{
    "ID": "W03-1025",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "The paper presents a maximum entropy Chinese character-based parser trained on the Chinese Treebank (\u201cCTB\u201d henceforth).",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Word-based parse trees in CTB are first converted into character- based trees, where word-level part-of- speech (POS) tags become constituent labels and character-level tags are derived from word-level POS tags.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A maximum entropy parser is then trained on the character-based corpus.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The parser does word-segmentation, POS- tagging and parsing in a unified framework.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An average label F-measure and word-segmentation F-measure are achieved by the parser.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our results show that word-level POS tags can improve significantly word-segmentation, but higher-level syntactic strutures are of little use to word segmentation in the maximum entropy parser.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A word-dictionary helps to improve both word-segmentation and parsing accuracy.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 Introduction: Why Parsing Characters?",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After Linguistic Data Consortium (LDC) released the Chinese Treebank (CTB) developed at UPenn (Xia et al., 2000), various statistical Chinese parsers (Bikel and Chiang, 2000; Xu et al., 2002) have been built.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Techniques used in parsing English have been shown working fairly well when applied to parsing Chinese text.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As there is no word boundary in written Chinese text, CTB is manually segmented into words and then labeled.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Parsers described in (Bikel and Chiang, 2000) and (Xu et al., 2002) operate at word-level with the assumption that input sentences are pre-segmented.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The paper studies the problem of parsing Chi-",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "motivation",
            "number": "1",
            "sents": [
                {
                    "text": "is that a character-based parser can be used directly in natural language applications that operate at character level, whereas a word-based parser requires a separate word-segmenter.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second and more important reason is that the availability of CTB, a large corpus with high quality syntactic annotations, provides us with an opportunity to create a highly-accurate word-segmenter.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is widely known that Chinese word-segmentation is a hard problem.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower . The agreement between multiple human subjects is even lower (Wu and Fung, 1994).",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The reason is that human subjects may differ in segmenting things like personal names (whether family and given names should be one or two words), number and measure units and compound words, although these ambiguities do not change a human being\u2019s understanding of a sentence.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Low agreement between humans affects directly evaluation of machines\u2019 performance (Wu and Fung, 1994) as it is hard to define a gold standard.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It does not necessarily imply that machines cannot do better than humans.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Indeed, if we train a model with consistently segmented data, a machine may do a better job in \u201cremembering\u201d word segmentations.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As will be shown shortly, it is straightforward to encode word-segmentation information in a character based parse tree.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Parsing Chinese character streams therefore does effectively word-segmentation, part- of-speech (POS) tagging and constituent labeling at the same time.",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since syntactical information influences directly word-segmentation in the proposed character-based parser, CTB allows us to test whether or not syntactic information is useful for word-segmentation.",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A third advantage of parsing Chinese character streams is that Chinese words are more or less an open concept and the out-of- vocabulary (OOV) word rate is high.",
                    "sid": 25,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As morphology of the Chinese language is limited, extra care is needed to model unknown words when building a word-based model.",
                    "sid": 26,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Xu et al.",
                    "sid": 27,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2002), for example, uses an independent corpus to derive word classes so that unknown words can be parsed reliably.",
                    "sid": 28,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Chinese characters, on the other hand, are almost closed.",
                    "sid": 29,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To demonstrate the OOV problem, we collect a word and character vocabulary from the first sentences of CTB, and compute their coverages on the corresponding word and character tokenization of the last of the corpus.",
                    "sid": 30,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The word-based OOV rate is while the character-based OOV rate is only . The first step of training a character-based parser is to convert word-based parse trees into character- based trees.",
                    "sid": 31,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We derive character-level tags from word-level POS tags and encode word-boundary information with a positional tag.",
                    "sid": 32,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Word-level POSs become a constituent label in character-based trees.",
                    "sid": 33,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A maximum entropy parser (Ratnaparkhi, 1997) parser is then built and tested.",
                    "sid": 34,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Many language- independent feature templates in the English parser can be reused.",
                    "sid": 35,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Lexical features, which are language- dependent, are used to further improve the baseline models trained with language-independent features only.",
                    "sid": 36,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Word-segmentation results will be presented and it will be shown that POSs are very helpful while higher-level syntactic structures are of little use to word-segmentation \u2013 at least in the way they are used in the parser.",
                    "sid": 37,
                    "ssid": 37,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "word-tree to character-tree. ",
            "number": "2",
            "sents": [
                {
                    "text": "CTB is manually segmented and is tokenized at word level.",
                    "sid": 38,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To build a Chinese character parser, we first need to convert word-based parse trees into character trees.",
                    "sid": 39,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A few simple rules are employed in this conversion to encode word boundary information: 1.",
                    "sid": 40,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Word-level POS tags become labels in charac-.",
                    "sid": 41,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ter trees.",
                    "sid": 42,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.",
                    "sid": 43,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Character-level tags are inherited from word-.",
                    "sid": 44,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "level POS tags after appending a positional tag;",
                    "sid": 45,
                    "ssid": 8,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "for single-character words, the positional tag is. ",
            "number": "3",
            "sents": [
                {
                    "text": "\u201cs\u201d; for multiple-character words, the first character is appended with a positional tag \u201cb\u201d, last character with a positional tag \u201ce\u201d, and all middle characters with a positional tag \u201cm\u201d.",
                    "sid": 46,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An example will clarify any ambiguity of the rules.",
                    "sid": 47,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, a word-parse tree \u201c(IP (NP (NP /NR ) (NP /NN /NN ) ) (VP /VV ) /PU )\u201d would become \u201c(IP (NP (NP (NR /nrb /nrm /nre ) ) (NP (NN /nnb /nne ) (NN /nnb /nne ) ) ) (VP (VV /vvb /vve ) ) (PU /pus ) ).\u201d (1) Note that the word-level POS \u201cNR\u201d becomes a label of the constituent spanning the three characters \u201c \u201d.",
                    "sid": 48,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The character-level tags of the constituent \u201c \u201d are the lower-cased word-level POS tag plus a positional letter.",
                    "sid": 49,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, the first character \u201c \u201d is assigned the tag \u201cnrb\u201d where \u201cnr\u201d is from the word-level POS tag and \u201cb\u201d denotes the beginning character; the second (middle) character \u201c \u201d gets the positional letter \u201cm\u201d, signifying that it is in the middle, and the last character \u201c \u201d gets the positional letter \u201ce\u201d, denoting the end of the word.",
                    "sid": 50,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other words in the sentence are mapped similarly.",
                    "sid": 51,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After the mapping, the number of terminal tokens of the character tree is larger than that of the word tree.",
                    "sid": 52,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is clear that character-level tags encode word boundary information, and chunk-level1 labels are word-level POS tags.",
                    "sid": 53,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, parsing a Chinese character sentence is effectively doing word- segmentation, POS-tagging and constructing syntactic structure at the same time.",
                    "sid": 54,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3 Model and Features.",
                    "sid": 55,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The maximum entropy parser (Ratnaparkhi, 1997) is used in this study, for it offers the flexibility of integrating multiple sources of knowledge into a model.",
                    "sid": 56,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The maximum entropy model decomposes , the probability of a parse tree given a sentence , into the product of probabilities of individual parse 1 A chunk is here defined as a constituent whose children are all preterminals.",
                    "sid": 57,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "actions, i.e., . The parse actions are an ordered sequence, where is the number of actions associated with the parse . The mapping from a parse tree to its unique sequence ofactions is 1-to-1.",
                    "sid": 58,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each parse action is either tagging a word, chunking tagged words, extend ing an existing constituent to another constituent, or checking whether an open constituent shouldbe closed.",
                    "sid": 59,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each component model takes the expo nential form: (2) where is a normalization term to ensure that is a probability, is a feature function (often binary) and is the weight of . Given a set of features and a corpus of training data, there exist efficient training algorithms (Darroch and Ratcliff, 1972; Berger et al., 1996) to find the optimal parameters . The art of building a maximum entropy parser then reduces to choosing \u201cgood\u201d features.",
                    "sid": 60,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We break features used in this study into two categories.",
                    "sid": 61,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first set of features are derived from predefined templates.",
                    "sid": 62,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When these templates are applied to training data, features are generated automatically.",
                    "sid": 63,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since these templates can be used in any language, features generated this way are referred to language-independent features.",
                    "sid": 64,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second category of features incorporate lexical information into the model and are primarily designed to improve word-segmentation.",
                    "sid": 65,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This set of features are language-dependent since a Chinese word dictionary is required.",
                    "sid": 66,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.1 Language-Independent Feature Templates.",
                    "sid": 67,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The maximum entropy parser (Ratnaparkhi, 1997) parses a sentence in three phases: (1) it first tags the input sentence.",
                    "sid": 68,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Multiple tag sequences are kept in the search heap for processing in later stages; (2) Tagged tokens are grouped into chunks.",
                    "sid": 69,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is possible that a tagged token is not in any chunk; (3) A chunked sentence, consisting of a forest of many subtrees, is then used to extend a subtree to a new constituent or join an existing constituent.",
                    "sid": 70,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each extending action is followed by a checking action which decides whether or not to close the extended constituent.",
                    "sid": 71,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In general, when a parse action is carried out, the context information, i.e., the input sentence and preceding parse actions , is represented by a forest of subtrees.",
                    "sid": 72,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Feature functions operate on the forest context and the next parse action.",
                    "sid": 73,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They are all of the form: (3) where is a binary function on the context.",
                    "sid": 74,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some notations are needed to present features.",
                    "sid": 75,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use to denote an input terminal token, its tag (preterminal), a chunk, and a constituent label, where the index is relative to the current subtree: the subtree immediately left to the current is indexed as , the second left to the current sub- tree is indexed as , the subtree immediately to the right is indexed as , so on and so forth.",
                    "sid": 76,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "represents the root label of the -child of the subtree.",
                    "sid": 77,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If , the child is counted from right.",
                    "sid": 78,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With these notations, we are ready to introduce language-independent features, which are broken down as follows: Tag Features In the tag model, the context consists of a window of five tokens \u2013 the token being tagged and two tokens to its left and right \u2013 and two tags on the left of the current word.",
                    "sid": 79,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The feature templates are tabulated in Table 1 (to save space, templates are grouped).",
                    "sid": 80,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "At training time, feature templates are in- stantiated by the training data.",
                    "sid": 81,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, when the template \u201c \u201d is applied to the first character of the sample sentence, \u201c(IP (NP (NP (NR /nrb /nrm /nre ) ) (NP (NN /nnb /nne ) (NN /nnb /nne ) ) ) (VP (VV /vvb /vve ) ) (PU /pus ) )\u201d, a feature *BOUNDARY* is generated.",
                    "sid": 82,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that is the token on the left and in this case, the boundary of the sentence.",
                    "sid": 83,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The template \u201c \u201d is instantiated similarly as . Chunk Features As character-level tags have encoded the chunk label information and the uncertainly about a chunk action is low given character-level tags, we limit the chunk context to a window of three subtrees \u2013 the current one plus its left and right subtree.",
                    "sid": 84,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "in Table 2 denotes the label of the subtree if it is not Index Template (context,future) 1 2 3 4 5 Table 1: Tag feature templates: : current token (if ) or token on the left (if ) or right (if ).",
                    "sid": 85,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": ": tag.",
                    "sid": 86,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "a chunk, or the chunk label plus the tag of its rightmost child if it is a chunk.",
                    "sid": 87,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Index Template (context,future) 1 2 Table 2: Chunk feature templates: is the chunk label plus the tag of its right most child if the tree is a chunk; Otherwise is the constituent label of the tree.Again, we use the sentence (1) as an example.",
                    "sid": 88,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As sume that the current forest of subtrees is (NR /nrb /nrm /nre ) /nnb /nne /nnb /nne /vvb /vve /pus , and the current subtree is \u201c /nnb\u201d, then instantiating the template would result in a feature . Extend Features Extend features depend on previous subtree and the two following subtrees.",
                    "sid": 89,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some features uses child labels of the previous subtree.",
                    "sid": 90,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the interpretation of the template on line 4 of Table 3 is that is the root label of the previous subtree, is the label of the rightmost child of the previous tree, and is the root label of the current subtree.",
                    "sid": 91,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Check Features Most of check feature templates again use constituent labels of the surrounding subtrees.",
                    "sid": 92,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The template on line 1 of Table 4 is unique to the check model.",
                    "sid": 93,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It essentially looks at children of the current constituent, which is intuitively a strong indication whether or not the current constituent should be closed.",
                    "sid": 94,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Index Template (context,future) 1 2 3 4 5 6 Table 3: Extend feature templates: is the root constituent label of the subtree (relative to the current one); is the label of the rightmost child of the previous subtree.",
                    "sid": 95,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Index Template (context,future) 1 2 3 4 5 6 7 Table 4: Check feature templates: is the constituent label of the subtree (relative to the current one).",
                    "sid": 96,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "is the child label of the current constituent.",
                    "sid": 97,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2 Language-Dependent Features.",
                    "sid": 98,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The model described so far does not depend on any Chinese word dictionary.",
                    "sid": 99,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All features derived from templates in Section 3.1 are extracted from training data.",
                    "sid": 100,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A problem is that words not seen in training data may not have \u201cgood\u201d features associated with them.",
                    "sid": 101,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Fortunately, the maximum entropy framework makes it relatively easy to incorporate other sources of knowledge into the model.",
                    "sid": 102,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We present a set of language-dependent features in this section, primarily for Chinese word segmentation.",
                    "sid": 103,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The language-dependent features are computed from a word list and training data.",
                    "sid": 104,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Formerly, let be a list of Chinese words, where characters are separated by spaces.",
                    "sid": 105,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "At the time of tagging characters (recall word-segmentation information is encoded in character-level tags), we test characters within a window of five (that is, two characters to the left and two to the right) and see if a character either starts, occurs in any position of, or ends any word on the list . This feature templates are summarized in Ta ble 5.",
                    "sid": 106,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "tests if the character starts any word on the list . Similarly, tests if the character occurs in any position of any word on the list , and tests if the character is the last position of any word on the list . Index Template (context,future) 1 2 3 Table 5: Language-dependent lexical features.",
                    "sid": 107,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A word list can be collected to encode different semantic or syntactic information.",
                    "sid": 108,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, a list of location names or personal names may help the model to identify unseen city or personal names; Or a closed list of functional words can be collected to represent a particular set of words sharing a POS.",
                    "sid": 109,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This type of features would improve the model robustness since unseen words will share features fired for seen words.",
                    "sid": 110,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will show shortly that even a relatively small word-list improves significantly the word-segmentation accuracy.",
                    "sid": 111,
                    "ssid": 66,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "experiments. ",
            "number": "4",
            "sents": [
                {
                    "text": "All experiments reported here are conducted on the latest LDC release of the Chinese Treebank, which consists of about words.",
                    "sid": 112,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Word parse trees are converted to character trees using the procedure described in Section 2.",
                    "sid": 113,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All traces and functional tags are stripped in training and testing.",
                    "sid": 114,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Two results are reported for the character-based parsers: the F-measure of word segmentation and F-measure of constituent labels.",
                    "sid": 115,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Formally, let be the number of words of the reference sentence and its parser output, respectively, and be the number of common words in the sentence of test set, then the word segmentation F-measure is output, respectively, and is the number of common constituents.",
                    "sid": 116,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Chunk-level labels converted from POS tags (e.g., \u201cNR\u201d, \u201cNN\u201d and \u201cVV\u201d etc in (1)) are included in computing label F-measures for character-based parsers.",
                    "sid": 117,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.1 Impact of Training Data.",
                    "sid": 118,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first question we have is whether CTB is large enough in the sense that the performance saturates.",
                    "sid": 119,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first set of experiments are intended to answer this question.",
                    "sid": 120,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In these experiments, the first CTB is used as the training set and the rest as the test set.",
                    "sid": 121,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We start with of the training set and increase the training set each time by . Only language-independent features are used in these experiments.",
                    "sid": 122,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1 shows the word segmentation F-measure and label F-measure versus the amount of training data.",
                    "sid": 123,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As can be seen, F-measures of both word segmentation and constituent label increase monotonically as the amount of training data increases.",
                    "sid": 124,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If all training data is used, the word segmentation F-measure is and label F-measure . These results show that language-independent features work fairly well \u2013 a major advantage of data- driven statistical approach.",
                    "sid": 125,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The learning curve also shows that the current training size has not reached a saturating point.",
                    "sid": 126,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This indicates that there is room to improve our model by getting more training data.",
                    "sid": 127,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Word seg F\u2212measure and Label F\u2212measure vs. training size 1 0.95 0.9 (4) 0.85 0.8 The F-measure of constituent labels is computed similarly: 0.75 0.7 0.65 Segmentation Label (5) where and are the number of constituents in the reference parse tree and parser 0 20 40 60 80 100 Percent of training data Figure 1: Learning curves: word-segmentation F- measure and parsing label F-measure vs. percentage of training data.",
                    "sid": 128,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.2 Effect of Lexical Features.",
                    "sid": 129,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section, we present the main parsing results.",
                    "sid": 130,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As it has not been long since the second release of CTB and there is no commonly-agreed training and test set, we divide the entire corpus into 10 equal partitions and hold each partition as a test set while the rest are used for training.",
                    "sid": 131,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each training-test configuration, a baseline model is trained with only language independent features.",
                    "sid": 132,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Baseline word segmentation and label F-measures are plotted with dotted- line in Figure 2.",
                    "sid": 133,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then add extra lexical features described in Section 3.1 to the model.",
                    "sid": 134,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Lexical questions are derived from a 58K-entry word list.",
                    "sid": 135,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The word list is broken into 4 sub-lists based on word length, ranging from 2 to 5 characters.",
                    "sid": 136,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Lexical features are computed by answering one of the three questions in Table 5.",
                    "sid": 137,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Intuitively, these questions would help the model to identify word boundaries, which in turn ought to improve the parser.",
                    "sid": 138,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is confirmed by results shown in Figure 2.",
                    "sid": 139,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The solid two lines represent results with enhanced lexical questions.",
                    "sid": 140,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As can be seen, lexical questions improve significantly both word segmentation and parsing across all experiments.",
                    "sid": 141,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is not surprising as lexical features derived from the word list are complementary to language-independent features computed from training sentences.",
                    "sid": 142,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "trained with lexical features, the second experiment has a label F-measure and word- segmentation F-measure , while the sixth experiment has a label F-measure and word- segmentation F-measure . The large variances justify multiple experiment runs.",
                    "sid": 143,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To reduce the variances, we report numbers averaged over the 10 experiments in Table 6.",
                    "sid": 144,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Numbers on the row starting with \u201cWS\u201d are word-segmentation results, while numbers on the last row are F-measures of constituent labels.",
                    "sid": 145,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second column are average F- measures for the baseline model trained with only language-independent features.",
                    "sid": 146,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The third column contains F-measures for the model trained with extra lexical features.",
                    "sid": 147,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The last column are releative error reduction.",
                    "sid": 148,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The best average word-segmentation F- measure is and label F-measure is . F m e a s u r e Re lat ive (% ) ba sel in e LexFeat W S ( % ) La be l( %) 9 4 . 6 96.0 8 0 . 0 81.4 2 6 7 Table 6: WS: word-segmentation.",
                    "sid": 149,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Baseline: language-independent features.",
                    "sid": 150,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "LexFeat: plus lexical features.",
                    "sid": 151,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Numbers are averaged over the 10 experiments in Figure 2.",
                    "sid": 152,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 0.95 Results of 10 experime nts Segment ation (with LexFeat) 4.3 Effect of Syntac tic Infor matio n on W o r d s e g m e n t a t i o n Since CTB provi des us with full parse trees, we want to know how synta ctic infor matio n affect s word 0.9 Segmentation (baseline).",
                    "sid": 153,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Label (with LexFeat) Label (baseline) segmentation.",
                    "sid": 154,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To this end, we devise two sets of experiments: 0.85 0.8 0.75 0.7 1 2 3 4 5 6 7 8 9 10 Experiment Number Figure 2: Parsing and word segmentation F- measures vs. the experiment numbers.",
                    "sid": 155,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Lines with triangles: segmentation; Lines with circles: label; Dotted-lines: language-independent features only; Solid lines: plus lexical features.",
                    "sid": 156,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Another observation is that results vary greatly across experiment configurations: for the model 1.",
                    "sid": 157,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We strip all POS tags and labels in the Chinese.",
                    "sid": 158,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Treebank and retain only word boundary information.",
                    "sid": 159,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To use the same maximum entropy parser, we represent word boundary by dummy constituent label \u201cW\u201d.",
                    "sid": 160,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the sample sentence (1) in Section 2 is represented as: (W /wb /wm /we ) (W /wb /we ) (W /wb /we ) (W /wb /we ) (W /ws ).",
                    "sid": 161,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.",
                    "sid": 162,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We remove all labels but retain word-level POS.",
                    "sid": 163,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "information.",
                    "sid": 164,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The sample sentence above is represented as: (NR /nrb /nrm /nre ) (NN /nnb /nne ) (NN /nnb /nne ) (VV /vvb /vve ) (PU /pus ).",
                    "sid": 165,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that positional tags are used in both setups.",
                    "sid": 166,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Effect of Syntactic Info on Word Segmentation Word\u2212boundary but enhanced with lexical features derived from the ASBC corpus2 . Xu et al.",
                    "sid": 167,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2002) reports an overall F-measure when the same training and test set as (Bikel and Chiang, 2000) are used.",
                    "sid": 168,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since our 0.975 0.97 0.965 0.96 0.955 0.95 0.945 0.94 0.935 0.93 POS Full Tree pars er ope rate s at cha ract er leve l, and mor e trai nin g data is use d, the best resu lts are not dire ctly co mp a- rabl e. The mid dle poi nt of the lear nin g cur ve in Figure 1, whi ch is trai ned wit h rou ghl y 100 K wor ds, is at the sam e ball par k of (Xu et al., 200 2).",
                    "sid": 169,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The con - trib utio n of this wor k is that the pro pos ed cha ract erbas ed pars er doe s word seg me ntat ion, PO S tag gin g and pars ing in a unif ied fra me wor k. It is the first at- tem pt to our kno wle dge that synt acti c info rma tion is use d in word seg me ntat ion.",
                    "sid": 170,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 2 3 4 5 6 7 8 9 10 Experiment Number Figure 3: Usefulness of syntactic information: (black) dash-dotted line \u2013 word boundaries only, (red) dashed line \u2013 POS info, and (blue) solid line \u2013 full parse trees.",
                    "sid": 171,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With these two representations of CTB, we repeat the 10 experiments of Section 4.2 using the same lexical features.",
                    "sid": 172,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Word-segmentation results are plotted in Figure 3.",
                    "sid": 173,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The model trained with word boundary information has the worst performance, which is not surprising as we would expect information such as POS tags to help disambiguate word boundaries.",
                    "sid": 174,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "What is surprising is that syntactic information beyond POS tags has little effect on word- segmentation \u2013 there is practically no difference between the solid line (for the model trained with full parse trees) and the dashed-line (for the model trained with POS information) in Figure 3.",
                    "sid": 175,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This result suggests that most ambiguities of Chinese word boundaries can be resolved at lexical level, and high- level syntactic information does not help much to word segmentation in the current parser.",
                    "sid": 176,
                    "ssid": 65,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "related work. ",
            "number": "5",
            "sents": [
                {
                    "text": "Bikel and Chiang (2000) and Xu et al.",
                    "sid": 177,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study.",
                    "sid": 178,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Bikel and Chiang (2000) in fact contains two parsers: one is a lexicalized probabilistic context- free grammar (PCFG) similar to (Collins, 1997); the other is based on statistical TAG (Chiang, 2000).",
                    "sid": 179,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.",
                    "sid": 180,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Without knowing and controlling testing conditions, it is nearly impossible to compare results in a meaningful way.",
                    "sid": 181,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, we will compare our approach with some related work only without commenting on segmentation accuracy.",
                    "sid": 182,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Wu and Tseng (1993) contains a good problem statement of Chinese word- segmentation and also outlines a few segmentation algorithms.",
                    "sid": 183,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our method is supervised in that the training data is manually labeled.",
                    "sid": 184,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Palmer (1997) uses transform-based learning (TBL) to correct an initial segmentation.",
                    "sid": 185,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sproat et al.",
                    "sid": 186,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1996) employs stochastic finite state machines to find word boundaries.",
                    "sid": 187,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Luo and Roukos (1996) proposes to use a language model to select from ambiguous word- segmentations.",
                    "sid": 188,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All these work assume that a lexicon or some manually segmented data or both are available.",
                    "sid": 189,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are numerous work exploring semi- supervised or unsupervised algorithms to segment Chinese text.",
                    "sid": 190,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ando and Lee (2003) uses a heuristic method that does not require segmented training data.",
                    "sid": 191,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Peng and Schuurmans (2001) learns a lexicon and its unigram probability distribution.",
                    "sid": 192,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The automatically learned lexicon is pruned using a mutual information criterion.",
                    "sid": 193,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Peng and Schuurmans (2001) requires a validation set and is therefore semi- supervised.",
                    "sid": 194,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "About F-measure is reported in (Bikel and Chi- ang, 2000).",
                    "sid": 195,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Xu et al.",
                    "sid": 196,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2002) is also based on PCFG, 2 See http://godel.iis.sinica.edu.tw/ROCLING..",
                    "sid": 197,
                    "ssid": 21,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusions. ",
            "number": "6",
            "sents": [
                {
                    "text": "We present a maximum entropy Chinese character- based parser which does word-segmentation, POS tagging and parsing in a unified framework.",
                    "sid": 198,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The flexibility of maximum entropy model allows us to integrate into the model knowledge from other sources, together with features derived automatically from training corpus.",
                    "sid": 199,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have shown that a relatively small word-list can reduce word- segmentation error by as much as , and a word- segmentation F-measure and label F-measure are obtained by the character-based parser.",
                    "sid": 200,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our results also show that POS information is very useful for Chinese word-segmentation, but higher- level syntactic information benefits little to word- segmentation.",
                    "sid": 201,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments",
            "number": "",
            "sents": [
                {
                    "text": "Special thanks go to Hongyan Jing and Judith Hochberg who proofread the paper and corrected many typos and ungrammatical errors.",
                    "sid": 202,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The author is also grateful to the anonymous reviewers for their insightful comments and suggestions.",
                    "sid": 203,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No.",
                    "sid": 204,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "N6600199-28916.",
                    "sid": 205,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.",
                    "sid": 206,
                    "ssid": 9,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}