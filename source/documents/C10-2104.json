{
    "ID": "C10-2104",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "We present novel kernels based on structured and unstructured features for reranking the N-best hypotheses of conditional random fields (CRFs) applied to entity extraction.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The former features are generated by a polynomial kernel encoding entity features whereas tree kernels are used to model dependencies amongst tagged candidate examples.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The experiments on two standard corpora in two languages, i.e. the Italian EVALITA 2009 and the English CoNLL 2003 datasets, show a large improvement on CRFs in F-measure, i.e. from 80.34% to 84.33% and from 84.86% to 88.16%, respectively.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our analysis reveals that both kernels provide a comparable improvement over the CRFs baseline.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additionally, their combination improves CRFs much more than the sum of the individual contributions, suggesting an interesting kernel synergy.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Reranking is a promising computational framework, which has drawn special attention in the Natural Language Processing (NLP) community.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Basically, this method first employs a probabilistic model to generate a list of top-n candidates and then reranks this n-best list with additional features.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One appeal of this approach is its flexibility of incorporating arbitrary features into a model.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These features help in discriminating good from bad hypotheses and consequently their automatic learning.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Various algorithms have been applied for reranking in NLP applications (Huang, 2008; Shen et al., 2004; Collins, 2002b; Collins and Koo, 2000), including parsing, name tagging and machine translation.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This work has exploited the disciminative property as one of the key criterion of the reranking algorithm.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Reranking appears extremely interesting if coupled with kernel methods (Dinarelli et al., 2009; Moschitti, 2004; Collins and Duffy, 2001), as the latter allow for extracting from the ranking hypotheses a huge amount of features along with their dependencies.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Indeed, while feature-based learning algorithms involve only the dot-product between feature vectors, kernel methods allow for a higher generalization by replacing the dot- product with a function between pairs of linguistic objects.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Such functions are a kind of similarity measure satisfying certain properties.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An example is the tree kernel (Collins and Duffy, 2001), where the objects are syntactic trees that encode grammatical derivations and the kernel function computes the number of common subtrees.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similarly, sequence kernels (Lodhi et al., 2002) count the number of common subsequences shared by two input strings.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Named-entities (NEs) are essential for defining the semantics of a document.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "NEs are objects that can be referred by names (Chinchor and Robinson, 1998), such as people, organizations, and locations.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The research on NER has been promoted by the Message Understanding Conferences (MUCs, 19871998), the shared task of the Conference on Natural Language Learning (CoNLL, 20022003), and the Automatic Content Extraction program (ACE, 20022005).",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the literature, there exist various learning approaches to extract named-entities from text.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A NER sys 901 Coling 2010: Poster Volume, pages 901\u2013909, Beijing, August 2010 tem often builds some generative/discriminative model, then, either uses only one classifier (Car- reras et al., 2002) or combines many classifiers using some heuristics (Florian et al., 2003).",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To the best of our knowledge, reranking has not been applied to NER except for the reranking algorithms defined in (Collins, 2002b; Collins, 2002a), which only targeted the entity detection (and not entity classification) task.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Besides, since kernel methods offer a natural way to exploit linguistic properties, applying kernels for NE reranking is worthwhile.",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we describe how kernel methods can be applied for reranking, i.e. detection and classification of named-entities, in standard corpora for Italian and English.",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The key aspect of our reranking approach is how structured and flat features can be employed in discriminating candidate tagged sequences.",
                    "sid": 25,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this purpose, we apply tree kernels to a tree structure encoding NE tags of a sentence and combined them with a polynomial kernel, which efficiently exploits global features.",
                    "sid": 26,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our main contribution is to show that (a) tree kernels can be used to define general features (not merely syntactic) and (b) using appropriate algorithms and features, reranking can be very effective for named-entity recognition.",
                    "sid": 27,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our study demonstrates that the composite kernel is very effective for reranking named-entity sequences.",
                    "sid": 28,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Without the need of producing and heuristically combining learning models like previous work on NER, the composite kernel not only captures most of the flat features but also efficiently exploits structured features.",
                    "sid": 29,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More interestingly, this kernel yields significant improvement when applied to two corpora of two different languages.",
                    "sid": 30,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The evaluation in the Italian corpus shows that our method outperforms the best reported methods whereas on the English data it reaches the state-of-the-art.",
                    "sid": 31,
                    "ssid": 31,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "background. ",
            "number": "2",
            "sents": [
                {
                    "text": "2.1 The data.",
                    "sid": 32,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Different languages exhibit different linguistic phenomena and challenges.",
                    "sid": 33,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A robust NER system is expected to be well-adapted to multiple domains and languages.",
                    "sid": 34,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, we experimented with two datasets: the EVALITA 2009 Italian corpus and the well-known CoNLL 2003 English shared task corpus.",
                    "sid": 35,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The EVALITA 2009 Italian dataset is based on I-CAB, the Italian Content Annotation Bank (Magnini et al., 2006), annotated with four entity types: Person (PER), Organization (ORG), GeoPolitical Entity (GPE) and Location (LOC).",
                    "sid": 36,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The training data, taken from the local newspaper \u201cL\u2019Adige\u201d, consists of 525 news stories which belong to five categories: News Stories, Cultural News, Economic News, Sports News and Local News.",
                    "sid": 37,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Test data, on the other hand, consist of completely new data, taken from the same newspaper and consists of 180 news stories.",
                    "sid": 38,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The CoNLL 2003 English dataset is created within the shared task of CoNLL2003 (Sang and Meulder, 2003).",
                    "sid": 39,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is a collection of news wire articles from the Reuters Corpus, annotated with four entity types: Person (PER), Location (LOC), Organization (ORG) and Miscellaneous name (MISC).",
                    "sid": 40,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The training and the development datasets are news feeds from August 1996, while the test set contains news feeds from December 1996.",
                    "sid": 41,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Accordingly, the named entities in the test dataset are considerably different from those that appear in the training or the development set.",
                    "sid": 42,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Italian GPE LOC ORG PER Train 2813 24.65% 362 3.17% 3658 32.06% 4577 40.11% Test 1143 23.02% 156 3.14% 1289 25.96% 2378 47.89% English LOC MISC ORG PER Train 7140 30.38% 3438 14.63% 6321 26.90% 6600 28.09% Dev 1837 30.92% 922 15.52% 1341 22.57% 1842 31.00% Test 1668 29.53% 702 12.43% 1661 29.41% 1617 28.63% Table 1: Statistics on the Italian EVALITA 2009 and English CoNLL 2003 corpora.",
                    "sid": 43,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.2 The baseline algorithm.",
                    "sid": 44,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We selected Conditional Random Fields (Lafferty et al., 2001) as the baseline model.",
                    "sid": 45,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Conditional random fields (CRFs) are a probabilistic framework for labeling and segmenting sequence data.",
                    "sid": 46,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They present several advantages over other purely generative models such as Hidden Markov models (HMMs) by relaxing the independence assumptions required by HMMs.",
                    "sid": 47,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Besides, HMMs and other discriminative Markov models are prone to the label bias problem, which is effectively solved by CRFs.",
                    "sid": 48,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The named-entity recognition (NER) task is framed as assigning label sequences to a set of observation sequences.",
                    "sid": 49,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We follow the IOB notation where the NE tags have the format B-TYPE, I-TYPE or O, which mean that the word is a beginning, a continuation of an entity, or not part of an entity at all.",
                    "sid": 50,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, consider the sentence with their corresponding NE tags, each word is la The gazetteer lists are built with names imported from different sources.",
                    "sid": 51,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For English, the geographic features are imported from NIMA\u2019s GEOnet Names Server (GNS)2, The Alexandria Digital Library (ADL) gazetteer3.",
                    "sid": 52,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The company data is included with all the publicly traded companies listed in Google directory4, the European business directory5.",
                    "sid": 53,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For Italian, the generic proper nouns are extracted from Wikipedia and various Italian sites.",
                    "sid": 54,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.3 Support Vector Machines (SVMs).",
                    "sid": 55,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Support Vector Machines refer to a supervised machine learning technique based on the latest results of the statistical learning theory.",
                    "sid": 56,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a vector space and a set of training points, i.e. positive and negative examples, SVMs find a separatbeled with a tag indicating its appropriate named ing hyperplane H ( x) = \u03c9 \u00d7 x + b = 0 where entity, resulting in annotated text, such as: Il/O presidente/O della/O Fifa/B-ORG Sepp/B-PER Blatter/I-PER affermando/O che/O il/O torneo/O era/O stato/O ottimo/O (FIFA president Sepp Blatter says that the tournament was excellent) For our experiments, we used CRF++ 1 to build our recognizer, which is a model trained discriminatively with the unigram and bigram features.",
                    "sid": 57,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These are extracted from a window at k words centered in the target word w (i.e. the one we want to classify with the B, O, I tags).",
                    "sid": 58,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More in detail such features are: \u2022 The word itself, its prefixes, suffixes, and part-of-speech \u2022 Orthographic/Word features.",
                    "sid": 59,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These are binary and mutually exclusive features that test whether a word contains all upper-cased, initial letter upper-cased, all lower-cased, roman-number, dots, hyphens, acronym, lonely initial, punctuation mark, single-char, and functional-word.",
                    "sid": 60,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 Gazetteer features.",
                    "sid": 61,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Class (geographical, \u03c9 \u2208 Rn and b \u2208 R are learned by applying the Structural Risk Minimization principle (Vapnik, 1998).",
                    "sid": 62,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "SVMs are a binary classifier, but they can be easily extended to multi-class classifier, e.g. by means of the one-vs-all method (Rifkin and Poggio, 2002).",
                    "sid": 63,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One strong point of SVMs is the possibility to apply kernel methods to implicitly map data in a new space where the examples are more easily separable as described in the next section.",
                    "sid": 64,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.4 Kernel methods.",
                    "sid": 65,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Kernel methods (Scho\u00a8 lkopf and Smola, 2001) are an attractive alternative to feature-based methods since the applied learning algorithm only needs to compute a product between a pair of objects (by means of kernel functions), avoiding the explicit feature representation.",
                    "sid": 66,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A kernel function is a scalar product in a possibly unknown feature space.",
                    "sid": 67,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More precisely, The object o is mapped in x with a feature function \u03c6 : O \u2192 'Rn, where O is the set of the objects.",
                    "sid": 68,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The kernel trick allows us to rewrite the decision hyperplane as:first name, surname, organization prefix, lo cation prefix) of words in the window.",
                    "sid": 69,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "H ( x) = i=1..l yi\u03b1i xi \u00b7 x + b = \u2022 Left Predictions.",
                    "sid": 70,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The predicted tags on the left of the word in the current classification.",
                    "sid": 71,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 http://crfpp.sourceforge.net 2 http://www.nima.mil/gns/html 3 http://www.alexandria.ucsb.edu 4 http://directory.google.com/Top/Business 5 http://www.europages.net i=1..l yi\u03b1ixi \u00b7 x + b = i=1..l yi\u03b1i\u03c6 (oi) \u00b7 \u03c6(o) + b, where yi is equal to 1 for positive and -1 for negative examples, \u03b1i \u2208 'R with \u03b1i \u2265 0, oi \u2200i \u2208 {1, .., l} are the training instances and the product K (oi, o) = \u03c6(oi) \u00b7 \u03c6(o)) is the kernel function associated with the mapping \u03c6.",
                    "sid": 72,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Kernel engineering can be carried out by combining basic kernels with additive or multiplicative operators or by designing specific data objects (vectors, sequences and tree structures) for the target tasks.",
                    "sid": 73,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Regarding NLP applications, kernel methods have attracted much interest due to the ability of implicitly exploring huge amounts of structural features.",
                    "sid": 74,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The parse tree kernel (Collins and Duffy, 2001) and string kernel (Lodhi et al., 2002) are examples of the well-known convolution kernels used in various NLP tasks.",
                    "sid": 75,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.5 Tree Kernels.",
                    "sid": 76,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Tree kernels represent trees in terms of their substructures (called tree fragments).",
                    "sid": 77,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Such fragments form a feature space which, in turn, is mapped into a vector space.",
                    "sid": 78,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Tree kernels measure the similarity between pair of trees by counting the number of fragments in common.",
                    "sid": 79,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are three important characterizations of fragment type: the Sub- Trees (ST), the SubSet Trees (SST) and the Partial Trees (PT).",
                    "sid": 80,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For sake of space, we do not report the mathematical description of them, which is available in (Vishwanathan and Smola, 2002), (Collins and Duffy, 2001) and (Moschitti, 2006), respectively.",
                    "sid": 81,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast, we report some descriptions in terms of feature space that may be useful to understand the new engineered kernels.",
                    "sid": 82,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In principle, a SubTree (ST) is defined by taking any node along with its descendants.",
                    "sid": 83,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A SubSet Tree (SST) is a more general structure which does not necessarily include all the descendants.",
                    "sid": 84,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The distinction is that an SST must be generated by applying the same grammatical rule set which generated the original tree, as pointed out in (Collins and Duffy, 2001).",
                    "sid": 85,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A Partial Tree (PT) is a more general form of substructures obtained by relaxing constraints over the SSTs.",
                    "sid": 86,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1 shows the overall fragment set of the ST, SST and PT kernels for the syntactic parse tree of the sentence frag Figure 1: Three kinds of tree kernels.",
                    "sid": 87,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ment: gives a talk . In the next section, we will define new structures for tagged sequences of NEs which along with the application of the PT kernel produce innovative tagging kernels for reranking.",
                    "sid": 88,
                    "ssid": 57,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "reranking method. ",
            "number": "3",
            "sents": [
                {
                    "text": "3.1 Reranking Strategy.",
                    "sid": 89,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a baseline we trained the CRFs model to generate 10-best candidates per sentence, along with their probabilities.",
                    "sid": 90,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each candidate was then represented by a semantic tree together with a feature vector.",
                    "sid": 91,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We consider our reranking task as a binary classification problem where examples are pairs of hypotheses < Hi, Hj >.",
                    "sid": 92,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a sentence \u201cSouth African Breweries Ltd bought stakes in the Lech and Tychy brewers\u201d and three of its candidate tagged sequences: H1 B-ORG I-ORG I-ORG I-ORG O O O O B-ORG O B-ORG O (the correct sequence) H2 B-MISC I-MISC B-ORG I-ORG O O O O B-ORG I-ORG I-ORG O H3 B-ORG I-ORG I-ORG I-ORG O O O O B-ORG O BLOC O where B-ORG, I-ORG, BLOC, O are the generated NE tags according to IOB notation as described in Section 3.2.",
                    "sid": 93,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With the above data (an original sentence together with a list of candidate tagged sequences), the following pairs of hypotheses will be gener ated < H1, H2 >, < H1, H3 >,< H2, H1 > and < H3, H1 >, where the first two pairs are positive and the latter pairs are negative instances.",
                    "sid": 94,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then a binary classifier based on SVMs and kernel methods can be trained to discriminate between the best hypothesis, i.e. < H1 > and the others.",
                    "sid": 95,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "At testing time the hypothesis receiving the highest score is selected (Collins and Duffy, 2001).",
                    "sid": 96,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2 Representation of Tagged Sequences in.",
                    "sid": 97,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Semantic Trees We now consider the representation that exploits the most discriminative aspects of candidate structures.",
                    "sid": 98,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As in the case of NER, an input candidate is a sequence of word/tag pairs x = {w1/t1...wn/tn} where wi is the i th word andti is the i th NE tag for that word.",
                    "sid": 99,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first repre sentation we consider is the tree structure.",
                    "sid": 100,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "See figure 2 as an example of candidate tagged sequence and its semantic tree.",
                    "sid": 101,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With the sentence \u201cSouth African Breweries Ltd bought stakes in the Lech and Tychy brewers\u201d and three of its candidate tagged sequences in the previous section, the training algorithm considers to construct a tree for each sequence, with the named- entity tags as pre-terminals and the words as leaves.",
                    "sid": 102,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "See figure 2 for an example of the semantic tree for the first tagged sequence.",
                    "sid": 103,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With this tree representation, for a word wi, the target NE tag would be set at parent and the features for this word are at child nodes.",
                    "sid": 104,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This allows us to best exploit the inner product between competing candidates.",
                    "sid": 105,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Indeed, in the kernel space, the inner product counts the number of common subtrees thus sequences with similar NE tags are likely to have higher score.",
                    "sid": 106,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the similarity between H1 and H3 will be higher than the similarity of the previous hypotheses with H2; this is reasonable since these two also have higher F1.It is worth noting that another useful modifica tion is the flexibility of incorporate diverse, arbitrary features into this tree structure by adding children to the parent node that contains entity tag.",
                    "sid": 107,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These characteristics can be exploited efficiently with the PT kernel, which relaxes constraints of production rules.",
                    "sid": 108,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The inner product can implicitly include these features and deal better with sparse data.",
                    "sid": 109,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.3 Global features.",
                    "sid": 110,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Mixed n-grams features In previous works, some global features have been used (Collins, 2002b; Collins, 2002a) but the employed algorithm just exploited arbitrary information regarding word types and linguistic patterns.",
                    "sid": 111,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast, we define and study diverse features by also considering n-grams patterns preceding, and following the target entity.",
                    "sid": 112,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Complementary context In supervised learning, NER systems often suffer from low recall, which is caused by lack of both resource and context.",
                    "sid": 113,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, a word like \u201cArkansas\u201d may not appear in the training set and in the test set, there may not be enough context to infer its NE tag.",
                    "sid": 114,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.",
                    "sid": 115,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To overcome this deficiency, we employed the following unsupervised procedure: first, the baseline NER is applied to the target un-annotated corpus.",
                    "sid": 116,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, we associate each word of the corpus with the most frequent NE category assigned in the previous step.",
                    "sid": 117,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, the above tags are used as features during the training of the improved NER and also for building the feature representation for a new classification instance.",
                    "sid": 118,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This way, for any unknown word w of the test set, we can rely on the most probable NE category as feature.",
                    "sid": 119,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The advantage is that we derived it by using the average over many possible contexts of w, which are in the different instances of the unnanotated corpus.",
                    "sid": 120,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The unlabeled corpus for Italian was collected from La Repubblica 6 and it contains over 20 millions words.",
                    "sid": 121,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Whereas the unlabeled corpus for English was collected mainly from The New York Times 7 and BBC news stories 8 with more than 35 millions words.",
                    "sid": 122,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Head word As the head word of an entity plays an important role in information extraction (Bunescu and Mooney, 2005a; Surdeanu et al., 2003), it is in 6 http://www.repubblica.it/ 7 http://www.nytimes.com/ 8 http://news.bbc.co.uk/ Figure 2: Semantic structure of the first sequence cluded in the global set together with its orthographic feature.",
                    "sid": 123,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We now describe some primitives for our global feature framework.",
                    "sid": 124,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.",
                    "sid": 125,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "wi for i = 1 . . .",
                    "sid": 126,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "n is the i th word 2.",
                    "sid": 127,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ti is the NE tag of wi 3.",
                    "sid": 128,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "gi is the gazetteer feature of the word wi 3.4 Reranking with Composite Kernel.",
                    "sid": 129,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section we describe our novel tagging kernels based on diverse global features as well as semantic trees for reranking candidate tagged sequences.",
                    "sid": 130,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As mentioned in the previous section, we can engineer kernels by combining tree and entity kernels.",
                    "sid": 131,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus we focus on the problem to define structure embedding the desired relational information among tagged sequences.",
                    "sid": 132,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Partial Tree Kernel 4.",
                    "sid": 133,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "fi is the most frequent NE tag seen in a large corpus of wi Let F = f1, f2, . . .",
                    "sid": 134,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", f |F | be a tree fragment 5.",
                    "sid": 135,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "hi is the head word of the entity.",
                    "sid": 136,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We normally set the head word of an entity as its last space of type PTs and let the indicator function Ii(n) be equal to 1 if the target f1 is rooted at node n and 0 otherwise, we define the PT kernel as: word.",
                    "sid": 137,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, when a preposition exists in the entity string, its head word is set as the last word before the preposition.",
                    "sid": 138,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For exam K (T1, T2) = n1 \u2208NT1 n2 \u2208NT2 \u2206(n1, n2) ple, the head word of the entity \u201cUniversity of Pennsylvania\u201d is \u201cUniversity\u201d.",
                    "sid": 139,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6.",
                    "sid": 140,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Mixed n-grams features of the words and.",
                    "sid": 141,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "their gazetteers/frequent-tag before/after the start/end of an entity.",
                    "sid": 142,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to the normal n-grams solely based on words, we mixed words with gazetteers/frequent-tag seen from a large corpus and create mixed n-grams features.",
                    "sid": 143,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 shows the full set of global features in our reranking framework.",
                    "sid": 144,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Features are anchored to each entity instance and adapted to entity types.",
                    "sid": 145,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This helps to discriminate different entities with the same surface forms.",
                    "sid": 146,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, they can be combined with n-grams patterns to learn and explicitly push the score of the correct sequence above the score of competing sequences.",
                    "sid": 147,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "where NT1 and NT2 are the set of nodes in T1 and T2 respectively and \u2206(n1, n2) = i=1 Ii(n1)Ii(n2), i.e. the number of common fragments rooted at the n1 and n2 nodes of the type shown in Figure 1.c. The Polynomial Kernel The polynomial kernel between two candidate tagged sequences is defined as: K (x, y) = (1 + x1 \u00b7 x2)2, where x1 and x2 are two feature vectors extracted from the two sequences with the global feature template.",
                    "sid": 148,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Tagging Kernels In our reranking framework, we incorporate the probability from the original model with the tree structure as well as the feature vectors.",
                    "sid": 149,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let us consider the following notations: Feature Description ws ws+1 . . .",
                    "sid": 150,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "we Entity string gs gs+1 . . .",
                    "sid": 151,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ge The gazetteer feature within the entity fs fs+1 . . .",
                    "sid": 152,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "fe The most frequent NE tag feature (seen from a large corpus) within the entity hw The head word of the entity lhw Indicates whether the head word is lower-cased ws\u22121 ws ; ws\u22121 gs ; gs\u22121 ws ; gs\u22121 gs Mixed bigrams of the words/gazetteer features before/after the start of the entity we we+1 ; we ge+1 ; ge we+1 ; ge ge+1 Mixed bigrams of the words/gazetteer features before/after the end of the entity ws\u22121 ws ; ws\u22121 fs ; fs\u22121 ws ; fs\u22121 fsMixed bigrams of the words/frequent-tag fea tures before/after the start of the entity we we+1 ; we fe+1 ; fe we+1 ; fe fe+1Mixed bigrams of the words/frequent-tag fea tures before/after the end of the entity ws\u22122 ws\u22121 ws ; ws\u22121 ws ws+1 ; we\u22121 we we+1 ; we\u22122 we\u22121 we Trigram features of the words before/after the start/end of the entity ws\u22122 ws\u22121 gs ; ws\u22122 gs\u22121 ws ; ws\u22122 gs\u22121 gs ; gs\u22122 ws\u22121 ws ; gs\u22122 ws\u22121 gs ; gs\u22122 gs\u22121 ws ; gs\u22122 gs\u22121 gs ; ws\u22121 ws gs+1 ; ws\u22121 gs ws+1 ; ws\u22121 gs gs+1 ; gs\u22121 ws ws+1 ; gs\u22121 ws gs+1 ; gs\u22121 gs ws+1 ; gs\u22121 gs gs+1 Mixed trigrams of the words/gazetteer features before/after the start of the entity we\u22121 we ge+1 ; we\u22121 ge we+1 ; we\u22121 ge ge+1 ; ge\u22121 we we+1 ; ge\u22121 we ge+1 ; ge\u22121 ge we+1 ; ge\u22121 ge ge+1 ; we\u22122 we\u22121 ge ; we\u22122 ge\u22121 we ; we\u22122 ge\u22121 ge ; ge\u22122 we\u22121 we ; ge\u22122 we\u22121 ge ; ge\u22122 ge\u22121 we ; ge\u22122 ge\u22121 ge Mixed trigrams of the words/gazetteer features before/after the end of the entity ws\u22122 ws\u22121 fs ; ws\u22122 fs\u22121 ws ; ws\u22122 fs\u22121 fs ; fs\u22122 ws\u22121 ws ; fs\u22122 ws\u22121 fs ; fs\u22122 fs\u22121 ws ; fs\u22122 fs\u22121 fs ; ws\u22121 ws fs+1 ; ws\u22121 fs ws+1 ; ws\u22121 fs fs+1 ; fs\u22121 ws ws+1 ; fs\u22121 ws fs+1 ; fs\u22121 fs ws+1 ; fs\u22121 fs fs+1Mixed trigrams of the words/frequent-tag fea tures before/after the start of the entity we\u22121 we fe+1 ; we\u22121 fe we+1 ; we\u22121 fe fe+1 ; fe\u22121 we we+1 ; fe\u22121 we fe+1 ; fe\u22121 fe we+1 ; fe\u22121 fe fe+1 ; we\u22122 we\u22121 fe ; we\u22122 fe\u22121 we ; we\u22122 fe\u22121 fe ; fe\u22122 we\u22121 we ; fe\u22122 we\u22121 fe ; fe\u22122 fe\u22121 we ; fe\u22122 fe\u22121 feMixed trigrams of the words/frequent-tag fea tures before/after the end of the entity Table 2: Global features in the entity kernel for reranking.",
                    "sid": 153,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These features are anchored for each entity instance and adapted to entity categories.",
                    "sid": 154,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the entity string (first feature) of the entity \u201cUnited Nations\u201d with entity type \u201cORG\u201d is \u201cORG United Nations\u201d.",
                    "sid": 155,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 K (x, y) = L(x) \u00b7 L(y) is the basic kernelwhere L(x) is the log probability of a can didate tagged sequence x under the original probability model.",
                    "sid": 156,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 T K (x, y) = t(x) \u00b7 t(y) is the partial tree kernel under the structure representation \u2022 F K (x, y) = f (x) \u00b7 f (y) is the polynomial kernel under the global features The tagging kernels between two tagged sequences are defined in the following combinations: 1.",
                    "sid": 157,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "C T K = \u03b1 \u00b7 K + (1 \u2212 \u03b1) \u00b7 T K 2.",
                    "sid": 158,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "C F K = \u03b2 \u00b7 K + (1 \u2212 \u03b2) \u00b7 F K 3.",
                    "sid": 159,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "C T F K = \u03b3 \u00b7 K + (1 \u2212 \u03b3) \u00b7 (T K + F K ) where \u03b1, \u03b2, \u03b3 are parameters weighting the two participating terms.",
                    "sid": 160,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Experiments on the validation set showed that these combinations yield the best performance with \u03b1 = 0.2 for both languages, \u03b2 = 0.4 for English and \u03b2 = 0.3 for and Italian, \u03b3 = 0.24 for English and \u03b3 = 0.2 for Italian.",
                    "sid": 161,
                    "ssid": 73,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "experimens and results. ",
            "number": "4",
            "sents": [
                {
                    "text": "4.1 Experimental Setup.",
                    "sid": 162,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a baseline we trained the CRFs classifier on the full training portion (11,227 sentences in the Italian and 14,987 sentences in the English corpus).",
                    "sid": 163,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In developing a reranking strategy for both English and Italian, the training data was split into 5 sections, and in each case the baseline classifier was trained on 4/5 of the data, then used to decode the remaining 1/5.",
                    "sid": 164,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The top 10 hypotheses together with their log probabilities were recovered for each training sentence.",
                    "sid": 165,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similarly, a model trained on the whole training data was used to produce 10 hypotheses for each sentence in the development set.",
                    "sid": 166,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the reranking experiments, we applied different kernel setups to the two corpora described in Section 2.1.",
                    "sid": 167,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The three kernels were trained on the training.",
                    "sid": 168,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "portion.",
                    "sid": 169,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Italian Test P R F C RF s 83.43 77.48 80.34 C T K 84.97 78.03 81.35 C F K 84.93 79.13 81.93 CTFK 85.99 82.73 84.33 (Zanoli et al., 2009) 84.07 80.02 82.00 English Test P R F C RF s 85.37 84.35 84.86 C T K 87.19 84.79 85.97 C F K 86.53 86.75 86.64 CTFK 88.07 88.25 88.16 (Ratinov and Roth, ) N/A N/A 90.57 Table 3: Reranking results of the three tagging kernels on the Italian and English testset.",
                    "sid": 170,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.2 Discussion.",
                    "sid": 171,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 3 presents the reranking results on the test data of both corpora.",
                    "sid": 172,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results show a 20.29% relative improvement in F-measure for Italian and 21.79% for English.",
                    "sid": 173,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "C F K based on unstructured features achieves higher accuracy than C T K based on structured features.",
                    "sid": 174,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the huge amount of subtrees generated by the PT kernel may limit the expressivity of some structural features, e.g. many fragments may only generate noise.",
                    "sid": 175,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This problem is less important with the polynomial kernel where global features are tailored for individual entities.",
                    "sid": 176,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In any case, the experiments demonstrate that both tagging kernels C T K and C F K give improvement over the CRFs baseline in both languages.",
                    "sid": 177,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This suggests that structured and unstructured features are effective in discriminating between competing NE annotations.",
                    "sid": 178,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, the combination of the two tagging kernels on both standard corpora shows a large improvement in F-measure from 80.34% to 84.33% for Italian and from 84.86% to 88.16% for English data.",
                    "sid": 179,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This suggests that these two kernels, corresponding to two kinds of feature, complement each other.",
                    "sid": 180,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To better collocate our results with previous work, we report the best NER outcome on the Italian (Zanoli et al., 2009) and the English (Ratinov and Roth, ) datasets, in the last row (in italic) of each table.",
                    "sid": 181,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This shows that our model outperforms the best Italian NER system and it is close to the state-of-art model for English, which exploits many complex features9.",
                    "sid": 182,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Also note that we are very close to the F1 achieved by the best system of CoNLL 2003, i.e. 88.8.",
                    "sid": 183,
                    "ssid": 22,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusion. ",
            "number": "5",
            "sents": [
                {
                    "text": "We analyzed the impact of kernel-based approaches for modeling dependencies between tagged sequences for NER.",
                    "sid": 184,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our study illustrates that each individual kernel, either with structured or with flat features clearly gives improvement to the base model.",
                    "sid": 185,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most interestingly, as we showed, these contributions are independent and, the approaches can be used together to yield better results.",
                    "sid": 186,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The composite kernel, which combines both kinds of features, can outperform the state-of-the- art.",
                    "sid": 187,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the future, it will be very interesting to use syntactic/semantic kernels, as for example in (Basili et al., 2005; Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b).",
                    "sid": 188,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Another promising direction is the use of syntactic trees, feature sequences and pairs of instances, e.g.",
                    "sid": 189,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Nguyen et al., 2009; Moschitti, 2008).",
                    "sid": 190,
                    "ssid": 7,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments",
            "number": "",
            "sents": [
                {
                    "text": "We would like to thank Roberto Zanoli and Marco Dinarelli for helpful explanation about their work.",
                    "sid": 191,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant.",
                    "sid": 192,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 In the future we will be able to integrate them with the authors collaboration.",
                    "sid": 193,
                    "ssid": 10,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}