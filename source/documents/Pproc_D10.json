{
    "ID": "Pproc_D10",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "The notion of fertility in word alignment (the number of words emitted by a sin\u00ad gle state) is useful but difficult to model.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Initial attempts at modeling fertility used heuristic search methods.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recent ap\u00ad proaches instead use more principled ap\u00ad proximate inference techniques such as Gibbs sampling for parameter estimation.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Yet in practice we also need the single best alignment, which is difficult to find us\u00ad ing Gibbs.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Building on recent advances in dual decomposition, this paper introduces an exact algorithm for finding the sin\u00ad gle best alignment with a fertility HMM.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finding the best alignment appears impor\u00ad tant, as this model leads to a substantial improvement in alignment quality.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Word-based translation models intended to model the translation process have found new uses iden\u00ad tifying word correspondences in sentence pairs.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These word alignments are a crucial training com\u00ad ponent in most machine translation systems.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Fur\u00ad thermore, they are useful in other NLP applica\u00ad tions, such as entailment identification.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The simplest models may use lexical infor\u00ad mation alone.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The seminal Model 1 (Brown et al., 1993) has proved very powerful, per\u00ad forming nearly as well as more complicated models in some phrasal systems (Koehn et al., 2003).",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With minor improvements to initializa\u00ad tion (Moore, 2004) (which may be important (Toutanova and Galley, 2011)), it can be quite competitive.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Subsequent IBM models include more detailed information about context.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Models 2 and 3 incorporate a positional model based on the absolute position of the word; Models 4 and 5 use a relative position model instead (an English word tends to align to a French word that is nearby the French word aligned to the previous English word).",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Models 3, 4, and 5 all incorporate a no\u00ad tion of\"fertility\": the number of French words that align to any English word.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although these latter models covered a broad range of phenomena, estimation techniques and MAP inference were challenging.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The au\u00ad thors originally recommended heuristic proce\u00ad dures based on local search for both.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Such meth\u00ad ods work reasonably well, but can be computation\u00ad ally inefficient and have few guarantees.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, many researchers have switched to the HMM model (Vogel et al., 1996) and variants with more parameters (He, 2007).",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This captures the posi\u00ad tional information in the IBM models in a frame\u00ad work that admits exact parameter estimation infer\u00ad ence, though the objective function is not concave: local maxima are a concern.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Modeling fertility is challenging in the HMM framework as it violates the Markov assump\u00ad tion.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Where the HMM jump model considers only the prior state, fertility requires looking across the whole state space.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, the standard forward-backward and Viterbi algorithms do not apply.",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recent work (Zhao and Gildea, 2010) de\u00ad scribed an extension to the HMM with a fertility model, using MCMC techniques for parameter es\u00ad timation.",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, they do not have a efficient means of MAP inference, which is necessary in many applications such as machine translation.",
                    "sid": 25,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This paper introduces a method for exact MAP inference with the fertility HMM using dual de\u00ad composition.",
                    "sid": 26,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The resulting model leads to sub\u00ad stantial improvements in alignment quality.",
                    "sid": 27,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 711, Sofia, Bulgaria, August 49 2013.",
                    "sid": 28,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "@2013 Association for Computational Linguistics",
                    "sid": 29,
                    "ssid": 29,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "iimm alignment. ",
            "number": "2",
            "sents": [
                {
                    "text": "Let us briefly review the HMM translation model as a starting point.",
                    "sid": 30,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We are given a sequence of English words e = e, 1.",
                    "sid": 31,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This model pro\u00ad duces distributions over French word sequences f = !I, ...",
                    "sid": 32,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", !J and word alignment vectors a = at, ...",
                    "sid": 33,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", aJ, where aj E [O..J] indicates the En\u00ad glish word generating the jth French word, 0 rep\u00ad resenting a special NULL state to handle systemat\u00ad ically unaligned words.",
                    "sid": 34,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "J Pr(f, ale) = p(JII) ITp(ajlaj-t) p(Jilea;) j=l The generative story begins by predicting the num\u00ad ber of words in the French sentence (hence the number of elements in the alignment vector).",
                    "sid": 35,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then for each French word position, first the alignment variable (English word index used to generate the current French word) is selected based on only the prior alignment variable.",
                    "sid": 36,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Next the French word is predicted based on its aligned English word.",
                    "sid": 37,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis\u00ad tribution.",
                    "sid": 38,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam\u00ad pling (Zhao and Gildea, 2010).",
                    "sid": 39,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this case, we make some initial estimate of the a vector, potentially randomly.",
                    "sid": 40,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then repeatedly re\u00ad sample each element of that vector conditioned on all other positions according to the distribu\u00ad tion Pr(aj la-j, e, f).",
                    "sid": 41,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a complete assign\u00ad ment of the alignment for all words except the cur\u00ad rent, computing the complete probability includ\u00ad ing transition, emission, and jump, is straightfor\u00ad ward.",
                    "sid": 42,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This estimate comes with a computational cost: we must cycle through all positions of the vector repeatedly to gather a good estimate.",
                    "sid": 43,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In practice, a small number of samples will suffice.",
                    "sid": 44,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.2 MAP inference with dual decomposition.",
                    "sid": 45,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Dual decomposition, also known as Lagrangian relaxation, is a method for solving complex combinatorial optimization problems (Rush and Collins, 2012).",
                    "sid": 46,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These complex problems are sepa\u00ad rated into distinct components with tractable MAP inference procedures.",
                    "sid": 47,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The subproblems are re\u00ad peatedly solved with some communication over consistency until a consistent and globally optimal solution is found.",
                    "sid": 48,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here we are interested in the problem of find\u00ad i=l J (1) ing the most likely alignment of a sentence pair ITp(ajlaj-t) p(/j leai) j=l where cPi = L,f=l 8(i, aj) indicates the number of times that state j is visited.",
                    "sid": 49,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This deficient model wastes some probability mass on inconsistent con\u00ad figurations where the number of times that a state iis visited does not match its fertility cPi\u00b7 Follow\u00ad ing in the footsteps of older, richer, and wiser col\u00ad leagues (Brown et al., 1993),we forge ahead un\u00ad concerned by this complication.",
                    "sid": 50,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.1 Parameter estimation.",
                    "sid": 51,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Of greater concern is the exponential complex\u00ad ity of inference in this model.",
                    "sid": 52,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the standard HMM, there is a dynamic programming algorithm to compute the posterior probability over word alignments Pr(ale, f).",
                    "sid": 53,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These are the sufficient statistics gathered in the E step of EM.",
                    "sid": 54,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The structure of the fertility model violates the Markov assumptions used in this dynamic pro\u00ad gramming method.",
                    "sid": 55,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, we may empirically e, f. Thus, we need to solve the combinatorial op\u00ad timization problem arg maxa Pr(f, ale).",
                    "sid": 56,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let us rewrite the objective function as follows: h(a) t,(logp(<P.Ie;) +;;logp ;le;)) +t,(logp(a;l\u2022;-d + logp(;;je.,)) Because f is fixed, the p( J I I) term is constant and may be omitted.",
                    "sid": 57,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note how we've split the opti\u00ad mization into two portions.",
                    "sid": 58,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first captures fer\u00ad tility as well as some component of the translation distribution, and the second captures the jump dis\u00ad tribution and the remainder of the translation dis\u00ad tribution.",
                    "sid": 59,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our dual decomposition method follows this segmentation.",
                    "sid": 60,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Define Ya as Ya (i, j) = 1 if aj = i, and 0 otherwise.",
                    "sid": 61,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let z E {0, 1}/xJ be a binary u(o)(i,j) := 0 'ViE l..J,j E l..J fork=1toK a(k) := argmaxa f(a)+I;.,,J. u(k-l)(i,j)ya(i,j)) z(k) := argmaxz g(z)- I;.,,J. u(k-l)(i,j)z(i, j)) if Ya = Z return a(k) end if u(k)(i,j) := u(k)(i,j) + lik (Ya<\u2022l (i,j)- z(k)(i,j)) end for return a(K) Figure 1: The dual decomposition algorithm for the fertility HMM, where 6k is the step size at the kth iteration for 1kK, and K is the max number of iterations.",
                    "sid": 62,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "matrix.",
                    "sid": 63,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Define the functions f and g as /(a) t,(logp(a;la;-1) + logp(/; !e.;)) g(z) L ( logp(\u00a2 (zi)lei) + i=l z(i,j) ) L.J 2logp(filei) j=l Then we want to find argmaxf(a) + g(z) a,z subject to the constraints Ya(i,j) = z(i,j)Vi,j. Note how this recovers the original objective func\u00ad tion when matching variables are found.",
                    "sid": 64,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the dual decomposition algorithm from Rush and Collins (2012), reproduced here in Figure 1.",
                    "sid": 65,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note how the langrangian adds one additional term word, scaled by a value indicating whether that word is aligned in the current position.",
                    "sid": 66,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Because it is only added for those words that are aligned, we can merge this with the log p(fi I eai) terms in both f and g. Therefore, we can solve argmax8 (!(a)+ i,j u<k-l)(i,j)ya(i,j)) us\u00ad ing the standard Viterbi algorithm.",
                    "sid": 67,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The g function, on the other hand, does not have a commonly used decomposition structure.",
                    "sid": 68,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Luck\u00ad ily we can factor this maximization into pieces that allow for efficient computation.",
                    "sid": 69,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that g sums over arbitrary binary matrices.",
                    "sid": 70,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unlike the HMM, where each French word must have exactly one English generator, this maximization allows each z(i,j) := 0 \\:l(i,j) E [l..J] X [l..J] v :=0 fori=1toJ forj=1toJ x(j) := (logpCJile;) ,j) end for sort x in descending order by first component max:= logp(\u00a2> =Ole;), arg := 0, sum:= 0 forf=ltoJ sum:= sum+ x[f, 1] if sum+ logp(\u00a2> =fie;) >max max:= sum+ logp(\u00a2> =fie;) arg := f end if end for v := v+max for f = 1 to arg z(i, x[f, 2]) := 1 end for end for retumz,v Figure 2: Algorithm for finding the arg max and max of g, the fertility-related component of the dual decomposition objective.",
                    "sid": 71,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "French word to have zero or many generators.",
                    "sid": 72,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Be\u00ad cause assignments that are in accordance between this model and the HMM will meet the HMM's constraints, the overall dual decomposition algo\u00ad rithm will return valid assignments, even though individual selections for this model may fail to meet the requirements.",
                    "sid": 73,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As the scoring function g can be decomposed into a sum of scores for each row9i (i.e., there are no interactions between distinct rows of the matrix) we can maximize each row independently: I I m:X L9i(zi) = Lm:Xgi(zi) i=l i=l Within each row, we seek the best of all 2J pos\u00ad sible configurations.",
                    "sid": 74,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These configurations may be grouped into equivalence classes based on the number of nonzero entries.",
                    "sid": 75,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In each class, the max assignment is the one using words with the highest log probabilities; the total score of this as\u00ad signment is the sum those log probabilities and the log probability of that fertility.",
                    "sid": 76,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sorting the scores of each cell in the row in descending or\u00ad der by log probability allows for linear time com\u00ad putation of the max for each row.",
                    "sid": 77,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The algorithm described in Figure 2 finds this maximal assign\u00ad ment in O(J Jlog J) time, generally faster than the 0(!2 J) time used by Viterbi.",
                    "sid": 78,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We note in passing that this maximizer is pick\u00ading from an unconstrained set of binary matri ces.",
                    "sid": 79,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since each English word may generate as many French words as it likes, regardless of all other words in the sentence, the underlying ma\u00ad trix have many more or many fewer nonzero en\u00ad tries than there are French words.",
                    "sid": 80,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A straightfor\u00ad ward extension to the algorithm of Figure 2 returns only z matrices with exactly J nonzero entries.",
                    "sid": 81,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Rather than maximizing each row totally indepen\u00ad dently, we keep track of the best configurations for each number of words generated in each row, and then pick the best combination that sums to J: another straightforward exercise in dynamic pro\u00ad gramming.",
                    "sid": 82,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This refinement does not change the correctness of the dual decomposition algorithm; rather it speeds the convergence.",
                    "sid": 83,
                    "ssid": 54,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "fertility distribution parameters. ",
            "number": "3",
            "sents": [
                {
                    "text": "Original IBM models used a categorical distribu\u00ad tion of fertility, one such distribution for each En\u00ad glish word.",
                    "sid": 84,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This gives EM a great amount of free\u00ad dom in parameter estimation, with no smoothing or parameter tying of even rare words.",
                    "sid": 85,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Prior work addressed this by using the single parameter Pois\u00ad son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).",
                    "sid": 86,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We explore instead a feature-rich approach to address this issue.",
                    "sid": 87,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Prior work has explored feature-rich approaches to modeling the transla\u00ad tion distribution (Berg-Kirkpatrick et al., 2010); we use the same technique, but only for the fertil\u00ad ity model.",
                    "sid": 88,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The fertility distribution is modeled as a log-linear distribution ofF, a binary feature set: p(\u00a21e) ex exp (0 \u00b7 F(e, \u00a2)).",
                    "sid": 89,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We include a simple set of features: \u2022 A binary indicator for each fertility \u00a2.",
                    "sid": 90,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This feature is present for all words, acting as smoothing.",
                    "sid": 91,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 A binary indicator for each word id and fer\u00ad tility, if the word occurs more than 10 times.",
                    "sid": 92,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 A binary indicator for each word length (in letters) and fertility.",
                    "sid": 93,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 A binary indicator for each four letter word prefix and fertility.",
                    "sid": 94,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Together these produce a distribution that can learn a reasonable distribution not only for com\u00ad mon words, but also for rare words.",
                    "sid": 95,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Including word length information aids in for languages with Algorithm AE R(G +E ) AE R(E +G ) H M M FH M M Vit erbi FH M M Dual dec 2 4 . 0 1 9 . 7 1 8 . 0 2 1 . 8 1 9 . 6 1 7 . 4 Table 1: Experimental results over the 120 evalu\u00ad ation sentences.",
                    "sid": 96,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Alignment error rates in both di\u00ad rections are provided here.",
                    "sid": 97,
                    "ssid": 14,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "evaluation. ",
            "number": "4",
            "sents": [
                {
                    "text": "We explore the impact of this improved MAP in\u00ad ference procedure on a task in GermanEnglish word alignment.",
                    "sid": 98,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For training data we use the news commentary data from the WMT 2012 translation task.1 120 of the training sentences were manually annotated with word alignments.",
                    "sid": 99,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results in Table 1 compare several differ\u00ad ent algorithms on this same data.",
                    "sid": 100,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first line is a baseline HMM using exact posterior computa\u00ad tion and inference with the standard dynamic pro\u00ad gramming algorithms.",
                    "sid": 101,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The next line shows the fer\u00ad tility HMM with approximate posterior computa\u00ad tion from Gibbs sampling but with final alignment selected by the Viterbi algorithm.",
                    "sid": 102,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Clearly fertil\u00ad ity modeling is improving alignment quality.",
                    "sid": 103,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).",
                    "sid": 104,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here, however, the difference be\u00ad tween a dual decomposition and Viterbi is signifi\u00ad cant: their results were likely due to search error.",
                    "sid": 105,
                    "ssid": 8,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusions and future work. ",
            "number": "5",
            "sents": [
                {
                    "text": "We have introduced a dual decomposition ap\u00ad proach to alignment inference that substantially reduces alignment error.",
                    "sid": 106,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unfortunately the algo\u00ad rithm is rather slow to converge: after 40 iterations of the dual decomposition, still only 55 percent of the test sentences have converged.",
                    "sid": 107,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We are ex\u00ad ploring improvements to the simple sub-gradient method applied here in hopes of finding faster con\u00ad vergence, fast enough to make this algorithm prac\u00ad tical.",
                    "sid": 108,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Alternate parameter estimation techniques appear promising given the improvements of dual decomposition over sampling.",
                    "sid": 109,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Once the perfor\u00ad mance issues of this algorithm are improved, ex\u00ad ploring hard EM or some variant thereof might lead to more substantial improvements.",
                    "sid": 110,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "compounding: long words in one language may correspond to multiple words in the other.",
                    "sid": 111,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1www.statmt.org/wmt12/trans1ation-task.html",
                    "sid": 112,
                    "ssid": 7,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}