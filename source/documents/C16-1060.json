{
    "ID": "C16-1060",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Current methods for word alignment require considerable amounts of parallel text to deliver accurate results, a requirement which is met only for a small minority of the world\u2019s approximately 7,000 languages.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We show that by jointly performing word alignment and annotation transfer in a novel Bayesian model, alignment accuracy can be improved for language pairs where annotations are available for only one of the languages\u2014a finding which could facilitate the study and processing of a vast number of low-resource languages.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also present an evaluation where our method is used to perform single-source and multi-source part-of-speech transfer with 22 translations of the same text in four different languages.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This allows us to quantify the considerable variation in accuracy depending on the specific source text(s) used, even with different translations into the same language.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Word alignment is the problem of identifying translationally equivalent words across the languages of a parallel text.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It has found widespread use for enabling applications such as statistical machine translation (Brown et al., 1993; Koehn et al., 2003), annotation transfer (Yarowsky et al., 2001), word sense disambiguation (Diab and Resnik, 2002) and lexicon extraction (Wu and Xia, 1994).",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although many types of algorithms have been explored, the main line of research through the last couple of decades has been based on the generative IBM models introduced by Brown et al.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1993).",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "What these models have in common is that they are unsupervised, asymmetric models, assuming one of the languages in a bitext (the source language) generates the corresponding text in the other language (the target language), word by word.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most often, a variant of the Expectation-Maximization algorithm (Dempster et al., 1977) has been used for inference in these models, but recently there has been some work using Bayesian alignment models using Gibbs sampling for inference (DeNero et al., 2008; Mermer and Sarac\u00b8lar, 2011; Gal and Blunsom, 2013).",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The incorporation of Bayesian priors into these models has been shown to improve accuracy, since they provide a flexible way of biasing the model towards empirical observations about language, most importantly that a given word type tends to have a very limited number of translations.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While the basic word alignment models use only lexical co-occurrence and word order, lexical data tends to be sparse and a number of authors have explored the usefulness of other information sources.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Toutanova et al.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2002) showed that Part of Speech (PoS) tags can be integrated into the IBM models to improve word alignment accuracy, and others have reported similar results for dependency (Cherry and Lin, 2003; Wang and Zong, 2013) and phrase-structure (Yamada and Knight, 2001) parse trees, and for lemmatized texts (Bojar and Prokopov, 2006).",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to the studies just mentioned that showed how various types of linguistic annotation can be used to guide word alignment, there has been research showing that the reverse also holds: word-aligned parallel texts can be used to transfer annotations and models from languages where those resources exist to languages where they do not.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pioneering work by Yarowsky et al.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2001) explored tasks such as PoS This work is licenced under a Creative Commons Attribution 4.0 International License.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "License details: http:// creativecommons.org/licenses/by/4.0/ 620 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 620\u2013629, Osaka, Japan, December 1117 2016.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "tagging, shallow parsing and lemmatization, which was followed by e.g. dependency parsing (Hwa et al., 2005).",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The present work combines these previous lines of work by exploring joint models of word alignment and annotation transfer (of PoS tags), within a Bayesian framework.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The source code of our implementation is available at http://www.ling.su.se/spacos.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "methods. ",
            "number": "2",
            "sents": [
                {
                    "text": "This section first discusses Bayesian word alignment using IBM-based models along with extensions to these, and finally describes our model of joint PoS transfer and word alignment.",
                    "sid": 22,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.1 Bayesian IBM models.",
                    "sid": 23,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The IBM 1 alignment model can be extended with sparse Dirichlet priors, and efficient inference is possible using Gibbs sampling (Mermer and Sarac\u00b8lar, 2011; Mermer et al., 2013; Gal and Blunsom, 2013) or Variational Bayesian techniques (Riley and Gildea, 2012).",
                    "sid": 24,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "IBM model 1 assumes each target word tj = f of a sentence is generated by one source word saj = e through the alignment variable aj , and that all words are generated independently and do not depend on the sentence positions i and j. The probability of a target sentence t (of length J ) and an alignment a given a source sentence s (of length I ) then becomes J P (t, a|s) \u221d p(J |I ) TI P (tj |sa ) (1) j=1 One drawback of this model (apart from the extreme independence assumptions addressed by laterIBM models) is that there is no penalty for having very flat distributions P (f |e) for target words con ditioned on a source word, a fact that causes the so-called garbage collection effect where rare source words are incorrectly linked to a large number of target words.",
                    "sid": 25,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By using priors on the translation distributions that discourage such solutions, it is possible to improve alignment accuracy.",
                    "sid": 26,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Mermer and Sarac\u00b8lar (2011) introduced the use of Dirichlet priors for this task.",
                    "sid": 27,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the Dirichlet parameter \u03b1 is 1, this reduces to the uniform distribution, but it turns out that by using much smaller values of \u03b1, below about 10\u22122 (Riley and Gildea, 2012, Figure 1), the model better reflects the empirical observation that words tend to have very few possible translations.",
                    "sid": 28,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.2 Inference.",
                    "sid": 29,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the standard IBM models, the EM algorithm is normally used for inference.",
                    "sid": 30,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the Bayesian version with Dirichlet priors, we mentioned above that two main options have been investigated: Variational Bayes and Gibbs sampling.",
                    "sid": 31,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While both methods have been shown to improve word alignment accuracy for IBM model 1, the computational complexity of Gibbs sampling is lower with more complex models (O\u00a8 stling and Tiedemann, 2016, Section 3.2).",
                    "sid": 32,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this reason, Gibbs sampling is used in the present work and will be discussed in further detail.",
                    "sid": 33,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Gibbs sampling (Gelfand and Smith, 1991) is a specific instance of the more general Markov Chain Monte Carlo algorithm, which is used to draw samples from a model M which defines a probability function pM (x) over the variable space x. This is done by constructing a Markov chain with pM as its stationary distribution and performing a sufficiently long random walk in it.",
                    "sid": 34,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A Gibbs sampler achieves this by specifying for each variable xi in x a sampling distribution P (xi = a|x\u2212i) for xi conditionedon x\u2212i, which denotes all variables in x except xi.",
                    "sid": 35,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For IBM model 1, this gives the following sam pling equation, which we also use, and for more complex models extend with additional factors given Equation (4): P (aj = i) = na\u2212j ,si ,tj + \u03b1tj (2) Here, na f ( n a \u2212 j , s i , f + \u03b1 f ) ,e,f is a count vector representing the number of times each source type e is aligned to each target type f under the alignment a, not counting the alignment at position j. In the end, we are interested in computing the expectations E [\u03b4aj ,i] under the alignment model, where \u03b4 is the Kronecker delta.",
                    "sid": 36,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a series of samples of a(t) for t \u2208 1 . . .",
                    "sid": 37,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "T , we approximate this using 1 E [\u03b4aj ,i] \u2248 T T t=1 P (aj = i|a(t) , s, t) (3) The initial alignments a(0) are sampled from a uniform distribution, and in order to reduce initialization bias we average the marginals from eight independently initialized samplers.",
                    "sid": 38,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For details on the tradeoffs involved in choosing the number of samplers and sampling iterations, we refer to Table 2 of O\u00a8 stling and Tiedemann (2016).",
                    "sid": 39,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.3 Word order and fertility.",
                    "sid": 40,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Even with good prior parameters, IBM model 1 is a poor model of word alignment because it ignores two important characteristics of parallel text: word order and morpheme counts.",
                    "sid": 41,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While different distortion models have been used to model word order, we use the HMM-based model of Vogel et al.",
                    "sid": 42,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1996), which has been demonstrated to deliver better performance than either no distortion model (like IBM model 1) or models based on absolute sentence positions (IBM models 2 and 3).",
                    "sid": 43,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This introduces a distribution P (aj \u2212 aj\u22121|I ) of the \u201cjump\u201d aj \u2212 aj\u22121 in the source sentence when moving one step in the target sentence.",
                    "sid": 44,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a way of modeling the relative number of morphemes in a word for a pair of languages, the fertility of a source word e is defined as the number of target words it is aligned to in a particular context.",
                    "sid": 45,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is modeled using a distribution P (\u03c6i = k|si = e), where the fertility \u03c6i at position i is conditioned on the word e at that position.",
                    "sid": 46,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is particularly important when the languages have large differences in word formation strategies and the general level of morphological complexity.",
                    "sid": 47,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.",
                    "sid": 48,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An important conclusion from their work is that a simple HMM with fertility model is competitive with the more complex IBM model 4, and we follow them in using this model as our baseline.",
                    "sid": 49,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our full baseline model is given by P s, t, a,\u03b8, \u03c8, \u03c0, \u03b1, \u03b2, \u03b3) \uf8eb K J (k) \uf8f6 \uf8eb E F \uf8f6 TI TI \u221d \uf8ed k=1 j=1 \u03b8s(k) a(k) j ,t(k) \uf8f8 \u00b7 \uf8ed TI TI e=1 f =1 \u03b1f \u22121 e,f \uf8f8 \uf8eb K J (k) +1 TI TI \uf8f6 \uf8eb Imax TI mmax TI \uf8f6 \u03b2I ,m \u22121 (4) \u00b7 \uf8ed \u03c8a(k) (k) \uf8f8 \u00b7 \uf8ed \u03c8m \uf8f8 k=1 j=1 j \u2212aj\u22121 I =Imin m=mmin \uf8eb K I (k) \uf8f6 E nmax \\ TI TI \u00b7 \uf8ed k=1 i=1 i ,\u03c6i \uf8f8 TI TI e=1 n=0 \u03b3n \u22121 e,n where K is the number of parallel sentences, \u03b8 \u223c Dir \u03b1) are the lexical translation parameters, \u03c8 \u223c Dir \u03b2) are the categorical distribution parameters for the word order model P aj \u2212 aj\u22121 = m|I ), and \u03c0e \u223c Dir \u03b3) for the fertility model P (\u03c6i = k|si = e).",
                    "sid": 50,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.4 PoS-guided word alignment.",
                    "sid": 51,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This work follows Toutanova et al.",
                    "sid": 52,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2002) in adding another factor to the model, akin to the lexical translation probability P (f |e) but using the PoS tags of the respective words, P (T t |T s).",
                    "sid": 53,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The main f e difference is that in their work PoS tags for both source and target languages were assumed, whereas here only one of the languages is assumed to be PoS-annotated.",
                    "sid": 54,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the other language, the PoS tags are sampled using the method described below.",
                    "sid": 55,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Algorithm 1 Alternating alignment-annotation.",
                    "sid": 56,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "[> Align a single sentence pair s, t. The extension to multiple sentences is straightforward.",
                    "sid": 57,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "function AAA(s, t) [> initialize alignments using the baseline HMM + fertility model [> the forward direction uses alignment vector a a \u2190 Baseline(s, t) [> the backward direction uses alignment vector b b \u2190 Baseline(t, s) while sampling do M \u2190 estimate bigram HMM model using a, b, s, t, T s [> set the target sentence tags T t using the Viterbi algorithm T t \u2190 arg maxT P (T |M ) [> sample alignment variables a, b for all j \u2190 1 . . .",
                    "sid": 58,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "J do aj \u223c P (aj = i|a\u2212j , s, t, T end for for all i \u2190 1 . . .",
                    "sid": 59,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "I do s, T t) bi \u223c P (bi = j|b\u2212i, s, t, T end for end while s, T t) [> return expected values for PoS tags alignments, as in Equation (3) return E [a], E [b], E [T t] end function 2.5 PoS transfer.",
                    "sid": 60,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We focus on applying PoS transfer as a way of obtaining better word alignment accuracy, rather than improving PoS transfer as such.",
                    "sid": 61,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These are largely complementary goals, as our evaluation in Section 3 shows that small changes in PoS tagging accuracy do not seem to influence alignment accuracy.",
                    "sid": 62,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this reason, and because of our focus on low-resource languages precludes using data-intensive approaches like that of Das and Petrov (2011), we choose the simple method of Yarowsky and Ngai (2001) as a starting point for the PoS transfer part of our model.",
                    "sid": 63,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The basis of this method is to use heuristics to estimate a robust first-order HMM tagger from (noisy) projected tags, and to re-tag the data using this tagger.",
                    "sid": 64,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore we extended the tagger using the affix-tree method of Schmid (1994) for rare words, in order to be able to handle morphologically complex languages better.",
                    "sid": 65,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While it would have been preferable for reasons of theoretical elegance to use a simpler PoS transfer model, matching the alignment model, such attempts by O\u00a8 stling et al.",
                    "sid": 66,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2015) resulted in very modest improvements for their sign language data set, and their model gave no improvement at all for our data sets.",
                    "sid": 67,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.6 Alternating alignment-annotation (AAA).",
                    "sid": 68,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Algorithm 1 summarizes our method, which can be viewed as a modified Gibbs sampler of the latent alignment variables a and b (in the forward and backward alignment direction) as well as the target-side PoS tags T t. While the PoS transfer part is not stochastic,1 it operates on samples of the alignment variables a and b and can be seamlessly integrated into the sampler.",
                    "sid": 69,
                    "ssid": 48,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "evaluation. ",
            "number": "3",
            "sents": [
                {
                    "text": "The empirical evaluation aims at investigating whether the alternating alignment-annotation (AAA) algorithm improves word alignment and/or PoS transfer accuracy, compared to the corresponding PoS 1 We also tried sampling T t using marginal distributions computed by the forward-backward algorithm, but found no effect on the accuracy of the algorithm.",
                    "sid": 70,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Corpus Sentences |S| |P | E ng lish Fr en ch 1 13 0 5 8 8 4 03 8 17 43 8 Ro m an ian En gli sh 4 8 6 4 1 5 03 4 5 0 3 4 E ng lish In uk tit ut 3 3 3 1 8 5 2 9 3 1 9 7 2 E ng lish Hi nd i 3 5 5 6 1 40 9 1 4 0 9 E ng lish S w ed is h 6 9 2 6 6 2 3 34 0 4 5 7 7 Table 1: Total corpus sizes (in sentences) and number of (S)ure and (P)ossible alignment links in their respective evaluation sets.",
                    "sid": 71,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "unaware Bayesian IBM model with an HMM word order model and fertility.",
                    "sid": 72,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.1 Data.",
                    "sid": 73,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to assess the general usefulness of the method presented, a number of parallel corpora representing a diverse set of languages and genres are used: the English-French Hansards corpus in the version presented by Mihalcea and Pedersen (2003), the RomanianEnglish, EnglishInuktitut and EnglishHindi corpora from Martin et al.",
                    "sid": 74,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2005), as well as parts of the SwedishEnglish Europarl corpus (Koehn, 2005) with the evaluation set of Holmqvist and Ahrenberg (2011).",
                    "sid": 75,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, a set of translations of the New Testament will be used to investigate the quality of the transfered PoS tags.",
                    "sid": 76,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some properties of these corpora are summarized in Table 1.",
                    "sid": 77,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Silver-standard PoS annotations were provided for English, French and German by the Stanford Tag- ger (Toutanova et al., 2003) and for Swedish by Stagger (O\u00a8 stling, 2013).",
                    "sid": 78,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The native tagsets were mapped to the Universal PoS Tagset of Petrov et al.",
                    "sid": 79,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2012).",
                    "sid": 80,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2 Experimental setup.",
                    "sid": 81,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The main intended use case is a fairly short parallel text, with two very different languages of which only one has an accurate PoS tagger available.",
                    "sid": 82,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This excludes the possibility of extensive per-language tuning (unlike some of the previous results cited), and in this evaluation the different language pairs use identical parameters to the largest possible extent.2 We fixed the hyperparameters in Equation (4) to \u03b1 = 10\u22125, \u03b2 = \u03b3 = 1.",
                    "sid": 83,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The experiments use eight individually initialized samplers, each of which used a pipelined approach where initially a lexical-only model equivalent to that of Mermer and Sarac\u00b8lar (2011) was used, then a word order term using the HMM model was added, then the fertility term, and finally (when applicable) the PoS translation probability.",
                    "sid": 84,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "No burn-in period was used during sampling, since the initial value of the last pipeline step is already quite good.",
                    "sid": 85,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the model is asymmetric, the alignments are run in both directions and symmetrized.",
                    "sid": 86,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A soft variant of the intersection heuristic is used, where the final set of links L is defined as L = {(i, j) | P (aj = i)P (bi = j) > t}.",
                    "sid": 87,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "for a threshold value t, in these evaluations fixed to 0.25.",
                    "sid": 88,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This gives a fairly conservative set of links, favoring precision before recall.",
                    "sid": 89,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note however that the model does not use NULL words, so this conservatism is not as severe as in models with NULL words.3 Heuristics based on the union on the contrary tend to over-generate links under these conditions.",
                    "sid": 90,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.3 Results.",
                    "sid": 91,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The systems used as baselines in the evaluation are mainly from the Workshop on Parallel Text shared tasks (Mihalcea and Pedersen, 2003; Martin et al., 2005), where most of the data sets used were intro 2 The main exception is that some of the language pairs (RomanianEnglish and EnglishHindi), following previous work, use a poor man\u2019s stemming trick where only the first four letters of each token is used.",
                    "sid": 92,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The only other exception is that the English-French evaluation did not use the fertility parameter, since it showed no further improvement beyond the plain HMM model.",
                    "sid": 93,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3 Later experiments with a related model (O\u00a8 stling and Tiedemann, 2016, compare their Table 2 with our Table 2) show that for some language pairs in particular, using NULL with standard symmetrization heuristics gives considerably worse AER scores.",
                    "sid": 94,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "duced: ISI2 (Fraser and Marcu, 2005), JHU (Schafer and Dra\u00b4bek, 2005), UMIACS2 (Lopez and Resnik, 2005) and XRCE (Dejean et al., 2003).",
                    "sid": 95,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The SwedishEnglish figures, LIU, are from Holmqvist and Ahrenberg (2011).",
                    "sid": 96,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we have run GIZA++ (Och and Ney, 2003) on the corpora as an additional baseline.4 Results from previous work using more data than a bitext plus PoS tags for one of the languages are not included, although some such systems have obtained better results on some of the corpora used, using e.g. semi-supervised discriminative training (Liu et al., 2010).",
                    "sid": 97,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 summarizes the main results of the evaluation.",
                    "sid": 98,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In all cases, the alternating alignment- annotation method surpasses the baseline model that does not use PoS tags.",
                    "sid": 99,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The model is generally competitive compared to previous work, in particular for the smaller corpora and where the languages are substantially different.",
                    "sid": 100,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The improvement compared to the non-Bayesian baselines is particularly good for the EnglishInuktitut corpus, which could be due to the fact that the morpheme/word ratio of Inuktitut is very high, resulting in very many low-frequency words that tend to function as garbage collectors in non-Bayesian models.",
                    "sid": 101,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The situation is similar for the EnglishHindi corpus, although in this case the cause for the many rare words is rather the short bitext than the languages themselves.",
                    "sid": 102,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is interesting to compare the corresponding AAA and Supervised figures in Table 2, where the only difference is that AAA uses a supervised PoS tag on one language (English) and annotation transfer to the other, whereas Supervised uses supervised PoS tags on both languages.",
                    "sid": 103,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The overall accuracy figures are nearly identical, even though the accuracy of the transfered tags is lower.",
                    "sid": 104,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This indicates that the word alignment algorithm is not very sensitive to PoS tagging accuracy, so that the relatively simple PoS transfer method used is sufficient for the purpose of increasing word alignment accuracy.",
                    "sid": 105,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are multiple translations of the New Testament into each of English, French, German and Swedish, which can be exploited for multi-source transfer.",
                    "sid": 106,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our model, multi-source transfer can be done trivially by averaging the expectations returned by Algorithm 1.",
                    "sid": 107,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 3 shows that this overall has a large positive effect on PoS accuracy, with an average error reduction of one fourth compared to the median single-source result, and one tenth compared to the best (out of 22) single-source result.",
                    "sid": 108,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using many translations in each language allows us to see how widely the accuracy varies, even when using the same source (or target) language.",
                    "sid": 109,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is due to many factors, including the large time span (hundreds of years) between the different translations.",
                    "sid": 110,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast, the multi-source results are, as could be expected, much more robust.",
                    "sid": 111,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is an encouraging result, given that the New Testament is perhaps the most widely translated text of significant length, and offers a great possibility to transfer linguistic annotations to languages where little other data is available.",
                    "sid": 112,
                    "ssid": 43,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusions and future work. ",
            "number": "4",
            "sents": [
                {
                    "text": "We have presented a model for joint word alignment and PoS annotation transfer, and shown empirically that it leads to improved word alignment accuracy, in particular for low-resource languages.",
                    "sid": 113,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using automatically transfered PoS tags led to improvements that were as big as the improvements seen when using PoS tags from supervised taggers on both sides of a bitext.",
                    "sid": 114,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, we took the opportunity to perform an evaluation investigating what kind of variation can be expected depending on which translation(s) are used as source texts in PoS annotation transfer, and found that this variation can be great, even among translations into the same language.",
                    "sid": 115,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using multi- source transfer reduces this variation considerably and typically gives better accuracy than even the best single-source transfer among many.",
                    "sid": 116,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this study, only PoS annotations were considered, but there are other types of annotation such as parse trees, named entities and word senses which potentially could be transfered jointly with word alignment.",
                    "sid": 117,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is left to future work, as are improvements to the baseline alignment model.",
                    "sid": 118,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments",
            "number": "",
            "sents": [
                {
                    "text": "Thanks to the anonymous reviewers, Mats Wire\u00b4n, Jo\u00a8 rg Tiedemann and Joakim Nivre for advice.",
                    "sid": 119,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4 In order to provide a competitive but fair baseline, the same general approach was used with GIZA++ as with the new system presented, using default parameters and no language-specific tuning.",
                    "sid": 120,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The specific alignment pipeline used was 13 h5 33 410 , and the symmetrization that provides the best alignment on the test set is chosen.",
                    "sid": 121,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This gives GIZA++ some advantage, but ensures that any claimed improvements by our algorithm over GIZA++ are not simply due to symmetrization.",
                    "sid": 122,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Model |A| |A \u2229 S| |A \u2229 P | P R F AER English-French (|S| = 4038, |P | = 17438.",
                    "sid": 123,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 130 588 sentences) B as eli ne 53 59 37 17 5 1 3 4 9 5 . 8 92 .1 93 .9 5 . 8 A A A 55 05 37 51 5 2 5 4 9 5 . 4 92 .9 94 .1 5 . 6 Su pe rvi se d 55 42 37 78 5 2 6 3 9 5 . 0 93 .6 94 .3 5 . 6 GI Z A ++ 48 31 35 31 4 7 1 5 9 7 . 6 87 .4 92 .2 7 . 0 X R C E 9 0 . 1 93 .8 91 .9 8 . 5 RomanianEnglish (|S| = |P | = 6201.",
                    "sid": 124,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "48 641 sentences) B as eli ne 33 74 30 70 3 0 7 0 9 1 . 0 61 .0 73 .0 27 .0 A A A 34 47 31 20 3 1 2 0 9 0 . 5 62 .0 73 .6 26 .4 GI Z A ++ 37 30 31 61 3 1 6 1 8 4 . 7 62 .8 72 .1 27 .9 IS I2 8 7 . 9 63 .1 73 .5 26 .6 R A C AI 7 6 . 8 71 .2 73 .9 26 .1 EnglishInuktitut (|S| = 293, |P | = 1972.",
                    "sid": 125,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "333 185 sentences) B as eli ne 5 9 8 2 6 7 5 5 9 9 3 . 5 91 .1 92 .3 7 . 3 A A A 6 3 0 2 7 3 5 9 5 9 4 . 4 93 .2 93 .8 6 . 0 GI Z A ++ 3 4 2 1 7 0 3 0 6 8 9 . 5 58 .0 70 .4 25 .0 J H U 9 6 . 7 76 .8 85 .6 9 . 5 J H U 8 4 . 4 92 .2 88 .1 14 .3 EnglishHindi (|S| = |P | = 1409.",
                    "sid": 126,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3 556 sentences) B as eli ne 7 1 2 6 0 6 6 0 6 8 5 . 1 43 .0 57 .1 42 .9 A A A 8 1 7 6 7 7 6 7 7 8 2 . 9 48 .0 60 .8 39 .2 GI Z A ++ 9 8 4 6 1 5 6 1 5 6 2 . 5 43 .6 51 .4 48 .6 U MI A C S 2 4 3 . 7 56 .1 49 .1 50 .9 EnglishSwedish (|S| = 3340, |P | = 4577.",
                    "sid": 127,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "692 662 sentences) B as eli ne 31 83 27 42 2 9 3 3 9 2 . 1 82 .1 86 .8 13 .0 A A A 31 25 27 74 2 9 6 1 9 4 . 8 83 .1 88 .5 11 .3 Su pe rvi se d 32 62 28 23 3 0 3 4 9 3 . 0 84 .5 88 .6 11 .3 GI Z A ++ 34 36 28 90 3 1 3 6 9 1 . 3 86 .5 88 .8 11 .1 LI U 8 5 . 3 \u2013 \u2013 12 .6 Table 2: Results from the empirical evaluation, including the Bayesian model without PoS tags (Baseline), the alternating alignment-annotation algorithm (AAA), the corresponding method but with supervised PoS taggers for both languages (Supervised), and comparable previous results on the same data.",
                    "sid": 128,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The number of alignment links |A|, of which |A \u2229 S| are considered (S)ure, and |A \u2229 P | (P)ossible, are reported.",
                    "sid": 129,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For convenience, precision (P ), recall (R), F1 score (F ) and Alignment Error Rate (AER) are also given.",
                    "sid": 130,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Target Source texts Multi deu (8) eng (5) fra (5) swe (4) All (22) de u1 d e u 2 d e u 3 d e u 4 d e u 5 d e u 6 d e u 7 d e u 8 e n g 1 e n g 2 e n g 3 e n g 4 e n g 5 f r a 1 f r a 2 f r a 3 f r a 4 f r a 5 s w e 1 s w e 2 s w e 3 s w e 4 74 .2 76.2 79.4 79 .2 81.8 84.2 80 .1 81.7 83.7 79 .5 81.4 84.3 80 .3 81.5 84.1 80 .2 82.9 83.5 80 .3 83.5 84.5 80 .5 83.1 83.5 80 .3 83.5 83.8 80 .5 83.2 83.8 80 .4 81.2 81.8 76 .3 77.5 79.4 81 .3 82.1 82.5 81 .8 82.3 82.7 79 .0 80.1 80.9 79 .3 80.8 81.4 79 .8 80.6 81.7 80 .6 81.1 82.4 80 .2 80.7 81.7 79 .4 81.3 81.9 79 .9 81.8 82.3 80 .0 81.4 82.3 80 .1 81.3 81.5 80 .7 80.9 81.1 79 .9 81.2 81.9 80 .0 81.1 81.2 80 .1 81.2 81.4 82 .8 84.0 85.4 80 .9 81.7 82.4 83 .4 85.4 86.4 82 .7 84.8 85.4 80 .1 81.1 81.4 79 .5 80.5 80.7 81 .1 81.9 82.0 81 .0 81.8 81.8 81 .3 81.6 82.2 80 .0 80.7 81.3 81 .1 81.2 81.9 81 .0 81.4 82.0 76 .2 77.0 77.8 80 .9 81.4 82.0 80 .6 81.0 81.6 80 .3 80.7 81.3 80 .7 81.0 81.8 80 .2 81.0 81.9 76 .9 77.9 79.4 81 .1 82.5 82.8 81 .8 82.7 83.3 75 .0 83.7 85.1 78 .3 83.5 85.2 77 .1 84.4 85.9 75 .3 83.2 85.4 76 .5 84.9 85.9 80 .7 85.0 86.0 76 .6 85.6 86.3 76 .3 84.8 85.7 76 .6 81.5 81.7 79 .8 85.8 86.2 80 .7 85.7 86.3 78 .8 85.8 86.5 80 .3 86.0 86.7 78 .0 83.8 84.5 77 .0 83.6 84.7 77 .6 84.3 85.1 77 .4 84.1 84.6 77 .2 84.0 84.8 75 .0 81.0 85.1 78 .3 80.7 85.2 77 .1 81.7 85.9 75 .3 81.6 85.4 76 .5 81.5 85.9 79 .4 81.0 86.0 76 .6 81.7 86.3 76 .3 81.6 85.7 74 .2 76.7 81.7 79 .2 81.8 86.2 80 .1 81.6 86.3 78 .8 81.3 86.5 80 .3 81.5 86.7 78 .0 82.5 84.5 77 .0 83.0 84.7 77 .6 82.7 85.1 77 .4 82.7 84.6 77 .2 82.9 84.8 80 .2 81.2 85.4 76 .3 78.3 82.4 81 .1 82.5 86.4 81 .8 82.7 85.4 8 6 . 8 8 5 . 8 8 8 . 2 8 7 . 3 8 6 . 8 8 5 . 4 8 6 . 4 8 6 . 5 8 3 . 8 8 5 . 5 8 5 . 4 8 5 . 1 8 4 . 7 8 5 . 8 8 6 . 0 8 5 . 3 8 5 . 3 8 6 . 1 9 0 . 3 8 5 . 7 9 0 . 6 9 0 . 7 A vg . 79 .6 81.6 82.9 80 .5 81.7 82.4 80 .2 80.9 81.5 77 .7 84.4 85.4 77 .9 81.5 85.3 8 6 . 5 Table 3: PoS transfer accuracy (in percent) using single-source (first five columns) and multi-source (rightmost column) transfer in the New Testament corpus.",
                    "sid": 131,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Rows are target texts and columns are source languages.",
                    "sid": 132,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each language (with number of translations), the worst/median/best results are given for the different translations.",
                    "sid": 133,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The All columns summarize the results over all the source texts from the preceding columns.",
                    "sid": 134,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, Multi is the result of multi-source transfer using the sums of tag marginal distributions.",
                    "sid": 135,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The best result on each row is bold-faced.",
                    "sid": 136,
                    "ssid": 24,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}