{
    "ID": "W01-0712",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "This paper reports on the LEARNING COMPUTATIONAL GRAMMARS (LCG) project, a postdoc network devoted to studying the application of machine learning techniques to grammars suit\u00adable for computational use.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We were in\u00adterested in a more systematic survey to understand the relevance of many fac\u00adtors to the success of learning, esp. the availability of annotated data, the kind of dependencies in the data, and the availability of knowledge bases (gram\u00admars).",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We focused on syntax, esp. noun phrase (NP) syntax.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "This paper reports on the still preliminary, but al\u00adready satisfying results of the LEARNING COM\u00adPUTATIONAL GRAMMARS (LCG) project, a postdoc network devoted to studying the application of machine learning techniques to grammars suit\u00adable for computational use.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The member insti\u00adtutes are listed with the authors and also included ISSCO at the University of Geneva.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We were im\u00adpressed by early experiments applying learning to natural language, but dissatis.ed with the con\u00adcentration on a few techniques from the very rich area of machine learning.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We were interested in University of Groningen, {nerbonne,konstant}@let.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "rug.nl, osborne@cogsci.ed.ac.uk u SRI Cambridge, anja.belz@cam.sri.com, Rob.Koe\u00adling@netdecisions.co.uk ttXRCE Grenoble, nicola.cancedda@xrce.xerox.com University of T\u00a8ubingen, Herve.Dejean@xrce.xerox.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "com, thollard@sfs.nphil.unituebingen.de o .University College Dublin, james.hammerton@ucd.ie University of Antwerp, erikt@uia.ua.ac.be a more systematic survey to understand the rele\u00advance of many factors to the success of learning, esp. the availability of annotated data, the kind of dependencies in the data, and the availability of knowledge bases (grammars).",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We focused on syntax, esp. noun phrase (NP) syntax from the beginning.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The industrial partner, Xerox, focused on more immediate applications (Cancedda and Samuelsson, 2000).",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The network was focused not only by its sci\u00adenti.c goal, the application and evaluation of machine-learning techniques as used to learn nat\u00adural language syntax, and by the subarea of syn\u00adtax chosen, NP syntax, but also by the use of shared training and test material, in this case ma\u00adterial drawn from the Penn Treebank.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we were curious about the possibility of combining different techniques, including those from statisti\u00adcal and symbolic machine learning.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The network members played an important role in the organi\u00adsation of three open workshops in which several external groups participated, sharing data and test materials.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "method. ",
            "number": "2",
            "sents": [
                {
                    "text": "This section starts with a description of the three tasks that we have worked on in the framework of this project.",
                    "sid": 15,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After this we will describe the ma\u00adchine learning algorithms applied to this data and conclude with some notes about combining dif\u00adferent system results.",
                    "sid": 16,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.1 Task descriptions.",
                    "sid": 17,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the framework of this project, we have worked on the following three tasks: 1.",
                    "sid": 18,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "base phrase (chunk) identi.cation 2.",
                    "sid": 19,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "base noun phrase recognition 3.",
                    "sid": 20,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": ".nding arbitrary noun phrases Text chunks are non-overlapping phrases which contain syntactically related words.",
                    "sid": 21,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the sentence: adwPak PadwP He ]reckons ]the current akoP account de.cit ]will narrow ] aPcPadwP to ]only \u00a31.8 billion ] aPcPadwP in ]September ].",
                    "sid": 22,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "contains eight chunks, four NP chunks, two VP chunks and two PP chunks.",
                    "sid": 23,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The latter only con\u00adtain prepositions rather than prepositions plus the noun phrase material because that has already been included in NP chunks.",
                    "sid": 24,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The process of .nding these phrases is called CHUNKING.",
                    "sid": 25,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The project provided a data set for this task at the CoNLL2000 workshop (Tjong Kim Sang and Buchholz, 2000)1.",
                    "sid": 26,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It consists of sections 1518 of the Wall Street Journal part of the Penn Treebank II (Marcus et al., 1993) as training data (211727 tokens) and section 20 as test data (47377 tokens).",
                    "sid": 27,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A specialised version of the chunking task is NP CHUNKING or baseNP identi.cation in which the goal is to identify the base noun phrases.",
                    "sid": 28,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The .rst work on this topic was done back in the eighties (Church, 1988).",
                    "sid": 29,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The data set that has become standard for evaluation machine learn\u00ading approaches is the one .rst used by Ramshaw and Marcus (1995).",
                    "sid": 30,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It consists of the same train\u00ading and test data segments of the Penn Treebank as the chunking task (respectively sections 1518 and section 20).",
                    "sid": 31,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, since the data sets have been generated with different software, the NP boundaries in the NP chunking data sets are slightly different from the NP boundaries in the general chunking data.",
                    "sid": 32,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Noun phrases are not restricted to the base lev\u00adels of parse trees.",
                    "sid": 33,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, in the sentence In early trading in Hong Kong Monday , gold was quoted at $ 366.50 an ounce ., the noun phrase adnP $ 366.50 an ounce ]contains two embedded adnPadwP noun phrases $ 366.50 ]and an ounce ].",
                    "sid": 34,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the NP BRACKETING task, the goal is to .nd all noun phrases in a sentence.",
                    "sid": 35,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Data sets for this task were de.ned for CoNLL992.",
                    "sid": 36,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The data con\u00adsist of the same segments of the Penn Treebank as 1Detailed information about chunking, the CoNLL\u00ad2000 shared task, is also available at http://lcg\u00adwww.uia.ac.be/conll2000/chunking/ 2Information about NP bracketing can be found at http://lcgwww.uia.ac.be/conll99/npb/ the previous two tasks (sections 1518) as train\u00ading material and section 20 as test material.",
                    "sid": 37,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This material was extracted directly from the Treebank and therefore the NP boundaries at base levels are different from those in the previous two tasks.",
                    "sid": 38,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the evaluation of all three tasks, the accu\u00adracy of the learners is measured with three rates.",
                    "sid": 39,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We compare the constituents postulated by the learners with those marked as correct by experts (gold standard).",
                    "sid": 40,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, the percentage of detected constituents that are correct (precision).",
                    "sid": 41,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, the percentage of correct constituents that are de\u00adtected (recall).",
                    "sid": 42,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "And third, a combination of pre\u00adcision and recall, the F/1 = rate which is equal to (2*precision*recall)/(precision+recall).",
                    "sid": 43,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.2 Machine Learning Techniques.",
                    "sid": 44,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This section introduces the ten learning meth\u00adods that have been applied by the project members to the three tasks: LSCGs, ALLiS, LSOMMBL, Maximum Entropy, Aleph, MDL-based DCG learners, Finite State Transducers, IB1IG, IGTREE and C5.0.",
                    "sid": 45,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Local Structural Context Grammars (LSCGs) (Belz, 2001) are situated between conventional probabilistic context-free produc\u00adtion rule grammars and DOP-Grammars (e.g., Bod and Scha (1997)).",
                    "sid": 46,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "LSCGs outperform the former because they do not share their inher\u00adent independence assumptions, and are more computationally ef.cient than the latter, because they incorporate only subsets of the context included in DOP-Grammars.",
                    "sid": 47,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Local Structural Context (LSC) is (partial) information about the immediate neighbourhood of a phrase in a parse.",
                    "sid": 48,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By conditioning bracketing probabilities on LSC, more .negrained probability distributions can be achieved, and parsing performance increased.",
                    "sid": 49,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given corpora of parsed text such as the WSJ, LSCGs are used in automatic grammar construc\u00adtion as follows.",
                    "sid": 50,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An LSCG is derived from the cor\u00adpus by extracting production rules from bracket\u00adings and annotating the rules with the type(s) of LSC to be incorporated in the LSCG (e.g. parent category information, depth of embedding, etc.).",
                    "sid": 51,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Rule probabilities are derived from rule frequen\u00adcies (currently by Maximum Likelihood Estima\u00adtion).",
                    "sid": 52,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In a separate optimisation step, the resulting LSCGs are optimised in terms of size and pars\u00ading performance for a given parsing task by an automatic method (currently a version of beam search) that searches the space of partitions of a grammar\u2019s set of nonterminals.",
                    "sid": 53,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The LSCG research efforts differ from other approaches reported in this paper in two respects.",
                    "sid": 54,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Firstly, no lexical information is used at any point, as the aim is to investigate the upper limit of pars\u00ading performance without lexicalisation.",
                    "sid": 55,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Secondly, grammars are optimised for parsing performance and size, the aim being to improve performance but not at the price of arbitrary increases in gram\u00admar complexity (hence the cost of parsing).",
                    "sid": 56,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The automatic optimisation of corpus-derived LSCGs is the subject of ongoing research and the results reported here for this method are therefore pre\u00adliminary.",
                    "sid": 57,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Theory Re.nement (ALLiS).",
                    "sid": 58,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ALLiS ((D\u00b4ejean, 2000b), (D\u00b4ejean, 2000c)) is a in\u00adductive rule-based system using a traditional general-to-speci.c approach (Mitchell, 1997).",
                    "sid": 59,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After generating a default classi.cation rule (equivalent to the n-gram model), ALLiS tries to re.ne it since the accuracy of these rules is usually not high enough.",
                    "sid": 60,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Re.nement is done by adding more premises (contextual elements).",
                    "sid": 61,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ALLiS uses data encoded in XML, and also learns rules in XML.",
                    "sid": 62,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From the perspective of the XML formalism, the initial rule can be viewed as a tree with only one leaf, and re.nement is done by adding adjacent leaves until the accuracy of the rule is high enough (a tuning threshold is used).",
                    "sid": 63,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These additional leaves correspond to more precise contextual elements.",
                    "sid": 64,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using the hierarchical structure of an XML document, re.nement begins with the highest available hierarchical level and goes down in the hierarchy (for example, starting at the chunk level and then word level).",
                    "sid": 65,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Adding new low level elements makes the rules more speci.c, increasing their accuracy but decreasing their coverage.",
                    "sid": 66,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After the learning is completed, the set of rules is transformed into a proper formalism used by a given parser.",
                    "sid": 67,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Labelled SOM and Memory Based Learn\u00ading (LSOMMBL) is a neurally inspired technique which incorporates a modi.ed self-organising map (SOM, also known as a \u2018Kohonen Map\u2019) in memory-based learning to select a subset of the training data for comparison with novel items.",
                    "sid": 68,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The SOM is trained with labelled inputs.",
                    "sid": 69,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Dur\u00ading training, each unit in the map acquires a la\u00adbel.",
                    "sid": 70,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When an input is presented, the node in the map with the highest activation (the \u2018winner\u2019) is identi.ed.",
                    "sid": 71,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the winner is unlabelled, then it ac\u00adquires the label from its input.",
                    "sid": 72,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Labelled units only respond to similarly labelled inputs.",
                    "sid": 73,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other\u00adwise training proceeds as with the normal SOM.",
                    "sid": 74,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When training ends, all inputs are presented to the SOM, and the winning units for the inputs are noted.",
                    "sid": 75,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Any unused units are then discarded.",
                    "sid": 76,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus each remaining unit in the SOM is associ\u00adated with the set of training inputs that are closest to it.",
                    "sid": 77,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is used in MBL as follows.",
                    "sid": 78,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The labelled SOM is trained with inputs labelled with the out\u00adput categories.",
                    "sid": 79,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When a novel item is presented, the winning unit for each category is found, the training items associated with the winning units are searched for the closest item to the novel item and the most frequent classi.cation of that item is used as the classi.cation for the novel item.",
                    "sid": 80,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Maximum Entropy When building a classi\u00ad.er, one must gather evidence for predicting the correct class of an item from its context.",
                    "sid": 81,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Maximum Entropy (MaxEnt) framework is espe\u00adcially suited for integrating evidence from var\u00adious information sources.",
                    "sid": 82,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Frequencies of evi\u00addence/class combinations (called features) are ex\u00adtracted from a sample corpus and considered to be properties of the classi.cation process.",
                    "sid": 83,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Attention is constrained to models with these properties.",
                    "sid": 84,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The MaxEnt principle now demands that among all the probability distributions that obey these constraints, the most uniform is chosen.",
                    "sid": 85,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During training, features are assigned weights in such a way that, given the MaxEnt principle, the train\u00ading data is matched as well as possible.",
                    "sid": 86,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During evaluation it is tested which features are active (i.e., a feature is active when the context meets the requirements given by the feature).",
                    "sid": 87,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For every class the weights of the active features are com\u00adbined and the best scoring class is chosen (Berger et al., 1996).",
                    "sid": 88,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the classi.er built here we use as evidence the surrounding words, their POS tags and baseNP tags predicted for the previous words.",
                    "sid": 89,
                    "ssid": 75,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A mixture of simple features (consisting of one of the mentioned information sources) and com\u00adplex features (combinations thereof) were used.",
                    "sid": 90,
                    "ssid": 76,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The left context never exceeded 3 words, the right context was maximally 2 words.",
                    "sid": 91,
                    "ssid": 77,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The model was calculated using existing software (Dehaspe, 1997).",
                    "sid": 92,
                    "ssid": 78,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Inductive Logic Programming (ILP) Aleph is an ILP machine learning system that searches for a hypothesis, given positive (and, if avail\u00adable, negative) data in the form of ground Prolog terms and background knowledge (prior knowl\u00adedge made available to the learning algorithm) in the form of Prolog predicates.",
                    "sid": 93,
                    "ssid": 79,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The system, then, constructs a set of hypothesis clauses that .t the data and background as well as possible.",
                    "sid": 94,
                    "ssid": 80,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to approach the problem of NP chunk\u00ading in this context of single-predicate learning, it was reformulated as a tagging task where each word was tagged as being \u2018inside\u2019 or \u2018outside\u2019 a baseNP (consecutive NPs were treated appropri\u00adately).",
                    "sid": 95,
                    "ssid": 81,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then, the target theory is a Prolog program that correctly predicts a word\u2019s tag given its con\u00adtext.",
                    "sid": 96,
                    "ssid": 82,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The context consisted of PoS tagged words and syntactically tagged words to the left and PoS tagged words to the right, so that the resulting tag\u00adger can be applied in the left-to-right pass over PoS-tagged text.",
                    "sid": 97,
                    "ssid": 83,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Minimum Description Length (MDL) Esti\u00admation using the minimum description length principle involves .nding a model which not only \u2018explains\u2019 the training material well, but also is compact.",
                    "sid": 98,
                    "ssid": 84,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The basic idea is to balance the gener\u00adality of a model (roughly speaking, the more com\u00adpact the model, the more general it is) with its spe\u00adcialisation to the training material.",
                    "sid": 99,
                    "ssid": 85,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have ap\u00adplied MDL to the task of learning broad-covering de.nite-clause grammars from either raw text, or else from parsed corpora (Osborne, 1999a).",
                    "sid": 100,
                    "ssid": 86,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pre\u00adliminary results have shown that learning using just raw text is worse than learning with parsed corpora, and that learning using both parsed cor\u00adpora and a compression-based prior is better than when learning using parsed corpora and a uniform prior.",
                    "sid": 101,
                    "ssid": 87,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, we have noted that our in\u00adstantiation of MDL does not capture dependen\u00adcies which exist either in the grammar or else in preferred parses.",
                    "sid": 102,
                    "ssid": 88,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ongoing work has focused on applying random .eld technology (maximum en\u00adtropy) to MDL-based grammar learning (see Os\u00adborne (2000a) for some of the issues involved).",
                    "sid": 103,
                    "ssid": 89,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finite State Transducers are built by inter\u00ad preting probabilistic automata as transducers.",
                    "sid": 104,
                    "ssid": 90,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use a probabilistic grammatical algorithm, the DDSM algorithm (Thollard, 2001), for learning automata that provide the probability of an item given the previous ones.",
                    "sid": 105,
                    "ssid": 91,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The items are described by bigrams of the format feature:class.",
                    "sid": 106,
                    "ssid": 92,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the re\u00adsulting automata we consider a transition labeled feature:class as the transducer transition that takes as input the .rst part (feature) of the bigram and outputs the second part (class).",
                    "sid": 107,
                    "ssid": 93,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By applying the Viterbi algorithm on such a model, we can .nd out the most probable set of class values given an input set of feature values.",
                    "sid": 108,
                    "ssid": 94,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As the DDSM algo\u00adrithm has a tuning parameter, it can provide many different automata.",
                    "sid": 109,
                    "ssid": 95,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We apply a majority vote over the propositions made by the so computed au\u00adtomata/transducers for obtaining the results men\u00adtioned in this paper.",
                    "sid": 110,
                    "ssid": 96,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Memory-based learning methods store all training data and classify test data items by giving them the classi.cation of the training data items which are most similar.",
                    "sid": 111,
                    "ssid": 97,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have used three differ\u00adent algorithms: the nearest neighbour algorithm IB1IG, which is part of the Timbl software pack\u00adage (Daelemans et al., 1999), the decision tree learner IGTREE, also from Timbl, and C5.0, a commercial version of the decision tree learner C4.5 (Quinlan, 1993).",
                    "sid": 112,
                    "ssid": 98,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They are classi.ers which means that they assign phrase classes such as I (inside a phrase), B (at the beginning of a phrase) and O (outside a phrase) to words.",
                    "sid": 113,
                    "ssid": 99,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to improve the classi.cation process we provide the systems with extra information about the words such as the previous n words, the next n words, their part-of-speech tags and chunk tags estimated by an earlier classi.cation process.",
                    "sid": 114,
                    "ssid": 100,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the de\u00adfault settings of the software except for the num\u00adber of examined nearest neighbourhood regions for IB1IG (k, default is 1) which we set to 3.",
                    "sid": 115,
                    "ssid": 101,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.3 Combination techniques.",
                    "sid": 116,
                    "ssid": 102,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When different systems are applied to the same problem, a clever combination of their results will outperform all of the individual results (Diette\u00adrich, 1997).",
                    "sid": 117,
                    "ssid": 103,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The reason for this is that the systems often make different errors and some of these er\u00adrors can be eliminated by examining the classi.\u00adcations of the others.",
                    "sid": 118,
                    "ssid": 104,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The most simple combina\u00adtion method is MAJORITY VOTING.",
                    "sid": 119,
                    "ssid": 105,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It examines the classi.cations of the test data item and for each item chooses the most frequently predicted classi.cation.",
                    "sid": 120,
                    "ssid": 106,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Despite its simplicity, majority vot\u00ading has found to be quite useful for boosting per\u00adformance on the tasks that we are interested in.",
                    "sid": 121,
                    "ssid": 107,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have applied majority voting and nine other combination methods to the output of the learning systems that were applied to the three tasks.",
                    "sid": 122,
                    "ssid": 108,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Nine combination methods were originally suggested by Van Halteren et al.",
                    "sid": 123,
                    "ssid": 109,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1998).",
                    "sid": 124,
                    "ssid": 110,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Five of them, including majority voting, are so-called voting methods.",
                    "sid": 125,
                    "ssid": 111,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Apart from majority voting, all assign weights to the predictions of the different systems based on their performance on non-used train\u00ading data, the tuning data.",
                    "sid": 126,
                    "ssid": 112,
                    "kind_of_tag": "s"
                },
                {
                    "text": "TOTPRECISION uses classi.er weights based on their accuracy.",
                    "sid": 127,
                    "ssid": 113,
                    "kind_of_tag": "s"
                },
                {
                    "text": "TAG\u00adPRECISION applies classi.cation weights based on the accuracy of the classi.er for that classi\u00ad.cation.",
                    "sid": 128,
                    "ssid": 114,
                    "kind_of_tag": "s"
                },
                {
                    "text": "PRECISION-RECALL uses classi.cation weights that combine the precision of the classi\u00ad.cation with the recall of the competitors.",
                    "sid": 129,
                    "ssid": 115,
                    "kind_of_tag": "s"
                },
                {
                    "text": "And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).",
                    "sid": 130,
                    "ssid": 116,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The remaining four combination methods are so-called STACKED CLASSIFIERS.",
                    "sid": 131,
                    "ssid": 117,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The idea is to make a classi.er process the output of the indi\u00advidual systems.",
                    "sid": 132,
                    "ssid": 118,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used the two memory-based learners IB1IG and IGTREE as stacked classi.ers.",
                    "sid": 133,
                    "ssid": 119,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Like Van Halteren et al.",
                    "sid": 134,
                    "ssid": 120,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1998), we evaluated two features combinations.",
                    "sid": 135,
                    "ssid": 121,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The .rst consisted of the predictions of the individual systems and the sec\u00adond of the predictions plus one feature that de\u00adscribed the data item.",
                    "sid": 136,
                    "ssid": 122,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used the feature that, according to the memory-based learning metrics, was most relevant to the tasks: the part-of-speech tag of the data item.",
                    "sid": 137,
                    "ssid": 123,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the course of this project we have evalu\u00adated another combination method: BEST-N MA\u00adJORITY VOTING (Tjong Kim Sang et al., 2000).",
                    "sid": 138,
                    "ssid": 124,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is similar to majority voting except that in\u00adstead of using the predictions of all systems, it uses only predictions from some of the systems for determining the most probable classi.cations.",
                    "sid": 139,
                    "ssid": 125,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have experienced that for different reasons some systems perform worse than others and in\u00adcluding their results in the majority vote decreases the combined performance.",
                    "sid": 140,
                    "ssid": 126,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore it is a good idea to evaluate majority voting on subsets of all systems rather than only on the combination of all systems.",
                    "sid": 141,
                    "ssid": 127,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Apart from standard majority voting, all com\u00adbination methods require extra data for measur\u00ading their performance which is required for de\u00adtermining their weights, the tuning data.",
                    "sid": 142,
                    "ssid": 128,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This data can be extracted from the training data or the training data can be processed in an n-fold cross-validation process after which the performance on the complete training data can be measured.",
                    "sid": 143,
                    "ssid": 129,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Al\u00adthough some work with individual systems in the project has been done with the goal of combining the results with other systems, tuning data is not always available for all results.",
                    "sid": 144,
                    "ssid": 130,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore it will not always be possible to apply all ten combina\u00adtion methods to the results.",
                    "sid": 145,
                    "ssid": 131,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In some cases we have to restrict ourselves to evaluating majority voting only.",
                    "sid": 146,
                    "ssid": 132,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "results. ",
            "number": "3",
            "sents": [
                {
                    "text": "This sections presents the results of the different systems applied to the three tasks which were cen\u00adtral to this this project: chunking, NP chunking and NP bracketing.",
                    "sid": 147,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.1 Chunking.",
                    "sid": 148,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Chunking was the shared task of CoNLL2000, the workshop on Computational Natural Lan\u00adguage Learning, held in Lisbon, Portugal in 2000 (Tjong Kim Sang and Buchholz, 2000).",
                    "sid": 149,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Six members of the project have performed this task.",
                    "sid": 150,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results of the six systems (precision, recall and F/1 =can be found in table 1.",
                    "sid": 151,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Belz (2001) used Local Structural Context Grammars for .nd\u00ading chunks.",
                    "sid": 152,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "D\u00b4ejean (2000a) applied the the\u00adory re.nement system ALLiS to the shared task data.",
                    "sid": 153,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Koeling (2000) evaluated a maximum en\u00adtropy learner while using different feature com\u00adbinations (ME).",
                    "sid": 154,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Osborne (2000b) used a maxi\u00admum entropy-based part-of-speech tagger for as\u00adsigning chunk tags to words (ME Tag).",
                    "sid": 155,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thollard (2001) identi.ed chunks with Finite State Trans\u00adducers generated by a probabilistic grammar algo\u00adrithm (FST).",
                    "sid": 156,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Tjong Kim Sang (2000b) tested dif\u00adferent con.gurations of combined memory-based learners (MBL).",
                    "sid": 157,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The FST and the LSCG results are lower than those of the other systems because they were obtained without using lexical informa\u00adTable 1: The chunking results for the six systems associated with the project (shared task CoNLL\u00ad2000).",
                    "sid": 158,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The baseline results have been obtained by selecting the most frequent chunk tag associ\u00adated with each part-of-speech tag.",
                    "sid": 159,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The best results at CoNLL2000 were obtained by Support Vector Machines.",
                    "sid": 160,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A majority vote of the six LCG sys\u00adtems does not perform much worse than this best result.",
                    "sid": 161,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A majority vote of the .ve best systems outperforms the best result slightly (5 %error re\u00adduction).",
                    "sid": 162,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "precision recall F/1 = MBL ALLiS ME ME Tag LSCG FST 94.04% 91.87% 92.08% 91.65% 87.97% 84.92% 91.00% 92.31% 91.86% 92.23% 88.17% 86.75% 92.50 92.09 91.97 91.94 88.07 85.82 combination 93.68% 92.98% 93.33 best baseline 93.45% 72.58% 93.51% 82.14% 93.48 77.07 precision recall F/, = MBL ME ALLiS IGTree C5.0 SOM 93.63% 93.20% 92.49% 92.28% 89.59% 89.29% 92.88% 93.00% 92.69% 91.65% 90.66% 89.73% 93.25 93.10 92.59 91.96 90.12 89.51 combination 93.78% 93.52% 93.65 best baseline 94.18% 78.20% 93.55% 81.87% 93.86 79.99 tion.",
                    "sid": 163,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The best result at the workshop was obtained with Support Vector Machines (Kudoh and Mat\u00adsumoto, 2000).",
                    "sid": 164,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Because there was no tuning data available for the systems, the only combination technique we could apply to the six project results was majority voting.",
                    "sid": 165,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We applied majority voting to the output of the six systems while using the same approach as Tjong Kim Sang (2000b): combining start and end positions of chunks separately and restoring the chunks from these results.",
                    "sid": 166,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The combined per\u00adformance (F/1 ==93.33) was close to the best re\u00adsult published at CoNLL2000 (93.48).",
                    "sid": 167,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2 NP chunking.",
                    "sid": 168,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The NP chunking task is the specialisation of the chunking task in which only base noun phrases need to be detected.",
                    "sid": 169,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Standard data sets for ma\u00adchine learning approaches to this task were put forward by Ramshaw and Marcus (1995).",
                    "sid": 170,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Six project members have applied a total of seven different systems to this task, most of them in the context of the combination paper Tjong Kim Sang et al.",
                    "sid": 171,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2000).",
                    "sid": 172,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Daelemans applied the de\u00adcision tree learner C5.0 to the task.",
                    "sid": 173,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "D\u00b4ejean used the theory re.nement system ALLiS for .nding Table 2: The NP chunking results for six sys\u00adtems associated with the project.",
                    "sid": 174,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The baseline results have been obtained by selecting the most frequent chunk tag associated with each part-of\u00adspeech tag.",
                    "sid": 175,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The best results for this task have been obtained with a combination of seven learn\u00aders, .ve of which were operated by project mem\u00adbers.",
                    "sid": 176,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The combination of these .ve performances is not far off these best results.",
                    "sid": 177,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "noun phrases in the data.",
                    "sid": 178,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hammerton (2001) pre\u00addicted NP chunks with the connectionist methods based on self-organising maps (SOM).",
                    "sid": 179,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Koeling detected noun phrases with a maximum entropy-based learner (ME).",
                    "sid": 180,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Konstantopoulos (2000) used Inductive Logic Programming (ILP) techniques for .nding NP chunks in unseen texts3.",
                    "sid": 181,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Tjong Kim Sang applied combinations of IB1IG systems (MBL) and combinations of IGTREE learners to this task.",
                    "sid": 182,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results of the six of the seven sys\u00adtems can be found in table 2.",
                    "sid": 183,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results of C5.0 and SOM are lower than the others because nei\u00adther of these systems used lexical information.",
                    "sid": 184,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all of the systems except SOM we had tun\u00ading data and an extra development data set avail\u00adable.",
                    "sid": 185,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We tested all ten combination methods on the development set and best-3 majority voting came out as the best (F/, == 93.30; it used the MBL, ME and ALLiS results).",
                    "sid": 186,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When we applied best-3 majority voting to the standard test set, we obtained F/1 == 93.65 which is close to the best result we know for this data set (F/, == 93.86) (Tjong Kim Sang et al., 2000).",
                    "sid": 187,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The latter result was obtained by a combination of seven learning systems, .ve of which were operated by members of this project.",
                    "sid": 188,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3Results are unavailable for the ILP approach.",
                    "sid": 189,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "precision recall F/, = MBL 90.00% 78.38% 83.79 LSCG 80.04% 80.25% 80.15 MDL 53.2% 68.7% 59.9 best 91.28% 76.06% 82.98 baseline 77.57% 59.85% 67.56 Table 3: The results for three systems associ\u00adated with the project for the NP bracketing task, the shared task at CoNLL99.",
                    "sid": 190,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The baseline re\u00adsults have been obtained by .nding NP chunks in the text with an algorithm which selects the most frequent chunk tag associated with each part-of\u00adspeech tag.",
                    "sid": 191,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The best results at CoNLL99 was obtained with a bottom-up memory-based learner.",
                    "sid": 192,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An improved version of that system (MBL) deliv\u00adered the best project result.",
                    "sid": 193,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The MDL results have been obtained on a different data set and therefore combination of the three systems was not feasible.",
                    "sid": 194,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The original Ramshaw and Marcus (1995) pub\u00adlication evaluated their NP chunker on two data sets, the second holding a larger amount of train\u00ading data (Penn Treebank sections 0221) while us\u00ading 00 as test data.",
                    "sid": 195,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Tjong Kim Sang (2000a) has applied a combination of memory-based learners to this data set and obtained F/, == 94.90, an im\u00adprovement on Ramshaw and Marcus\u2019s 93.3.",
                    "sid": 196,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.3 NP bracketing.",
                    "sid": 197,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finding arbitrary noun phrases was the shared task of CoNLL99, held in Bergen, Norway in 1999.",
                    "sid": 198,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Three project members have performed this task.",
                    "sid": 199,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Belz (2001) extracted noun phrases with Local Structural Context Grammars, a variant of Data-Oriented Parsing (LSCG).",
                    "sid": 200,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Osborne (1999b) used a De.nite Clause Grammar learner based on Minimum Description Length for .nding noun phrases in samples of Penn Treebank material (MDL).",
                    "sid": 201,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Tjong Kim Sang (2000a) detected noun phrases with a bottom-up cascade of combina\u00adtions of memory-based classi.ers (MBL).",
                    "sid": 202,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The performance of the three systems can be found in table 3.",
                    "sid": 203,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this task it was not possible to apply system combination to the output of the system.",
                    "sid": 204,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The MDL results have been obtained on a differ\u00adent data set and this left us with two remaining systems.",
                    "sid": 205,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A majority vote of the two will not im\u00adprove on the best system and since there was no tuning data or development data available, other combination methods could not be applied.",
                    "sid": 206,
                    "ssid": 60,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "prospects. ",
            "number": "4",
            "sents": [
                {
                    "text": "The project has proven to be successful in its re\u00adsults for applying machine learning techniques to all three of its selected tasks: chunking, NP chunking and NP bracketing.",
                    "sid": 207,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We are looking for\u00adward to applying these techniques to other NLP tasks.",
                    "sid": 208,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Three of our project members will take part in the CoNLL2001 shared task, \u2018clausing\u2019, hope\u00adfully with good results.",
                    "sid": 209,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Two more have started working on the challenging task of full parsing, in particular by starting with a chunker and build\u00ading a bottom-up arbitrary phrase recogniser on top of that.",
                    "sid": 210,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The preliminary results are encouraging though not as good as advanced statistical parsers like those of Charniak (2000) and Collins (2000).",
                    "sid": 211,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is fair to characterise LCG\u2019s goals as pri\u00admarily technical in the sense that we sought to maximise performance rates, esp. the recognition of different levels of NP structure.",
                    "sid": 212,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our view in the project is certainly broader, and most project members would include learning as one of the language processes one ought to study from a computational perspective\u2014like parsing or gen\u00aderation.",
                    "sid": 213,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This suggest several further avenues, e.g., one might compare the learning progress of sim\u00adulations to humans (mastery as a function of ex\u00adperience).",
                    "sid": 214,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One might also be interested in the exact role of supervision, in the behaviour (and availability) of incremental learning algorithms, and also in comparing the simulation\u2019s error func\u00adtions to those of human learners (wrt to phrase length or construction frequency or similarity).",
                    "sid": 215,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This would add an interesting cognitive perspec\u00adtive to the work, along the lines begun by Brent (1997), but we note it here only as a prospect for future work.",
                    "sid": 216,
                    "ssid": 10,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgement",
            "number": "",
            "sents": [
                {
                    "text": "LCG\u2019s work has been supported by a grant from the European Union\u2019s programme Training and Mobility of Researchers, ERBFMRXCT980237.",
                    "sid": 217,
                    "ssid": 11,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}