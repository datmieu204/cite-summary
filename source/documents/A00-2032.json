{
    "ID": "A00-2032",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Given the lack of word delimiters in written Japanese, word segmentation is generally consid\u00ad ered a crucial first step in processing Japanese texts.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Typical Japanese segmentation algorithms rely ei\u00ad ther on a lexicon and grammar or on pre-segmented data.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast, we introduce a novel statistical method utilizing unsegmentd training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological analyz\u00ad ers over a variety of error metrics.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Because Japanese is written without delimiters be\u00ad tween words, 1 accurate word segmentation to re\u00ad cover the lexical items is a key step in Japanese text processing.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) er\u00ad rors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996; Fung, 1998).",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Typically, Japanese word segmentation is per\u00ad formed by morphological analysis based on lexical and grammatical knowledge.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This analysis is aided by the fact that there are three types of Japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, al\u00ad though using this heuristic alone achieves less than 60% accuracy (Nagata, 1997).",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Character sequences consisting solely of kanji pose a challenge to morphologically-based seg\u00ad menters for several reasons.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First and most importantly, kanji sequences often contain domain terms and proper nouns: Fung (1998) notes that5085% of the terms in various technical dictio 1The analogous situation in English would be if words were written without spaces between them.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1: Statistics from 1993 Japanese newswire (NIKKEI), 79,326,406 characters total.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "naries are composed at least partly of kanji.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Such words tend to be missing from general-purpose lexicons, causing an unknown word problem for morphological analyzers; yet, these terms are quite important for information retrieval, information extraction, and text summarization, making correct segmentation of these terms critical.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, kanji sequences often consist of compound nouns, so grammatical constraints are not applicable.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, the sequence shachohjkenjgyoh-mujbu\u00ad choh (presidentjandjbusinessjgeneral manager = \"a president as well as a general manager of business\") could be incorrectly segmented as: sha\u00ad chohjkengyohjmujbu-choh (presidentjsubsidiary business!Tsutomu [a name]jgeneral manager); since both alternatives are four-noun sequences, they cannot be distinguished by part-of-speech information alone.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, heuristics based on changes in character type obviously do not apply to kanji-only sequences.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although kanji sequences are difficult to seg\u00ad ment, they can comprise a significant portion of Japanese text, as shown in Figure 1.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since se\u00ad quences of more than 3 kanji generally consist of more than one word, at least 21.2% of 1993 Nikkei newswire consists of kanji sequences requiring seg\u00ad mentation.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, accuracy on kanji sequences is an important aspect of the total segmentation process.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As an alternative to lexica-grammatical and su\u00adpervised approaches, we propose a simple, effi cient segmentation method which learns mostly from very large amounts of unsegmented training data, thus avoiding the costs of building a lexicon or grammar or hand-segmenting large amounts of training data.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some key advantages of this method are: \u2022 No Japanese-specific rules are employed, en\u00ad hancing portability to other languages.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 A very small number of pre-segmented train\u00ad ing examples (as few as 5 in our experiments) are needed for good performance, as long as large amounts of unsegmented data are avail\u00ad able.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 For long kanji strings, the method produces re\u00ad sults rivalling those produced by Juman 3.61 (Kurohashi and Nagao, 1998) and Chasen 1.0 (Matsumoto et al., 1997), two morphological analyzers in widespread use.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, we achieve 5% higher word precision and 6% bet\u00ad ter morpheme recall.",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 2: Collecting evidence for a word boundary - are the non-straddling n-grams 8 1 and 82 more frequent than the straddling n-grams t 1, t2, and t3?",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let I> (y, z) be an indicator function that is 1 when y > z, and 0 otherwise.2 In order to compensate for the fact that there are more n-gram questions than (n -1)-gram questions, we calculate the fraction of affirmative answers separately for each n in N: 2 n-1",
                    "sid": 25,
                    "ssid": 25,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "algorithm. ",
            "number": "2",
            "sents": [
                {
                    "text": "Our algorithm employs counts of character n-grams Vn(k) = 2(n 1 L L I>(#(8f), #(tj)) i=l j=l in an unsegmented corpus to make segmentation de\u00ad cisions.",
                    "sid": 26,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We illustrate its use with an example (see Then, we average the contributions of each n-gram order: Figure 2).",
                    "sid": 27,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let \"A B C D W X Y Z\" represent an eight-kanji \u00b7 VN(k) = 1 INI L Vn(k) nEN sequence.",
                    "sid": 28,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To decide whether there should be a word boundary between D and W, we check whether n\u00ad grams that are adjacent to the proposed boundary, such as the 4-grams s1 =\"A B C D\" and 82 =\"W X Y Z\", tend to be more frequent than n-grams that straddle it, such as the 4-gram t 1 =\"BCD W\".",
                    "sid": 29,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If so, we have evidence of a word boundary between D and W, since there seems to be relatively little cohesion between the characters on opposite sides of this gap.",
                    "sid": 30,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The n-gram orders used as evidence in the seg\u00ad mentation decision are specified by the set N. For instance, if N = {4} in our example, then we pose the six questions of the form, \"Is #(8i) > #(tj)?\", where #(x) denotes the number of occurrences of x in the (unsegmented) training corpus.",
                    "sid": 31,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If N = {2,4}, then two more questions (Is \"#(CD) > #(D W)?\" and \"Is #(W X) > #(D W)?\") are added.",
                    "sid": 32,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More formally, let 8 and 8 be the non\u00ad straddling n-grams just to the left and right of lo\u00ad cation k, respectively, and let tj be the straddling n-gram with j characters to the right of location k. After vN (k) is computed for every location, bound\u00ad aries are placed at all locations f such that either: \u2022 VN(f) > VN(f- 1) and VN(\u00a3) > VN(f. + 1) (that is, R. is a local maximum), or \u2022 vN (R.)t, a threshold parameter.",
                    "sid": 33,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second condition is necessary to allow for single-character words (see Figure 3).",
                    "sid": 34,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that it also controls the granularity of the segmentation: low thresholds encourage shorter segments.",
                    "sid": 35,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both the count acquisition and the testing phase are efficient.",
                    "sid": 36,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Computing n-gram statistics for all possible values of n simultaneously can be done in 0(m log m) time using suffix arrays, where m is the training corpus size (Manber and Myers, 1993; Nagao and Mori, 1994).",
                    "sid": 37,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, if the set N of n gram orders is known in advance, conceptually simpler algorithms suffice.",
                    "sid": 38,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Memory allocation for 2Note that we do not take into account the magnitude of the difference between the two frequencies; see section 5 for discussion.",
                    "sid": 39,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "v,./k) A BI C DI W XI Y1 Z the word level, a stem and its affixe s are brack eted toget her as a single unit.",
                    "sid": 40,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "At the morp heme level, stems are divid ed from their affixe s. For exam ple, altho ugh both naga no (Naga no) and shi (city) can appea r as indivi dual words , nagano shi (Nag ano city) is brack eted as [[naga no][s hi]], since here shi Figure 3: Determining word boundaries.",
                    "sid": 41,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The X- Y boundary is created by the threshold criterion, the other three by the local maximum condition.",
                    "sid": 42,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "count tables can be significantly reduced by omit\u00ad ting n-grams occurring only once and assuming the count of unseen n-grams to be one.",
                    "sid": 43,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the applica\u00ad tion phase, the algorithm is clearly linear in the test corpus size if INI is treated as a constant.",
                    "sid": 44,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we note that some pre-segmented data is necessary in order to set the parameters N and t. However, as described below, very little such data was required to get good performance; we therefore deem our algorithm to be \"mostly unsupervised\".",
                    "sid": 45,
                    "ssid": 20,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "experimental framework. ",
            "number": "3",
            "sents": [
                {
                    "text": "Our experimental data was drawn from 150 megabytes of 1993 Nikkei newswire (see Figure I).",
                    "sid": 46,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Five 500-sequence held-out subsets were ob\u00ad tained from this corpus, the rest of the data serv\u00ad ing as the unsegmented corpus from which to derive character n-gram counts.",
                    "sid": 47,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each held-out subset was hand-segmented and then split into a 50-sequence parameter-training set and a 450-sequence test set.",
                    "sid": 48,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, any sequences occurring in both a test set and its corresponding parameter-training set were discarded from the parameter-training set, so that these sets were disjoint.",
                    "sid": 49,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Typically no more than five sequences were removed.)",
                    "sid": 50,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.1 Held-out set annotation.",
                    "sid": 51,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each held-out set contained 500 randomly-extracted kanji sequences at least ten characters long (about twelve on average), lengthy sequences being the most difficult to segment (Takeda and Fujisaki, 1987).",
                    "sid": 52,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To obtain the gold-standard annotations, we segmented the sequences by hand, using an observa\u00ad tion of Takeda and Fujisaki (1987) that many kanji compound words consist of two-character stem words together with one-character prefixes and suf\u00ad fixes.",
                    "sid": 53,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using this terminology, our two-level bracket\u00ad serves as a suffix.",
                    "sid": 54,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Loosely speaking, word-level bracketing demarcates discourse entities, whereas morpheme-level brackets enclose strings that cannot be further segmented without loss of meaning.4 For instance, if one segments naga-no in naga-no-shi into naga (long) and no (field), the intended mean\u00ad ing disappears.",
                    "sid": 55,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here is an example sequence from our datasets: ['J' f'.X:J [mli'JJ [[iiibJ [J J [ J Three native Japanese speakers participated in the annotation: one segmented all the held-out data based on the above rules, and the other two reviewed 350 sequences in total.",
                    "sid": 56,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The percentage of agree\u00ad ment with the first person's bracketing was 98.42%: only 62 out of 3927 locations were contested by a verifier.",
                    "sid": 57,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Interestingly, all disagreement was at the morpheme level.",
                    "sid": 58,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2 Baseline algorithms.",
                    "sid": 59,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluated our segmentation method by com\u00ad paring its performance against Chasen 1.05 (Mat\u00ad sumoto et al., 1997) and Juman 3.61,6 (Kurohashi and Nagao, 1998), two state-of-the-art, publically\u00ad available, user-extensible morphological analyzers.",
                    "sid": 60,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In both cases, the grammars were used as distributed without modification.",
                    "sid": 61,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The sizes of Chasen's and Ju\u00ad man's default lexicons are approximately 115,000 and 231,000 words, respectively.",
                    "sid": 62,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Comparison issues An important question that arose in designing our experiments was how to en\u00ad able morphological analyzers to make use of the parameter-training data, since they do not have pa\u00ad rameters to tune.",
                    "sid": 63,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The only significant way that they can be updated is by changing their grammars or lexicons, which is quite tedious (for instance, we had to add part-of-speech information\u00b7 to new en\u00ad tries by hand).",
                    "sid": 64,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We took what we felt to be a rea\u00ad sonable, but not too time-consuming, course of cre\u00ad ating new lexical entries for all the bracketed words in the parameter-training data.",
                    "sid": 65,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Evidence that this ing annotation may be summarized as follows.3 At 4This level of segmentation is consistent with Wu's (1998) 3A complete description of the annotation policy, including the treatment of numeric expressions, may be found in a tech\u00ad nical report (Ando and Lee, 1999).",
                    "sid": 66,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Monotonicity Principle for segmentation.",
                    "sid": 67,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5http://cactus.aistnara.ac.jp/lab/nltlchasen.html 6http://pine.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html Word accuracy 90 CHASEN JUMAN opllnizt oplnuo recall opiJTozt F Figure 4: Word accuracy.",
                    "sid": 68,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The three rightmost groups represent our algorithm with parameters tuned for different optimization criteria.",
                    "sid": 69,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "was appropriate comes from the fact that these ad\u00ad ditions never degraded test set performance, and in\u00ad deed improved it by one percent in some cases (only small improvements are to be expected because the parameter-training sets were fairly small).",
                    "sid": 70,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is important to note that in the end, we are com\u00ad paring algorithms with access to different sources of knowledge.",
                    "sid": 71,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Juman and Chasen use lexicons and grammars developed by human experts.",
                    "sid": 72,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our al\u00ad gorithm, not having access to such pre-compiled knowledge bases, must of necessity draw on other information sources (in this case, a very large un\u00ad segmented corpus and a few pre-segmented exam\u00ad ples) to compensate for this lack.",
                    "sid": 73,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since we are in\u00ad terested in whether using simple statistics can match the performance of labor-intensive methods, we do not view these information sources as conveying an unfair advantage, especially since the annotated training sets were small, available to the morpho\u00ad logical analyzers, and disjoint from the test sets.",
                    "sid": 74,
                    "ssid": 29,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "results. ",
            "number": "4",
            "sents": [
                {
                    "text": "We report the average results over the five test sets using the optimal parameter settings for the corre\u00ad sponding training sets (we tried all nonempty sub\u00ad sets of {2, 3, 4, 5, 6} for the set of n-gram orders N and all values in {.05, .1, .15, ...",
                    "sid": 75,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", 1} for the thresh\u00ad old t)7.",
                    "sid": 76,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In all performance graphs, the \"error bars\" represent one standard deviation.",
                    "sid": 77,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results for Chasen and Juman reflect the lexicon additions de 7 For simplicity, ties were deterministically broken by pre\u00ad ferring smaller sizes of N, shorter n-grams in N, and larger threshold values, in that order.",
                    "sid": 78,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "scribed in section 3.2.",
                    "sid": 79,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Word and morpheme accuracy The standard metrics in word segmentation are word precision and recall.",
                    "sid": 80,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Treating a proposed segmentation as a non-nested bracketing (e.g., \"IABICI\" corresponds to the bracketing \"[AB][C]\"), word precision (P) is defined as the percentage of proposed brackets that exactly match word-level brackets in the annotation; word recall (R) is the percentage of word-level an\u00ad notation brackets that are proposed by the algorithm in question; and word F combines precision and re\u00ad call: F = 2PR/(P + R)..",
                    "sid": 81,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One problem with using word metrics is that morphological analyzers are designed to produce morpheme-level segments.",
                    "sid": 82,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To compensate, we al\u00ad tered the segmentations produced by Juman and Chasen by concatenating stems and affixes, as iden\u00ad tified by the part-of-speech information the analyz\u00ad ers provided.",
                    "sid": 83,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(We also measured morpheme accu\u00ad racy, as described below.)",
                    "sid": 84,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figures 4 and 8 show word accuracy for Chasen, Juman, and our algorithm for parameter settings optimizing word precision, recall, and F-measure rates.",
                    "sid": 85,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our algorithm achieves 5.27% higher preci\u00ad sion and 0.26% better F-measure accuracy than Ju\u00ad man, and does even better (8.8% and 4.22%, respec\u00ad tively) with respect to Chasen.",
                    "sid": 86,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The recall perfor\u00ad mance falls (barely) between that of Juman and that of Chasen.",
                    "sid": 87,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As noted above, Juman and Chasen were de\u00ad signed to produce morpheme-level segmentations.",
                    "sid": 88,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We therefore also measured morpheme precision, recall, and F measure, all defined analogously to their word counterparts.",
                    "sid": 89,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 5 shows our morpheme accuracy results.",
                    "sid": 90,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We see that our algorithm can achieve better recall (by 6.51%) and F-measure (by 1.38%) than Juman, and does better than Chasen by an even wider mar\u00ad gin (11.18% and 5.39%, respectively).",
                    "sid": 91,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Precision was generally worse than the morphological analyz\u00ad ers.",
                    "sid": 92,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Compatible Brackets Although word-level accu\u00ad racy is a standard performance metric, it is clearly very sensitive to the test annotation.",
                    "sid": 93,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Morpheme ac\u00ad curacy suffers the same problem.",
                    "sid": 94,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Indeed, the au\u00ad thors of Juman and Chasen may well have con\u00ad structed their standard dictionaries using different notions of word and morpheme than the definitions we used in annotating the data.",
                    "sid": 95,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We therefore devel\u00ad oped two new, more robust metrics to measure the number of proposed brackets that would be incor [[data][base]][system](annotation brackets) Pr op os ed se g m en tat io n w o r d e rr o r s m o r p h e m e e r r o r s c o m p a t i b l e b r a c k e t e r r o r s cr os sin g m o r p h e m e d i v i d i n g [ d a t a ] [ b a s e ] [ s y s t e m ] [ d a t a ] [ b a s e s y s t e m ] [ d a t a b a s e ] [ s y s ] [ t e r n ] 2 2 2 0 1 3 0 1 0 0 0 2 Figure 6: Examples of word, morpheme, and compatible-bracket errors.",
                    "sid": 96,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The sequence \"data base\" has been annotated as \"[[data][base]]\" because \"data base\" and \"database\" are interchangeable.",
                    "sid": 97,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "BS , I M o r p h e m e a c c u r a c y CHASEN JUMAN opJrrlze rocal oplmlzo F p r e d l i o n Fi g ur e 5: M or p h e m e ac c ur ac y. segme ntatio ns can be count ed as correc t. W e also use the all comp atibl e brac kets rate, whic h is the fracti on of seque nces for whic h all the prop osed brack ets are comp atible . Intuit ively, this funct ion meas ures the ease with whic h a huma n could corre ct the outpu t of the segm entati on algo\u00ad rithm : if the all comp atible brack ets rate is high, then the error s are conc entrat ed in relati vely few seque nces; if it is low, then a huma n doing post\u00ad proce ssing woul d have to corre ct many sequ ences . Fi gure 7 depic ts the comp atible brack ets and all\u00ad comp atible brack ets rates.",
                    "sid": 98,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our algor ithm does bet\u00ad ter on both metri cs (for insta nce, when F meas ure is opti mize d, by 2.16 % and 1.9% , respe ctivel y, in rect with respect to any reasonable annotation.",
                    "sid": 99,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our novel metrics account for two types of er\u00ad rors.",
                    "sid": 100,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first, a crossing bracket, is a proposed bracket that overlaps but is not contained within an annotation bracket (Grishman et al., 1992).",
                    "sid": 101,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Cross\u00ad ing brackets cannot coexist with annotation brack\u00ad ets, and it is unlikely that another human would create such brackets.",
                    "sid": 102,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second type of er\u00ad ror, a morpheme-dividing bracket, subdivides a morpheme-level annotation bracket; by definition, such a bracket results in a loss of meaning.",
                    "sid": 103,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "See Fig\u00ad ure 6 for some examples.",
                    "sid": 104,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We define a compatible bracket as a proposed bracket that is neither crossing nor morpheme\u00ad dividing.",
                    "sid": 105,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The compatible brackets rate is simply the compatible brackets precision.",
                    "sid": 106,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that this met\u00ad ric accounts for different levels of segmentation si\u00ad multaneously, which is beneficial because the gran\u00ad ularity of Chasen and Juman's segmentation varies from morpheme level to compound word level (by our definition).",
                    "sid": 107,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, well-known university names are treated as single segments by virtue of be\u00ad ing in the default lexicon, whereas other university names are divided into the name and the word \"uni\u00ad versity\".",
                    "sid": 108,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using the compatible brackets rate, both comparison to Chasen, and by 3.15% and 4.96%, respectively, in comparison to Juman), regardless of training optimization function (word precision, re\u00ad call, or F -we cannot directly optimize the com\u00ad patible brackets rate because \"perfect\" performance is possible simply by making the entire sequence a single segment).",
                    "sid": 109,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Compatible and all-compatible bracket.s rates CHASEN JUIAAH cpWnizo Pf\"\"\"\"\" O!)lllnllt riCIII CJI'IINZ!",
                    "sid": 110,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "F l!!",
                    "sid": 111,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "conpaliblebroclr!!Urtlos _brad<JIUIIU Figure 7: Compatible brackets and all-compatible bracket rates when word accuracy is optimized.",
                    "sid": 112,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Juman5 vs. Juman50 Our50 vs Juman50 Our5 vs. Juman5 Our5 vs. Juman50 precision recall F-measure -1.040.630.84 +5.274.39 +0.26 +6.183.73 +1.14 +5.144.36 +0.30 Figure 8: Relative word accuracy as a function of training set size.",
                    "sid": 113,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\"5\" and \"50\" denote training set size before discarding overlaps with the test sets.",
                    "sid": 114,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.1 Discussion.",
                    "sid": 115,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Minimal human effort is needed.",
                    "sid": 116,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast to our mostly-unsupervised method, morphological analyzers need a lexicon and grammar rules built using human expertise.",
                    "sid": 117,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The workload in creating dictionaries on the order of hundreds of thousands of words (the size of Chasen's and Juman's de\u00ad fault lexicons) is clearly much larger than annotat\u00ad ing the small parameter-training sets for our algo\u00ad rithm.",
                    "sid": 118,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also avoid the need to segment a large amount of parameter-training data because our al\u00ad gorithm draws almost all its information from an unsegmented corpus.",
                    "sid": 119,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Indeed, the only human effort involved in our algorithm is pre-segmenting the five 50-sequence parameter training sets, which took only 42 minutes.",
                    "sid": 120,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast, previously proposed supervised approaches have used segmented train\u00ad ing sets ranging from 10005000 sentences (Kash\u00ad ioka et al., 1998) to 190,000 sentences (Nagata, l996a).",
                    "sid": 121,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To test how much annotated training data is actu\u00ad ally necessary, we experimented with using minis\u00ad cule parameter-training sets: five sets of only five strings each (from which any sequences repeated in the test data were discarded).",
                    "sid": 122,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It took only 4 minutes to perform the hand segmentation in this case.",
                    "sid": 123,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As shown in Figure 8, relative word performance was not degraded and sometimes even slightly better.",
                    "sid": 124,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In fact, from the last column of Figure 8 we see that even if our algorithm has access to only five anno\u00ad tated sequences when Juman has access to ten times as many, we still achieve better precision and better F measure.",
                    "sid": 125,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both the local maximum and threshold condi\u00ad tions contribute.",
                    "sid": 126,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our algorithm, a location k is deemed a word boundary if vN (k) is either (1) a local maximum or (2) at least as big as the thresh\u00ad old t. It is natural to ask whether we really need two conditions, or whether just one would suffice.",
                    "sid": 127,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We therefore studied whether optimal perfor\u00ad mance could be achieved using only one of the con\u00ad ditions.",
                    "sid": 128,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 9 shows that in fact both contribute to producing good segmentations.",
                    "sid": 129,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Indeed, in some cases, both are needed to achieve the best perfor\u00ad mance; also, each condition when used in isolation yields suboptimal performance with respect to some performance metrics.",
                    "sid": 130,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "a c c ur a c y o p ti m i z e p r e c i s i o n o p t i m i z e r e c a l l o p ti m iz eF m ea su re w or d M M & T M m or p h e m e M & T T T Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both.",
                    "sid": 131,
                    "ssid": 57,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "related work. ",
            "number": "5",
            "sents": [
                {
                    "text": "Japanese Many previously proposed segmenta\u00ad tion methods for Japanese text make use of either a pre existing lexicon (Yamron et al., 1993; Mat\u00ad sumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papa georgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998).",
                    "sid": 132,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996).",
                    "sid": 133,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability.",
                    "sid": 134,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes.",
                    "sid": 135,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Takeda and Fujisaki (1987) propose the short unit model, a type of Hidden Markov Model with linguistically\u00ad determined topology, to segment kanji compound words.",
                    "sid": 136,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, their method does not handle three-character stem words or single-character stem words with affixes, both of which often occur in proper nouns.",
                    "sid": 137,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our five test datasets, we found that 13.56% of the kanji sequences contain words that cannot be handled by the short unit model.Nagao and Mori (1994) propose using the heuris tic that high-frequency character n-grams may rep\u00ad resent (portions of) new collocations and terms, but the results are not experimentally evaluated, nor is a general segmentation algorithm proposed.",
                    "sid": 138,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The work of Ito and Kohda (1995) similarly relies on high-frequency character n-grams, but again, is more concerned with using these frequent n-grams as pseudo-lexicon entries; a standard segmentation algorithm is then used on the basis of the induced lexicon.",
                    "sid": 139,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our algorithm, on the hand, is fundamen\u00ad tally different in that it incorporates no explicit no\u00ad tion of word, but only \"sees\" locations between characters.",
                    "sid": 140,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Chinese According to Sproat et al.",
                    "sid": 141,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pub\u00ad lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical ap\u00ad proach.",
                    "sid": 142,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In a later paper, Palmer (1997) presents a transformation-based algorithm, which requires pre-segmented training data.",
                    "sid": 143,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To our knowledge, the Chinese segmenter most similar to ours is that of Sun et al.",
                    "sid": 144,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1998).",
                    "sid": 145,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They also avoid using a lexicon, determining whether a given location constitutes a word boundary in part by deciding whether the two characters on either side tend to occur together; also, they use thresholds and several types of local minima and maxima to make segmentation decisions.",
                    "sid": 146,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the statis\u00ad tics they use (mutual information and t-score) are more complex than the simple n-gram counts that we employ.",
                    "sid": 147,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our preliminary reimplementation of their method shows that it does not perform as well as the morphological analyzers on our datasets, al\u00ad though we do not want to draw definite conclusions because some aspects of Sun et al's method seem incomparable to ours.",
                    "sid": 148,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We do note, however, that their method incorporates numerical differences between statistics, whereas we only use indicator functions; for example, once we know that one trigram is more common than another, we do not take into account the difference between the two frequencies.",
                    "sid": 149,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We conjecture that using absolute differences may have an adverse effect on rare sequences.",
                    "sid": 150,
                    "ssid": 19,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusion. ",
            "number": "6",
            "sents": [
                {
                    "text": "In this paper, we have presented a simple, mostly\u00adunsupervised algorithm that segments Japanese se quences into words based on statistics drawn from a large unsegmented corpus.",
                    "sid": 151,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluated per\u00ad formance on kanji with respect to several metrics, including the novel compatible brackets and all\u00ad compatible brackets rates, and found that our al\u00ad gorithm could yield performances rivaling that of lexicon-based morphological analyzers.",
                    "sid": 152,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In future work, we plan to experiment on Japanese sentences with mixtures of character types, possibly in combination with morphologi\u00ad cal analyzers in order to balance the strengths and weaknesses of the two types of methods.",
                    "sid": 153,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since our method does not use any Japanese-dependent heuristics, we also hope to test it on Chinese or other languages as well.",
                    "sid": 154,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments",
            "number": "",
            "sents": [
                {
                    "text": "We thank Minoru Shindoh and Takashi Ando for reviewing the annotations, and the anonymous re\u00ad viewers for their comments.",
                    "sid": 155,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This material was sup\u00ad ported in part by a grant from the GE Foundation.",
                    "sid": 156,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}