{
    "ID": "P134_n09",
    "sections": [
        {
            "text": "initial system development. ",
            "number": "1",
            "sents": [
                {
                    "text": "We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (CallisonBurch et al., 2012).",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The notable features of these systems are: \u2022 Moses phrase-based models with mostly de\u00ad fault settings \u2022 training on all available parallel data, includ\u00ad ing the large UN parallel data, the French\u00ad English 109 parallel data and the LDC Giga\u00ad worddata \u2022 very large tuning set consisting of the test sets from 20082010, with a total of 7,567 sen\u00ad tences per language \u2022 GermanEnglish with syntactic pre\u00ad reordering (Collins et al., 2005), compound splitting (Koehn and Knight, 2003) and use of factored representation for a POS target sequence model (Koehn and Hoang, 2007) \u2022 EnglishGerman with morphological target sequence model Note that while our final 2012 systems in\u00ad cluded subsarnpling of training data with modified MooreLewis filtering (Axelrod et al., 2011), we did not use such filtering at the slatting point of our development.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will report on such filtering in Section 2.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, our system development initially used the WMT 2012 data condition, since it took place throughout 2012, and we switched to WMT 2013 traitting data at a later stage.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this sec\u00ad tion, we report cased BLEU scores (Papineni et al., 2001) on newstest20ll.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.1 Factored Backoff (GermanEnglish).",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have consistently used factored models in past WMT systems for the GermanEnglish language pairs to include POS and morphological target se\u00ad quence models.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But we did not use the factored decomposition of translation options into multi\u00ad ple mapping steps, since this usually lead to much slower systems with usually worse results.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A good place, however, for factored decompo\u00ad sition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a).",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here, we used ouly factored backoff for unknown words, giving gains in BLEU of +.12 for GermanEnglish.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.2 Tuning with k-best MIRA.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In preparation for training with sparse features, we moved away from MERT which is known to fall 114 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114121, Sofia, Bulgaria, August 89, 2013 @2013 Association for Computational Unguistics apart with many more than a couple of dozen fea\u00ad tures.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead, we used k-best MIRA (Cherry and Foster, 2012).",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the different language pairs, we saw improvements in BLEU of -.05 to +.39, with an average of +.09.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There was only a minimal change in the length ratio (Table 1) The lexical features were restricted to the 50 most frequent words.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All these features together only gave minor improvements (Table 3).",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 1:Thning with k-best MIRA instead of MERT (cased BLEU scores with length ratio) 1.3 Translation Table Smoothing with.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "KneserNey Discounting Previously, we smoothed counts for the phrasal conditional probability distributions in the trans\u00ad lation model with Good Turing discounting.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We explored the use of KneserNey discounting, but resnlts are mixed (no difference on average, see Table 2), so we did not pursue this further.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 3: Sparse features We also explored domain features in the sparse feature framework, in three different variations.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Assume that we have three domains, and a phrase pair occurs in domain A 15 times, in domain B 5 times, and in domain C never.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We compute three types of domain features: \u2022 binary indicator, if phrase-pairs occurs in do\u00ad main (example: indA = 1, indB = 1, indo = 0) \u2022 ratio how frequent the phrase pairs occurs in domain (example: ratioA = 1i!s = .75, ratioB = 15 5 = .25, ratioc = 0) \u2022 subset of domains in which phrase pair oc\u00ad curs (example: subsetAB = 1, other subsets 0) We tested all three feature types, and found the biggest gain with the domain indicator feature (+.11, Table 4).",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that we define as domain the different corpora (Europarl, etc.).",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The number of domains ranges from 2 to 9 (see column #d).1 Table 2: Translation model smoothing with KneserNey 1.4 Sparse Features.",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A significant extension of the Moses system over the last couple of years was the support for large numbers of sparse features.",
                    "sid": 25,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This year, we tested this capability on our big WMT systems.",
                    "sid": 26,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we used features proposed by Chiang et al.",
                    "sid": 27,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2009): \u2022 phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) \u2022 target word insertion features \u2022 source word deletion features \u2022 word translation features \u2022 phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).",
                    "sid": 28,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the domain indicator fea\u00ad ture and the other sparse features in subsequent ex\u00ad periments.",
                    "sid": 29,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1In the final experiments on the 2013 data condition, one domain (commoncrawl) was added for all language pairs.",
                    "sid": 30,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "bas elin e in di ca to r r a t i o s u b s e t de en fr e n es en cs en en de enfr en es en cs 22 .1 0 30 .1 1 30 .6 3 25 .4 9 1 6.",
                    "sid": 31,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 2 29 .6 5 31 .9 5 1 7.",
                    "sid": 32,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4 2 22.",
                    "sid": 33,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "18 +.0 8 30.",
                    "sid": 34,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "41 +.3 0 30.",
                    "sid": 35,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "75 +.1 2 25.",
                    "sid": 36,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "56 +.0 7 15.",
                    "sid": 37,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "95 .17 29.",
                    "sid": 38,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "96 +.3 1 32.",
                    "sid": 39,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "12 +.1 7 17.",
                    "sid": 40,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "38 .04 22.",
                    "sid": 41,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "10 \u00b1.0 0 30.",
                    "sid": 42,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "49 +.3 8 30.",
                    "sid": 43,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "56 .07 25.",
                    "sid": 44,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "63 +.1 4 15 .96 .1 6 29.",
                    "sid": 45,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "88 +.2 3 32.",
                    "sid": 46,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "16 +.2 1 17 .35 .0 7 22.",
                    "sid": 47,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "16 +.0 6 30.",
                    "sid": 48,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "36 +.2 5 30.",
                    "sid": 49,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "85 +.2 2 25.43 .06 16.",
                    "sid": 50,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "05 .07 29.",
                    "sid": 51,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "92 +.2 7 32.",
                    "sid": 52,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "08 +.2 3 17.",
                    "sid": 53,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "40 .02 avg . + . 1 1 + . 0 9 + . 1 1 Table 5: Combining domain and other sparse features 1.5 Thning Settings.",
                    "sid": 54,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given the opportunity to explore the parameter tuning of models with sparse features across many language pairs, we investigated a number of set\u00ad tings.",
                    "sid": 55,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We expect tuning to work better with more iterations, longer n-best lists and bigger cube prun\u00ad ing pop limits.",
                    "sid": 56,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our baseline settings are 10 itera\u00ad tions with 100-best lists (accumulating) and a pop limit ofI000 for tuning and 5000 for testing.",
                    "sid": 57,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 7: Maximum phrase length, reduced from baseline 1.7 Unpruned Language Models.",
                    "sid": 58,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previously, we trained 5-gram language models using the default settings of the SRILM toolkit in terms of singleton pruning.",
                    "sid": 59,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, training throws out all singletons n-grams of order 3 and higher.",
                    "sid": 60,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We explored whether unpruned language models could give better performance, even if we are ouly able to train 4-gram models due to memory con\u00ad straints.",
                    "sid": 61,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "At the time, we were not able to build un\u00ad pruned 4-gram language models for English, but for the other language pairs we did see improve\u00ad ments of -.07 to +.13 (Table 8).",
                    "sid": 62,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We adopted such models for these language pairs.",
                    "sid": 63,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5g pru ne d 4gu npr une den fr en es en cs 2 9 . 8 9 3 2 . 2 7 1 7 . 4 1 2 9 . 8 3 3 2 . 3 4 1 7 . 5 4 .0 7 +.0 7 +.1 3 Table 8: Language models without singleton pruning Table 6: Tuning settings (number of iterations, size of n-best list, and cube pruning pup limit) Results support running tuning for 25 iterations but we see no gains for 5000 pops.",
                    "sid": 64,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There is ev\u00ad idence that ann-best list size of 1000 is better in tuning but we did not adopt this since these large lists take up a lot of disk space and slow down the MIRA optimization step (Table 6).",
                    "sid": 65,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.6 Smaller Phrases.",
                    "sid": 66,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given the very large corpus sizes (up to a billion words of parallel data for French-Englisb), the size of translation model and lexicalized reorder\u00ad ing model becomes a challenge.",
                    "sid": 67,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hence, we want to examine if restriction to smaller phrases is fea\u00ad sible without loss in translation quality.",
                    "sid": 68,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Results in Table 7 suggest that a maximum phrase length of 5 gives almost identical results, and ouly with a phrase length limit of 4 significant losses occur.",
                    "sid": 69,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We adopted the limit of 5.",
                    "sid": 70,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.8 Translations per Input Phrase.",
                    "sid": 71,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we explored one more parameter: the limit on how many translation options are considered per input phrase.",
                    "sid": 72,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The default for this setting is 20.",
                    "sid": 73,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, our experiments (Table 9) sbow that we can get better results with a translation table limit of I00, so we adopted this.",
                    "sid": 74,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ttl 20 ttl 30 ttl 50 ttll OO de enfr en es en cs en en de enfr en es en cs 21.",
                    "sid": 75,
                    "ssid": 75,
                    "kind_of_tag": "s"
                },
                {
                    "text": "05 30.",
                    "sid": 76,
                    "ssid": 76,
                    "kind_of_tag": "s"
                },
                {
                    "text": "39 30.",
                    "sid": 77,
                    "ssid": 77,
                    "kind_of_tag": "s"
                },
                {
                    "text": "86 25.",
                    "sid": 78,
                    "ssid": 78,
                    "kind_of_tag": "s"
                },
                {
                    "text": "53 15.",
                    "sid": 79,
                    "ssid": 79,
                    "kind_of_tag": "s"
                },
                {
                    "text": "97 29.",
                    "sid": 80,
                    "ssid": 80,
                    "kind_of_tag": "s"
                },
                {
                    "text": "83 32.",
                    "sid": 81,
                    "ssid": 81,
                    "kind_of_tag": "s"
                },
                {
                    "text": "34 17.",
                    "sid": 82,
                    "ssid": 82,
                    "kind_of_tag": "s"
                },
                {
                    "text": "54 +.0 6 .0 2 \u00b1.0 0 +.2 4 +.0 3 +.1 4 +.0 8 .0 5 +.0 9 +.0 5 .0 3 +.1 3 +.0 7 +.1 9 +.1 0 .0 2 +.",
                    "sid": 83,
                    "ssid": 83,
                    "kind_of_tag": "s"
                },
                {
                    "text": "01 +.",
                    "sid": 84,
                    "ssid": 84,
                    "kind_of_tag": "s"
                },
                {
                    "text": "07 .0 7 +.",
                    "sid": 85,
                    "ssid": 85,
                    "kind_of_tag": "s"
                },
                {
                    "text": "20 +.",
                    "sid": 86,
                    "ssid": 86,
                    "kind_of_tag": "s"
                },
                {
                    "text": "11 +.",
                    "sid": 87,
                    "ssid": 87,
                    "kind_of_tag": "s"
                },
                {
                    "text": "13 +.",
                    "sid": 88,
                    "ssid": 88,
                    "kind_of_tag": "s"
                },
                {
                    "text": "07 +.",
                    "sid": 89,
                    "ssid": 89,
                    "kind_of_tag": "s"
                },
                {
                    "text": "01 avg +.0 6 +.0 7 +.",
                    "sid": 90,
                    "ssid": 90,
                    "kind_of_tag": "s"
                },
                {
                    "text": "07 Table 9: Maximal number translations per input phrase 1.9 Other Experiments.",
                    "sid": 91,
                    "ssid": 91,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We explored a number of other settings and fea\u00ad tures, but did not observe any gains.",
                    "sid": 92,
                    "ssid": 92,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 Using HMM alignment instead of IBM Model4 leads to losses of -.01 to -.27.",
                    "sid": 93,
                    "ssid": 93,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 An earlier check of modified MooreLewis filtering (see also below in Section 3) gave very inconsistent results.",
                    "sid": 94,
                    "ssid": 94,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 Filtering the phrase table with significance filtering (Johnson eta!., 2007) leads to losses of -.19 to -.63.",
                    "sid": 95,
                    "ssid": 95,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 Throwing out phrase pairs with direct transla\u00ad tion probability cf>( elf) of less than w-5 has almost no effect.",
                    "sid": 96,
                    "ssid": 96,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 Double-checking the contribution of the sparse lexical features in the final setup, we observe an average losses of -.07 when drop\u00ad ping these features.",
                    "sid": 97,
                    "ssid": 97,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 For the GermanEnglish language pairs we saw some benefits to using sparse lexical fea\u00ad tures over POS tags instead of words, so we used this in the final system.",
                    "sid": 98,
                    "ssid": 98,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.10 Summary.",
                    "sid": 99,
                    "ssid": 99,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We adopted a number of changes that improved our baseline system by an average of +.30, see Ta\u00ad ble 10 for a breakdown.",
                    "sid": 100,
                    "ssid": 100,
                    "kind_of_tag": "s"
                },
                {
                    "text": "avg.",
                    "sid": 101,
                    "ssid": 101,
                    "kind_of_tag": "s"
                },
                {
                    "text": "method +.01 factored backoff +.09 kbest MIRA +.11 sparse features and domain indicator +.03 tuning with 25 iterations -.03 maximum phrase length 5 +.02 unpruned4gramLM +.07 translation table limit 100 +.30 total Table 10: Summary of impact of clumges Minor improvements that we did not adopt was avoiding reducing maximum phrase length to 5 (average +.03) and turting with 1000-best lists (+.02).",
                    "sid": 102,
                    "ssid": 102,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The improvements differed significantly by lan\u00ad guage pair, as detailed in Table 11, with the biggest gains for English-French (+.70), no gain for EnglishGerman and no gain for English\u00ad German.",
                    "sid": 103,
                    "ssid": 103,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.11 New Data.",
                    "sid": 104,
                    "ssid": 104,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The final experiment of the initial system devel\u00ad opment phase was to train the systems on the new data, adding newstest2011 to the turting set (now 10,068 sentences).",
                    "sid": 105,
                    "ssid": 105,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 12 reports the gains on newstest2012 due to added data, indicating very clearly that valuable new data resources became available this year.",
                    "sid": 106,
                    "ssid": 106,
                    "kind_of_tag": "s"
                },
                {
                    "text": "bas elin e imp rov ed L J .de enfr en es en cs en en de enfr en es en cs 2 1.",
                    "sid": 107,
                    "ssid": 107,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 9 3 0.",
                    "sid": 108,
                    "ssid": 108,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 0 3 0.",
                    "sid": 109,
                    "ssid": 109,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4 2 2 5.",
                    "sid": 110,
                    "ssid": 110,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 4 1 6.",
                    "sid": 111,
                    "ssid": 111,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 8 2 9.",
                    "sid": 112,
                    "ssid": 112,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 6 3 1.",
                    "sid": 113,
                    "ssid": 113,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 2 1 7.",
                    "sid": 114,
                    "ssid": 114,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3 8 2 2.",
                    "sid": 115,
                    "ssid": 115,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 9 3 0 . 4 6 3 0 . 7 9 2 5.",
                    "sid": 116,
                    "ssid": 116,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7 3 1 6 . 0 8 2 9.",
                    "sid": 117,
                    "ssid": 117,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 6 3 2 . 4 1 1 7 . 5 5 +.1 0 +.4 6 +.3 7 +.1 9 \u00b1.0 0 +.7 0 +.4 9 +.1 7 Table 11: Overall improvements per language pair W M T2 01 2 W M T2 01 3 L J .de enfr en es en cs en ru en en de enfr en -es en -cs enru 2 3 . 1 1 2 9 . 2 5 3 2 . 8 0 2 2 . 5 3 1 6 . 7 8 2 7 . 9 2 3 3 . 4 1 1 5 . 5 1 2 4 . 0 1 3 0 . 7 7 3 3 . 9 9 2 2 . 8 6 3 1 . 6 7 1 7 . 9 5 2 8 . 7 6 3 4 . 0 0 1 5 . 7 8 2 3 . 7 8 +0.",
                    "sid": 118,
                    "ssid": 118,
                    "kind_of_tag": "s"
                },
                {
                    "text": "90 +1.",
                    "sid": 119,
                    "ssid": 119,
                    "kind_of_tag": "s"
                },
                {
                    "text": "52 +1.",
                    "sid": 120,
                    "ssid": 120,
                    "kind_of_tag": "s"
                },
                {
                    "text": "19 +0.",
                    "sid": 121,
                    "ssid": 121,
                    "kind_of_tag": "s"
                },
                {
                    "text": "33 +1.",
                    "sid": 122,
                    "ssid": 122,
                    "kind_of_tag": "s"
                },
                {
                    "text": "17 +0.",
                    "sid": 123,
                    "ssid": 123,
                    "kind_of_tag": "s"
                },
                {
                    "text": "84 +0.",
                    "sid": 124,
                    "ssid": 124,
                    "kind_of_tag": "s"
                },
                {
                    "text": "59 +0.",
                    "sid": 125,
                    "ssid": 125,
                    "kind_of_tag": "s"
                },
                {
                    "text": "27 Table 12: Training with new data (newstest2012 scores)",
                    "sid": 126,
                    "ssid": 126,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "domain adaptation techniques. ",
            "number": "2",
            "sents": [
                {
                    "text": "We explored two additional domain adaptation techniques: phrase table interpolation and modi\u00ad fied MooreLewis filtering.",
                    "sid": 127,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.1 Phrase Table Interpolation.",
                    "sid": 128,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We experimented with phrase-table interpolation using perplexity minimisation (Foster eta!., 2010; Sennrich, 2012).",
                    "sid": 129,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1n particular, we used the im\u00ad plementation released with Sennrich (2012) and available in Moses, comparing both the naive and modified interpolation methods from that paper.",
                    "sid": 130,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each language pair, we took the alignments created from all the data concatenated, built sepa\u00ad rate phrase tables from each of the individual cor\u00ad para, and interpolated using each method.",
                    "sid": 131,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The re\u00ad sults are shown in Table 13 bas elin e n a i v e m od ifi ed f r e n es en \u2022 cs en \"' ru en e n f r e n e s e n c s e n r u 3 0.",
                    "sid": 132,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7 7 3 3.",
                    "sid": 133,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 8 2 3.",
                    "sid": 134,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 9 3 1.",
                    "sid": 135,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6 7 28 .7 6 3 4.",
                    "sid": 136,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 0 1 5.",
                    "sid": 137,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7 8 23 .7 8 30.",
                    "sid": 138,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "63 .14 33.",
                    "sid": 139,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "83 .15 22.77 .42 31.",
                    "sid": 140,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "42 .25 28.",
                    "sid": 141,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "88 +.1 2 34.",
                    "sid": 142,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "07 +.0 7 15.",
                    "sid": 143,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "88 +.1 0 23.",
                    "sid": 144,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "84 +.0 6 34.",
                    "sid": 145,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "03 +.0 5 23.03 .17 31.59 .08 34.",
                    "sid": 146,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "31 +.3 1 15.",
                    "sid": 147,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "87 +.0 9 23.68 .10 Table 13: Comparison of phrase-table interpolation (two methods) with baseline (on newstest2012).",
                    "sid": 148,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The baselines are as Table 12 except for the starred rows where tuning with PRO was found to be better.",
                    "sid": 149,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The modified interpolation was not possible in fr++en as it uses to much RAM.",
                    "sid": 150,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results from the phrase-table interpolation are quite mixed, and we ouly used the technique for the final system in en-es.",
                    "sid": 151,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An interpolation based on PRO has recently been shown (Haddow, 2013) to improve on perplexity minimisation is some cases, but the current implementation of this method is limited to 2 phrase-tables, so we did not use it in this evaluation.",
                    "sid": 152,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.2 Modified MooreLewis Filtering.",
                    "sid": 153,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In last year's evaluation (Koehn and Haddow, 2012b) we had some success with modified MooreLewis filtering (Moore and Lewis, 2010; Axelrod et al., 2011) of the training data.",
                    "sid": 154,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This year we conducted experiments in most of the lan\u00ad guage pairs using MML filtering, and also exper\u00ad imented using instance weighting (Mansour and Ney, 2012) using the (exponential of) the MML weights.",
                    "sid": 155,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results are show in Table 14 ba se lin e M M L 2 0 % Tn st. W t Tn st. W t (s c al e ) f r e n es en * cs en * r u e n e n f r e n e s e n c s e n r u 30.",
                    "sid": 156,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "77 33.",
                    "sid": 157,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "98 23.",
                    "sid": 158,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "19 31.",
                    "sid": 159,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "67 28.",
                    "sid": 160,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "67 34.",
                    "sid": 161,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "00 15.",
                    "sid": 162,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "78 23.",
                    "sid": 163,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "78 34.",
                    "sid": 164,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "26 +.2 8 22 .62 .5 7 31 .58 .0 9 28.",
                    "sid": 165,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "74 +.0 7 34.",
                    "sid": 166,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "07 +.0 7 15 .37 .4 1 22 .90 .8 8 33.85 .13 23.17 .02 31.57 .10 28.",
                    "sid": 167,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "81 +.1 7 34.",
                    "sid": 168,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "27 +.2 7 15.",
                    "sid": 169,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "87 +.0 9 23.",
                    "sid": 170,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "82 +.0 5 33.",
                    "sid": 171,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "98 \u00b1.0 0 23.13 .06 31.62 .05 28.63 .04 34.",
                    "sid": 172,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "03 +.0 3 15.",
                    "sid": 173,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "89 +.I I 23.72 .06 Table 14: Comparison of MML filtering and weighting with baseline.",
                    "sid": 174,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The MML uses monolingual news as in-domain, and selects from all training data after alignment.The weight\u00ad ing uses the MML weights, optionally downscaled by 10, then exponentiated.",
                    "sid": 175,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Baselines are as Table 13.",
                    "sid": 176,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As with phrase-table interpolation, MML filter\u00ad ing and weighting shows a very mixed picture, and not the consistent improvements these techniques offer on IWSLT data.",
                    "sid": 177,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the final systems, we used MML filtering only for es-en.",
                    "sid": 178,
                    "ssid": 52,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "operation sequence model (osm). ",
            "number": "3",
            "sents": [
                {
                    "text": "We enhanced the phrase segmentation and re\u00ad ordering mechanism by integrating OSM: an op\u00ad eration sequence N-gram-based translation and re\u00ad ordering model (Durrani et al., 2011) into the Moses phrase-based decoder.",
                    "sid": 179,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The model is based on minimal translation units (MTUs) and Markov chains over sequences of operations.",
                    "sid": 180,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An opera\u00ad tion can be (a) to jointly generate a bi-language MTU, composed from sou rce and target words, or (b) to perform reordering by inserting gaps and do\u00ad ing jumps.",
                    "sid": 181,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Model: Given a bilingual sentence pair < F, E > and its alignment A, we transform it to If h g aycht z/Jn hars I do o tfthhotse Figure 1: Bilingual Sentence with Alignments sequence of operations (01, 02 , . . .",
                    "sid": 182,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", OJ) and learn a Markov model over this sequence as: JPosm(F, E,A) = p(o{) = IJ p(ojlOj-n+l, ...,Oj 1) j = l By coupling reordering with lexical generation, each (translation or reordering) decision condi\u00ad tions on n - 1 previous (translation and reorder\u00ad ing) decisions spann ing across phrasal boundaries thus overcoming the problematic phrasal indepen\u00ad dence assumption in the phrase-based model.",
                    "sid": 183,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the OSM model, the reordering decisions influ\u00ad ence lexical selection and vice versa.",
                    "sid": 184,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Lexical gen\u00ad eration is strongly coupled with reordering thus improving the overall reordering mechanism.",
                    "sid": 185,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used the modified version of the OSM model (Durrani et al., 2013b) that addition\u00ad ally handles discontinuous and unaligned target MTUs3.",
                    "sid": 186,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We borrow 4 count based supportive fea\u00ad tures, the Gap, Open Gap, Gap-width and Dele\u00ad tion penalties from Durrani eta!.",
                    "sid": 187,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2011).",
                    "sid": 188,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Training: During training, each bilingual sen\u00ad tence pair is deterministically converted to a unique sequence of operations.",
                    "sid": 189,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Please refer to Durrani et al.",
                    "sid": 190,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2011) for a list of operations and the conversion algorithm and see Figure 1 and Ta\u00ad ble 15 for a sample bilingual sentence pair and its step-wise conversion into a sequence of oper\u00ad ation.",
                    "sid": 191,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A 9-gram KneserNey smoothed operation sequence model is trained with SRILM.",
                    "sid": 192,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Search: Although the OSM model is based on minimal uni ts, phrase-based search on top of OSM model was found to be superior to the MTU-based decoding in Durrani et al.",
                    "sid": 193,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2013a).",
                    "sid": 194,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Following this framework allows us to use OSM model in tandem with phrase-based models.",
                    "sid": 195,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We integrated the gen\u00ad erative story of the OSM model into the hypothe\u00ad sis extension of the phrase-based Moses decoder.",
                    "sid": 196,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Please refer to (Durrani et al., 2013b) for details.",
                    "sid": 197,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Results: Table 16 shows case-sensitive BLEU scores on newstest2012 and newstest2013 for fi 3 In the original OSM model these are removed from the alignments through a post-processing heuristic which hurt.s in some language pairs.",
                    "sid": 198,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "See Durrani et al.",
                    "sid": 199,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2013b) for deta1led experiments.",
                    "sid": 200,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "393m 3,775m 17,629m 39,919m 59,794m Table 17: Counts of unique n-grams (m for millions) for the 5 orders in the unconstrained language model Table 15: Step-Wise Generation ofFtgore I The large language model was 1hen quantized to 10 bits and compressed to 643 GB wi1h KenLM (Heafield, 2011), loaded onto a machine wi1h 1 TB RAM, and used as an additional feature in unconstrained French-English, SpanishEnglish, and CzechEnglish submissions.",
                    "sid": 201,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This additional language model is 1he ouly difference between our final constrained and unconstrained submissions; no additional parallel data was used.",
                    "sid": 202,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Results are shown in Table 18.",
                    "sid": 203,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Improvement from large lan\u00ad guage models is not a new result (Brants et al., 2007); 1he primary contribution is estimating on a single machine.",
                    "sid": 204,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Co nst rai ned Un con stra ine d f ) . fr e n es en cs en ru eo 3 1 . 4 6 3 0 . 5 9 2 7 . 3 8 2 4 . 3 3 3 2 . 2 4 3 1 . 3 7 2 8 . 1 6 2 5 . 1 4 +.7 8 +.7 8 +.7 8 +.8 1 Table 16: Results using the OSM Feature nal systems from Section 1 and 1hese systems aug\u00ad mented wi1h 1he operation sequence model.",
                    "sid": 205,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The model gives gains for all language pairs (BLEU +.09 to +.90, average +.37, on newstest2013).",
                    "sid": 206,
                    "ssid": 28,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "huge language models. ",
            "number": "4",
            "sents": [
                {
                    "text": "To overcome 1he memory limitations of SRILM, we implemented modified KneserNey (Kneser and Ney, 1995; Chen and Goodman, 1998) smoothlng from scratch using disk-based stream\u00ad ing algori1hms.",
                    "sid": 207,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This open-source4 tool is de\u00ad scribed fully by Heafield et al.",
                    "sid": 208,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2013).",
                    "sid": 209,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used it to estimate an unpruned 5-gram language model on web pages from ClueWeb09.5 The corpus was preprocessed by removing spam (Cormack et al., 2011), selecting English documents, splitting sen\u00ad tences, deduplicating, tokenizing, and truecasing.",
                    "sid": 210,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Estimation on 1he remaining 126 billion tokens took 2.8 days on a single machine wi1h 140 GB RAM (of which 123GB was used at peak) and sbr.",
                    "sid": 211,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "hard drives in a RAIDS configuration.",
                    "sid": 212,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Statistics about 1he resulting model are shown in Table 17.",
                    "sid": 213,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4http://kheafield.com/code/ 5http://lemurproject.org/clueweb&9/ Table 18: Gain on newstest2013 from the unconstrained lan\u00ad guage model.",
                    "sid": 214,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our time on shared machines with 1 TB is limited so RussianEnglish was run after the deadline and GermanEnglish was not ready in time.",
                    "sid": 215,
                    "ssid": 9,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "summary. ",
            "number": "5",
            "sents": [
                {
                    "text": "Table 19 breaks down 1he gains over 1he final sys\u00ad tem from Section 1 from using 1he operation se\u00ad quence models (OSM), modified MooreLewis fil\u00ad tering (MML), fixing a bug wi1h 1he sparse lex\u00ad ical features (Sparse-Lex Bugfix), and instance weighting (Instance Wt.), translation model com\u00ad bination (TM-Combine), and use of 1he huge lan\u00ad guage model (ClueWeb09 LM).",
                    "sid": 216,
                    "ssid": 1,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments",
            "number": "",
            "sents": [
                {
                    "text": "Thanks to Miles Osborne for preprocessing the ClueWeb09 corpus.",
                    "sid": 217,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The research leading to these results has re\u00ad ceived funding from the European Union Seventh Framework Programme (FP7n0072013) under graot agreement 287658 (EU BRIDGE) aod grant agreement 288487(MosesCore).This work made use of the resources pruvided by the Edioburgh Compure aod Data Facility'.",
                    "sid": 218,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The ECDF is partilllly supported by the eDIKT initia\u00ad tive7.",
                    "sid": 219,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This work also used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number OCI1053575.",
                    "sid": 220,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Specifically, Stampede was used under allocation TGCCR110017.",
                    "sid": 221,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6http://www.ecdf.ed.ac.uk/ 7http://www.edikt.org.uk/ System 1 2012 2013 SpanishEnglish BnglishSpamsh CzechEnglish",
                    "sid": 222,
                    "ssid": 7,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}