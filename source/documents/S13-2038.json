{
    "ID": "S13-2038",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "In this paper, we describe the UKP Lab system participating in the Semeval2013 task\u201cWord Sense Induction and Disambiguationwithin an End-User Application\u201d.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our approach uses preprocessing, co-occurrence extraction, graph clustering, and a state-of-the-art word sense disambiguation system.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Wedeveloped a configurable pipeline which canbe used to integrate and evaluate other components for the various steps of the complextask.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "The task \u201cEvaluating Word Sense Induction andWord Sense Disambiguation in an End-User Application\u201d of SemEval2013 (Navigli and Vannella,2013) aims at an extrinsic evaluation scheme forWSI to overcome the difficulties inherent to WSIevaluation.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The task requires building a WSI system and combining it with a WSD step to assign theinduced sentences to example instances.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Word sense disambiguation (WSD) is the taskof determining the correct meaning for an ambiguous word from its context.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "WSD algorithms usually choose one sense out of a given set of possiblesenses for each word.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A resource that enumeratespossible senses for each word is called a sense inventory.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Manually created inventories come usuallyin form of lexical semantic resources, such as Word-Net or more specifically created inventories such asOntoNotes (Hovy et al., 2006).",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Word sense induction (WSI) on the other handaims to create such an inventory from a corpus in an unsupervised manner.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each word that shouldbe disambiguated, a WSI algorithm creates a set ofcontext clusters that will be used to define and describe the senses.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We build our system upon the open-source DKProframework 1 and a corresponding WSD component(upcoming).",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Input for the task comes as two files.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One containsthe search queries, also referred as topics.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sense induction will be performed for each of those topics.The second file contains 6400 entries from the result pages of a search engine.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each entry consists ofthe title, a snippet and the URL of the correspondingweb page.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "related work. ",
            "number": "2",
            "sents": [
                {
                    "text": "One of the early approaches to WSI (Schu\u00a8tze, 1998)maps words into a vector space and representsword contexts as vector-sums and use cosine vector similarity, clustering is performed by expectationmaximization (EM) clustering.",
                    "sid": 17,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold.",
                    "sid": 18,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They perform Markov clustering on this graph.",
                    "sid": 19,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pantel and Lin (2002) proposesa clustering approach called clustering by committee (CBC).",
                    "sid": 20,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This algorithm first selects the wordswith the highest similarity based on mutual information and then builds groups of highly connectedwords called committees.",
                    "sid": 21,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It then iteratively assignsthe remaining words to one of the committee clusters by comparing them to the averaged the com 1http://code.google.com/p/dkpro-core-asl/ 212 mittee feature vectors.",
                    "sid": 22,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This exploits the assumptionthat two or more words together disambiguate eachother, Bordag (2006) extends on this idea by usingword triples to form non-ambiguous seed-clusters.Many approaches use a variety of graph clusteringalgorithms for WSI: Others (Klapaftis and Manandhar, 2010) use hierarchical agglomerative clusteringon hierarchical random graphs created from wordco-occurrences.",
                    "sid": 23,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Di Marco and Navigli (2013) useword sense induction for web search result clustering.",
                    "sid": 24,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They introduce a maximum spanning tree algorithm that operates on co-occurrence graphs builtfrom large corpora, such as ukWaC (Baroni et al.,2009).",
                    "sid": 25,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The system by Pedersen (2010) employsclustering first- and second-order co-occurrences aswell as singular value decomposition on the co-occurrence matrix, which is clustered using repeatedbisections.",
                    "sid": 26,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Jurgens (2011) employ a graph-basedcommunity detection algorithm on a cooccurrencegraph.",
                    "sid": 27,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Distributional approaches for WSI includeLSA Apidianaki and Van de Cruys (2011) or LDA(Brody and Lapata, 2009).",
                    "sid": 28,
                    "ssid": 12,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "our approach. ",
            "number": "3",
            "sents": [
                {
                    "text": "Our system consists of two independent parts.",
                    "sid": 29,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thefirst is a batch process that creates database containing co-occurrence statistics derived from a background corpus.",
                    "sid": 30,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second is the actual WSI andWSD pipeline doing the result clustering.",
                    "sid": 31,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both partsinclude identical preprocessing steps for segmentation and lemmatization.",
                    "sid": 32,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The pipeline (Figure 1) first performs Word SenseInduction, resulting in an induced sense inventory.A WSD algorithm then uses this inventory to disambiguate all instances of the search query within aweb-page.",
                    "sid": 33,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A majority voting finally assigns a senseto each result-snippet.",
                    "sid": 34,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The sense induction algorithm is based on graphclustering on a co-occurrence graph, similar to theapproach by Di Marco and Navigli (2013).",
                    "sid": 35,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our approach differs from previous work in the way weperform a greedy search for additional context andhow it combines WSI with an advanced WSD stepusing lexical expansions.",
                    "sid": 36,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, we considerour generic UIMA-based WSD and WSI system asa useful basis for experimentation and evaluation ofWSI systems.",
                    "sid": 37,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "# words # cooccurrencesWikipedia 3,011,397 96,979,920ukWaC 8,687,711 441,005,478 Table 1: Size of co-occurrence databases 3.1 Preprocessing.",
                    "sid": 38,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The pipeline first reads topics and snippets.",
                    "sid": 39,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If theweb-page can be downloaded at the URL that corresponds to the result, it is cleaned by an HTMLparser and the plain text is appended to the snippet.As further steps we segment and lemmatize the input.",
                    "sid": 40,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We apply the same preprocessing to snippets,queries and the corpora.",
                    "sid": 41,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2 Co-occurrence Extraction.",
                    "sid": 42,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We calculate the log-likelihood ratio (LLR) (Dunning, 1993) and point-wise mutual information(PMI) (Church and Hanks, 1990) of a word pair co-occurring at sentence level using a modified versionof the collocation statistics implemented in ApacheMahout 2.",
                    "sid": 43,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Even when sorting the co-occurrences byPMI, we employ a minimum support cutoff basedon the LLR, which is based on significance.",
                    "sid": 44,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Allpairs with a log-likelihood ratio < 1 are discarded.This value is lower than the significance level of 3\u02dc.8we found in the literature, but because in the expand step (see algorithm 2) we require more thantwo words to co-occur with the target word, we useda lower value.",
                    "sid": 45,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the English Wikipedia 3 andukWaC (Baroni et al., 2009) as background corpus.Table 1 gives an overview about the obtained co-occurrence pairs.",
                    "sid": 46,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.3 Clustering Algorithm.",
                    "sid": 47,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The algorithm is a two-step approach that first creates an initial clustering of a graph G = (V,E)and then improves this clustering in a second step.The initial step (Algorithm 1) starts by retrieving thetop n = 150 most similar terms for the target wordby querying the co-occurrence database we createdin section 3.2.",
                    "sid": 48,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These represent vertices in a graph.We then construct 4 a minimum spanning tree (mst)2http://mahout.apache.org3Dump from April 20114For all of our graph operations, we employ the igraph li brary for R, http://igraph.sf.net 213 Figure 1: WSI and WSD Pipeline by inserting edges {vi, vj} from the cooccurrencedatabase.",
                    "sid": 49,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The weight w({vi, vj}) of each edge isset to the inverse of the used similarity measuredist (LLR or PMI) between those terms.",
                    "sid": 50,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The minimum spanning tree then is cut into subtrees be it-eratively removing the edge with the highest edge-betweenness (Freeman, 1977) (betweeness) untilthe size of the largest component of G falls belowa threshold Sinitial.",
                    "sid": 51,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Algorithm 1 initialClustersV (G0)?",
                    "sid": 52,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "top n most similar words to target wordw(vi, vj)?",
                    "sid": 53,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "dist(termi, termj)G? mst(G0)V (G)?",
                    "sid": 54,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "V (G) \\ vtargetwhilemax(|C(G)|) > Sintitial doE(G)?",
                    "sid": 55,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "E(G) \\ argmaxe(betweeness(e)) end whileThe resulting partitioning of the graph is the starting point for the second phase of the algorithm,which we call expand/join step (Algorithm 2).",
                    "sid": 56,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During this step, the algorithm looks iteratively at allclusters Csmall of size s smaller than Smax = 9 (determined empirically), starting with the largest ones.From each of these clusters, it constructs a queryto the co-occurrence database, retrieving all termsthat significantly co-occur together with all terms inthe respective cluster (querys) and with the targetword (E).",
                    "sid": 57,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This list of terms is then compared toall clusters Clarge with |C| > s . If the normalizedintersection between one of those Clarge is above athreshold t = 0.3 (determined empirically), we assume that the Csmall represents the same sense asthe Clarge and merge those clusters.",
                    "sid": 58,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If this is not thecase for any of the larger clusters, we assume thatCsmall represents a sense of its own extend the clus ter by adding edges between vertices representingthe expansion terms and Csmall.",
                    "sid": 59,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Algorithm 2 expandJoinRequire: G is a minimum spanning forestfor s = Smax ? 1 dofor all Csmall(G), |Csmall| = s doE ? querys(v1, .., vi)for all Clarge ? G, |Clarge| > s doif |Clarge n E|/|Clarge| > t thenClarge ? Clarge ? Csmall elseCsmall ? Csmall ? E end ifend for end forend for 3.4 Word Sense Disambiguation.",
                    "sid": 60,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the DKPro WSD framework, which implements various WSD algorithms, with the same system configuration as reported by Miller et al.",
                    "sid": 61,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2012).It uses a variant of the Simplified Lesk Algorithm(Kilgarriff et al., 2000).",
                    "sid": 62,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This algorithm measuresthe overlap between a words context and the textual descriptions of senses within a machine readable dictionary, such as WordNet.",
                    "sid": 63,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The senses thathave been induced in the previous step are providedto the framework as a sense inventory.",
                    "sid": 64,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead ofusing sense descriptions, we now compute the overlap between the sense clusters and the context ofthe target word.",
                    "sid": 65,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The WSD system expands boththe word context and the sense clusters with synonyms from a distributional thesaurus (DT), similarto Lin (1998).",
                    "sid": 66,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The DT has been created from 10Mdependency-parsed sentences of English newswire 214 Run F1 ARI RI JI # clusters avg cl. sizewackyllr 0.5826 0.0253 0.5002 0.3394 3.6400 32.3434wpllr 0.5864 0.0377 0.5109 0.3177 4.1700 21.8702wppmi 0.6048 0.0364 0.5050 0.2932 5.8600 30.3098 Table 2: Results for the submitted runs from the Leipzig Corpora Collection (Biemann etal., 2007) for word similarity5.",
                    "sid": 67,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Besides knowledge-based WSD, the DT also has been successfully usedfor improving the performance of semantic text similarity (Ba\u00a8r et al., 2012).",
                    "sid": 68,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The WSD component disambiguates each instance of the search query withinthe snippet and web page individually.",
                    "sid": 69,
                    "ssid": 41,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "results. ",
            "number": "4",
            "sents": [
                {
                    "text": "The clustering was evaluated using four differentmetrics as described by Di Marco and Navigli(2013).",
                    "sid": 70,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Rand index and its chance-adjustedvariant ARI are common cluster evaluation metrics.The adjusted rand index gives special weight to lessfrequent senses.",
                    "sid": 71,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Jaccard index (JI) disregardsthe cases where two results are assigned to different clusters in the gold standard, therefore it is lesssensitive to the granularity of the clustering.",
                    "sid": 72,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "TheF1-Measure gives more attention to the individualclusters and how they cover the topics in the goldstandard.",
                    "sid": 73,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We submitted several runs for different configurations of the co-occurrence extraction (Table 2).Between runs, we did not modify the configurationof the sense induction or disambiguation step.",
                    "sid": 74,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thefirst run used collocations extracted from ukWaCscored by LLR metric (wacky-llr), and two othersused Wikipedia as background corpus.",
                    "sid": 75,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One of theWP-based runs used PMI as association metric (wppmi), the other one used LLR (wpllr).",
                    "sid": 76,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The run onthe larger ukWaC corpus scored best with regard tothe Jaccard measure, but worst in the adjusted Randindex measure.",
                    "sid": 77,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We attribute low scores for ARI tothe fact that our system did not induce certain lessfrequent senses, resulting in small average numberof clusters.",
                    "sid": 78,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The coarse grained clusters however,have been assigned quite well by our WSD system,as shown by relatively high Jaccard Index.",
                    "sid": 79,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the 5The software used to create the DT is available fromhttp://www.jobimtext.org WP-based runs, the clustering based on PMI produced more clusters and therefore scored higher onthe F1 measure than the LLR-based run.",
                    "sid": 80,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From anexploratory analysis of the created clusters, we assume that the WP-based runs have a higher chanceto find more rare senses in this specific task, sincethe gold standard was also based on Wikipedia disambiguation pages.",
                    "sid": 81,
                    "ssid": 12,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}