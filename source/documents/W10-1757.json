{
    "ID": "W10-1757",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "We propose a new framework for N-best reranking on sparse feature sets.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The idea is to reformulate the reranking problem as a Multitask Learning problem, where each N-best list corresponds to a distinct task.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is motivated by the observation that N-best lists often show significant differences in feature distributions.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Training a single reranker directly on this heteroge- nous data can be difficult.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our proposed meta-algorithm solves this challenge by using multitask learning (such as \u21131/\u21132 regularization) to discover common feature representations across N- best lists.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This meta-algorithm is simple to implement, and its modular approach allows one to plugin different learning algorithms from existing literature.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a proof of concept, we show statistically significant improvements on a machine translation system involving millions of features.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Many natural language processing applications, such as machine translation (MT), parsing, and language modeling, benefit from the N-best reranking framework (Shen et al., 2004; Collins and Koo, 2005; Roark et al., 2007).",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The advantage of N-best reranking is that it abstracts away the complexities of first-pass decoding, allowing the researcher to try new features and learning algorithms with fast experimental turnover.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the N-best reranking scenario, the training data consists of sets of hypotheses (i.e. N-best lists) generated by a first-pass system, along with their labels.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a new N-best list, the goal is to rerank it such that the best hypothesis appears near the top of the list.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Existing research have focused on training a single reranker directly on the entire data.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This approach is reasonable if the data is homogenous, but it fails when features vary significantly across different N-best lists.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, when one employs sparse feature sets, one seldom finds features that are simultaneously active on multiple N-best lists.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this case, we believe it is more advantageous to view the N-best reranking problem as a multi- task learning problem, where each N-best list corresponds to a distinct task.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Multitask learning, a subfield of machine learning, focuses on how to effectively train on a set of different but related datasets (tasks).",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our heterogenous N-best list data fits nicely with this assumption.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The contribution of this work is threefold: 1.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We introduce the idea of viewing N-best.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "reranking as a multitask learning problem.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This view is particularly apt to any general reranking problem with sparse feature sets.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "we propose a simple meta-algorithm that. ",
            "number": "2",
            "sents": [
                {
                    "text": "first discovers common feature representations across N-bests (via multitask learning) before training a conventional reranker.",
                    "sid": 22,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus it is easily applicable to existing systems.",
                    "sid": 23,
                    "ssid": 2,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "we demonstrate that our proposed method. ",
            "number": "3",
            "sents": [
                {
                    "text": "outperforms the conventional reranking approach on a EnglishJapanese biomedical machine translation task involving millions of features.",
                    "sid": 24,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The paper is organized as follows: Section 2 describes the feature sparsity problem and Section 3 presents our multitask solution.",
                    "sid": 25,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The effectiveness of our proposed approach is validated by experiments demonstrated in Section 4.",
                    "sid": 26,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, Sections 5 and 6 discuss related work and conclusions.",
                    "sid": 27,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 The Problem of Sparse Feature Sets.",
                    "sid": 28,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For concreteness, we will describe N-best reranking in terms of machine translation (MT), though 375 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 375\u2013383, Uppsala, Sweden, 1516 July 2010.",
                    "sid": 29,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Qc 2010 Association for Computational Linguistics our approach is agnostic to the application.",
                    "sid": 30,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In MT reranking, the goal is to translate a foreign language sentence f into an English sentence e by picking from a set of likely translations.",
                    "sid": 31,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A standard approach is to use a linear model: e\u02c6 = arg max wT \u00b7 h(e, f ) (1) e\u2208N (f ) where h(e, f ) is a D-dimensional feature vector, w is the weight vector to be trained, and N (f ) is the set of likely translations of f , i.e. the N-best list.",
                    "sid": 32,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The feature h(e, f ) can be any quantity defined in terms of the sentence pair, such as translation model and language model probabilities.",
                    "sid": 33,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here we are interested in situations where the feature definitions can be quite sparse.",
                    "sid": 34,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A common methodology in reranking is to first design feature templates based on linguistic intuition and domain knowledge.",
                    "sid": 35,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then, numerous features are instantiated based on the training data seen.",
                    "sid": 36,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the work of (Watanabe et al., 2007) defines feature templates based on bilingual word alignments, which lead to extraction of heavily- lexicalized features of the form: \uf8f1 1 if foreign word \u201cMonsieur\u201d \uf8f4 sense ngrams will appear given a different test sentence.",
                    "sid": 37,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In summary, the following issues compound to create extremely sparse feature sets: 1.",
                    "sid": 38,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Feature templates are heavily-lexicalized,.",
                    "sid": 39,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "which causes the number of features to grow unbounded as the the amount of data increases.",
                    "sid": 40,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.",
                    "sid": 41,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The input (f ) has high variability (e.g. large.",
                    "sid": 42,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "vocabulary size), so that features for different inputs are rarely shared.",
                    "sid": 43,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.",
                    "sid": 44,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The N-best list output also exhibits high vari-.",
                    "sid": 45,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ability (e.g. many different word reorder- ings).",
                    "sid": 46,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Larger N may improve reranking performance, but may also increase feature sparsity.",
                    "sid": 47,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.",
                    "sid": 48,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our goal here is to address this situation.",
                    "sid": 49,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3 Proposed Reranking Framework.",
                    "sid": 50,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the following, we first give an intuitive com h(e, f ) = \uf8f2 \uf8f4 and English word \u201cMr.\u201d co-occur in e,f parison between single vs. multiple task learning (Section 3.1), before presenting the general meta \uf8f3 0 otherwise (2) algorithm (Section 3.2) and particular instantia One can imagine that such features are sparse because it may only fire for input sentences that contain the word \u201cMonsieur\u201d.",
                    "sid": 51,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all other input sentences, it is an useless, inactive feature.",
                    "sid": 52,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Another common feature involves word ngram templates, for example: \uf8f1 1 if English trigram tions (Section 3.3).",
                    "sid": 53,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.1 Single vs. Multiple Tasks.",
                    "sid": 54,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a set of I input sentences {f i}, the training data for reranking consists of a set of I N-best lists {(Hi, yi )}i=1,...,I , where Hi are features and yi are labels.",
                    "sid": 55,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To clarify the notation:1 for an input sentence h(e, f ) = \uf8f2 \u201cMr.",
                    "sid": 56,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Smith said\u201d occurs in e f i, there is a N-best list N (f i).",
                    "sid": 57,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For a N best list \uf8f3 0 otherwise (3) N (f i), there are N feature vectors corresponding to the N hypotheses, each with dimension D. The In this case, all possible trigrams seen in the N- best list are extracted as features.",
                    "sid": 58,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One can see that this kind of feature can be very sensitive to the first-pass decoder: if the decoder has loose reordering constraints, then we may extract exponentially many nonsense ngram features such as \u201cSmith said Mr.\u201d and \u201csaid Smith Mr.\u201d.",
                    "sid": 59,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Granted, the reranker training algorithm may learn that these nonsense ngrams are indicative of poor hypotheses, but it is unlikely that the exact same non collection of feature vectors for N (f i) is represented by Hi, which can be seen as a D \u00d7 N matrix.",
                    "sid": 60,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, the N -dimensional vector of labels yi indicates the translation quality of each hypothesis in N (f i).",
                    "sid": 61,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The purpose of the reranker training algorithm is to find good parameters from {(Hi, yi )}.",
                    "sid": 62,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 Generally we use bold font h to represent a vector, bold- capital font H to represent a matrix.",
                    "sid": 63,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Script h and h(\u00b7) may be scalar, function, or sentence (depends on context).",
                    "sid": 64,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The conventional method of training a single reranker (single task formulation) involves optimizing a generic objective such as: I arg min ) L(w, Hi , yi ) + \u03bb\u2126(w) (4) w i=1 where w \u2208 RD is the reranker trained on all lists, and L(\u00b7) is some loss function.",
                    "sid": 65,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2126(w) is an op tional regularizer, whose effect is traded-off by the constant \u03bb.",
                    "sid": 66,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the SVM reranker for MT (Shen et al., 2004) defines L(\u00b7) to be some 3.2 Proposed Meta-algorithm.",
                    "sid": 67,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We are now ready to present our general reranking meta-algorithm (see Algorithm 1), termed Reranking by Multitask Learning (RML).",
                    "sid": 68,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Algorithm 1 Reranking by Multitask Learning Input: N-best data {(Hi , yi )}i=1,...,I Output: Common feature representation hc(e, f ) and weight vector wc 1: [optional] RandomHashing({Hi }) 2: W = MultitaskLearn({(Hi , yi )}) 3: hc = ExtractCommonFeature(W) function of sentence-level BLEU score, and \u2126(w) i i to be the large margin regularizer.2 On the other hand, multitask learning involves solving for multiple weights, w1, w2 , . . .",
                    "sid": 69,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", wI , one for each N-best list.",
                    "sid": 70,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One class of multitask learning algorithms, Joint Regularization, solves the following objective: I 4: {Hc} = RemapFeature({H }, hc ) 5: wc = ConventionalReranker({(Hi , yi )}) The first step, random hashing, is optional.",
                    "sid": 71,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Random hashing is an effective trick for reducing the dimension of sparse feature sets without suffering losses in fidelity (Weinberger et al., 2009; Ganchev and Dredze, 2008).",
                    "sid": 72,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It works by collaps arg min ) L(wi, Hi, yi ) + \u03bb\u2126(w1, .., wI ) ing random subsets of features.",
                    "sid": 73,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This step can be w1 ,..,wI i=1 (5) performed to speedup multitask learning later.",
                    "sid": 74,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In some cases, the original feature dimension may beThe loss decomposes by task but the joint regu larizer \u2126(w1, .., wI ) couples together the different weight parameters.",
                    "sid": 75,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The key is to note that multiple weights allow the algorithm to fit the heteroge nous data better, compared to a single weight vector.",
                    "sid": 76,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Yet these weights are still tied together so that some information can be shared across N-best lists (tasks).",
                    "sid": 77,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One instantiation of Eq. 5 is \u21131 /\u21132 regular- ization: \u2126(w1 , .., wI ) A ||W||1,2 , where W = [w1 |w2| . . .",
                    "sid": 78,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "|wI ]T is a I -by-D matrix of stacked weight vectors.",
                    "sid": 79,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The norm is computed by first taking the 2-norm on columns of W, then taking a 1-norm on the resulting D-length vector.",
                    "sid": 80,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This encourages the optimizer to choose a small subset of features that are useful across all tasks.",
                    "sid": 81,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, suppose two different sets ofweight vectors Wa and Wb for a 2 lists, 4 fea tures reranking problem.",
                    "sid": 82,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The \u21131/\u21132 norm for Wa is 14; the \u21131 /\u21132 norm for Wb is 12.",
                    "sid": 83,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If both have the same loss L(\u00b7) in Eq. 5, the multitask opti mizer would prefer Wb since more features are shared: so large that hashed representations may be necessary.",
                    "sid": 84,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The next two steps are key.",
                    "sid": 85,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A multitask learning algorithm is run on the N-best lists, and a common feature space shared by all lists is extracted.",
                    "sid": 86,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, if one uses the multitask objective of Eq. 5, the result of step 2 is a set of weights W. ExtractCommonFeature(W) then returns the feature id\u2019s (either from original or hashed representation) that receive nonzero weight in any of W.3 The new features hc(e, f ) are expected to have lower dimension than the original features h(e, f ).",
                    "sid": 87,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Section 3.3 describes in detail different multitask methods that can be plugged-in to this step.",
                    "sid": 88,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The final two steps involve a conventional reranker.",
                    "sid": 89,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In step 4, we remap the N-best list data according to the new feature representations hc(e, f ).",
                    "sid": 90,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In step 5, we train a conventional reranker on this common representation, which by now should have overcome sparsity issues.",
                    "sid": 91,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using a conventional reranker at the end allows us Wa : \u00bb 4 0 0 3 \u2013 0 4 3 0 Wb : \u00bb 4 3 0 0 \u2013 0 4 3 0 to exploit existing reranker s designed for specific NLP applicati ons.",
                    "sid": 92,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In a sense, our meta algorith m simply involves a change of representation for the conventional reranking scenario, where the 2 In MT, evaluation metrics like BLEU do not exactly de-.",
                    "sid": 93,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "compose across sentences, so for some training algorithms this loss is an approximation.",
                    "sid": 94,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3 For example in Wb , features 13 have nonzero weights.",
                    "sid": 95,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "and are extracted.",
                    "sid": 96,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Feature 4 is discarded.",
                    "sid": 97,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "new representation is found by multitask methods 0 which are well-suited to heterogenous data.",
                    "sid": 98,
                    "ssid": 75,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u22121 \u22122 3.3 Multitask Objective Functions 10.",
                    "sid": 99,
                    "ssid": 76,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here, we describe various multitask methods that can be plugged in Step 2 of Algorithm 1.",
                    "sid": 100,
                    "ssid": 77,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our goal is to demonstrate that a wide range of existing methods from the multitask learning literature can be brought to our problem.",
                    "sid": 101,
                    "ssid": 78,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We categorize multi \u22123 10 \u22124 10 \u22125 10 \u22126 10 \u22127 10 0 1 2 3 4 task methods into two major approaches: 10 10 10 10 10 x 1.",
                    "sid": 102,
                    "ssid": 79,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Joint Regularization: Eq. 5 is an example of joint regularization, with \u21131/\u21132 norm being a particular regularizer.",
                    "sid": 103,
                    "ssid": 80,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The idea is to use the regularizer to ensure that the learned functions of related tasks are close to each other.",
                    "sid": 104,
                    "ssid": 81,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The popular \u21131/\u21132 objective can be optimized by various methods, such as boosting (Obozinski et al., 2009) and convex programming (Argyriou et al., 2008).",
                    "sid": 105,
                    "ssid": 82,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Yet another regularizer is the \u21131/\u2113\u221e norm (Quattoni et al., 2009), which replaces the 2-norm with a max.",
                    "sid": 106,
                    "ssid": 83,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One could also define a regularizer to ensure that each task-specific wi is close to some averageparameter, e.g. Li ||wi \u2212 wavg ||2 . If we interpret wavg as a prior, we begin to see links to Hier archical Bayesian methods for multitask learning (Finkel and Manning, 2009; Daume, 2009).",
                    "sid": 107,
                    "ssid": 84,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.",
                    "sid": 108,
                    "ssid": 85,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Shared Subspace: This approach assumes that there is an underlying feature subspace that is common to all tasks.",
                    "sid": 109,
                    "ssid": 86,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Early works on multi- task learning implement this by neural networks, where different tasks have different output layers but share the same hidden layer (Caruana, 1997).",
                    "sid": 110,
                    "ssid": 87,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Another method is to write the weight vector as two parts w = [u; v] and let the task-specific function be uT \u00b7 h(e, f ) + vT \u00b7 \u0398 \u00b7 h(e, f ) (Ando and Zhang, 2005).",
                    "sid": 111,
                    "ssid": 88,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u0398 is a D\u2032 \u00d7 D matrix that maps the original features to a subspace common to all tasks.",
                    "sid": 112,
                    "ssid": 89,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The new feature representation is computed by the projection hc(e, f ) A \u0398 \u00b7 h(e, f ).",
                    "sid": 113,
                    "ssid": 90,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Multitask learning is a vast field and relates to areas like collaborative filtering (Yu and Tresp, 2005) and domain adaptation.",
                    "sid": 114,
                    "ssid": 91,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most methods assume some common representation and is thus applicable to our framework.",
                    "sid": 115,
                    "ssid": 92,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The reader is urged to refer to citations in, e.g.",
                    "sid": 116,
                    "ssid": 93,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Argyriou et al., 2008) for a survey.",
                    "sid": 117,
                    "ssid": 94,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "experiments and results. ",
            "number": "4",
            "sents": [
                {
                    "text": "As a proof of concept, we perform experiments on a MT system with millions of features.",
                    "sid": 118,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use a hierarchical phrase-based system (Chiang, Figure 1: This log-log plot shows that there are many rare features and few common features.",
                    "sid": 119,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The probability that a feature occurs in x number of N best lists behaves according to the power-law x\u2212\u03b1 , where \u03b1 = 2.28.",
                    "sid": 120,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2007) to generate N-best lists (N=100).",
                    "sid": 121,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sparse features used in reranking are extracted according to (Watanabe et al., 2007).",
                    "sid": 122,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Specifically, the majority are lexical features involving joint occurrences of words within the N-best lists and source sentences.",
                    "sid": 123,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is worth noting that the fact that the first pass system is a hierarchical system is not essential to the feature extraction step; similar features can be extracted with other systems as first-pass, e.g. a phrase-based system.",
                    "sid": 124,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That said, the extent of the feature sparsity problem may depend on the performance of the first-pass system.",
                    "sid": 125,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We experiment with medical domain MT, where large numbers of technical vocabulary cause sparsity challenges.",
                    "sid": 126,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our corpora consists of English abstracts from PubMed4 with their Japanese translations.",
                    "sid": 127,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first-pass system is built on hierarchical phrases extracted from 17k sentence pairs and target (Japanese) language models trained on 800k medical-domain sentences.",
                    "sid": 128,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our reranking experiments, we used 500 lists as the training set5 , 500 lists as held-out, and another 500 for test.",
                    "sid": 129,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.1 Data Characteristics.",
                    "sid": 130,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We present some statistics to illustrate the feature sparsity problem: From 500 N-best lists, we extracted a total of 2.4 million distinct features.",
                    "sid": 131,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By type, 75% of these features occur in only one N- best list in the dataset.",
                    "sid": 132,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Less than 3% of features 4 A database of the U.S. National Library of Medicine.",
                    "sid": 133,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 In MT, training data for reranking is sometimes referred to as \u201cdev set\u201d to distinguish from the data used in first-pass.",
                    "sid": 134,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Also, while the 17k bitext may seem small compared to other MT work, we note that 1st pass translation quality (around 28 BLEU) is high enough to evaluate reranking methods.",
                    "sid": 135,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "occur in ten or more lists.",
                    "sid": 136,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The distribution of feature occurrence is clearly Zipfian, as seen in the power-law plot in Figure 1.We can also observe the feature growth rate (Ta ble 1).",
                    "sid": 137,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is the number of new features introduced when an additional N-best list is seen.",
                    "sid": 138,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is important to note that on average, 2599 new features are added everytime a new N-best list is seen.",
                    "sid": 139,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is as much as 2599/4188 = 62% of the active features.",
                    "sid": 140,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Imagine an online training algorithm (e.g. MIRA or perceptron) on this kind of data: whenever a loss occurs and we update the weight vector, less than half of the weight vector update applies to data we have seen thus far.",
                    "sid": 141,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Herein lies the potential for overfitting.",
                    "sid": 142,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From observing the feature grow rate, one may hypothesize that adding large numbers of N-best lists to the training set (500 in the experiments here) may not necessarily improve results.",
                    "sid": 143,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While adding data potentially improves the estimation process, it also increases the feature space dramatically.",
                    "sid": 144,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus we see the need for a feature extraction procedure.",
                    "sid": 145,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Watanabe et al., 2007) also reports the possibility of overfitting in their dataset (ArabicEnglish newswire translation), especially when domain differences are present.",
                    "sid": 146,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here we observe this tendency already on the same domain, which is likely due to the highly-specialized vocabulary and the complex sentence structures common in research paper abstracts.",
                    "sid": 147,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.2 MT Results.",
                    "sid": 148,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our goal is to compare different feature representations in reranking: The baseline reranker uses the original sparse feature representation.",
                    "sid": 149,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is compared to feature representations discovered by three different multitask learning methods: \u2022 Joint Regularization (Obozinski et al., 2009) \u2022 Shared Subspace (Ando and Zhang, 2005) \u2022 Unsupervised Multitask Feature Selection (Abernethy et al., 2007).6 We use existing implementations of the above methods.7 The conventional reranker (Step 5, Al 6 This is not a standard multitask algorithm since most multitask algorithms are supervised.",
                    "sid": 150,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We include it to see if unsupervised or semi-supervised multitask algorithms is promising.",
                    "sid": 151,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Intuitively, the method tries to select subsets of features that are correlated across multiple tasks using ra n- dom sampling (MCMC).",
                    "sid": 152,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Features that co-occur in different tasks form a high probability path.",
                    "sid": 153,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7 Available at http://multitask.cs.berkeley.edu.",
                    "sid": 154,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "N be st id # Ne w Ft #S oF ar # Ac tiv e 1 2 3 4 5 6 ....",
                    "sid": 155,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "10 0 10 1 10 2 10 3 3 9 0 0 7 5 3 5 6 0 7 8 3 8 6 8 1 8 9 6 3 5 4 2 2 4 4 0 1 6 3 9 3 4 6 8 2 3 5 0 3 9 0 0 1 1 4 3 5 1 7 5 1 3 2 1 3 8 1 2 3 2 7 7 2 6 8 1 9 2 8 9 1 1 8 2 9 0 7 5 7 2 9 4 2 2 5 2 9 6 5 7 5 3 9 0 0 7 9 1 3 7 0 8 7 4 7 4 7 2 6 4 5 4 7 4 7 4 2 9 9 2 3 9 0 4 7 5 5 3 8 2 4 Av er ag e 2 5 9 9 \u2013 4 1 8 8 Table 1: Feature growth rate: For N-best list i in the table, we have (#NewFt = number of new fea tures introduced since N-best i \u2212 1) ; (#SoFar =Total number of features defined so far); and (#Ac tive = number of active features for N-best i).",
                    "sid": 156,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "E.g., we extracted 7535 new features from N-best 2; combined with the 3900 from N-best 1, the total features so far is 11435.",
                    "sid": 157,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "gorithm 1) used in all cases is SVMrank.8 Our initial experiments show that the SVM baseline performance is comparable to MIRA training, so we use SVM throughout.",
                    "sid": 158,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The labels for the SVM are derived as in (Shen et al., 2004), where top 10% of hypotheses by smoothed sentence-BLEU is ranked before the bottom 90%.",
                    "sid": 159,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All multitask learning methods work on hashed features of dimension 4000 (Step 1, Algorithm 1).",
                    "sid": 160,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This speeds up the training process.",
                    "sid": 161,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All hyperparameters of the multitask method are tuned on the held-out set.",
                    "sid": 162,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, the most important is the number of common features to extract, which we pick from {250, 500, 1000}.",
                    "sid": 163,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 shows the results by BLEU (Papineni et al., 2002) and PER.",
                    "sid": 164,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Oracle results are obtained by choosing the best hypothesis per N-best list by sentence-level BLEU, which achieved 36.9 BLEU in both Train and Test.",
                    "sid": 165,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A summary of our observations is: 1.",
                    "sid": 166,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The baseline (All sparse features) overfits.",
                    "sid": 167,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It.",
                    "sid": 168,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "achieves the oracle BLEU score on the train set (36.9) but performs poorly on the test (28.6).",
                    "sid": 169,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.",
                    "sid": 170,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similar overfitting occurs when traditional \u21131.",
                    "sid": 171,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "regularization is used to select features on 8 Available at http://svmlight.joachims.org.",
                    "sid": 172,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "the sparse feature representation9 . \u21131 regularization is a good method of handling sparse features for classification problems, but in reranking the lack of tying between lists makes this regularizer inappropriate.",
                    "sid": 173,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A small set of around 1200 features are chosen: they perform well independently on each task in the training data, but there is little sharing with the test data.",
                    "sid": 174,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.",
                    "sid": 175,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All three multitask methods obtained features.",
                    "sid": 176,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "that outperformed the baseline.",
                    "sid": 177,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The BLEU scores are 28.8, 28.9, 29.1 for Unsupervised Feature Selection, Joint Regularization, and Shared Subspace, respectively, which all outperform the 28.6 baseline.",
                    "sid": 178,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All improvements are statistically significant by bootstrap sampling test (1000 samples, p < 0.05) (Zhang et al., 2004).",
                    "sid": 179,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.",
                    "sid": 180,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Shared Subspace performed the best.",
                    "sid": 181,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We.",
                    "sid": 182,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "conjecture this is because its feature projection can create new feature combinations that is more expressive than the feature selection used by the two other methods.",
                    "sid": 183,
                    "ssid": 66,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "per results are qualitatively similar to bleu. ",
            "number": "5",
            "sents": [
                {
                    "text": "results.",
                    "sid": 184,
                    "ssid": 1,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "as a further analysis, we are interested in see-. ",
            "number": "6",
            "sents": [
                {
                    "text": "ing whether multitask learning extracts novel features, especially those that have low frequency.",
                    "sid": 185,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, we tried an additional feature representation (feature threshold) which only keeps features that occur in more than x N- bests, and concatenate these high-frequency features to the multitask features.",
                    "sid": 186,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The feature threshold alone achieves nice BLEU results (29.0 for x > 10), but the combination outperforms it by statistically significant margins (29.329.6).",
                    "sid": 187,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This implies that multitask learning is extracting features that complement well with high frequency features.",
                    "sid": 188,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the multitask features, improvements of 0.2 to 1.0 BLEU are modest but consistent.",
                    "sid": 189,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 2 shows the BLEU of bootstrap samples obtained as part of the statistical significance test.",
                    "sid": 190,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We see that multitask almost never underperform baseline in any random sampling of the data.",
                    "sid": 191,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This implies that the proposed meta-algorithm is very sta ble, i.e. it is not a method that sometimes improves and sometimes degrades.",
                    "sid": 192,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, a potential question to ask is: what kinds of features are being selected by the multitask learning algorithms?",
                    "sid": 193,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We found that that two kinds of features are usually selected: one is general features that are not lexicalized, such as \u201ccount of phrases\u201d, \u201ccount of deletions/insertions\u201d, \u201cnumber of punctuation marks\u201d.",
                    "sid": 194,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The other kind is lexicalized features, such as those in Equations 2 and 3, but involving functions words (like the Japanese characters \u201cwa\u201d, \u201cga\u201d, \u201cni\u201d, \u201cde\u201d) or special characters (such as numeral symbol and punctuation).",
                    "sid": 195,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These are features that can be expected to be widely applicable, and it is promising that multitask learning is able to recover these from the millions of potential features.",
                    "sid": 196,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "10 300 250 200 150 100 50 0 \u22120.2 0 0.2 0.4 0.6 0.8 1 1.2 BLEU(shared subspace)\u2212BLEU(baseline sparse feature) Figure 2: BLEU difference of 1000 bootstrap samples.",
                    "sid": 197,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "95% confidence interval is [.15, .90] The proposed approach therefore seems to be a stable method.",
                    "sid": 198,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 Related Work in NLP.",
                    "sid": 199,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previous reranking work in NLP can be classified into two different research focuses: 1.",
                    "sid": 200,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Engineering better features: In MT, (Och and others, 2004) investigates features extracted from a wide variety of syntactic representations, such as parse tree probability on the outputs.",
                    "sid": 201,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although their results show that the proposed syntactic features gave little improvements, they point to some potential reasons, such as domain mismatch for the parser and overfitting by the reranking 10 Note: In order to do this analysis, we needed to run Joint Regularization on the original feature representation, since the hashed representations are less interpretable.",
                    "sid": 202,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This turns out to be computationally prohibitive in the time being so we only ran on a smaller data set of 50 lists.",
                    "sid": 203,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recently new optimization methods that are orders of magnitude faster have 9 O p t i m i z e d by the Vo wp al Wa bbi t too lkit : be en de ve lo pe d (L iu et al. , 20 09 ), w hi ch m ak es lar ger sc al e http: //hu nch.",
                    "sid": 204,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "net/ vw/ ex pe ri m en ts po ssi bl e. Fe at ur e R ep re se nt ati on #F ea tu re Tr ai n B L E U Te st B L E U Te st P E R (b as eli ne s) Fir st pa ss Al l sp ars e fe at ur es ( M ai n ba sel in e) Al l sp ars e fe at ur es w/ \u21131 re gu lar iza tio n Ra nd o m ha sh re pr es en tat io n 20 2 . 4 M 1 2 0 0 4 0 0 0 29 .5 36 .9 36 .5 33 .0 28 .5 28 .6 28 .5 28 .5 38 .3 38 .2 38 .6 38 .2 (m ult ita sk le ar ni ng ) U ns up er vis ed Fe at ur eS ele ct Joi nt Re gu lar iza tio n Sh ar ed Su bs pa ce 5 0 0 2 5 0 1 0 0 0 32 .0 31 .8 32 .9 28 .8 28 .9 29 .1 37 .7 37 .5 37 .3 (c o m bi na tio n w/ hi gh fre qu en cy fe at ur es) (a) Fe at ur e thr es ho ld x > 10 0 (b) Fe at ur e thr es ho ld x > 10 U n s u p e r v i s e d F e a t u r e S e l e c t + ( b ) J o i n t R e g u l a r i z a t i o n + ( b ) Sh ar ed Su bs pa ce + (b) 3k 6 0 k 6 0 . 5 k 60 .2 5k 6 1 k 31 .7 35 .8 36 .2 36 .1 36 .2 27 .9 29 .0 29 .3 29 .4 29 .6 38 .2 37 .9 37 .6 37 .5 37 .3 Or acl e (b est po ssi bl e) \u2013 36 .9 36 .9 33 .1 Table 2: Results for different feature sets, with corresponding feature size and train/test BLEU/PER.",
                    "sid": 205,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All multitask features give statistically significant improvements over the baselines (boldfaced), e.g. Shared Subspace: 29.1 BLEU vs Baseline: 28.6 BLEU.",
                    "sid": 206,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Combinations of multitask features with high frequency features also give significant improvements over the high frequency features alone.",
                    "sid": 207,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "method.",
                    "sid": 208,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.",
                    "sid": 209,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Evaluation campaigns like WMT (CallisonBurch et al., 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks.",
                    "sid": 210,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.",
                    "sid": 211,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Designing better training algorithms: N- best reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f.",
                    "sid": 212,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Bakir et al., 2007)) can be applied.",
                    "sid": 213,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process.",
                    "sid": 214,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al., 2005).",
                    "sid": 215,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The division into two research focuses is convenient, but may be suboptimal if the training algorithm and features do not match well together.",
                    "sid": 216,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our work can be seen as reconnecting the two focuses, where the training algorithm is explicitly used to help discover better features.",
                    "sid": 217,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Multitask learning is currently an active subfield within machine learning.",
                    "sid": 218,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on part- of-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements.",
                    "sid": 219,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Deselaers et al., 2009) applies similar methods for machine transliteration.",
                    "sid": 220,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009).",
                    "sid": 221,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our work can be seen as following the same philosophy, but applied to N-best lists.",
                    "sid": 222,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data.",
                    "sid": 223,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP.",
                    "sid": 224,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We expect that more novel applications of multitask learning will appear in NLP as the techniques become scal- able and standard.",
                    "sid": 225,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6 Discussion and Conclusion.",
                    "sid": 226,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "N-best reranking is a beneficial framework for experimenting with large feature sets, but unfortunately feature sparsity leads to overfitting.",
                    "sid": 227,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We addressed this by recasting N-best lists as multitask learning data.",
                    "sid": 228,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our MT experiments show consistent statistically significant improvements.",
                    "sid": 229,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From the Bayesian view, multitask formulation of N-best lists is actually very natural: Each N- best is generated by a different data-generating distribution since the input sentences are different,i.e. p(e|f 1) j= p(e|f 2).",
                    "sid": 230,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Yet these N-bests are re lated since the general p(e|f ) distribution depends on the same first-pass models.",
                    "sid": 231,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The multitask learning perspective opens up interesting new possibilities for future work, e.g.: \u2022 Different ways to partition data into tasks, e.g. clustering lists by document structure, or hierarchical clustering of data \u2022 Multitask learning on lattices or N-best lists with larger N. It is possible that a larger hypothesis space may improve the estimation of task-specific weights.",
                    "sid": 232,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 Comparing multitask learning to sparse online learning of batch data, e.g.",
                    "sid": 233,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Tsuruoka et al., 2009).",
                    "sid": 234,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 Modifying the multitask objective to incorporate application-specific loss/decoding, such as Minimum Bayes Risk (Kumar and Byrne, 2004) \u2022 Using multitask learning to aid large-scale feature engineering and visualization.",
                    "sid": 235,
                    "ssid": 51,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments",
            "number": "",
            "sents": [
                {
                    "text": "We have received numerous helpful comments throughout the course of this work.",
                    "sid": 236,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, we would like to thank Albert Au Yeung, Jun Suzuki, Shinji Watanabe, and the three anonymous reviewers for their valuable suggestions.",
                    "sid": 237,
                    "ssid": 53,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}