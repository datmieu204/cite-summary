{
    "ID": "W10-1704",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "This paper describes our Statistical Machine Translation systems for the WMT10 evaluation, where LIMSI participated for two language pairs (French-English and GermanEnglish, in both directions).",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For GermanEnglish, we concentrated on normalizing the German side through a proper preprocessing, aimed at reducing the lexical redundancy and at splitting complex compounds.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For French-English, we studied two extensions of our in-house N -code decoder: firstly, the effect of integrating a new bilingual reordering model; second, the use of adaptation techniques for the translation model.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For both set of experiments, we report the improvements obtained on the development and test data.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "LIMSI took part in the WMT 2010 evaluation campaign and developed systems for two languages pairs: French-English and GermanEnglish in both directions.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For GermanEnglish, we focused on preprocessing issues and performed a series of experiments aimed at normalizing the German side by removing some of the lexical redundancy and by splitting compounds.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this pair, all the experiments were performed using the Moses decoder (Koehn et al., 2007).",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For French- English, we studied two extensions of our n-gram based system: first, the effect of integrating a new bilingual reordering model; second, the use of adaptation techniques for the translation model.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Decoding is performed using our in-house N -code (Marin\u02dc o et al., 2006) decoder.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "system architecture and resources. ",
            "number": "2",
            "sents": [
                {
                    "text": "In this section, we describe the main characteristics of the phrase-based systems developed for this evaluation and the resources that were used to train our models.",
                    "sid": 10,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As far as resources go, we used all the data supplied by the 2010 evaluation organizers.",
                    "sid": 11,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Based on our previous experiments (De\u00b4chelotte et al., 2008) which have demonstrated that better normalization tools provide better BLEU scores (Pap- ineni et al., 2002), we took advantage of our in- house text processing tools for the tokenization and detokenization steps.",
                    "sid": 12,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Only for German data did we used the TreeTagger (Schmid, 1994) tokenizer.",
                    "sid": 13,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similar to last year\u2019s experiments, all of our systems are built in \u201dtrue-case\u201d.",
                    "sid": 14,
                    "ssid": 5,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "german-english systems. ",
            "number": "3",
            "sents": [
                {
                    "text": "As German is morphologically more complex than English, the default policy which consists in treating each word form independently from the others is plagued with data sparsity, which poses a number of difficulties both at training and decoding time.",
                    "sid": 15,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When aligning parallel texts at the word level, German compound words typically tend to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable.",
                    "sid": 16,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In decoding, new compounds or unseen morphological variants of existing words artificially increase the number out- of-vocabulary (OOV) forms, which severely hurts the overall translation quality.",
                    "sid": 17,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Several researchers have proposed normalization (Niessen and Ney, 2004; Corstonoliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods.",
                    "sid": 18,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques.",
                    "sid": 19,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.1 Reducing the lexical redundancy.",
                    "sid": 20,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) 54 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 54\u201359, Uppsala, Sweden, 1516 July 2010.",
                    "sid": 21,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Qc 2010 Association for Computational Linguistics Input POS Lemma Analysis In APPR in APPR.In der* ART d ART.Def.Dat.Sg.Fem Folge NN Folge N.Reg.Dat.Sg.Fem befand VVFIN befinden VFIN.Full.3.Sg.Past.Ind die* ART d ART.Def.Nom.Sg.Fem derart ADV derart ADV gesta\u00a8rkte* ADJA gesta\u00a8rkt ADJA.Pos.Nom.Sg.Fem Justiz NN Justiz N.Reg.Nom.Sg.Fem wiederholt ADJD wiederholt ADJD.Pos gegen APPR gegen APPR.Acc die* ART d ART.Def.Acc.Sg.Fem Regierung NN Regierung N.Reg.Acc.Sg.Fem und KON und CONJ.Coord.-2 insbesondere ADV insbesondere ADV gegen APPR gegen APPR.Acc deren* PDAT d PRO.Dem.Subst.-3.Gen.Sg.Fem Geheimdienste* NN Geheimdienst N.Reg.Acc.Pl.Masc . $.",
                    "sid": 22,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "SYM.Pun.Sent Table 1: TreeTagger and RFTagger outputs.",
                    "sid": 23,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Starred word forms are modified during preprocessing.",
                    "sid": 24,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "so as to satisfy agreement constraints.",
                    "sid": 25,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Inflections vary according to gender, case, and number information.",
                    "sid": 26,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, the German definite determiner could be marked in sixteen different ways according to the possible combinations of genders (3), case (4) and number (2)1, which are fused in six different tokens der, das, die, den, dem, des.",
                    "sid": 27,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With the exception of the plural and genitive cases, all these words translate to the same English word: the.",
                    "sid": 28,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to reduce the size of the German vocabulary and to improve the robustness of the alignment probabilities, we considered various normalization strategies for the different word classes.",
                    "sid": 29,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In a nutshell, normalizing amounts to collapsing several German forms of a given lemma into a unique representative, using manually written normalization patterns.",
                    "sid": 30,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A pattern typically specifies which forms of a given morphological paradigm should be considered equivalent when translating into English.",
                    "sid": 31,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.",
                    "sid": 32,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 1 displays the analysis of an example sentence.",
                    "sid": 33,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 In most cases, normalization patterns replace a word form by its lemma; in order to partially pre 1 For the plural forms, gender distinctions are neutralized and the same 4 forms are used for all genders . 2 The English reference: Subsequently , the energized judiciary continued ruling against government decisions , embarrassing the government \u2013 especially its intelligence agencies . serve some inflection marks, we introduced two generic suffixes, +s and +en which respectively denote plural and genitive wherever needed.",
                    "sid": 34,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Typical normalization rules take the following form:\u2022 For articles, adjectives, and pronouns (Indef inite , possessive, demonstrative, relative and reflexive), if a token has; \u2013 Genitive case: replace with lemma+en (Ex.",
                    "sid": 35,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "des, der, des, der \u2192 d+en) \u2013 Plural number: replace with lemma+s (Ex.",
                    "sid": 36,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "die, den \u2192 d+s)\u2013 All other gender, case and number: re place with lemma (Ex.",
                    "sid": 37,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "der, die, das, die \u2192 d) \u2022 For nouns; \u2013 Plural number: replace with lemma+s (Ex.",
                    "sid": 38,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Bilder, Bildern, Bilder \u2192 Bild+s)) \u2013 All other gender and case: replace with lemma (Ex Bild, Bilde, Bildes \u2192 Bild; Using these tags, a normalized version of previous sentence is as follows: In d Folge befand d derart gesta\u00a8 rkt Justiz wiederholt gegen d Regierung und insbesondere gegen d+en Geheimdienst+s. Several experiments were carried out to assess the effect of different normalization schemes.",
                    "sid": 39,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Removing all gender and case information, except for the genitive for articles, adjectives and pronouns, allowed to achieve the best BLEU scores.",
                    "sid": 40,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2 Compound Splitting.",
                    "sid": 41,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Combining nouns, verbs and adjectives to forge new words is a very common process in German.",
                    "sid": 42,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It partly explains the difference between the number of types and tokens between English and German in parallel texts.",
                    "sid": 43,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In most cases, compounds are formed by a mere concatenation of existing word forms, and can easily be split into simpler units.",
                    "sid": 44,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As words are freely conjoined, the vocabulary size increases vastly, yielding to sparse data problems that turn into unreliable parameter estimates.",
                    "sid": 45,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used the frequency-based segmentation algorithm initially introduced in (Koehn and Knight, 2003) to handle compounding.",
                    "sid": 46,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our implementation extends this technique to handle the most common letter fillers at word junctions.",
                    "sid": 47,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our experiments, we investigated different splitting schemes in a manner similar to the work of (Stymne, 2008).",
                    "sid": 48,
                    "ssid": 34,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "french-english systems. ",
            "number": "4",
            "sents": [
                {
                    "text": "4.1 Baseline N -coder systems.",
                    "sid": 49,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this language pair, we used our in-house N -code system, which implements the n-gram- based approach to SMT.",
                    "sid": 50,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In a nutshell, the translation model is implemented as a stochastic finite- state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004).",
                    "sid": 51,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Training this model requires to reorder source sentences so as to match the target word order.",
                    "sid": 52,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is performed by a stochastic finite- state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities.",
                    "sid": 53,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to the translation model, our system implements eight feature functions which are optimally combined using a discriminative training framework (Och, 2003): a target-language model; two lexicon models, which give complementary translation scores for each tuple; two lexicalized reordering models aiming at predicting the orientation of the next translation unit; previous (respectively next phrase pair).",
                    "sid": 54,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our implementation, we modified the three orientation types originally introduced and consider: a consecutive type, where the original monotone and swap orientations are lumped together, a forward type, specifying a discontiguous forward orientation, and a backward type, specifying a discontiguous backward orientation.",
                    "sid": 55,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Empirical results showed that in our case, the new orientations slightly outperform the original ones.",
                    "sid": 56,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This may be explained by the fact that the model is applied over tuples instead of phrases.",
                    "sid": 57,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Counts of these three types are updated for each unit collected during the training process.",
                    "sid": 58,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given these counts, we can learn probability dis tributions of the form pr (orientation|(st)) where orientation \u2208 {c, f, b} (consecutive, forward and backward) and (st) is a translation unit.",
                    "sid": 59,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Counts are typically smoothed for the estimation of the probability distribution.",
                    "sid": 60,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The overall search process is performed by our in-house n-code decoder.",
                    "sid": 61,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It implements a beam- search strategy on top of a dynamic programming algorithm.",
                    "sid": 62,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process.",
                    "sid": 63,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The resulting reordering hypotheses are passed to the decoder in the form of word lattices (Crego and no, 2006).",
                    "sid": 64,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.2 A bilingual POS-based reordering model For this year evaluation, we also experimented with an additional reordering model, which is estimated as a standard n-gram language model, over generalized translation units.",
                    "sid": 65,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the experiments reported below, we generalized tuples using POS tags, instead of raw word forms.",
                    "sid": 66,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1 displays the same sequence of tuples when built from surface word forms (top), and from POS tags (bottom).",
                    "sid": 67,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "a \u2019weak\u2019 distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations.",
                    "sid": 68,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One novelty this year are the introduction of lexicalized reordering mod els (Tillmann, 2004).",
                    "sid": 69,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Such models require to estimate reordering probabilities for each phrase pairs, typically distinguishing three case, depending whether the current phrase is translated monotone, swapped or discontiguous with respect to the 3 Part-of-speech information for English and French is computed using the above mentioned TreeTagger.",
                    "sid": 70,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1: Sequence of units built from surface word forms (top) and POS-tags (bottom).",
                    "sid": 71,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Generalizing units greatly reduces the number of symbols in the model and enables to take larger n-gram contexts into account: in the experiments reported below, we used up to 6-grams.",
                    "sid": 72,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This new model is thus helping to capture the mid-range syntactic reorderings that are observed in the training corpus.",
                    "sid": 73,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This model can also be seen as a translation model of the sentence structure.",
                    "sid": 74,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It models the adequacy of translating sequences of source POS tags into target POS tags.",
                    "sid": 75,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additional details on these new reordering models can be found in (Crego and Yvon, 2010).",
                    "sid": 76,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.3 Combining translation models.",
                    "sid": 77,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our main translation model being a conventional n-gram model over bilingual units, it can directly take advantage of all the techniques that exist for these models.",
                    "sid": 78,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To take the diversity of the available parallel corpora into account, we independently trained several translation models on subpart of the training data.",
                    "sid": 79,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These translation models were then linearly interpolated, where the interpolation weights are chosen so as to minimize the perplexity on the development set.",
                    "sid": 80,
                    "ssid": 32,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "language models. ",
            "number": "5",
            "sents": [
                {
                    "text": "The English and French language models (LMs) are the same as for the last year\u2019s French-English task (Allauzen et al., 2009) and are heavily tuned to the newspaper/newswire genre, using the first part of the WMT09 official development data (dev2009a).",
                    "sid": 81,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used all the authorized news corpora, including the French and English Gigaword corpora, for translating both into French (1.4 billion tokens) and English (3.7 billion tokens).",
                    "sid": 82,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To estimate such LMs, a vocabulary was defined for both languages by including all tokens in the WMT parallel data.",
                    "sid": 83,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This initial vocabulary of 130K words was then extended with the most frequent words observed in the training data, yielding a vocabulary of one million words in both languages.",
                    "sid": 84,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The training data was divided into several sets based on dates and genres (resp.",
                    "sid": 85,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7 and 9 sets for English and French).",
                    "sid": 86,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On each set, a standard 4-gram LM was estimated from the 1M word vocabulary with in-house tools using KneserNey discounting interpolated with lower order models (Kneser and Ney, 1995; Chen and Goodman, 1998)4.",
                    "sid": 87,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The resulting LMs were then linearly combined using interpolation coefficients 4 Given the amount of training data, the use of the modified KneserNey smoothing is prohibitive while previous experiments did not show significant improvements.",
                    "sid": 88,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "chosen so as to minimize perplexity of the development set (dev2009a).",
                    "sid": 89,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The final LMs were finally pruned using perplexity as pruning criterion (Stolcke, 1998).",
                    "sid": 90,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For German, since we have less training data, we only used the German monolingual texts (Europarlv5, News Commentary and News Monolingual) provided by the organizers to train a single n-gram language model, with modified KneserNey smoothing scheme (Chen and Goodman, 1998), using the SRILM toolkit (Stolcke, 2002).",
                    "sid": 91,
                    "ssid": 11,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "tuning. ",
            "number": "6",
            "sents": [
                {
                    "text": "Moses-based systems were tuned using the implementation of minimum error rate training (MERT) (Och, 2003) distributed with the Moses decoder, using the development corpus (news-test2008).",
                    "sid": 92,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The N -code systems were also tuned by the same implementation of MERT, which was slightly modified to match the requirements of our decoder.",
                    "sid": 93,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The BLEU score is used as objective function for MERT and to evaluate test performance.",
                    "sid": 94,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The interpolation experiment for French- English was tuned on news-test2008a (first 1025 lines).",
                    "sid": 95,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Optimization was carried out over new- stest2008b (last 1026 lines).",
                    "sid": 96,
                    "ssid": 5,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "experiments. ",
            "number": "7",
            "sents": [
                {
                    "text": "For each system, we used all the available parallel corpora distributed for this evaluation.",
                    "sid": 97,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used Europarl and News commentary corpora for GermanEnglish task and Europarl, News commentary, United Nations and Gigaword corpora for the French-English tasks.",
                    "sid": 98,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All corpora were aligned with GIZA++ for word-to-word alignments with grow-diag-final-and and default settings.",
                    "sid": 99,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the GermanEnglish tasks, we applied normalization and compound splitting as a pre- processing step.",
                    "sid": 100,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the French-English tasks, we used new POS-based reordering model and interpolation.",
                    "sid": 101,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7.1 GermanEnglish Tasks.",
                    "sid": 102,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We combined our two preprocessing schemes (see Section 3) by applying compound splitting over normalized data.",
                    "sid": 103,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our experiments showed that for German to English, using 4 characters as the minimum split length and 8 characters as the minimum compound candidate, and allowing the insertion of -s -n -en -nen -e -es -er -ien) and the truncation of -e -en -n yielded the best BLEU scores.",
                    "sid": 104,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the reverse direction, the best setting is different: 5 characters as minimum split length, 10 characters as minimum compound candidate, no truncation.",
                    "sid": 105,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These processes are performed before alignment, training, tuning and decoding.",
                    "sid": 106,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Before decoding, we also replaced all OOV words with their lemma.",
                    "sid": 107,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used the Moses (Koehn et al., 2007) decoder, with default settings, to obtain the translations.",
                    "sid": 108,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For translating from English to German, we used a two-level decoding.",
                    "sid": 109,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first decoding step translates English to \u201cpreprocessed German\u201d, which is then turned into German by undoing the effect of normalization.",
                    "sid": 110,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this second step, we thus aim at restoring inflection marks and at merging compounds.",
                    "sid": 111,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this second \u201ctranslation\u201d step, we also use a Moses-based system.",
                    "sid": 112,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To point out the error rate of the second step, we also translated the preprocessed reference German text and computed the BLEU score as 97.05.",
                    "sid": 113,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our experiments showed that this two-level decoding strategy was not improving the direct baseline systems.",
                    "sid": 114,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 reports the BLEU scores5 on newstest2010 of our official submissions.",
                    "sid": 115,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "System De \u2192 En En \u2192 De Baseline 20.0 15.3 Norm+Split 21.3 15.0 Table 2: Results for GermanEnglish 7.2 French-English tasks.",
                    "sid": 116,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As explained above, in addition to the baseline system (base), two contrast systems were built.",
                    "sid": 117,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first introduces an additional POS-based bilingual 6-gram reordering model (bilrm), the second implements the bilingual n-gram model after interpolating 4 models trained respectively on the news, epps, UNdoc and gigaword subparts of the parallel corpus (interp).",
                    "sid": 118,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Optimization was carried out over newstest2008b (last 1026 lines) and tested over newstest2010 (2489 lines).",
                    "sid": 119,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 3 reports translation accuracy for the three systems and for both translation directions.",
                    "sid": 120,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As can be seen, the system using the new reordering model (base+bilrm) outperformed the baseline system when translating into French, while no difference was measured when translating into English.",
                    "sid": 121,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The interpolation experiments 5 Scores are computed with the official script mtevalv11b.pl S y s t e m F r \u2192 E n E n \u2192 F r b a s e 2 6 . 5 2 2 7 . 2 2 b a s e + b i l r m 26.50 27.84 bas e+b ilr m+ inte rp 26.84 27.62 Table 3: Results for French-English did not show any clear impact on performance.",
                    "sid": 122,
                    "ssid": 26,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusions. ",
            "number": "8",
            "sents": [
                {
                    "text": "In this paper, we presented our statistical MT systems developed for the WMT\u201910 shared task, including several novelties, namely the preprocessing of German, and the integration of several new techniques in our n-gram based decoder.",
                    "sid": 123,
                    "ssid": 1,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments",
            "number": "",
            "sents": [
                {
                    "text": "This work was partly realized as part of the Quaero Program, funded by OSEO, the French agency for innovation.",
                    "sid": 124,
                    "ssid": 2,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}