{
    "ID": "W97-0316",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "We investigate the effects of lexicon size and stopwords on Chinese information retrieval using our method of short-word segmentation based on simple language usage rules and statistics.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These rules allow us to employ a small lexicon of only 2,175 entries and provide quite admirable retrieval results.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is noticed that accurate segmentation is not essential for good retrieval.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Larger lexicons can lead to incremental improvements.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The presence of stopwords do not contribute much noise to IR.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Their removal risks elimination of crucial words in a query and adversely affect retrieval, especially when the queries are short.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Short queries of a few words perform more than 10% worse than paragraph-size queries.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "It is well known that a sentence in Chinese (or several other oriental languages) consists of a continuous string of 'characters' without delimiting white spaces to identify words.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Chinese, the characters are called ideographs.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This makes it difficult to do machine studies on these languages since isolated words are needed for many purposes, such as linguistic analysis, machine translation, etc. Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Information retrieval (IR) deals with the problem of selecting relevant documents for a user need that is expressed in free text.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The document collection is usually huge, of gigabyte size, and both queries and documents are domain unrestricted and unpredictable.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When one does IR in the Chinese language with its peculiar property, then one would assume that accurate word segmentation is also a crucial first step before other processing can begin.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, in the recent 5th Text REtrieval Conference (TREC5) where a fairly large scale Chinese IR experiment was performed [Kwok and Grunfeld, 199x], we have demonstrated that a simple word segmentation method, couple with a powerful retrieval algorithm, is sufficient to provide quite good retrieval results.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, experiments by others using even simpler bigram representation of text (i.e. all consecutive overlapping two characters), both within and outside the TREC environment, also produce good results [Ballerini et al., 199x, Buckley et al., 199x, Chien, 1995, Liang et al., 1996].",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is a bit counter-intuitive because the bigram method leads to three times as large an indexing feature space compared with our segmentation (approximately 1.5 million vs 0.5 million), and one would expect that there are many random, non-content matchings between queries and documents that may adversely affect precision.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Apparently, this is not so.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Based on this observation, we made some adjustments to our lexicon, and provide some experimental results of the lexicon effects on retrieval effectiveness.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "short-word segmentation. ",
            "number": "2",
            "sents": [
                {
                    "text": "While word segmentation for linguistic analysis may aim at the longest string that carry a specific semantic content, this may not be ideal for IR because one then has to deal with the problem of partial string matching when a query term matches only part of a document term or vice versa.",
                    "sid": 20,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead, we aim at segmenting texts into short words of one to three characters long that function like English content terms.",
                    "sid": 21,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our process is based on the following four steps A to D: A) facts- lookup on a manually created 2175-entry lexicon called LO.",
                    "sid": 22,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is small, consisting of commonly used words of 1 to 3 characters, with some proper nouns of size 4.",
                    "sid": 23,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each entry is tagged as 0 (useful: total 1337), 1 (stopword: 671), s (symbol: 88), 6 (numeric: 37), 4 (punctuation: 9), and 2 or 3 for the rules below.",
                    "sid": 24,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other researchers have used lexicons of hundreds of thousands.",
                    "sid": 25,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We do not have such a large resource; besides, maintenance of such a list is not trivial.",
                    "sid": 26,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We try to remedy this via rules.",
                    "sid": 27,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given an input string, we scan left to right and perform longest matching when searching on the lexicon.",
                    "sid": 28,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Any match will result in breaking a sentence into smaller chunks of texts.",
                    "sid": 29,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Fig.1b shows the result of processing an original TREC query (Fig.1a) after our lexicon lookup process.",
                    "sid": 30,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "B) rules - for performing further segmentation on chunks.",
                    "sid": 31,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Words in any language are dynamic and one can never capture 'all' Chinese words in a lexicon for segmentation purposes.",
                    "sid": 32,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We attempt to identify some common language usage ad-hoc rules that can be employed to further split the chunks into short words.",
                    "sid": 33,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The rules that we use, together with their rationale and examples and counterexamples are described below: Rule D (for double): any two adjacent similar characters xx are considered stopwords -- this identifies double same characters that are often used as adjectives or adverbs that do not carry much content below (ex.713).",
                    "sid": 34,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When character pis tagged '2', we also try to identify common words where p is used as a word in the construct yp, and these are entered into the lexicon.",
                    "sid": 35,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "yp may or may not be a stopword.",
                    "sid": 36,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This way a string like ..ypx..",
                    "sid": 37,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "would be split 'yp x' rather than 'y px', dictionary entries being of higher precedence.",
                    "sid": 38,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This rule works in many cases, but we believe that our list may be too long, and many words that have content (such as ex.1415) are stopped.",
                    "sid": 39,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Rule 3: xQ, where Q currently has only 2 special characters, are stopwords for any x -- these are tagged '3' and is a complement to Rule 2 (see ex.1619 and counterexamples ex.2021).",
                    "sid": 40,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Rule 3 Q = { 1\u00b7} Examples: (see ex.l-3 below).",
                    "sid": 41,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, some Chinese names do use double same characters (ex.4) and we would 'stop' them wrong.",
                    "sid": 42,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other cases such as 'Japan Honshu' (ex.5), 'U.S. Congress' (ex.6) requires splitting between the same two characters.",
                    "sid": 43,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In these cases we rely on 'Japan' or 'U.S.' being on the lexicon and identified first before applying this rule.",
                    "sid": 44,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(16) (, )fi} u7> ie l (18) (19) CounterExamples: (28) jft ijifiJ (21) Ibn we they those more teachers more diligent Rule D Examples: (1) Jt.Jt.",
                    "sid": 45,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2) +Itt (3> H CounterExamples: (1) -fiiJJj:r Jj:r (5> S*:t#l (6) 001il-\u20act daily slowly euery where person name Japan Honshu U.S. Congress Rule E (for even): any remaining sequence of even number of characters are segmented two by two -- this arises from the observation that 7080% of Chinese words are 2-characters long, and the rhythm of Chinese are often hi-syllable punctuated with mono\u00ad syllables and tri syllables.",
                    "sid": 46,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If one can identify where the single character words occur, the rest of the string quite often can be split as such when it is even.",
                    "sid": 47,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These single characters are often stopwords that hopefully are in our lexicon.",
                    "sid": 48,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Examples 22 to 26 below show chunks that are even, being surrounded by punctuation signs or stopwords.",
                    "sid": 49,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They will be segmented correctly.",
                    "sid": 50,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Examples 27 to 29 show counter examples with even Rule 2: Px, where P is a small set of 31 special characters, are stopwords for any x -- these characters are tagged '2' in our lexicon and examples are shown Rule 2 p = {-, J&, u. t,A \u2022.\u2022 } Examples: (7) - a branch/stick of (8) -!fl.",
                    "sid": 51,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "early (9) -jf together (18) (}!.$> (this, that)kind (11) c}!.$> (this, that)time (12) t,AJg consider to be (13) tAJt in earnest CounterExamples: (11) -00 one country (15) tAift admit mistake number of characters that do not obey this rule.",
                    "sid": 52,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, numeric entries are also removed as stopwords although one can often detect a sequence of them and have it identified as a number.",
                    "sid": 53,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "C) frequency filter - after a first pass through the test corpus via steps A and B, a list of candidate short\u00ad words will be generated with their frequency of occurrence.",
                    "sid": 54,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A threshold is used to extract the most commonly occurring ones.",
                    "sid": 55,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These are our new short\u00ad words that are 'data-mined' from the corpus itself.",
                    "sid": 56,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "D) iteration - using the newly identified short-words of Step C all tagged useful for segmentation purposes, we expand our initial lexicon in step A and reprocess the corpus.",
                    "sid": 57,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In theory, we could continue to iterate, but we have only done one round.",
                    "sid": 58,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With a frequency threshold value in Step C of 30, a final lexicon size of 15,234 called L01 was obtained.",
                    "sid": 59,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We believe the rules we use for Step B, though Rule E Examples: 1 CZ3) l(;j\" jlj it!s I \u00b7.",
                    "sid": 60,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 1 CZ1) !ilt.FD!",
                    "sid": 61,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "IUJ . 1 czs) tE .-&!krtiM I ..",
                    "sid": 62,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 1 Probabilistic Indexing and Retrieval - Components - System) engine that has been documented elsewhere [Kwok 1990,1995] and has participated in the past five TREC experiments with admirable results [see for automatic, learning-based IR system that is conceptualized as a 3-layer network and operates via activation spreading.",
                    "sid": 63,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It combines different probabilistic methods of retrieval that can account for local as well as global term usage evidence.",
                    "sid": 64,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our CZ6) ft;ft I l'.t J!.ltJlltffl .*.& 8 1 first is the initial retrieval where a raw query is used directly.",
                    "sid": 65,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The d best ranked documents from this Counterexamples: CZBl tl I fi\"-Jil.t I 1{-ij..",
                    "sid": 66,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CZ9) \u2022.",
                    "sid": 67,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "_FjtI jij.",
                    "sid": 68,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "simple, are useful.",
                    "sid": 69,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They naturally do not work always, but may work correctly often enough for IR purposes.",
                    "sid": 70,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Fig.lc shows the results of processing the TREC5 query #28 based on these rules after Step A. Comparison with a manual short word segmentation of the set of 28 TREC5 queries shows that we achieve 91.3% recall and 83% precision on average.",
                    "sid": 71,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is possible that these queries are easy to segment.",
                    "sid": 72,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our method of segmentation is certainly too approximate for other applications such as linguistic analysis, text\u00ad to-speech, etc. For IR, where the purpose is to detect documents with high probability of relevance rather than exact matching of meaning and is a more forgiving environment, it may be adequate.",
                    "sid": 73,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Besides, one also has other tools in IR to remedy the situation.",
                    "sid": 74,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These are discussed below.",
                    "sid": 75,
                    "ssid": 56,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "the retrieval environment. ",
            "number": "3",
            "sents": [
                {
                    "text": "Our investigations are based on the TREC5 Chinese collection of 24,988 Xinhua and 139,801 People's Daily news articles totaling about 170MB.",
                    "sid": 76,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To guard retrieval are then regarded as relevant without user initial query term weights and to add new terms to the query - query expansion.",
                    "sid": 77,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This process has been called pseudo-feedback.",
                    "sid": 78,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This expanded query retrieval then provides the final result.",
                    "sid": 79,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This second retrieval in general can provide substantially better results than the initial if the initial retrieval is reasonable and has some relevants within the d best-ranked documents.",
                    "sid": 80,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The process is like having a dynamic thesaurus bringing in synonymous or related terms to enrich the raw query.",
                    "sid": 81,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As an example of a retrieval, we have shown in Table 1 comparing the TREC5 Chinese experiment using bigram representation with our method of text segmentation in the PIRCS system.",
                    "sid": 82,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The table is a standard for the TREC evaluation.",
                    "sid": 83,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Precision is defined as the proportion of retrieved documents which are relevant, and recall that of relevant documents which are retrieved.",
                    "sid": 84,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In general when more documents are retrieved, precision falls as recall increases.",
                    "sid": 85,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It can be Represent'n: Bigram Short-Word Segm Total number of documents over all queries Retrieved: 28000 28000 Relevant: 2182 2182 Rel_ret: 2125 2015 Interpolated Recall - Precision Averages: words as described in Section 2.",
                    "sid": 86,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, the single characters from each word of length two or greater are also used for indexing purposes to guard against wrong segmentation.",
                    "sid": 87,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Provided with the TREC5 collection are 28 very long and rich Chinese topics, mostly on current affairs.",
                    "sid": 88,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They are processed like documents into queries.",
                    "sid": 89,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These topics representing user needs have also been manually judged with respect to the (most fruitful part of the) collection at NIST so that a set of relevant documents for each query is known.",
                    "sid": 90,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This allows retrieval results to be evaluated against known answers.",
                    "sid": 91,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For retrieval, we use our PffiCS (acronym for Average precision (non-interpolated) over all rei docs 0.4477 0.4516 Precision At: 5 docs: 0.6429 0.6643 10 docs: 0.6036 0.6000 20 docs: 0.5625 0.5482 30 docs: 0.5214 0.5321 100 docs: 0.3796 0.3693 Exact: 0.4557 0.4522 Table 1: Bigram and Short-Word Segmentation Retrieval Results Averaged over 28 Queries seen that the two methods provide quite similar performance - bigram method ranks 2125 of the 2182 known relevant documents within the first 1000 retrieved for the 28 queries while the short-word method has about 5% less, at 2015.",
                    "sid": 92,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The latter has a slight edge in average precision (0.4516 vs 0.4477).",
                    "sid": 93,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Average precision is often used as a standard for comparison.",
                    "sid": 94,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The precision at different number of documents retrieved, a user-oriented measure, are also comparable in both cases.",
                    "sid": 95,
                    "ssid": 20,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "lexicon effects on retrieval. ",
            "number": "4",
            "sents": [
                {
                    "text": "In bigram representation of text, no lexicon is used and many meaningless bigrams as well as many that are true stopwords are included.",
                    "sid": 96,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Yet they do not seem to affect retrieval effectiveness.",
                    "sid": 97,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We take this as a clue that stopword removal may not play an important role in Chinese IR and lead us to investigate its effect.",
                    "sid": 98,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also like to see how lexicon size can affect retrieval.",
                    "sid": 99,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "for IR processing: features with frequencies that are too high or too low have adverse effects on retrieval effectiveness and efficiency.",
                    "sid": 100,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is done as a default, and is also performed for bigrams.",
                    "sid": 101,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our lexicon-based stopwords consists of 671 entries in our list tagged as '1'.",
                    "sid": 102,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The major rule-based stopword removal is Rule 2, while others have minor effects because they occur much less often.",
                    "sid": 103,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A run through the collection shows that the number of times tag 1 and Rule 2 were exercised are about 1.9m and 2.1m.",
                    "sid": 104,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have enabled Rules D and E, tags 0,3 and 4 to be effective for segmentation as a default, and perform experiments where the lexicon (tag 1 & 6) and rule\u00ad based (Rule 2) stopword removal (and segmentation) can be activated or deactivated as follows: tag t,6 Rule 2------------------ segment yes yes ExpTyp.2 Usually one needs as large a dictionary as possible so that many segmentation patterns are available for the system to select the correct one.",
                    "sid": 105,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An entry in our lexicon list can serve the purpose of a segmentation marker or, in addition, for detection of stopwords.",
                    "sid": 106,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our system stopwords can be determined in three ways based on: lexicon, rule or frequency threshold (statistical).",
                    "sid": 107,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The last category stop & segm segment ExpTyp.3 stop & segm segment ExpTyp.4 yes yes yes yes arises from Zipfian behavior of terms and is standard stop & segm yes yes Lexicon:<--- LO ----> <---\u00b7 LOt ----> ExpTyp: t 2 3 4 5 t 2 3 4 5 Total number of documents over all queriesRetrieved: <----Relevant: <---- Rel_ret: 2059 2062 2047 204628000 - --->2182 - ---> 2013 2058 2060 204t 2040 20t2 Interpolated Recall - Precision Averages: at .1 .688 .682 .699 .689 .671 .673 .676 .678 .675 .655 at .3 .557 .557 .555 .555 .549 .564 .563 .564 .568 .560 at.5 .467 .473 .470 .473 .466 .474 .481 .475 .483 .469 at .7 .375 .374 .373 .367 .356 .378 .376 .380 .376 .365 at.9 .249 .253 .246 .239 .233 .252 .257 .250 .254 .246 Average precision (non-interpolated) over all rel docs .455 .457 .456 .457 .448 .461 .462 .460 .460 .451 Precision At: 5 docs: .650 .657 .664 .650 .650 .664 .657 .664 .643 .686 tO docs: .596 .589 .611 .611 .596 .593 .596 .621 .6t4 .607 20 docs: .564 .559 .557 .561 .566 .555 .552 .554 .558 .552 30 docs: .526 .533 .535 .537 .537 .531 .533 .525 .536 .535 . 100 docs: .373 .376 .372 .373 .368 .380 .377 .370 .371 .368 Exact: .455 .465 .460 .463 .455 .453 .457 .462 .462 .452 Table 2: Effect of Lexicon-based and Rule-based Stopwords on Long Query Retrieval using LO & LOt For example, ExpTyp.2 means lexicon entries with tags 1,6 are used for segmentation only, while those obeying Rule 2 serve to segment and removed as well.",
                    "sid": 108,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An ExpTyp.5 will be explained later.",
                    "sid": 109,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Retrievals using lexicons of four different sizes with long and short versions of the TREC5 queries were performed and evaluated.",
                    "sid": 110,
                    "ssid": 15,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "results and discussion. ",
            "number": "5",
            "sents": [
                {
                    "text": "5.1 Long Queries.",
                    "sid": 111,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 tabulates the precision and recall values averaged over 28 long queries using LO, the 2175- entry and L01, the 15234-entry lexicons.",
                    "sid": 112,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In ExpTyp.1 under LO for example, where tags 1 & 6 as well as Rule 2 are in effect for segmentation only, an average precision of 0.455 and recall of relevants (at 1000 retrieved) of 2059 out of 2182 are achieved.",
                    "sid": 113,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On average close to 5.96 out of the first 10 retrieved documents are relevant.",
                    "sid": 114,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is very good performance for a purely statistical retrieval system.",
                    "sid": 115,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is also interesting to see that the small lexicon is sufficient to yield this good result.",
                    "sid": 116,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Indirectly, it shows that our rule-based segmentation (Rule D, E, 2) can define sufficiently good features for retrieval, and remedies our deficiency in lexicon size.",
                    "sid": 117,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When both tag 1,6 entries and Rule 2 are used for stopword removal (ExpTyp.4, LO), average precision remains practically the same at 0.457.",
                    "sid": 118,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similarly for ExpTyps.2 & 3 LO, where either Rule2 or tag 1,6 are used for stopword removal, effectiveness does not seem to alter word 'hope' is often used in the context of 'We hope to/that..' or 'My hope is ..' and quite non-content bearing.",
                    "sid": 119,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is not unreasonable to regard it as a stopword in both English and Chinese.",
                    "sid": 120,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, for this query it is crucial.",
                    "sid": 121,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ExpTyp.5 LO is done under the same circumstances as ExpTyp.4 LO except that the word 'hope' is changed to be a stopword (tag 1).",
                    "sid": 122,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This query then practically accounts for all the adverse effect.",
                    "sid": 123,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the presence of stopwords has been shown to have a benign effect on Chinese retrieval, it appears advisable to keep them as indexing terms to guard against such unexpected results.",
                    "sid": 124,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Table 2 under LOl, we repeat the same experiments using our larger lexicon which is derived from the collection using LO as the basis.",
                    "sid": 125,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is seen that the larger lexicon improves average precision by about 1%, from around 0.456 to about 0.461.",
                    "sid": 126,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Otherwise, the two sets of experiments are qualitatively similar.",
                    "sid": 127,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since retrieval is crucially dependent on how well the queries are processed, it appears that the 28 are well-prepared for retrieval using the original 2175-entry lexicon.",
                    "sid": 128,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recently, we further augment our LO to a larger initial lexicon L1 with 27,147 entries.",
                    "sid": 129,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This derives Ll1, a 42,822-entry lexicon from the collection based on our segmentation procedure.",
                    "sid": 130,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Results of repeating the retrieval experiments using these two larger lexicons are shown in Table 3.",
                    "sid": 131,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There is incremental improvements in average precision by using the larger lexicon: e.g. for ExpTyp.1, from 0.455 (LO) to 0.463 (Ll1), about 2%.",
                    "sid": 132,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The removal of stopwords for L11 (ExpTyp.4 vs 1) does not lead to much difference, much.",
                    "sid": 133,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Removal of tag 1,6 words however decreases the number of relevants slightly from 2060 to around 2040.",
                    "sid": 134,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It appears that the presence of stopwords have Lexicon: ExpTyp.: <\u00b7 L1 \u00b7> 1 4 <\u00b7 Lll \u00b7> 1 4 5 little effect on Chinese IR, just as noticed for bigrams.",
                    "sid": 135,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ExpTyp.5 LO in Table 2 is included as a Total number of documents over all queries Retrieved: <------ 28000 Relevant: <------ 2182 -------> -------> demonstration of the perils associated with stopword removal.",
                    "sid": 136,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It shows about a 2% drop in average precision as well as in relevants retrieved compared Rel_ret: 2062 2056 2061 Interpolated Recall - Precision Averages: 2056 2008 with ExpTyp.4 LO due to bad result of one single query.",
                    "sid": 137,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Query #19 asks for documents on 'Project Hope', and the Chinese query is shown below.",
                    "sid": 138,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The TREC5 Chinese Query 819: (= hope)JCfl (=project).",
                    "sid": 139,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "at .1 .684 .673 .696 .695 .688 at .3 .555 .553 .558 .567 .558 at .5 .478 .475 .478 .479 .465 at .7 .381 .375 .384 .379 .358 at .9 .254 .262 .256 .262 .247 Average precision (non-interpolated) over all rei docs Precision At: .460 .459 .463 .464 .451 5 docs: .650 .643 .671 .693 .693 10 docs: .614 .604 .604 .611 .607 20 docs: .561 .555 .563 .550 .543 30 docs: .529 .524 .525 .521 .516 100 docs: .373 .373 .373 .374 .366 Exact: .460 .466 .461 .468 .458 Table 3: Effect of Lexicon-based and Rule-based Stopwords on Long Query Retrieval using L1 and Lll but the peril of accidentally removing a crucial word remains, leading again to about 2% drop in effectiveness (ExpTyp.5 vs 4 L11).",
                    "sid": 140,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.2 Short Queries.",
                    "sid": 141,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It has been pointed out that the paragraph-size TREC queries are long and unrealistic because real-life queries are usually very short, like one or two words.",
                    "sid": 142,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One or two words, on the other hand, often do not supply sufficient clues to a retrieval engine.",
                    "sid": 143,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To study the effects of lexicons on short queries, we further perform retrievals using only the first sentence of each query that belongs to the 'title' section of an original topic.",
                    "sid": 144,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They average to a few short-words and we hope to see more pronounced effects.",
                    "sid": 145,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These results are shown in Table 4.",
                    "sid": 146,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As expected, retrieval effectiveness decreases substantially over 10% compared to the full length queries: from around 0.463 to 0.409 (ExpTyp.1 L11, Tables 3&4).",
                    "sid": 147,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The larger lexicon L11 also has an edge over LO (average precision 0.409 vs 0.398 Table 4), and the use stopwords (ExpTyp.4 vs 1 L11) can improve precision as for long queries, but the accidental removal of a crucial word can lead to a much bigger adverse effect of 6% drop in average precision (ExpTyp.5 vs ExpTyp.4).",
                    "sid": 148,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Especially hard hit is the number of relevants at 1000 retrieved, which decreases by 11% (1962 vs 1732).",
                    "sid": 149,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The reason for this pronounced effect is that when a query is short (like two words 'Project Hope') and a crucial word ('Hope') is removed, what is left for retrieval is practically useless.",
                    "sid": 150,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In long queries however, many other terms are still available to remedy the removed crucial word, Lexicon: <- LO \u00b7> <\u00b7 L01 \u00b7> <\u00b7 L1 \u00b7> <\u00b7\u00b7 Lll \u00b7\u00b7> ExpTyp: 1 4 1 4 5 1 4 1 4 5 Total number of documents over all queriesRetrieved: <----Relevant: <---- Rel_ret: 1958 1929 1961 1914 28000 2182 1684 1970 1952 1975 -----> -----> 1962 1732 Interpolated Recall - Precision Averages: at .1 .608 .596 .609 .614 .579 .579 .578 .586 .605 .569 at.3 .502 .498 .500 .486 .456 .496 .495 .492 .493 .458 at .5 .410 .409 .410 .415 .383 .420 .426 .427 .434 .402 at .7 .336 .346 .345 .344 .321 .348 .349 .351 .355 .333 at .9 .217 .223 .227 .233 .232 .234 .235 .234 .241 .241 Average precision (non-interpolated) over all rei docs .398 .405 .408 .407 .382 .405 .409 .409 .417 .391 Precision At: 5 docs: .579 .550 .579 .564 .529 .586 .586 .593 .607 .571 10 docs: .534 .532 .568 .554 .518 .550 .554 .564 .571 .536 20 docs: .495 .496 .516 .502 .466 .488 .489 .488 .495 .459 30 docs: .466 .474 .481 .473 .437 .467 .464 .469 .475 .439 100 docs: .334 .336 .339 .335 .301 .330 .329 .333 .335 .301 Exact: .403 .404 .406 .406 .381 .399 .405 .398 .409 .385 Table 4: Effect of Lexicon-based and Rule-based Stopwords on Short Query Retrieval using LOO, L01, L1 & Lll.",
                    "sid": 151,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "and the effect is less pronounced.",
                    "sid": 152,
                    "ssid": 42,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusion. ",
            "number": "6",
            "sents": [
                {
                    "text": "For the TREC5 Chinese collection of documents and queries, it is found that a small 2175-lexicon coupled with some simple linguistic rules is sufficient to provide indexing features for good retrieval results.",
                    "sid": 153,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Larger lexicons can give incremental improvements.",
                    "sid": 154,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Lexicon or rule-based stopword removal have negligible effect on retrieval with long queries.",
                    "sid": 155,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For short queries with a large lexicon, stopword elimination can lead to some improvements, but runs the risks of accidentally deleting a crucial word in a query that can adversely affect retrieval significantly.",
                    "sid": 156,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It appears advisable to keep all stopwords and use them for segmentation purposes.",
                    "sid": 157,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One needs only retain high and low frequency thresholds to screen out frequency-based statistical stopwords.",
                    "sid": 158,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Experimentation with more varied queries is needed to verify these findings.",
                    "sid": 159,
                    "ssid": 7,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments. ",
            "number": "7",
            "sents": [
                {
                    "text": "This work is partially supported by a Tipster grant from the U.S. Department of Defense.",
                    "sid": 160,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Xianlin Zhang and Jing Yan helped prepare the lexicons.",
                    "sid": 161,
                    "ssid": 2,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}