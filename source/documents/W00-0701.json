{
    "ID": "W00-0701",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "This article summarizes work on developing a learning theory account for the major learning and statistics based approaches used in natural language processing.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It shows that these ap\u00ad proaches can all be explained using a single dis\u00ad tribution free inductive principle related to the pac model of learning.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, they all make predictions using the same simple knowl\u00ad edge representation - a linear representation over a common feature space.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is signifi\u00ad cant both to explaining the generalization and robustness properties of these methods and to understanding how these methods might be ex\u00ad tended to learn from more structured, knowl\u00ad edge intensive examples, as part of a learning centered approach to higher level natural lan\u00ad guage inferences.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Many important natural language inferences can be viewed as problems of resolving phonetic, syntactic, semantics or pragmatics ambiguities, based on properties of the surrounding context.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is generally accepted that a learning compo\u00ad nent must have a central role in resolving these context sensitive ambiguities, and a significant amount of work has been devoted in the last few years to developing learning methods for these tasks, with considerable success.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Yet, our un\u00ad derstanding of when and why learning works in this domain and how it can be used to support increasingly higher level tasks is still lacking.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This article summarizes work on developing a learning theory account for the major learning approaches used in NL.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While the major statistics based methods used in NLP are typically developed with a \u2022 This research is supported by NSF grants IIS9801638, SBR9873450 and IIS9984168.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Bayesian view in mind, the Bayesian principle cannot directly explain the success and robust\u00ad ness of these methods, since their probabilistic assumptions typically do not hold in the data.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead, we provide this explanation using a sin\u00ad gle, distribution free inductive principle related to the pac model of learning.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We describe the unified learning framework and show that, in addition to explaining the success and robust\u00ad ness of the statistics based methods, it also ap\u00ad plies to other machine learning methods, such as rule based and memory based methods.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An important component of the view devel\u00ad oped is the observation that most methods use the same simple knowledge representation.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is a linear representation over a new feature space - a transformation of the original instance space to a higher dimensional and more expres\u00ad sive space.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Methods vary mostly algorithmicly, in ways they derive weights for features in this space.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is significant both to explaining the generalization properties of these methods and to developing an understanding for how and when can these methods be extended to learn from more structured, knowledge intensive ex\u00ad amples, perhaps hierarchically.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These issues are briefly discussed and we emphasize the impor\u00ad tance of studying knowledge representation and inference in developing a learning centered ap\u00ad proach to NL inferences.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "learning frameworks. ",
            "number": "2",
            "sents": [
                {
                    "text": "Generative probability models provide a princi\u00ad pled way to the study of statistical classification in complex domains such as NL.",
                    "sid": 18,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is common to assume a generative model for such data, es\u00ad timate its parameters from training data and then use Bayes rule to obtain a classifier for this model.",
                    "sid": 19,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the context of NL most clas\u00ad sifiers are derived from probabilistic language models which estimate the probability of a sen\u00ad tence s using Bayes rule, and then decompose this probability into a product of conditional probabilities according to the generative model.",
                    "sid": 20,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pr(s) = Pr(w1,w2,\u00b7\u00b7\u00b7wn) = = IIf=1Pr(wilwi, ...",
                    "sid": 21,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Wi-d= IIf=1Pr(wilhi) where hi is the relevant history when predicting wi, and sis any sequence of tokens, words, part\u00ad of-speech (pos) tags or other terms.",
                    "sid": 22,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This general scheme has been used to de\u00ad rive classifiers for a variety of natural lan\u00ad guage applications including speech applica\u00ad tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and context\u00ad sensitive spelling correction (Gol95).",
                    "sid": 23,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While the use of Bayes rule is harmless, most of the work in statistical language modeling and ambiguity resolution is devoted to estimating terms of the form Pr(wlh).",
                    "sid": 24,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The generative models used to estimate these terms typically make Markov or other independence assumptions.",
                    "sid": 25,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is evident from studying language data that these assump\u00ad tions are often patently false and that there are significant global dependencies both within and across sentences.",
                    "sid": 26,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, when using (Hidden) Markov Model (HMM) as a generative model for pos tagging, estimating the probabil\u00ad ity of a sequence of tags involves assuming that the pos tag ti of the word Wi is independent of other words in the sentence, given the preced\u00ad ing tag ti-l\u00b7 It is not surprising therefore that this results in a poor estimate of the probabil\u00ad ity density function.",
                    "sid": 27,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, classifiers built based on these false assumptions nevertheless seem to behave quite robustly in many cases.",
                    "sid": 28,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A different, distribution free inductive princi\u00ad ple that is related to the pac model of learning is the basis for the account developed here.",
                    "sid": 29,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In an instance of the agnostic variant of pac learning (Val84; Hau92; KSS94), a learner is given data elements (x, l) that are sampled ac\u00ad cording to some fixed but arbitrary distribu\u00ad tion D on X x {0, 1}.",
                    "sid": 30,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "X is the instance space and l E {0, 1} is the label1.",
                    "sid": 31,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "D may simply re\u00ad flect the distribution of the data as it occurs \"in nature\" (including contradictions) without assuming that the labels are generated accord\u00ad ing to some \"rule\".",
                    "sid": 32,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a sample, the goal of the learning algorithm is to eventually out\u00ad put a hypothesis h from some hypothesis class 1-l that closely approximates the data.",
                    "sid": 33,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The 1The model can be extended to deal with any discrete or continuous range of the labels.",
                    "sid": 34,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "true error of the hypothesis h is defined to be errorn(h) = Pr(x,l)ED[h(x) -::1 l], and the goal of the (agnostic) pac learner is to com\u00ad pute, for any distribution D, with high prob\u00ad ability (> 18), a hypothesis h E 1i with true error no larger than f: + infhE1ierrorn(h).",
                    "sid": 35,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In practice, one cannot compute the true er\u00ad ror errorn(h).",
                    "sid": 36,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead, the input tthe learn\u00ad ing algorithm is a sample S = {(xt, F)} 1 of m labeled examples and the learner tries to find a hypothesis h with a small empirical er\u00ad ror errors(h) = l{x E Slh(x) \"=I l}I/ISI, and hopes that it behaves well on future examples.",
                    "sid": 37,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The hope that a classifier learned from a train\u00ad ing set will perform well on previously unseen examples is based on the basic inductive prin\u00ad ciple underlying learning theory (Val84; Vap95) which, stated informally, guarantees that if the training and the test data are sampled from the same distribution, good performance on large enough training sample guarantees good per\u00ad formance on the test data (i.e., good \"true\" er\u00ad ror).",
                    "sid": 38,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, the quality of the generalization is inversely proportional to the expressivity of the class 1i.",
                    "sid": 39,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Equivalently, for a fixed sample size lSI, the quantified version of this princi\u00ad ple (e.g.",
                    "sid": 40,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Hau92)) indicates how much can one count on a hypothesis selected according to its performance on S. Finally, notice the underly\u00ad ing assumption that the training and test data are sampled from the same distribution; this framework addresses this issue.",
                    "sid": 41,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(See (GR99).)",
                    "sid": 42,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our discussion functions learned over the instance space X are not defined directly over the raw instances but rather over a transforma\u00ad tion of it to a feature space.",
                    "sid": 43,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A feature is an in\u00ad dicator function x : X{0, 1} which defines a subset of the instance space - all those elements in X which are mapped to 1 by X\u00b7 X denotes a class of such functions and can be viewed as a transformation of the instance space; each ex\u00ad ample (x 1, ...xn) EX is mapped to an example (Xl, ...",
                    "sid": 44,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "XiXI) in the ew.",
                    "sid": 45,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "space.",
                    "sid": 46,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Wsometimes view a feature as an md1cator functiOn over the labeled instance space X x {0, 1} and say that x(x, l) = 1 for examples x E x(X) with labell.",
                    "sid": 47,
                    "ssid": 30,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "explaining probabilistic methods. ",
            "number": "3",
            "sents": [
                {
                    "text": "Using the abovementioned inductive principle we describe a learning theory account that ex\u00ad plains the success and robustness of statistics based classifiers (Rot99a).",
                    "sid": 48,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A variety of meth ods used for learning in NL are shown to make their prediction using Linear Statistical Queries {LSQ) hypotheses.",
                    "sid": 49,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is a family of linear predictors over a set of features which are di\u00ad rectly related to the independence assumptions of the probabilistic model assumed.",
                    "sid": 50,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The success of these classification methods is then shown to be due to the combination of two factors: \u2022 Low expressive power of the derived classifier.",
                    "sid": 51,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Prs[x(x, l)] = l{(x, l) : x(x, l) = 11}/ISI.",
                    "sid": 52,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Chernoff bounds guarantee that the number of examples required to achieve tolerance T with probability at least 18 is polynomial in 1/T and log 1/8.",
                    "sid": 53,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(See (Kea93; Dec93; AD95)).",
                    "sid": 54,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let X be a class of features and f : {0, 1} --+ a function that depends only on the values P[ ,l] for X E X. Given x E X, a Linear Statis\u00ad tical Queries (LSQ) hypothesis predicts \u2022 Robustness properties shared by all linear sta\u00ad tistical queries hypotheses.",
                    "sid": 55,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "l = argmaxlE{0,1} LxEX f[x,lJ( AD {P[x,lJ} ).",
                    "sid": 56,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "x(x).",
                    "sid": 57,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the hypotheses are computed over a fea\u00ad ture space chosen so that they perform well on training data, learning theory implies that they perform well on previously unseen data, irre\u00ad spective of whether the underlying probabilistic assumptions hold.",
                    "sid": 58,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.1 Robust Learning.",
                    "sid": 59,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This section defines a learning algorithm and a class of hypotheses with some generaliza\u00ad tion properties, that capture many probabilis\u00ad tic learning methods used in NLP.",
                    "sid": 60,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The learn\u00ad ing algorithm is a Statistical Queries{SQ) algo\u00ad rithm (Kea93).",
                    "sid": 61,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An SQ algorithm can be viewed as a learning algorithm that interacts with its environment in a restricted way.",
                    "sid": 62,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Rather than viewing examples, the algorithm only requests the values of various statistics on the distribu\u00ad tion of the examples to construct its hypothesis.",
                    "sid": 63,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(E.g. \"What is the probability that a randomly chosen example (x,l) has Xi= 0 and l = 1\"?)",
                    "sid": 64,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A statistical query has the form [x,l, T], where x E X is a feature, l E {0, 1} is a further (op\u00ad tional) restriction imposed on the query and T is an error parameter.",
                    "sid": 65,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A call to the SQ oracle returns an estimate f>[ ,l,r] of P[ ,l] = PrD{(x,i)lx(x) = 1 1\\i = l} which satisfies lf>x- Pxl < T.",
                    "sid": 66,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(We usually omit T and/or l from this notation.)",
                    "sid": 67,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A statistical queries algorithm is a learning algorithm that constructs its hypothesis only using information received from an SQ oracle.",
                    "sid": 68,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An algorithm is said to use a query space X if it only makes queries of the form [x, l, T] where x E X. An SQ algorithm is said to be a good learning al\u00ad gorithm if, with high probability, it outputs a hypothesis h with small error, using sample size that is polynomial in the relevant parameters.",
                    "sid": 69,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a query [x, l, T] the SQ oracle is sim\u00ad ulated by drawing a large sample S of labeled examples (x, l) according to D and evaluating Clearly, the LSQ is a linear discriminator over the feature space X, with coefficients f that are computed given (potentially all) the values f>[ ,l]' The definition generalizes naturally to non-binary classifiers; in this case, the discrim\u00ad inator between predicting l and other values is linear.",
                    "sid": 70,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A learning algorithm that outputs an LSQ hypothesis is called an LSQ algorithm.",
                    "sid": 71,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Example 3.1 The naive Bayes predictor (DH73) is derived using the assumption that given the label l E L the features' values are statistically independent.",
                    "sid": 72,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consequently, the Bayes optimal prediction is given by: h(x) = argmaxlELII 1Pr(xill)Pr(l), where Pr(l) denotes the prior probability of l {the fraction of examples labeled l) and Pr(xill) are the conditional feature probabilities {the fraction of the examples labeled l in which the ith feature has value Xi)\u00b7 Therefore, we get: Claim: The naive Bayes algorithm is an LSQ algorithm over a set X which consists of n + 1 features: xo 1, Xi = Xi for i = 1, ...",
                    "sid": 73,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", n and where f[l,l]0 = logf>[f,l] and f[x;,l]() = AD AD . logP[x;,l]/P[l,l]' z = 1, ...",
                    "sid": 74,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": ",n. The observation that the LSQ hypothesis is linear over X yields the first generalization property of LSQ.",
                    "sid": 75,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "VC theory implies that the VC dimension of the class of LSQ hypothe\u00ad ses is bounded above by IX I\u00b7 Moreover, if the LSQ hypothesis is sparse and does not make use of unobserved features in X (as in Ex.",
                    "sid": 76,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.1) it is bounded by the number of features used (RotOO).",
                    "sid": 77,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Together with the basic general\u00ad ization property described above this implies: Corollary 3.1 For LSQ, the number of train\u00ad ing examples required in order to maintain a specific generalization performance guarantee scales linearly with the number of features used.",
                    "sid": 78,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The robustness property of LSQ can be cast for the case in which the hypothesis is learned using a training set sampled according to a dis\u00ad tribution D, but tested over a sample from D'.",
                    "sid": 79,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It still performs well as long as the distributional distance d(D, D') is controlled (Rot99a; RotOO).",
                    "sid": 80,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Theorem 3.2 Let A be an SQ( r, X) learning algorithm for a function class Q over the distri\u00ad bution D and assume that d(D, D') < 1 (for 1 inversely polynomial in r ).",
                    "sid": 81,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then A is also an SQ( r, X) learning algorithm for g over D'.",
                    "sid": 82,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we mention that the robustness of the algorithm to different distributions depends on the sample size and the richness of the feature class X plays an important role here.",
                    "sid": 83,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, for a given size sample, the use of simpler fea\u00ad tures in the LSQ representation provides better robustness.",
                    "sid": 84,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This, in turn, can be traded off with the ability to express the learned function with an LSQ over a simpler set of features.",
                    "sid": 85,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2 Additional Examples.",
                    "sid": 86,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to the naive Bayes (NB) classifier described above several other widely used prob\u00ad abilistic classifiers can be cast as LSQ hypothe\u00ad ses.",
                    "sid": 87,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This property is maintained even if the NB predictor is generalized in several ways, by al\u00ad lowing hidden variables (GROO) or by assuming a more involved independence structure around the target variable.",
                    "sid": 88,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When the structure is mod\u00ad eled using a general Bayesian network (since we care only about predicting a value for a single variable having observed the others) the Bayes optimal predictor is an LSQ hypothesis over fea\u00ad tures that are polynomials x = Ilxi1Xi 2 \u2022.\u2022 Xik of degree that depends on the number of neighbors of the target variable.",
                    "sid": 89,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A specific case of great interest to NLP is that of hidden Markov Mod\u00ad els.",
                    "sid": 90,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this case there are two types of variables, state variables Sand observed ones, 0 (Rab89).",
                    "sid": 91,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The task of predicting the value of a state vari\u00ad able given values of the others can be cast as an LSQ, where X{S, 0, 1}x {S, 0, 1}, a suitably defined set of singletons and pairs of observables and state variables (Rot99a).",
                    "sid": 92,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, Maximum Entropy (ME) models (Jay82; Rat97) are also LSQ models.",
                    "sid": 93,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this framework, constrains correspond to features; the distribution (and the induced classifier) are defined in terms of the expected value of the fea\u00ad tures over the training set.",
                    "sid": 94,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The induced clas sifier is a linear classifier whose weights are de\u00ad rived from these expectations; the weights are computed iteratively (DR72) since no closed form solution is known for the optimal values.",
                    "sid": 95,
                    "ssid": 48,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "learning linear classifiers. ",
            "number": "4",
            "sents": [
                {
                    "text": "It was shown in (Rot98) that several other learning approaches widely used in NL work also make their predictions by utilizing a lin\u00ad ear representation.",
                    "sid": 96,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The SNoW learning archi\u00ad tecture (Rot98; CCRR99; MPRZ99) is explic\u00ad itly presented this way, but this holds also for methods that are presented in different ways, and some effort is required to cast them this way.",
                    "sid": 97,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These include Brill's transformation based method (Bri95)2 , decision lists (Yar94) and back-off estimation (Kat87; CB95).",
                    "sid": 98,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, even memory-based methods (ZD97; DBZ99) can be cast in a similar way (Rot99b).",
                    "sid": 99,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They can be reformulated as feature-based algorithms, with features of types that are commonly used by other features-based learning algorithms in NLP; the prediction com\u00ad puted by MBL can be computed by a linear function over this set of features.",
                    "sid": 100,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some other methods that have been recently used in language related applications, such as Boosting (CS99) and support vector machines are also making use of the same representation.",
                    "sid": 101,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "At a conceptual level all learning methods are therefore quite similar.",
                    "sid": 102,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They transform the original input (e.g., sentence, sentence+pos in\u00ad formation) to a new, high dimensional, feature space, whose coordinates are typically small conjunctions ( n-grams) over the original input.",
                    "sid": 103,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this new space they search for a linear func\u00ad tion that best separates the training data, and rely on the inductive principle mentioned to yield good behavior on future data.",
                    "sid": 104,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Viewed this way, methods are easy to compare and analyze for their suitability to NL applications and fu\u00ad ture extensions, as we sketch below.",
                    "sid": 105,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The goal of blowing up the instance space to a high dimensional space is to increase the expres\u00ad sivity of the classifier so that a linear function could represent the target concepts.",
                    "sid": 106,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Within this space, probabilistic methods are the most lim\u00ad ited since they do not actually search in the 2 This holds only in cases in which the TBL condi\u00ad tions do not depend on the labels, as in Context Sensi\u00ad tive Spelling (MB97) and Prepositional Phrase Attach\u00ad ment (BR94) and not in the general case.",
                    "sid": 107,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "space of linear functions.",
                    "sid": 108,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given the feature space they directly compute the classifier.",
                    "sid": 109,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In general, even when a simple linear function gen\u00ad erates the training data, these methods are not guaranteed to be consistent with it (Rot99a).",
                    "sid": 110,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, if the feature space is chosen so that they are, the robustness properties shown above become significant.",
                    "sid": 111,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Decision lists and MBL methods have advantages in their ability to rep\u00ad resent exceptions and small areas in the feature space.",
                    "sid": 112,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).",
                    "sid": 113,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Learning methods that attempt to find the best linear function (relative to some loss function) are typically more flexi\u00ad ble.",
                    "sid": 114,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Of these, we highlight here the SNoW ar\u00ad chitecture, which has some specific advantages that favor NLP-like domains.",
                    "sid": 115,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "SNoW determines the features' weights using an online algorithm that attempts to minimize the number of mistakes on the training data us\u00ad ing a multiplicative weight update rule (Lit88).",
                    "sid": 116,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The weight update rule is driven by the maxi\u00ad mum entropy principle (KW95).",
                    "sid": 117,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The main im\u00ad plication is that SNoW has significant advan\u00ad tages in sparse spaces, those in which a few of the features are actually relevant to the tar\u00ad get concept, as is typical in NLP.",
                    "sid": 118,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In domains with these characteristics, for a given number of training examples, SNoW generalizes better than additive update methods like perceptron and its close relative SVMs (Ros58; FS98) (and in general,it has better learning curves).",
                    "sid": 119,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, although in SNoW the transfor\u00ad mation to a large dimensional space needs to be done explicitly (rather than via kernel functions as is possible in perceptron and SVMs) its use of variable size examples nevertheless gives it com\u00ad putational advantages, due to the sparse feature space in NLP applications.",
                    "sid": 120,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is also significant for extensions to relational domain mentioned later.",
                    "sid": 121,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, SNoW is a multi-class classifier.",
                    "sid": 122,
                    "ssid": 27,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "future research issues. ",
            "number": "5",
            "sents": [
                {
                    "text": "Research on learning in NLP needs to be inte\u00ad grated with work on knowledge representation and inference to enable studying higher level NL tasks.",
                    "sid": 123,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We mention two important directions the implications on the learning issues.",
                    "sid": 124,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The unified view presented reveals that all methods blow up the dimensionality of the orig\u00ad inal space in essentially the same way; they gen\u00ad erate conjunctive features over the linear struc\u00ad ture of the sentence (i.e., n-gram like features in the word and/or pos space).",
                    "sid": 125,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This does not seem to be expressive enough.",
                    "sid": 126,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Expressing complex concepts and relations necessary for higher level inferences will re\u00ad quire more involved intermediate representa\u00ad tions (\"features\") over the input; higher order structural and semantic properties, long term dependencies and relational predicates need to be represented.",
                    "sid": 127,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Learning will stay manageable if done in terms of these intermediate represen\u00ad tations as done today, using functionally simple representations (perhaps cascaded).",
                    "sid": 128,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Inductive logic programming (MDR94; Coh95) is a natural paradigm for this.",
                    "sid": 129,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "How\u00ad ever, computational limitations that include both learnability and subsumption render this approach inadequate for large scale knowledge intensive problems (KRV99; CROO).",
                    "sid": 130,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In (CROO) we suggest an approach that ad\u00ad dresses the generation of complex and relational intermediate representations and supports effi\u00ad cient learning on top of those.",
                    "sid": 131,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It allows the generation and use of structured examples which could encode relational information and long term functional dependencies.",
                    "sid": 132,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is done us\u00ad ing a construct that defines \"types\" of (poten\u00ad tially, relational) features the learning process might use.",
                    "sid": 133,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These represent infinitely many fea\u00ad tures, and are not generated explicitly; only those present in the data are generated, on the fly, as part of the learning process.",
                    "sid": 134,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus it yields hypotheses that are as expressive as re\u00ad lational learners in a scalable fashion.",
                    "sid": 135,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This ap\u00ad proach, however, makes some requirements on the learning process.",
                    "sid": 136,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most importantly, the learning approach needs to be able to process variable size examples.",
                    "sid": 137,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "And, it has to be feature efficient in that its complexity depends mostly on the number of relevant features.",
                    "sid": 138,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This seems to favor the SNoW approach over other algo\u00ad rithms that learn the same representation.",
                    "sid": 139,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Eventually, we would like to perform infer\u00ad ences that depend on the outcomes of several different classifiers; together these might need to coherently satisfy some constrains arising from the sequential nature of the data or task and do\u00ad main specific issues.",
                    "sid": 140,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There is a need to study, along with learning and knowledge representa\u00ad tion, inference methods that suit this frame\u00ad work (KR97).",
                    "sid": 141,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Work in this direction requires a consistent semantics of the learners (Val99) and will have implications on the knowledge repre\u00ad sentations and learning methods used.",
                    "sid": 142,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Prelim\u00ad inary work in (PROO) suggests several ways to formalize this problem and is evaluated in the context of identifying phrase structure.",
                    "sid": 143,
                    "ssid": 21,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}