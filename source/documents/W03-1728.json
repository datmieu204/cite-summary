{
    "ID": "W03-1728",
    "sections": [
        {
            "text": "segmentation as tagging. ",
            "number": "1",
            "sents": [
                {
                    "text": "Unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are represented as strings of Chinese characters or hanzi without similar natural delimiters.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, the first step in a Chinese language processing task is to identify the sequence of words in a sentence and mark boundaries in appropriate places.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The key to accurate automatic word identification in Chinese lies in the successful resolution of ambiguities and a proper way to handle out-of-vocabulary words.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The ambiguities in Chinese word segmentation is due to the fact that a hanzi can occur in different word-internal positions (Xue, 2003).",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given the proper context, generally provided by the sentence in which it occurs, the position of a hanzi can be determined.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we model the Chinese word segmentation as a hanzi tagging problem and use a machine-learning algorithm to determine the appropriate position for a hanzi.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are several reasons why we may expect this approach to work.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, Chinese words generally have fewer than four characters.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a result, the number of positions is small.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, although each hanzi can in principle occur in all possible positions, not all hanzi behave this way.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A substantial number of hanzi are distributed in a constrained manner.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, , the plural marker, almost always occurs in the word-final position.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, although Chinese words cannot be exhaustively listed and new words are bound to occur in naturally occurring text, the same is not true for hanzi.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The number of hanzi stays fairly constant and we do not generally expect to see new hanzi.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We represent the positions of a hanzi with four different tags (Table 1): LM for a hanzi that occurs on the left periphery of a word, followed by other hanzi, MM for a hanzi that occurs in the middle of a word, MR for a hanzi that occurs on the right periphery of word, preceded by other hanzi, and LR for hanzi that is a word by itself.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We call this LMR tagging.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With this approach, word segmentation is a process where each hanzi is assigned an LMR tag and sequences of hanzi are then converted into sequences of words based on the LMR tags.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The use of four tags is linguistically intuitive in that LM tags morphemes that are prefixes or stems in the absence of prefixes, MR tags morphemes that are suffixes or stems in the absence of suffixes, MM tags stems with affixes and LR tags stems without affixes.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Representing the distributions of hanzi with LMR tags also makes it easy to use machine learning algorithms which has been successfully applied to other tagging problems, such as POS-tagging and IOB tagging used in text chunking.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Right Boundary (R) Not Right Boundary (M) Left Boundary (L) LR LM Not Left Boundary (M) MR MM Table 1: LMR Tagging",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "tagging algorithms. ",
            "number": "2",
            "sents": [
                {
                    "text": "Our algorithm consists of two parts.",
                    "sid": 22,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We first implement two Maximum Entropy taggers, one of which scans the input from left to right and the other scans the input from right to left.",
                    "sid": 23,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then we implement a Transformation Based Algorithm to combine the results of the two taggers.",
                    "sid": 24,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.1 The Maximum Entropy Tagger.",
                    "sid": 25,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Maximum Entropy Markov Model (MEMM) has been successfully used in some tagging problems.",
                    "sid": 26,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "MEMM models are capable of utilizing a large set of features that generative models cannot use.",
                    "sid": 27,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the other hand, MEMM approaches scan the input incrementally as generative models do.",
                    "sid": 28,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Maximum Entropy Markov Model used in POS-tagging is described in detail in (Ratnaparkhi, 1996) and the LMR tagger here uses the same probability model.",
                    "sid": 29,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The probability model is defined over , where is the set of possible contexts or \u201dhistories\u201d and is the set of possible tags.",
                    "sid": 30,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The model\u2019s joint probability of a history and a tag is defined as (1) where is a normalization constant, are the model parameters and are known as features, where . Each feature has a corresponding parameter , that effectively serves as a \u201dweight\u201d of this feature.",
                    "sid": 31,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the training process, given a sequence of characters and their LMR tags as train ing data, the purpose is to determine the parameters that maximize the likelihood of the training data using : (2) The success of the model in tagging depends to a large extent on the selection of suitable features.",
                    "sid": 32,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given , a feature must encode information that helps to predict . The features we used in our experiments are instantiations of the feature templates in (1).",
                    "sid": 33,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Feature templates (b) to (e) represent character features while (f) represents tag features.",
                    "sid": 34,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the following list, are characters and are LMR tags.",
                    "sid": 35,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1) Feature templates (a) Default feature (b) The current character ( ) (c) The previous (next) two characters ( , , , ) (d) The previous (next) character and the current character ( , ), the previous two characters ( ), and the next two characters ( ) (e) The previous and the next character ( ) (f) The tag of the previous character ( ), and the tag of the character two before the current character ( ) 2.2 Transformation-Based Learning.",
                    "sid": 36,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One potential problem with the MEMM is that it can only scan the input in one direction, from left to right or from right to left.",
                    "sid": 37,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is noted in (Lafferty et al., 2001) that non-generative finite-state models, MEMM models included, share a weakness which they call the Label Bias Problem (LBP): a transition leaving a given state compete only against all other transitions in the model.",
                    "sid": 38,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They proposed Conditional Random Fields (CRFs) as a solution to address this problem.",
                    "sid": 39,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A partial solution to the LBP is to compute the probability of transitions in both directions.",
                    "sid": 40,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This way we can use two MEMM taggers, one of which scans the input from left to right and the other scans the input from right to left.",
                    "sid": 41,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This strategy has been successfully used in (Shen and Joshi, 2003).",
                    "sid": 42,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In that paper, pairwise voting (van Halteren et al., 1998) has been used to combine the results of two supertaggers that scan the input in the opposite directions.",
                    "sid": 43,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The pairwise voting is not suitable in this application because we must make sure that the LMR tags assigned to consecutive words are compatible.",
                    "sid": 44,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, an LM tag cannot immediately follow an MM.",
                    "sid": 45,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pairwise voting does not use any contextual information, so it cannot prevent incompatible tags from occurring.",
                    "sid": 46,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, in our experiments described here, we use the Transformation-Based Learning (Brill, 1995) to combine the results of twoMEMM taggers.",
                    "sid": 47,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The feature set used in the TBL al 0.9148 0.9146 0.9144 0.9142 0.914 0.9138 0.9136 0.9134 0.9132 0.913 0.9128 0.9126 HK 100 150 200 250 300 iteration gorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001).",
                    "sid": 48,
                    "ssid": 27,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "experiments. ",
            "number": "3",
            "sents": [
                {
                    "text": "We conducted closed track experiments on three data sources: the Academia Sinica (AS) corpus, the Beijing University (PKU) corpus and the Hong Kong City University (CityU) corpus.",
                    "sid": 49,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We first split the training data from each of the three sources into two portions.",
                    "sid": 50,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "of the official training data is used to train the MEMM taggers, and the other is held out as the development test data (the development set).",
                    "sid": 51,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The development set is used to estimate the optimal number of iterations in the MEMM training.",
                    "sid": 52,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure (1), (2) and (3) show the curves of F-scores on the development set with respect to the number of iterations in MEMM training.",
                    "sid": 53,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 2: Learning curves on the development dataset of the HK City Univ. corpus.",
                    "sid": 54,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0.9391 PK 0.939 0.9389 0.9388 0.9387 0.9386 0.9385 0.9384 0.9383 0.9382 0.9381 200 300 400 500 600 700 800 iteration Figure 3: Learning curves on the development dataset of the Beijing Univ. corpus.",
                    "sid": 55,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0.9595 0.95945 0.9594 0.95935 0.9593 0.95925 0.9592 0.95915 0.9591 0.95905 0.959 AS a c h i e v e t h e b e s t r e s u l t s a f t e r 5 0 0 a n d 4 0 0 r o u n d s ( i t e r a t i o n s ) o f t r a i n i n g o n t h e A S d a t a a n d t h e P K U d a t a r e s p e c t i v e l y . H o w e v e r , t h e r e s u l t s o n t h e C i t y U d a t a i s n o t v e r y c l e a r . F r o m R o u n d 1 0 0 t h r o u g h 2 0 0 , t h e F s c o r e o n t h e d e v e l o p m e n t s e t a l m o s t s t a y s u n c h a n g e d . W e t h i n k t h i s i s b e c a u s e t h e C i t y U d a t a i s f r o m t h r e e d i f f e r e n t s o u r c e s , w h i c h d i f f e r i n t h e o p t i m a l n u m b e r o f i t e r a t i o n s . W e d e c i d e d t o t r a i n t h e M E M M t a g g e r s f o r 1 6 0 i t e r a t i o n s t h e H K C i t y U n i v e r s i t y d a t a . 200 300 400 500 600 700 800 iteration Figure 1: Learning curves on the development dataset of the Academia Sinica corpus.",
                    "sid": 56,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "X-axis stands for the number of iteration in training.",
                    "sid": 57,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Y-axis stands for the -score.",
                    "sid": 58,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Experiments show that the MEMM models We implemented two MEMM taggers, one scans the input from left to right and one from right to left.",
                    "sid": 59,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then used these two MEMM taggers to tag both the training and the development data.",
                    "sid": 60,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the LMR tagging output to train a Transformation- Based learner, using fast TBL (Ngai and Florian, 2001).",
                    "sid": 61,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The middle in Table 2 shows the F-score on the development set achieved by the MEMM tag- ger that scans the input from left to right and the last column is the results after the Transformation- Based Learner is applied.",
                    "sid": 62,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results show that using Transformation-Based learning only give rise to slight improvements.",
                    "sid": 63,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It seems that the bidirectional approach does not help much for the LMR tagging.",
                    "sid": 64,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, we only submitted the results of our left- to-right MEMM tagger, retrained on the entire training sets, as our official results.F sc or e M E M M M E M M +T B L A S 0.",
                    "sid": 65,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 5 9 5 0 . 9 6 0 3 H K 0.",
                    "sid": 66,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 1 4 3 N / A P K 0.",
                    "sid": 67,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 3 9 1 0 . 9 3 9 8 Table 2: F-score on development data The results on the official test data is similar to what we have got on our development set, except that the F-score on the Beijing Univ. corpus is over 2 lower in absolute accuracy than what we expected.",
                    "sid": 68,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The reason is that in the training data of Beijing University corpus, all the numbers are encoded in GBK, while in the test data many numbers are encoded in ASCII, which are unknown to our tagger.",
                    "sid": 69,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With this problem fixed, the results of the official test data are compatible with the results on our development set.",
                    "sid": 70,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, we have withdrawn our segmentation results on the Beijing University corpus.",
                    "sid": 71,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "co rp us R P F A S 0.",
                    "sid": 72,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "96 1 0.",
                    "sid": 73,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "95 8 0.",
                    "sid": 74,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "95 9 0.",
                    "sid": 75,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7 2 9 0.",
                    "sid": 76,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "96 6 H K 0.",
                    "sid": 77,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "91 7 0.",
                    "sid": 78,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "91 5 0.",
                    "sid": 79,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "91 6 0.",
                    "sid": 80,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6 7 0 0.",
                    "sid": 81,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "93 6 Table 3: Official Bakeoff Outcome",
                    "sid": 82,
                    "ssid": 34,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusions and future work. ",
            "number": "4",
            "sents": [
                {
                    "text": "Our closed track experiments on the first Sighan Bakeoff data show that the LMR algorithm produces promising results.",
                    "sid": 83,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our system ranks the second when tested on the Academia Sinica corpus and third on the Hong Kong City University corpus.",
                    "sid": 84,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the future, we will try to incorporate a large word list into our tagger to test its performance on open track experiments.",
                    "sid": 85,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Its high accuracy on makes it a good candidate as a general purpose segmenter.",
                    "sid": 86,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}