{
    "ID": "W02-1020",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "Text prediction is a form of interactive machine translation that is well suited to skilled translators.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In principle it can assist in the production of a target text with minimal disruption to a translator\u2019s normal routine.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, recent evaluations of a prototype prediction system showed that it significantly decreased the productivity of most translators who used it.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we analyze the reasons for this and propose a solution which consists in seeking predictions that maximize the expected benefit to the translator, rather than just trying to anticipate some amount of upcoming text.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using a model of a \u201ctypical translator\u201d constructed from data collected in the evaluations of the prediction prototype, we show that this approach has the potential to turn text prediction into a help rather than a hindrance to a translator.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "The idea of using text prediction as a tool for translators was first introduced by Church and Hovy as one of many possible applications for \u201ccrummy\u201d machine translation technology (Church and Hovy, 1993).",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Text prediction can be seen as a form of interactive MT that is well suited to skilled translators.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Compared to the traditional form of IMT based on Kay\u2019s original work (Kay, 1973)\u2014in which the user\u2019s role is to help disambiguate the source text\u2014 prediction is less obtrusive and more natural, allowing the translator to focus on and directly control the contents of the target text.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Predictions can benefit a translator in several ways: by accelerating typing, by suggesting translations, and by serving as an implicit check against errors.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first implementation of a predictive tool for translators was described in (Foster et al., 1997), in the form of a simple word-completion system based on statistical models.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Various enhancements to this were carried out as part of the TransType project (Langlais et al., 2000), including the addition of a realistic user interface, better models, and the capability of predicting multi-word lexical units.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the final TransType prototype for English to French translation, the translator is presented with a short pop- up menu of predictions after each character typed.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These may be incorporated into the text with a special command or rejected by continuing to type normally.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although TransType is capable of correctly anticipating over 70% of the characters in a freely-typed translation (within the domain of its training corpus), this does not mean that users can translate in 70% less time when using the tool.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In fact, in a trial with skilled translators, the users\u2019 rate of text production declined by an average of 17% as a result of using TransType (Langlais et al., 2002).",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are two main reasons for this.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, it takes time to read the system\u2019s proposals, so that in cases where they are wrong or too short, the net effect will be to slow the translator down.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, translators do not always act \u201crationally\u201d when confronted with a proposal; that is, they do not always accept correct proposals and they occasionally accept incorrect ones.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Many of the former cases correspond to translators simply ignoring proposals altogether, which is understandable behaviour given the first point.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This paper describes a new approach to text prediction intended to address these problems.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The main idea is to make predictions that maximize the expected benefit to the user in each context, rather than systematically proposing a fixed amount of text after each character typed.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The expected benefit is estimated from two components: a statistical translation model that gives the probability that a candidate prediction will be correct or incorrect, and a user model that determines the benefit to the translator in either case.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The user model takes into account the cost of reading a proposal, as well as the random nature of the decision to accept it or not.",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This approach can be characterized as making fewer but better predictions: in general, predictions will be longer in contexts where the translation model is confident, shorter where it is less so, and absent in contexts where it is very uncertain.",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other novel aspects of the work we describe here are the use of a more accurate statistical translation model than has previously been employed for text prediction, and the use of a decoder to generate predictions of arbitrary length, rather than just single words or lexicalized units as in the TransType prototype.",
                    "sid": 25,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The translation model is based on the maximum entropy principle and is designed specifically for this application.",
                    "sid": 26,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To evaluate our approach to prediction, we simulated the actions of a translator over a large corpus of previously-translated text.",
                    "sid": 27,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The result is an increase of over 10% in translator productivity when using the predictive tool.",
                    "sid": 28,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is a considerable improvement over the -17% observed in the TransType trials.",
                    "sid": 29,
                    "ssid": 29,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "the text prediction task. ",
            "number": "2",
            "sents": [
                {
                    "text": "In the basic prediction task, the input to the predictor is a source sentence s and a prefix h of its translation (ie, the target text before the current cursor position); the output is a proposed extension x to h. Figure 1 gives an example.",
                    "sid": 30,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unlike the TransType prototype, which proposes a set of single-word (or single-unit) suggestions, we assume that each prediction consists of only a single proposal, but one that may span an arbitrary number of words.",
                    "sid": 31,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As described above, the goal of the predictor is to find the prediction x\u02c6 that maximizes the expected s: Let us return to serious matters.",
                    "sid": 32,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "h x\u2217 t: O n v a r e venir aux ch o ses se\u00b4rieuses.",
                    "sid": 33,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "x: evenir a` Figure 1: Example of a prediction for English to French translation.",
                    "sid": 34,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "s is the source sentence, h is the part of its translation that has already been typed, x\u2217 is what the translator wants to type, and x is the prediction.",
                    "sid": 35,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "benefit to the user: x\u02c6 = argmax B(x, h, s), (1) x where B(x, h, s) measures typing time saved.",
                    "sid": 36,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This obviously depends on how much of x is correct, and how long it would take to edit it into the desired text.",
                    "sid": 37,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A major simplifying assumption we make is that the user edits only by erasing wrong characters from the end of a proposal.",
                    "sid": 38,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a TransType-style interface where acceptance places the cursor at the end of a proposal, this is the most common editing method, and it gives a conservative estimate of the cost attainable by other methods.",
                    "sid": 39,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With this assumption, the key determinant of edit cost is the length of the correct prefix of x, so the expected benefit can be written as: l B(x, h, s) = ) p(k|x, h, s) B(x, h, s, k), (2) k=0 where p(k|x, h, s) is the probability that exactly k characters from the beginning of x will be correct, l is the length of x, and B(x, h, s, k) is the benefit to the user given that the first k characters of x are correct.",
                    "sid": 40,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Equations (1) and (2) define three main problems: estimating the prefix probabilities p(k|x, h, s), estimating the user benefit function B(x, h, s, k), and searching for x\u02c6.",
                    "sid": 41,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The following three sections describe our solutions to these.",
                    "sid": 42,
                    "ssid": 13,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "translation model. ",
            "number": "3",
            "sents": [
                {
                    "text": "The correct-prefix probabilities p(k|x, h, s) are derived from a word-based statistical translation model.",
                    "sid": 43,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first step in the derivation is to convert these into a form that deals explicitly with character strings.",
                    "sid": 44,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is accomplished by noting that p(k|x, h, s) is the probability that the first k characters of x are correct and that the k + 1th character (if there is one) is incorrect.",
                    "sid": 45,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For k < l: p(k|x, h, s) = p(xk |h, s) \u2212 p(xk+1|h, s) likelihood that a word w will follow a previous sequence of words h in the translation of s.1 This is the family of distributions we have concentrated on modeling.",
                    "sid": 46,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our model for p(w|h, s) is a log-linear combination of a trigram language model for p(w|h) and a maximum-entropy translation model for p(w|s), de1 1 scribed in (Foster, 2000a; Foster, 2000b).",
                    "sid": 47,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The trans lation component is an analog of the IBM model 2 where xk = x1 . . .",
                    "sid": 48,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "xk . If k =.",
                    "sid": 49,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "l, p(k|x, h, s) = (Brown et al., 1993), with parameters that are op p(x |h, s).",
                    "sid": 50,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Also, p(x0) \u2261 1.",
                    "sid": 51,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "timiz ed for use with the trigr am.",
                    "sid": 52,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The com bine d The next step is to convert string probabilities into word probabilities.",
                    "sid": 53,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To do this, we assume that strings map one-to-one into token sequences, so that: model is shown in (Foster, 2000a) to have significantly lower test corpus perplexity than the linear combination of a trigram and IBM 2 used in the TransType experiments (Langlais et al., 2002).",
                    "sid": 54,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both models support O(mJ V 3) Viterbi-style searches for p(xk |h, s) \u2248 p(v1, w2, . . .",
                    "sid": 55,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", wm \u22121, um |h, s), the most likely sequence of m words that follows h, where v1 is a possibly-empty word suffix, each wi is a complete word, and um is a possibly empty word prefix.",
                    "sid": 56,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, if x in figure 1 were evenir aux choses, then x14 would map to v1 = evenir, w2 = aux, and u3 = cho.",
                    "sid": 57,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The one-to-one assumption is reasonable given that entries in our lexicon contain neither whitespace nor internal punctuation.",
                    "sid": 58,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To model word-sequence probabilities, we apply the chain rule: where J is the number of tokens in s and V is the size of the target-language vocabulary.",
                    "sid": 59,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Compared to an equivalent noisy-channel combination of the form p(t)p(s|t), where t is the target sentence, our model is faster but less accurate.",
                    "sid": 60,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J \u2014for in p(v1, w2, . . .",
                    "sid": 61,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", wm\u22121, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).",
                    "sid": 62,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It m\u22121 p(v1|h, s) n p(wi|h, v1, wi\u22121, s) \u00d7 i=2 p(um|h, v1, wm\u22121, s).",
                    "sid": 63,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(3) The probabilities of v1 and um can be expressed in terms of word probabilities as follows.",
                    "sid": 64,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Letting u1 be the prefix of the word that ends in v1 (eg, r in figure 1), w1 = u1v1, and h = htu1:is less accurate because it ignores the alignment rela tion between s and h, which is captured by even the simplest noisy-channel models.",
                    "sid": 65,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our model is therefore suitable for making predictions in real time, but not for establishing complete translations unassisted by a human.",
                    "sid": 66,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.1 Implementation.",
                    "sid": 67,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The most expensive part of the calculation in equation (3) is the sum in (4) over all words in the vo p(v1|h, s) = p(w1|ht, s)/ ) p(w|ht, s), cabulary, which according to (2) must be carried out w:w=u1 v where the sum is over all words that start with u1.",
                    "sid": 68,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similarly: p(um|ht, wm\u22121, s) = ) p(w|ht, wm\u22121, s).",
                    "sid": 69,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(4) 1 1 w:w=um v Thus all factors in (3) can be calculated from probabilities of the form p(w|h, s) which give the for every character position k in a given prediction x. We reduce the cost of this by performing sums only at the end of each sequence of complete tokens in x (eg, after revenir and revenir aux in the above example).",
                    "sid": 70,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "At these points, probabilities for all possible prefixes of the next word are calculated in a 1 Here we ignore the distinction between previous words that have been sanctioned by the translator and those that are hypothesized as part of the current prediction.",
                    "sid": 71,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "single recursive pass over the vocabulary and stored in a trie for later access.",
                    "sid": 72,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to the exact calculation, we also experimented with establishing exact probabilities via p(w|h, s) only at the end of each token in x, and assuming that the probabilities of the intervening characters vary linearly between these points.",
                    "sid": 73,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a result of this assumption, p(k|x, h, s) = p(xk |h, s) \u2212 1 0.9 0.8 0.7 0.6 0.5 0.4 raw smoothed model 1 |h, s) is constant for all k between the end of one word and the next, and therefore can be factored out of the sum in equation (2) between these points.",
                    "sid": 74,
                    "ssid": 32,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "user model. ",
            "number": "4",
            "sents": [
                {
                    "text": "The purpose of the user model is to determine the expected benefit B(x, h, s, k) to the translator of a prediction x whose first k characters match the text that the translator wishes to type.",
                    "sid": 75,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This will depend on whether the translator decides to accept or reject the prediction, so the first step in our model is the following expansion: B(x, h, s, k) = ) p(a|x, h, s, k) B(x, h, s, k, a), a\u2208{0,1} where p(a|x, h, s, k) is the probability that the translator accepts or rejects x, B(x, h, s, k, a) is the benefit they derive from doing so, and a is a random variable that takes on the values 1 for acceptance and 0 for rejection.",
                    "sid": 76,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first two quantities are the main elements in the user model, and are described in following sections.",
                    "sid": 77,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The parameters of both were estimated from data collected during the TransType trial described in (Langlais et al., 2002), which involved nine accomplished translators using a prototype prediction tool for approximately half an hour each.",
                    "sid": 78,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In all cases, estimates were made by pooling the data for all nine translators.",
                    "sid": 79,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.1 Acceptance Probability.",
                    "sid": 80,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ideally, a model for p(a|x, h, s, k) would take into account whether the user actually reads the proposal before accepting or rejecting it, eg: p(a|x, h, s, k) = ) p(a|r, x, h, s, k)p(r|x, h, s, k) r\u2208{0,1} where r is a boolean \u201cread\u201d variable.",
                    "sid": 81,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, this information is hard to extract reliably from the available data; and even if were obtainable, many of the 0.3 0.2 0.1 0 \u221260 \u221250 \u221240 \u221230 \u221220 \u221210 0 10 20 30 40 50 60 gain (length of correct prefix \u2212 length of incorrect suffix) Figure 2: Probability that a prediction will be accepted versus its gain.",
                    "sid": 82,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "factors which influence whether a user is likely to read a proposal\u2014such as a record of how many previous predictions have been accepted\u2014are not available to the predictor in our formulation.",
                    "sid": 83,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We thus model p(a|x, h, s, k) directly.",
                    "sid": 84,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our model is based on the assumption that the probability of accepting x depends only on what the user stands to gain from it, defined according to the editing scenario given in section 2 as the amount by which the length of the correct prefix of x exceeds the length of the incorrect suffix: p(a|x, h, s, k) \u2248 p(a|2k \u2212 l), where k \u2212 (l \u2212 k) = 2k \u2212 l is called the gain.",
                    "sid": 85,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, the gain for the prediction in figure 1 would be 2 \u00d7 7 \u2212 8 = 6.",
                    "sid": 86,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The strongest part of this assumption is dropping the dependence on h, because there is some evidence from the data that users are more likely to accept at the beginnings of words.",
                    "sid": 87,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, this does not appear to have a severe effect on the quality of the model.",
                    "sid": 88,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 2 shows empirical estimates of p(a = 1|2k \u2212 l) from the TransType data.",
                    "sid": 89,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There is a certain amount of noise intrinsic to the estimation procedure, since it is difficult to determine x\u2217, and there fore k, reliably from the data in some cases (when the user is editing the text heavily).",
                    "sid": 90,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Nonetheless, it is apparent from the plot that gain is a useful abstrac 4000 3500 3000 raw least\u2212squ ares fit 4000 3500 3000 r a w l e a s t \u2212 s q u a r e s f i t 2500 2500 2000 2000 1500 1500 1000 1000 500 500 0 0 Figure 3: Time to read and accept or reject proposals versus their length tion, because the empirical probability of acceptance is very low when it is less than zero and rises rapidly as it increases.",
                    "sid": 91,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This relatively clean separation supports the basic assumption in section 2 that benefit depends on k. The points labelled smoothed in figure 2 were obtained using a sliding-average smoother, and the model curve was obtained using two-component Gaussian mixtures to fit the smoothed empirical likelihoods p(gain|a = 0) and p(gain|a = 1).",
                    "sid": 92,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The model probabilities are taken from the curve at integral values.",
                    "sid": 93,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As an example, the probability of accepting the prediction in figure 1 is about .25.",
                    "sid": 94,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.2 Benefit.",
                    "sid": 95,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A natural unit for B(x, k, a) is the number of keystrokes saved, so all elements of the above equation are converted to this measure.",
                    "sid": 96,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is straightforward in the case of T (x, k) and E(x, k), which are estimated as k and l \u2212 k + 1 respectively\u2014for E(x, k), this corresponds to one keystroke for the command to accept a prediction, and one to erase each wrong character.",
                    "sid": 97,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is likely to slightly underestimate the true benefit, because it is usually harder to type n characters than to erase them.",
                    "sid": 98,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As in the previous section, read costs are interpreted as expected values with respect to the probability that the user actually does read x, eg, assuming 0 cost for not reading, R0(x) = p(r = 1|x)Rt (x), where Rt (x ) is the unknown true cost of reading The benefit B(x, h, s, k, a) is defined as the typing time the translator saves by accepting or rejecting a prediction x whose first k characters are correct.",
                    "sid": 99,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To determine this, we assume that the translator first reads x, then, if he or she decides to accept, uses a special command to place the cursor at the end of x and erases its last l \u2212 k characters.",
                    "sid": 100,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Assuming independence from h, s as before, our model is: r and rejecting x. To determine Ra(x), we measured the average elapsed time in the TransType data from the point at which a proposal was displayed to the point at which the next user action occurred\u2014either an acceptance or some other command signalling a rejection.",
                    "sid": 101,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Times greater than 5 seconds were treated as indicating that the translator was distracted and were filtered out.",
                    "sid": 102,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As shown in figure 3, read times are much higher for predictions that get accepted, re B(x, k, a) = \u2212R1(x) + T (x, k) \u2212 E(x, k), a = 1 \u2212R0(x), a = 0 flecting both a more careful perusal by the translator and the fact the rejected predictions are often simplywhere Ra(x) is the cost of reading x when it ulignored.2 In both cases there is a weak linear rela timately gets accepted (a = 1) or rejected (a = 0), T (x, k) is the cost of manually typing xk , and E(x, k) is the edit cost of accepting x and erasing to the end of its first k characters.",
                    "sid": 103,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 Here the number of characters read was assumed to include.",
                    "sid": 104,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "the whole contents of the TransType menu in the case of rejections, and only the proposal that was ultimately accepted in the case of acceptances.",
                    "sid": 105,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "tionship between the number of characters read and the time taken to read them, so we used the least- squares lines shown as our models.",
                    "sid": 106,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both plots are noisy and would benefit from a more sophisticated psycholinguistic analysis, but they are plausible and empirically-grounded first approximations.",
                    "sid": 107,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To convert reading times to keystrokes for the benefit function we calculated an average time per keystroke (304 milliseconds) based on sections of the trial where translators were rapidly typing and when predictions were not displayed.",
                    "sid": 108,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This gives an upper bound for the per-keystroke cost of reading\u2014 compare to, for instance, simply dividing the total time required to produce a text by the number of characters in it\u2014and therefore results in a conservative estimate of benefit.",
                    "sid": 109,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To illustrate the complete user model, in the figure 1 example the benefit of accepting would be7 \u2212 2 \u2212 4.2 = .8 keystrokes and the benefit of reject ing would be \u2212.2 keystrokes.",
                    "sid": 110,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Combining these with the acceptance probability of .25 gives an overall expected benefit B(x, h, s, k = 7) for this proposal of 0.05 keystrokes.",
                    "sid": 111,
                    "ssid": 37,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "search. ",
            "number": "5",
            "sents": [
                {
                    "text": "Searching directly through all character strings x in order to find x\u02c6 according to equation (1) would be very expensive.",
                    "sid": 112,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The fact that B(x, h, s) is non- monotonic in the length of x makes it difficult to organize efficient dynamic-programming search techniques or use heuristics to prune partial hypotheses.",
                    "sid": 113,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Because of this, we adopted a fairly radical search strategy that involves first finding the most likely sequence of words of each length, then calculating the benefit of each of these sequences to determine the best proposal.",
                    "sid": 114,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The algorithm is: 1.",
                    "sid": 115,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each length m = 1 . . .",
                    "sid": 116,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "M , find the best.",
                    "sid": 117,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "word sequence: M average time maximum time 1 2 3 4 5 0.0012 0.0038 0.0097 0.0184 0.0285 0.01 0.23 0.51 0.55 0.57 Table 1: Approximate times in seconds to generate predictions of maximum word sequence length M , on a 1.2GHz processor, for the MEMD model.",
                    "sid": 118,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In all experiments reported below, M was set to a maximum of 5 to allow for convenient testing.",
                    "sid": 119,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Step 1 is carried out using a Viterbi beam search.",
                    "sid": 120,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To speed this up, the search is limited to an active vocabulary of target words likely to appear in translations of s, defined as the set of all words connected by some word-pair feature in our translation model to some word in s. Step 2 is a trivial deterministic procedure that mainly involves deciding whether or not to introduce blanks between adjacent words (eg yes in the case of la + vie, no in the case of l\u2019 + an).",
                    "sid": 121,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This also removes the prefix u1 from the proposal.",
                    "sid": 122,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Step 3 involves a straightforward evaluation of m strings according to equation (2).",
                    "sid": 123,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 1 shows empirical search timings for various values of M , for the MEMD model described in the next section.",
                    "sid": 124,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Times for the linear model are similar.",
                    "sid": 125,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although the maximum times shown would cause perceptible delays for M > 1, these occur very rarely, and in practice typing is usually not noticeably impeded when using the TransType interface, even at M = 5.",
                    "sid": 126,
                    "ssid": 15,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "evaluation. ",
            "number": "6",
            "sents": [
                {
                    "text": "We evaluated the predictor for English to French translation on a section of the Canadian Hansard corpus, after training the model on a chronologi w\u02c6 m = argmax w1 :(w1 =u1 v), wm p(wm|ht, s), cally earlier section.",
                    "sid": 127,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The test corpus consisted of 5,020 sentence pairs and approximately 100k words where u1 and ht are as defined in section 3.",
                    "sid": 128,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2. Convert each w\u02c6 m to a corresponding character.",
                    "sid": 129,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "string x\u02c6m. in each language; details of the training corpus are given in (Foster, 2000b).",
                    "sid": 130,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To simulate a translator\u2019s responses to predictions, we relied on the user model, accepting prob 3.",
                    "sid": 131,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Output x\u02c6.",
                    "sid": 132,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "= argmaxm B(x\u02c6m, h, s), or the abilistically according to p(a|x, h, s, k), determinempty string if all B(x\u02c6m, h, s) are non positive.",
                    "sid": 133,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ing the associated benefit using B(x, h, s, k, a), and advancing the cursor k characters in the case of an config M 1 2 3 4 5 fixed linear exact corr best -8.50.43.6011.620.8 6.1 9.40 8.8 8.1 7.8 5.3 10.10 10.7 10.0 9.7 5.8 10.7 12.0 12.5 12.6 7.9 17.90 24.5 27.7 29.2 fixed exact best -11.59.315.122.028.2 3.0 4.3 5.0 5.2 5.2 6.2 12.1 15.4 16.7 17.3 Table 2: Results for different predictor configurations.",
                    "sid": 134,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Numbers give % reductions in keystrokes.",
                    "sid": 135,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "user M 1 2 3 4 5 superman rational real 48.6 53.5 51.8 51.1 50.9 11.7 17.8 17.2 16.4 16.1 5.3 10.10 10.7 10.0 9.7 Table 3: Results for different user simulations.",
                    "sid": 136,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Numbers give % reductions in keystrokes.",
                    "sid": 137,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "acceptance, 1 otherwise.",
                    "sid": 138,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here k was obtained by comparing x to the known x\u2217 from the test corpus.",
                    "sid": 139,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It may seem artificial to measure performance according to the objective function for the predictor, but this is biased only to the extent that it misrepresents an actual user\u2019s characteristics.",
                    "sid": 140,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are two cases: either the user is a better candidate\u2014types more slowly, reacts more quickly and rationally\u2014 than assumed by the model, or a worse one.",
                    "sid": 141,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The predictor will not be optimized in either case, but the simulation will only overestimate the benefit in the second case.",
                    "sid": 142,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By being conservative in estimating the parameters of the user model, we feel we have minimized the number of translators who would fall into this category, and thus can hope to obtain realistic lower bounds for the average benefit across all translators.",
                    "sid": 143,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 contains results for two different translation models.",
                    "sid": 144,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The top portion corresponds to the MEMD2B maximum entropy model described in (Foster, 2000a); the bottom portion corresponds to the linear combination of a trigram and IBM 2 used in the TransType experiments (Langlais et al., 2002).",
                    "sid": 145,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Columns give the maximum permitted number of words in predictions.",
                    "sid": 146,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Rows show different predic tor configurations: fixed ignores the user model and makes fixed M -word predictions; linear uses the linear character-probability estimates described in section 3.1; exact uses the exact character-probability calculation; corr is described below; and best gives an upper bound on performance by choosing m in step 3 of the search algorithm so as to maximize B(x, h, s, k) using the true value of k. Table 3 illustrates the effects of different components of the user model by showing results for simulated users who read infinitely fast and accept only predictions having positive benefit (superman); who read normally but accept like superman (rational); and who match the standard user model (real).",
                    "sid": 147,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each simulation, the predictor optimized benefits for the corresponding user model.",
                    "sid": 148,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Several conclusions can be drawn from these results.",
                    "sid": 149,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, it is clear that estimating expected benefit is a much better strategy than making fixed-word- length proposals, since the latter causes an increase in time for all values of M . In general, making \u201cexact\u201d estimates of string prefix probabilities works better than a linear approximation, but the difference is fairly small.",
                    "sid": 150,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, the MEMD2B model significantly outperforms the trigram+IBM2 combination, producing better results for every predictor configuration tested.",
                    "sid": 151,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The figure of -11.5% in bold corresponds to the TransType configuration, and corroborates the validity of the simulation.3 Third, there are large drops in benefit due to reading times and probabilistic acceptance.",
                    "sid": 152,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The biggest cost is due to reading, which lowers the best possible keystroke reduction by almost 50% for M = 5.",
                    "sid": 153,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Probabilistic acceptance causes a further drop of about 15% for M = 5.",
                    "sid": 154,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The main disappointment in these results is that performance peaks at M = 3 rather than continuing to improve as the predictor is allowed to consider longer word sequences.",
                    "sid": 155,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the predictor knows B(x, h, s, k), the most likely cause for this is that the estimates for p(w\u02c6 m|h, s) become worse with increasing m. Significantly, performance lev 3 Although the drop observed with real users was greater at about 20% (= 17% reduction in speed), there are many differences between experimental setups that could account for the discrepancy.",
                    "sid": 156,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, part of the corpus used for the TransType trials was drawn from a different domain, which would adversely affect predictor performance.",
                    "sid": 157,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "els off at three words, just as the search loses direct contact with h through the trigram.",
                    "sid": 158,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To correct for this, we used modified probabilities of the form \u03bbm p(w\u02c6 m|h, s), where \u03bbm is a length-specific correction factor, tuned so as to optimize benefit on a cross-validation corpus.",
                    "sid": 159,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results are shown in the corr row of table 2, for exact character-probability estimates.",
                    "sid": 160,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this case, performance improves with M , reaching a maximum keystroke reduction of 12.6% at M = 5.",
                    "sid": 161,
                    "ssid": 35,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusion and future work. ",
            "number": "7",
            "sents": [
                {
                    "text": "We have described an approach to text prediction for translators that is based on maximizing the benefit to the translator according to an explicit user model whose parameters were set from data collected in user evaluations of an existing text prediction prototype.",
                    "sid": 162,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using this approach, we demonstrate in simulated results that our current predictor can reduce the time required for an average user to type a text in the domain of our training corpus by over 10%.",
                    "sid": 163,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We look forward to corroborating this result in tests with real translators.",
                    "sid": 164,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are many ways to build on the work described here.",
                    "sid": 165,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The statistical models which are the backbone of the predictor could be improved by making them adaptive\u2014taking advantage of the user\u2019s input\u2014and by adding features to capture the alignment relation between h and s in such a way as to preserve the efficient search properties.",
                    "sid": 166,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The user model could also be made adaptive, and it could be enriched in many other ways, for instance so as to capture the propensity of translators to accept at the beginnings of words.",
                    "sid": 167,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We feel that the idea of creating explicit user models to guide the behaviour of interactive systems is likely to have applications in areas of NLP apart from translators\u2019 tools.",
                    "sid": 168,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For one thing, most of the approach described here carries over more or less directly to monolingual text prediction, which is an important tool for the handicapped (Carlberger et al., 1997).",
                    "sid": 169,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other possibilities include virtually any application where a human and a machine communicate through a language-rich interface.",
                    "sid": 170,
                    "ssid": 9,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}