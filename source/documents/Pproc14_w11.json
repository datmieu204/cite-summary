{
    "ID": "Pproc14_w11",
    "sections": [
        {
            "text": "compositionality of mwes. ",
            "number": "1",
            "sents": [
                {
                    "text": "Multiword expressions (hereafter MWEs) are combinations of words which are lexically, syntac\u00ad tically, semantically or statistically idiosyncratic (Sag et al., 2002; Baldwin and Kim, 2009).",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Much research has been carried out on the extraction and identification of MWEs1 in English (Schone and Jurafsky, 2001; Pecina, 2008; Fazly et al., 2009) and other languages (Dias, 2003; Evert and Krenn, 2005; Salehi et al., 2012).",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, considerably less work has addressed the task of predicting the meaning of MWEs, especially in non-English lan\u00ad guages.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a step in this direction, the focus of this study is on predicting the compositionality of MWEs.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An MWE is fully compositional if its meaning is predictable from its component words, and it is non-compositional (or idiomatic) if not.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For ex\u00ad ample, stand up \"rise to one's feet\" is composi 1In this paper, we follow Baldwin and Kim (2009) in considering MWE \"identification\" to be a token-level disam\u00ad biguation task, and MWE \"extraction\" to be a type-level lex\u00b7 icon induction task.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "tional, because its meaning is clear from the mean\u00ad ing of the components stand and up.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the meaning of strike up \"to start playing\" is largely unpredictable from the component words strike and up.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this study, following McCarthy et al.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2003) and Reddy et al.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2011), we consider composition\u00ad ality to be graded, and aim to predict the degree of compositionality.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, in the dataset of Reddy et al.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2011), climate change is judged to be 99% compositional, while silver screen is 48% compositional and ivory tower is 9% com\u00ad positional.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Formally, we model compositionality prediction as a regression task.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An explicit handling of MWEs has been shown to be useful in NLP applications (Rarnisch, 2012).",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As an example, Carpuat and Diab (2010) proposed two strategies for integrating MWEs into statisti\u00ad cal machine translation.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They show that even a large scale bilingual corpus cannot capture all the necessary information to translate MWEs, and that in adding the facility to model the compositional\u00ad ity ofMWEs into their system, they could improve translation quality.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Acosta et al.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2011) showed that treating non-compositional MWEs as a sin\u00ad gle unit in information retrieval improves retrieval effectiveness.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, while searching for documents related to ivory tower, we are almost certainly not interested in documents relating to elephant tusks.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our approach is to use a large-scale multi-way translation lexicon to source translations of MWEs and their component words, and then model the relative similarity between each of the component words and the MWE, using distributional similar\u00ad ity based on monolingual corpora for the source language and each of the target languages.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our hypothesis is that using distributional similarity in more than one language will improve the pre\u00ad diction of compositionality.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "hnportantly, in order to make the method as language-independent and 472 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 472--481, Gothenburg, Sweden, Apri12630 2014.",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "@2014 Association for Computational Linguistics broadly-applicable as possible, we make no use of corpus preprocessing such as lemmatisation, and rely only on the availability of a translation dictio\u00ad nary and monolingual corpora.",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our results confirm our hypothesis that distri\u00ad butional similarity over the source language in ad\u00ad dition to multiple target languages improves the quality of compositionality prediction.",
                    "sid": 25,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also show that our method can be complemented with string similarity (Salehi and Cook, 2013) to further improve compositionality prediction.",
                    "sid": 26,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We achieve state-of-the-art results over two datasets.",
                    "sid": 27,
                    "ssid": 27,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "related work. ",
            "number": "2",
            "sents": [
                {
                    "text": "Most recent work on predicting the composi\u00ad tionality of MWEs can be divided into two categories: language/construction-specific and general-purpose.",
                    "sid": 28,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This can be at either the token\u00ad level (over token occurrences of an MWE in a cor\u00ad pus) or type-level (over the MWE string, indepen\u00ad dent of usage).",
                    "sid": 29,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The bulk of work on composition\u00ad ality has been language/construction-specific and operated at the token-level, using dedicated meth\u00ad ods to identify instances of a given MWE, and specific properties of the MWE in that language to predict compositionality (Lin, 1999; Kim and Baldwin, 2007; Fazly et al., 2009).",
                    "sid": 30,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "General-purpose token-level approaches such as distributional similarity have been commonly applied to infer the semantics of a word/MWE (Schone and Jurafsky, 2001; Baldwin et al., 2003; Reddy et al., 2011).",
                    "sid": 31,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These techniques are based on the assumption that the meaning of a word is predictable from its context of use, via the neigh\u00ad bouring words of token-level occurrences of the MWE.",
                    "sid": 32,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to predict the compositionality of a given MWE using distributional similarity, the different contexts of the MWE are compared with the contexts of its components, and the MWE is considered to be compositional if the MWE and component words occur in similar contexts.",
                    "sid": 33,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Identifying token instances of MWEs is not al\u00ad ways easy, especially when the component words do not occur sequentially.",
                    "sid": 34,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example consider put on in put your jacket on, and put your jacket on the chair.",
                    "sid": 35,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the first example put on is an MWE while in the second example, put on is a simple verb with prepositional phrase and not an instance of an MWE.",
                    "sid": 36,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, if we adopt a con\u00ad servative identification method, the number of to\u00ad ken occurrences will be limited and the distribu tional scores may not be reliable.",
                    "sid": 37,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additionally, for morphologically-rich languages, it can be dif\u00ad ficult to predict the different word forms a given MWE type will occur across, posing a challenge for our requirement of no language-specific pre\u00ad processing.",
                    "sid": 38,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pichotta and DeNero (2013) proposed a token\u00ad based method for identifying English phrasal verbs based on parallel corpora for 50 languages.",
                    "sid": 39,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They show that they can identify phrasal verbs bet\u00ad ter when they combine information from multiple languages, in addition to the information they get from a monolingual corpus.",
                    "sid": 40,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This finding lends weight to our hypothesis that using translation data and distributional similarity from each of a range of target languages, can improve compositionality prediction.",
                    "sid": 41,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Having said that, the general applica\u00ad bility of the method is questionable -there are many parallel corpora involving English, but for other languages, this tends not to be the case.",
                    "sid": 42,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Salehi and Cook (2013) proposed a general\u00ad purpose type-based approach using translation data from multiple languages, and string similar\u00ad ity between the MWE and each of the compo\u00ad nent words.",
                    "sid": 43,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They use training data to identify the best-10 languages for a given family ofMWEs, on which to base the string similarity, and once again find that translation data improves their results substantially.",
                    "sid": 44,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Among the four string similarity measures they experimented with, longest com\u00ad mon substring was found to perform best.",
                    "sid": 45,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Their proposed method is general and applicable to dif\u00ad ferent families of MWEs in different languages.",
                    "sid": 46,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we reimplement the method of Salehi and Cook (2013) using longest common substring (LCS), and both benchmark against this method and combine it with our distributional similarity\u00ad based method.",
                    "sid": 47,
                    "ssid": 20,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "our approach. ",
            "number": "3",
            "sents": [
                {
                    "text": "To predict the compositionality of a given MWE, we first measure the semantic similarity between the MWE and each of its component words2 using distributional similarity based on a monolingual corpus in the source language.",
                    "sid": 48,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then repeat the process for translations of the MWE and its com\u00ad ponent words into each of a range of target lan\u00ad guages, calculating distributional similarity using 2Note that we will always assume that there are two component words, but the method is easily generalisable to MWEs with more than two components.",
                    "sid": 49,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "MWE component1 component2 Score1foreachlanguage Score2foreachlangua ge Translate (using Panlex) Translate (using Panlex) Translate (using Panlex) csmethod csmethod DS DS (using Wikiepdia) (using Wikiepdia) j j score1 score2 Figure 1: Outline of our approach to computing the distributional similarity (DS) of translations of an MWE with each of its component words, for a given target language.",
                    "sid": 50,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "score1 and score2 are the similarity for the first and second compo\u00ad nents, respectively.",
                    "sid": 51,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We obtain translations from Panlex, and use Wikipedia as our corpus for each language.",
                    "sid": 52,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "a monolingual corpus in the target language (Fig\u00ad ure 1).",
                    "sid": 53,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We additionally use supervised learning to identify which target languages (or what weights for each language) optimise the prediction of com\u00ad positionality (Figure 2).",
                    "sid": 54,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We hypothesise that by using multiple translations -rather than only in\u00ad formation from the source language -we will be able to better predict compositionality.",
                    "sid": 55,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We optionally combine our proposed approach with string similarity, calculated based on the method of Salehi and Cook (2013), using LCS.",
                    "sid": 56,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Below, we detail our method for calculating dis\u00ad tributional similarity in a given language, the dif\u00ad ferent methods for combining distributional simi\u00ad larity scores into a single estimate of composition\u00ad ality, and finally the method for selecting the target languages to use in calculating compositionality.",
                    "sid": 57,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.1 Calculating Distributional Similarity.",
                    "sid": 58,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to be consistent across all languages and be as language-independent as possible, we calcu Compositionality score Figure 2: Outline of the method for combin\u00ad ing distributional similarity scores from multiple languages, across the components of the MWE.",
                    "sid": 59,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CSmethod refers to one of the methods described in Section 3.2 for calculating compositionality.",
                    "sid": 60,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "late distributional similarity in the following man\u00ad ner for a given language.",
                    "sid": 61,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Tokenisation is based on whitespace delimiters and punctuation; no lemmatisation or case-folding is carried out.",
                    "sid": 62,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Token instances of a given MWE or component word are identified by full-token n\u00ad gram matching over the token stream.",
                    "sid": 63,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We assume that all full stops and equivalent characters for other orthographies are sentence boundaries, and chunk the corpora into (pseudo-)sentences on the basis of them.",
                    "sid": 64,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each language, we identify the 51st1050th most frequent words, and consider them to be content-bearing words, in the manner of Schtitze (1997).",
                    "sid": 65,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is based on the assump\u00ad tion that the top-50 most frequent words are stop words, and not a good choice of word for calculat\u00ad ing distributional similarity over.",
                    "sid": 66,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is not to say that we can't calculate the distributional similarity for stop words, however (as we will for the verb particle construction dataset-see Section 4.3.2) they are simply not used as the dimensions in our calculation of distributional similarity.",
                    "sid": 67,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We form a vector of content-bearing words across all token occurrences of the target word, on the basis of these content-bearing words.",
                    "sid": 68,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Dis\u00ad tributional similarity is calculated over these con\u00ad text vectors using cosine similarity.",
                    "sid": 69,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Accord\u00ad ing to Weeds (2003), using dependency rela\u00ad tions with the neighbouring words of the target word can better predict the meaning of the target word.",
                    "sid": 70,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, in line with our assumption of no language-specific preprocessing, we just use word co-occurrence.",
                    "sid": 71,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2 Calculating Compositionality.",
                    "sid": 72,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we need to calculate a combined composi\u00ad tionality score from the individual distributional similarities between each component word and the MWE.",
                    "sid": 73,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Following Reddy et al.",
                    "sid": 74,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2011), we combine the component scores using the weighted mean (as shown in Figure 2): comp = as1 + (1- a)s2 (1) where s1 and s2 are the scores for the first and the second component, respectively.",
                    "sid": 75,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use dif\u00ad ferent a settings for each dataset, as detailed in Section 4.3.",
                    "sid": 76,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We experiment with a range of methods for cal\u00ad culating compositionality, as follows: CS Lt : calculate distributional similarity using only distributional similarity in the source language corpus (This is the approach used by Reddy et al.",
                    "sid": 77,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2011), as discussed in Sec\u00ad tion 2).",
                    "sid": 78,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CS L2N: exclude the source language, and com\u00ad pute the mean of the distributional similarity scores for the best-N target languages.",
                    "sid": 79,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The value of N is selected according to training data, as detailed in Section 3.3.",
                    "sid": 80,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CS Lt +L2N: calculate distributional similarity over both the source language ( CS Lt) and the mean of the best-N languages ( CS L2N ), and combine via the arithmetic mean.3 This is to examine the hypothesis that using multiple target languages is better than just using the source language.",
                    "sid": 81,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CSsvR(Lt+L2): train a support vector regressor (SVR: Smola and SchOlkopf (2004)) over the distributional similarities for all 52 languages (source and target languages).",
                    "sid": 82,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3We also experimented with taking the mean over all the languages -target and source -but found it best to com\u00ad bine the scores for the target languages first, to give more weight to the source language.",
                    "sid": 83,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CS string: calculate string similarity using the LCS-based method of Salehi and Cook (2013).4 CS string+Lt: calculate the mean of the string similarity (CS string) and distributional sim\u00ad ilarity in the source language (Salehi and Cook, 2013).",
                    "sid": 84,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "CS all: calculate the mean of the string similarity ( CS string) and distributional similarity scores ( CS L1 and CS L2N ).",
                    "sid": 85,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.3 Selecting Target Languages.",
                    "sid": 86,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We experiment with two approaches for combin\u00ad ing the compositionality scores from multiple tar\u00ad get languages.",
                    "sid": 87,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, in CS L2N (and CS Lt +L2N and CS all that build off it), we use training data to rank the target languages according to Pearson's correlation be\u00ad tween the predicted compositionality scores and the gold-standard compositionality judgements.",
                    "sid": 88,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Based on this ranking, we take the best-N lan\u00ad guages, and combine the individual composition\u00ad ality scores by taking the arithmetic mean.",
                    "sid": 89,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We se\u00ad lect N by determining the value that optimises the correlation over the training data.",
                    "sid": 90,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In other words, the selection of Nand accordingly the best-N lan\u00ad guages are based on nested cross-validation over training data, independently of the test data for that iteration of cross-validation.",
                    "sid": 91,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second in CSsvR(Lt+L2), we combine the compositionality scores from the source and all 51 target languages into a feature vector, and train an SVR over the data using LIBSVM.5",
                    "sid": 92,
                    "ssid": 45,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "resources. ",
            "number": "4",
            "sents": [
                {
                    "text": "In this section, we describe the resources required by our method, and also the datasets used to eval\u00ad uate our method.",
                    "sid": 93,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.1 Monolingual Corpora for Different.",
                    "sid": 94,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Languages We collected monolingual corpora for each of 52 languages (51 target languages + 1 source lan\u00ad guage) from XML dumps of Wikipedia.",
                    "sid": 95,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These languages are based on the 54 target languages 4Due to differences in our random partitioning, our re\u00ad ported results over the two English datasets differ slightly over the results of Salehi and Cook (2013) using the same method.",
                    "sid": 96,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5http://www.csie.ntu.edu.tw/-cjlin/libsvm used by Salehi and Cook (2013), excluding Span\u00ad ish because we happened not to have a dump of Spanish Wikipedia, and also Chinese and Japanese because of the need for a language-specific word tokeniser.",
                    "sid": 97,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The raw corpora were preprocessed us\u00ad ing the WP2TXT toolbox6 to eliminate XML tags, HTML tags and hyperlinks, and then tokenisa\u00ad tion based on whitespace and punctuation was per\u00ad formed.",
                    "sid": 98,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The corpora vary in size from roughly 150M tokens for English, to roughly 640K tokens for Marathi.",
                    "sid": 99,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.2 Multilingual Dictionary.",
                    "sid": 100,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To translate the MWEs and their components, we follow Salehi and Cook (2013) in using Pan\u00ad lex (Baldwin et al., 2010).",
                    "sid": 101,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This online dictio\u00ad nary is massively multilingual, covering more than 1353 languages.",
                    "sid": 102,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each MWE dataset (see Sec\u00ad tion 4.3), we translate the MWE and component words from the source language into each of the 51 languages.",
                    "sid": 103,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In instances where there is no direct translation in a given language for a term, we use a pivot lan\u00ad guage to find translation(s) in the target language.",
                    "sid": 104,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the English noun compound silver screen has direct translations in only 13languages in Panlex, including Vietnamese (m2m bac) but not French.",
                    "sid": 105,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There is, however, a translation of man bac into French (cinema), allowing us to infer an indirect translation between silver screen and cinema.",
                    "sid": 106,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this way, if there are no direct translations into a particular target language, we search for a single-pivot translation via each of our other target languages, and combine them all to\u00ad gether as our set of translations for the target lan\u00ad guage of interest.",
                    "sid": 107,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the case that no translation (direct or indirect) can be found for a given source language term into a particular target language, the compositionality score for that target language is set to the average across all target languages for which scores can be calculated for the given term.",
                    "sid": 108,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If no translations are available for any target language (e.g. the term is not in Panlex) the compositionality score for each target language is set to the average score for that target language across all other source language terms.",
                    "sid": 109,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6http://wp2txt.rubyforge.org/ 4.3 Datasets.",
                    "sid": 110,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluate our proposed method over three datasets (two English, one German), as described below.",
                    "sid": 111,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.3.1 English Noun Compounds (ENC) Our first dataset is made up of 90 binary English noun compounds, from the work of Reddy et al.",
                    "sid": 112,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2011).",
                    "sid": 113,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each noun compound was annotated by multiple annotators using the integer scale 0 (fully non-compositional) to 5 (fully compositional).",
                    "sid": 114,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A final compositionality score was then calculated as the mean of the scores from the annotators.",
                    "sid": 115,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If we simplistically consider 2.5 as the threshold for compositionality, the dataset is relatively well balanced, containing 48% compositional and 52% non-compositional noun compounds.",
                    "sid": 116,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Following Reddy et al.",
                    "sid": 117,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2011), in combining the component\u00ad wise distributional similarities for this dataset, we weight the first component in Equation 1 higher than the second (a= 0.7).",
                    "sid": 118,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.3.2 English Verb Particle Constructions (EVPC) The second dataset contains 160 English verb par\u00ad ticle constructions (VPCs), from the work of Ban\u00ad nard (2006).",
                    "sid": 119,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this dataset, a verb particle con\u00ad struction consists of a verb (the head) and a prepo\u00ad sitional particle (e.g. hand in, look up or battle on).",
                    "sid": 120,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each component word (the verb and parti\u00ad cle, respectively), multiple annotators were asked whether the VPC entails the component word.",
                    "sid": 121,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to translate the dataset into a regression task, we calculate the overall compositionality as the number of annotations of entailment for the verb, divided by the total number of verb annotations for that VPC.",
                    "sid": 122,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, following Bannard et al.",
                    "sid": 123,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2003), we only consider the compositionality of the verb component in our experiments (and as such a = 1 in Equation 1).",
                    "sid": 124,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One area of particular interest with this dataset will be the robustness of the method to function words (the particles), both under translation and in terms of calculating distributional similarity, al\u00ad though the findings of Baldwin (2006) for English prepositions are at least encouraging in this re\u00ad spect.",
                    "sid": 125,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additionally, English VPCs can occur in \"split\" form (e.g. put your jacket on, from our earlier example), which will complicate identifi\u00ad cation, and the verb component will often be in\u00ad flected and thus not match under our identification strategy (for both VPCs and the component verbs).",
                    "sid": 126,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Dataset Language Frequency Family Italian 100 Romance French 99 Romance each N is selected over 100 folds on ENC, EVPC and GNC datasets, respectively.",
                    "sid": 127,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From the his\u00ad ENC EVPC GNC German 86 Germanic Vietnamese 83 VietMuong Portuguese 62 Romance Bulgarian 100 Slavic Breton 100 Celtic Occitan 100 Romance Indonesian 100 Indonesian Slovenian 100 Slavic Polish 100 Slavic Lithuanian 99 Baltic Finnish 74 Uralic Bulgarian 72 Slavic Czech 40 Slavic tograms, N = 6, N = 15 and N = 2 are the most commonl y selected settings for ENC, EVPC and GNC, respective ly.",
                    "sid": 128,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, multiple languages are generally used, but more languages are used for English VPCs than either of the compoun d noun datasets.",
                    "sid": 129,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The 5 most selected languages for ENC, EVPC and GNC are shown in Table 1.",
                    "sid": 130,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As we can see, there are some languages which are al\u00ad ways selected for a given dataset, but equally the commonl y-selected languages vary considera bly between datasets.",
                    "sid": 131,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Further analysis reveals that 32 (63%) target Table 1: The 5 best languages for the ENC, EVPC and GNC datasets.",
                    "sid": 132,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The language family is based on Voegelin and Voegelin (1977).",
                    "sid": 133,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.3.3 German Noun Compounds (GNC) Our final dataset is made up of 246 German noun compounds (von der Heide and Borgwaldt, 2009; Schulte im Walde et al., 2013).",
                    "sid": 134,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Multiple anno\u00ad tators were asked to rate the compositionality of each German noun compound on an integer scale of 1 (non-compositional) to 7 (compositional).",
                    "sid": 135,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The overall compositionality score is then calcu\u00ad lated as the mean across the annotators.",
                    "sid": 136,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that the component words are provided as part of the dataset, and that there is no need to perform de\u00ad compounding.",
                    "sid": 137,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Following Schulte im Walde et al.",
                    "sid": 138,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2013), we weight the first component higher in Equation 1 (a= 0.8) when calculating the overall compositionality score.",
                    "sid": 139,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This dataset is significant in being non-English, and also in that German has relatively rich mor\u00ad phology, which we expect to impact on the iden\u00ad tification of both the MWE and the component words.",
                    "sid": 140,
                    "ssid": 48,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "results. ",
            "number": "5",
            "sents": [
                {
                    "text": "All experiments are carried out using 10 iterations of 10-fold cross validation, randomly partitioning the data independently on each of the 10 iterations, and averaging across all 100 test partitions in our presented results.",
                    "sid": 141,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the case of CSL2N and other methods that make use of it (i.e. CSL1+L2N and CSau), the languages selected for a given training fold are then used to compute the compositionality scores for the instances in the test set.",
                    "sid": 142,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figures 3a, 3b and 3c are histograms of the number of times languages for ENC, 25 (49%) target languages for EVPC, and only 5 (10%) target languages for GNC have a correlation of r 2: 0.1 with gold\u00ad standard compositionality judgements.",
                    "sid": 143,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the other hand, 8 (16%) target languages for ENC, 2 (4%) target languages for EVPC, and no target lan\u00ad guages for GNC have a correlation of r :::; -0.1.",
                    "sid": 144,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.1 ENC Results.",
                    "sid": 145,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "English noun compounds are relatively easy to identify in a corpus,7 because the components oc\u00ad cur sequentially, and the only morphological vari\u00ad ation is in noun number (singular vs. plural).",
                    "sid": 146,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In other words, the precision for our token match\u00ad ing method is very high, and the recall is also acceptably high.",
                    "sid": 147,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Partly as a result of the ease of identification, we get a high correlation of r = 0.700 for CSL1 (using only source language data).",
                    "sid": 148,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using only target languages (CSL2N), the results drop tor = 0.434, but when we combine the two (CSL1+L2N), the correlation is higher than using only source or target language data, at r = 0.725.",
                    "sid": 149,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When we combine all languages us\u00ad ing SVR, the results rise slightly higher again to r = 0.744, which is slightly above the correla\u00ad tion of the state-of-the-art method of Salehi and Cook (2013), which combines their method with the method of Reddy et al.",
                    "sid": 150,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2011) (CS string+LJ ).",
                    "sid": 151,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These last two results support our hypothesis that using translation data can improve the prediction of compositionality.",
                    "sid": 152,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results for string similar\u00ad ity on its own (CSstring\u2022 r = 0.644) are slightly lower than those using only source language dis\u00ad tributional similarity, but when combined with 7Although see Lapata and Lascarides (2003) for discus\u00ad sion of the difficulty of reliably identifying low-frequency English noun compounds.",
                    "sid": 153,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(a) ENC (b)EVPC '\" 10 8 bestN (c) GNC Figure 3: Histograms displaying how many times a given N is selected as the best number of languages over each dataset.",
                    "sid": 154,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, according to the GNC chart, there is a peak for N = 2, which shows that over 100 folds, the best-2languages achieved the highest correlation on 18 folds.",
                    "sid": 155,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "M eth od Su m ma ry of the M eth od E N C E VP C G N C CS LJ So urc e lan gu ag e 0.7 00 0.",
                    "sid": 156,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "17 7 0.1 41 CS L2 N Best N tar get lan gu ag es 0.4 34 0.",
                    "sid": 157,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "39 8 0.1 13 CS LJ + L2 N So urc e + best N tar get lan gu ag es 0.7 25 0.",
                    "sid": 158,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "31 2 0.1 78 C S SV R( Ll + L2 ) SV R (S ou rc e+ all 51 tar get lan gu ag es) 0.7 44 0.",
                    "sid": 159,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "38 9 0.0 85 CS str in g Str ing Si mi lar ity (S ale hi an d Co ok, 20 13 ) 0.6 44 0.",
                    "sid": 160,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "38 5 0.3 72 C S str ing +L l C S stri ng + C S Ll (S ale hi an d Co ok , 20 13 ) 0.7 39 0.",
                    "sid": 161,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "36 0 0.3 53 C Sa u C S Ll + C S L2 N + C S stri ng 0.7 32 0.",
                    "sid": 162,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "41 7 0.3 64 Table 2: Pearson's correlation on the ENC, EVPC and GNC datasets CS Ll +L2N (i.e. CS au) there is a slight rise in cor\u00ad relation (from r = 0.725 to r = 0.732).",
                    "sid": 163,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.2 EVPC Results.",
                    "sid": 164,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "English VPCs are hard to identify.",
                    "sid": 165,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As discussed in Section 2, VPC components may not occur se\u00ad quentially, and even when they do occur sequen\u00ad tially, they may not be a VPC.",
                    "sid": 166,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As such, our sim\u00ad plistic identification method has low precision and recall (hand analysis of 927 identified VPC in\u00ad stances would suggest a precision of around 74%).",
                    "sid": 167,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There is no question that this is a contributor to the low correlation for the source language method ( CS LJ; r = 0.177).",
                    "sid": 168,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When we use target lan\u00ad guages instead of the source language ( CS L2N ), the correlation jumps substantially to r = 0.398.When we combine English and the target Ian guages ( CS Ll +L2N ), the results are actually lower than just using the target languages, because of the high weight on the target language, which is not desirable for VPCs, based on the source lan\u00ad guage results.",
                    "sid": 169,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Even for CS SVR(LJ+\u00a3 2 ), the re\u00ad sults (r = 0.389) are slightly below the target language-only results.",
                    "sid": 170,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This suggests that when predicting the compositionality of MWEs which are hard to identify in the source language, it may actually be better to use target languages only.",
                    "sid": 171,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results for string similarity (CS string: r = 0.385) are similar to those for CS L2N . However, as with the ENC dataset, when we combine string simi\u00ad larity and distributional similarity ( CS au), the re\u00ad sults improve, and we achieve the state-of-the-art for the dataset.In Table 3, we present classification-based eval M eth od Pre cis ion Re cal lF sco re ({3 = 1) Ac cur ac y Ba nn ard et al.",
                    "sid": 172,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2 00 3) 6 0 . 8 6 6.",
                    "sid": 173,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6 6 3 . 6 6 0 . 0 Sal ehi an d Co ok (2 01 3) 8 6 . 2 7 1.",
                    "sid": 174,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "8 7 7 . 4 6 9 . 3 CSau 79.5 89.3 82.0 74.5 Table 3: Results(%) for the binary compositionality prediction task on the EVPC dataset uation over a subset of EVPC, binarising the com\u00ad positionality judgements in the manner of Bannard et al.",
                    "sid": 175,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2003).",
                    "sid": 176,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our method achieves state-of-the-art results in terms of overall F-score and accuracy.",
                    "sid": 177,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.3 GNC Results.",
                    "sid": 178,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "German is a morphologically-rich language, with marking of number and case on nouns.",
                    "sid": 179,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given that we do not perform any lemmatization or other language-specific preprocessing, we inevitably achieve low recall for the identification of noun compound tokens, although the precision should be nearly 100%.",
                    "sid": 180,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Partly because of the resultant sparseness in the distributional similarity method, the results for CS L1 are low (r = 0.141), al\u00ad though they are lower again when using target lan\u00ad guages (r = 0.113).",
                    "sid": 181,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, when we combine the source and target languages ( CS L1+L2N) the results improve to r = 0.178.",
                    "sid": 182,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results for CSSVR(L1+L2)\u2022 on the other hand, are very low (r = 0.085).",
                    "sid": 183,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ultimately, simple string similar\u00ad ity achieves the best results for the dataset (r = 0.372), and this result actually drops slightly when combined with the distributional similarities.",
                    "sid": 184,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To better understand the reason for the lacklus\u00ad tre results using SVR, we carried out error analysis and found that, unlike the other two datasets, about half of the target languages return scores which correlate negatively with the human judgements.",
                    "sid": 185,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When we filter these languages from the data, the score for SVR improves appreciably.",
                    "sid": 186,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, over the best-3 languages overall, we get a corre\u00ad lation score of r = 0.179, which is slightly higher than CS L1+L2N.",
                    "sid": 187,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We further investigated the reason for getting very low and sometimes negative correlations with many of our target languages.",
                    "sid": 188,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We noted that about 24% of the German noun compounds in the dataset do not have entries in Panlex.",
                    "sid": 189,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This contrasts with ENC where only one instance does not have an entry in Panlex, and EVPC where all VPCs have translations in at least one language in Panlex.",
                    "sid": 190,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We experimented with using string sim\u00ad ilarity scores in the case of such missing transla tions, as opposed to the strategy described in Sec\u00ad tion 4.2.",
                    "sid": 191,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results for CS SVR(Ll +L2) rose to r = 0.269, although this is still below the correla\u00ad tion for just using string similarity.",
                    "sid": 192,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our results on the GNC dataset using string similarity are competitive with the state-of-the-art results (r = 0.45) using a window-based distribu\u00ad tional similarity approach over monolingual Ger\u00ad man data (Schulte im Walde et al., 2013).",
                    "sid": 193,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note, however, that their method used part-of-speech in\u00ad formation and lemmatisation, where ours does not, in keeping with the language-independent philos\u00ad ophy of this research.",
                    "sid": 194,
                    "ssid": 54,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusion and future work. ",
            "number": "6",
            "sents": [
                {
                    "text": "In this study, we proposed a method to predict the compositionality of MWEs based on monolingual distributional similarity between the MWE and each of its component words, under translation into multiple target languages.",
                    "sid": 195,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We showed that using translation and multiple target languages en\u00ad hances compositionality modelling, and also that there is strong complementarity between our ap\u00ad proach and an approach based on string similarity.",
                    "sid": 196,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In future work, we hope to address the ques\u00ad tion of translation sparseness, as observed for the GNC dataset.",
                    "sid": 197,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also plan to experiment with un\u00ad supervised morphological analysis methods to im\u00ad prove identification recall, and explore the impact of tokenization.",
                    "sid": 198,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, we would like to in\u00ad vestigate the optimal number of stop words and content-bearing words for each language, and to look into the development of general unsupervised methods for compositionality prediction.",
                    "sid": 199,
                    "ssid": 5,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgements",
            "number": "",
            "sents": [
                {
                    "text": "We thank the anonymous reviewers for their insightful comments and valuable suggestions.",
                    "sid": 200,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "NICTA is funded by the Australian government as represented by Department of Broadband, Com\u00ad munication and Digital Economy, and the Aus\u00ad tralian Research Council through the ICT Centre of Excellence programme.",
                    "sid": 201,
                    "ssid": 7,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}