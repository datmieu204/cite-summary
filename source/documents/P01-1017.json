{
    "ID": "P01-1017",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "We present two language models based upon an \u00f4immediate-head\u00f6 parser \u00f9 our name for a parser that conditions all events below a constituent c upon the head of c. While allofthe most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The perplexity for both of these models signi.cantly improve upon the trigram model baseline as well as the best previous grammar-based language model.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the better of our two models these improvements are 24% and 14% respectively.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also suggest that improvement of the un\u00a1derlying parser should signi.cantly im\u00a1prove the model\u00c6s perplexity and that even in the near term there is a lot of po\u00a1tential for improvement in immediate-head language models.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "All of the most accurate statistical parsers [1,3, 6,7,12,14] are lexicalized in that they condition probabilities on the lexical content of the sen\u00a1tences being parsed.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, all of these . This research was supported in part by NSF grant LIS SBR 9720368 and by NSF grant 00100203 IIS0085980.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The author would like to thank the members of the Brown Laboratory for Linguistic Information Processing (BLLIP) and particularly Brian Roark who gave very useful tips on conducting this research.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thanks also to Fred Jelinek and Ciprian Chelba for the use of their data and for detailed com\u00a1ments on earlier drafts of this paper.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "parsers are what we will call immediate-head parsers in that all of the properties of the imme\u00a1diate descendants of a constituent c are assigned probabilities that are conditioned on the lexical head of c. For example, in Figure 1 the probability that the vp expands into vnppp is conditioned on the head of the vp, \u00f4put\u00f6, as are the choices of the subheads under the vp, i.e., \u00f4ball\u00f6 (the head of the np) and \u00f4in\u00f6 (the head of the pp).",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is the ex\u00a1perience of the statistical parsing community that immediate-head parsers are the most accurate we can design.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is also worthy of note that many of these parsers [1,3,6,7] are generative \u00f9 thatis, fora sentence s they try to .nd the parse Jde.ned by Equation 1: arg maxJp(JIs)= arg maxJp(J, s) (1) This is interesting because insofar as they com\u00a1pute p(J, s) these parsers de.ne a language-model in that they can (in principle) assign a probability to all possible sentences in the language by com\u00a1 puting the sum in Equation 2: p(s)= .p(J, s) (2) J where p(J, s) is zero if the yield of J = s.Lan\u00a1 guage models, of course, are of interest because speech-recognition systems require them.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These systems determine the words that were spoken by solving Equation 3: arg maxsp(s IA)= arg maxsp(s)p(A Is) (3) where A denotes the acoustic signal.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The .rst term on the right, p(s), is the language model, and is what we compute via parsing in Equation 2.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "vp/put put the ball in the box Figure 1: A tree showing head information Virtually all current speech recognition sys\u00a1tems use the so-called trigram language model in which the probability of a string is broken down into conditional probabilities on each word given the two previous words.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "E.g., . p(w0,n)= p(wi Iwi1, wi2) (4) i=0,n-1 On the other hand, in the last few years there has been interest in designing language models based upon parsing and Equation 2.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We now turn to this previous research.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "previous work. ",
            "number": "2",
            "sents": [
                {
                    "text": "There is, of course, a very large body of litera\u00a1ture on language modeling (for an overview, see [10]) and even the literature on grammatical lan\u00a1guage models is becoming moderately large [4, 9,15,16,17].",
                    "sid": 17,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The research presented in this pa\u00a1per is most closely related to two previous efforts, that by Chelba and Jelinek [4] (C&J) and that by Roark [15], and this review concentrates on these two papers.",
                    "sid": 18,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While these two works differ in many particulars, we stress here the ways in which they are similar, and similar in ways that differ from the approach taken in this paper.",
                    "sid": 19,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In both cases the grammar based language model computes the probability of the next word based upon the previous words of the sentence.",
                    "sid": 20,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More speci.cally, these grammar-based models compute a subset of all possible grammatical re\u00a1lations for the prior words, and then compute \u00f2 the probability of the next grammatical situ\u00a1ation, and \u00f2 the probability of seeing the next word given each of these grammatical situations.",
                    "sid": 21,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Also, when computing the probability of the next word, both models condition on the two prior heads of constituents.",
                    "sid": 22,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, like a trigram model, they use information about triples of words.",
                    "sid": 23,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Neither of these models uses an immediate-head parser.",
                    "sid": 24,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Rather they are both what we will call strict left-to-right parsers.",
                    "sid": 25,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "At each sentence position in strict left-to-right parsing one com\u00a1putes the probability of the next word given the previous words (and does not go back to mod\u00a1ify such probabilities).",
                    "sid": 26,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is not possible in immediate-head parsing.",
                    "sid": 27,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sometimes the imme\u00a1diate head of a constituent occurs after it (e.g, in noun-phrases, where the head is typically the rightmost noun) and thus is not available for con\u00a1ditioning by a strict left-to-right parser.",
                    "sid": 28,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are two reasons why one might prefer strict left-to-right parsing for a language model (Roark [15] and Chelba, personal communica\u00a1tion).",
                    "sid": 29,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, the search procedures for guessing the words that correspond to the acoustic signal works left to right in the string.",
                    "sid": 30,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the language model is to offer guidance to the search procedure it must do so as well.",
                    "sid": 31,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second bene.t of strict left-to-right parsing is that it is easily combined with the standard tri\u00a1gram model.",
                    "sid": 32,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In both cases at every point in the sentence we compute the probability of the next word given the prior words.",
                    "sid": 33,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus one can inter\u00a1polate the trigram and grammar probability esti\u00a1mates for each word to get a more robust estimate.",
                    "sid": 34,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It turns out that this is a good thing to do, as is clear from Table 1, which gives perplexity results for a trigram model of the data in column one, re\u00a1sults for the grammar-model in column two, and results for a model in which the two are interpo\u00a1Table 1: Perplexity results for two previous grammar-based language models Model Trigram Perplexity Grammar Interpolation C&J 167.14 158.28 148.90 Roark 167.02 152.26 137.26 latedincolumnthree.",
                    "sid": 35,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both the were trained and tested on the same training and testing corpora, to be described in Section 4.1.",
                    "sid": 36,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As indicated in the table, the trigram model achieved a perplexity of 167 for the test\u00a1ing corpus.",
                    "sid": 37,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The grammar models did slightly bet\u00a1ter (e.g., 158.28 for the Chelba and Jelinek (C&J) parser), but it is the interpolation of the two that is clearly the winner (e.g., 137.26 for the Roark parser/trigram combination).",
                    "sid": 38,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In both papers the interpolation constants were 0.36 for the trigram estimate and 0.64 for the grammar estimate.",
                    "sid": 39,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While both of these reasons for strict-left-to\u00a1right parsing (search and trigram interpolation) are valid, they are not necessarily compelling.",
                    "sid": 40,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The ability to combine easily with trigram models is important only as long as trigram models can improve grammar models.",
                    "sid": 41,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A suf.ciently good grammar model would obviate the need for tri\u00a1grams.",
                    "sid": 42,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As for the search problem, we brie.y re\u00a1turn to this point at the end of the paper.",
                    "sid": 43,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here we simply note that while search requires that a language model provide probabilities in a left to right fashion, one can easily imagine proce\u00a1dures where these probabilities are revised after new information is found (i.e., the head of the constituent).",
                    "sid": 44,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that already our search pro\u00a1cedure needs to revise previous most-likely-word hypotheses when the original guess makes the subsequent words very unlikely.",
                    "sid": 45,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Revising the associated language-model probabilities compli\u00a1cates the search procedure, but not unimaginably so.",
                    "sid": 46,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus it seems to us that it is worth .nding out whether the superior parsing performance of immediate-head parsers translates into improved language models.",
                    "sid": 47,
                    "ssid": 31,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "the immediate-head parsing model. ",
            "number": "3",
            "sents": [
                {
                    "text": "We have taken the immediate-head parser de\u00a1scribed in [3] as our starting point.",
                    "sid": 48,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This parsing model assigns a probability to a parse Jby a top-down process of considering each constituent c in Jand, for each c, .rst guessing the pre-terminal of c, t(c)(t for \u00f4tag\u00f6), then the lexical head of c, h(c), and then the expansion of c into further con\u00a1stituents e(c).",
                    "sid": 49,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus the probability of a parse is given by the equation . p(J)= p(t(c) Il(c), H(c)) cEJ \u00c0p(h(c) It(c), l(c), H(c)) \u00c0p(e(c) Il(c), t(c), h(c), H(c)) where l(c) is the label of c (e.g., whether it is a noun phrase (np), verb phrase, etc.) and H(c)is the relevant history of c \u00f9 information outside c that our probability model deems important in de\u00a1termining the probability in question.",
                    "sid": 50,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In [3] H(c) approximately consists of the label, head, and head-part-of-speech for the parent of c: m(c), i(c), and u(c) respectively.",
                    "sid": 51,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One exception is the distri\u00a1bution p(e(c) Il(c), t(c), h(c), H(c)), where H only 1 includes m and u. Whenever it is clear to which constituent we are referring we omit the (c) in, e.g., h(c).",
                    "sid": 52,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this notation the above equation takes the following form: . p(J)= p(t Il, m, u, i) \u00c0p(h It, l, m, u, i) cEJ \u00c0p(e Il, t, h, m, u).",
                    "sid": 53,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(5) Because this is a point of contrast with the parsers described in the previous section, note that all of the conditional distributions are conditioned on one lexical item (either i or h).",
                    "sid": 54,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus only p(h It, l, m, u, i), the distribution for the head of c, looks at two lexical items (i and h itself), and none of the distributions look at three lexical items as do the trigram distribution of Equation 4 and the previously discussed parsing language models [4, 15].",
                    "sid": 55,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Next we describe how we assign a probabil\u00a1ity to the expansion e of a constituent.",
                    "sid": 56,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We break up a traditional probabilistic context-free gram\u00a1mar (PCFG) rule into a left-hand side with a label l(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence 1We simplify slightly in this section.",
                    "sid": 57,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "See [3] for all the details on the equations as well as the smoothing used.",
                    "sid": 58,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "of one or more such symbols.",
                    "sid": 59,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each expansion we distinguish one of the right-hand side labels as the \u00f4middle\u00f6 or \u00f4head\u00f6 symbol M(c).",
                    "sid": 60,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "M(c)is the constituent from which the head lexical item h is obtained according to deterministic rules that pick the head of a constituent from among the heads of its children.",
                    "sid": 61,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To the left of M is a sequence of one or more left labels Li(c) including the special ter\u00a1mination symbol , which indicates that there are no more symbols to the left, and similarly for the labels to the right, Ri(c).",
                    "sid": 62,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus an expansion e(c) looks like: lLm...",
                    "sid": 63,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "L1MR1...",
                    "sid": 64,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Rn .",
                    "sid": 65,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(6) The expansion is generated by guessing .rst M, then in order L1 through Lm+1 (= ), and similarly for R1 through Rn+1.",
                    "sid": 66,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In anticipation of our discussion in Section 4.2, note that when we are expanding an Li we do not know the lexical items to its left, but if we prop\u00a1erly dovetail our \u00f4guesses\u00f6 we can be sure of what word, if any, appears to its right and before M,and similarly for the word to the left of Rj.",
                    "sid": 67,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This makes such words available to be conditioned upon.",
                    "sid": 68,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, the parser of [3] deviates in two places from the strict dictates of a language model.",
                    "sid": 69,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, as explicitly noted in [3], the parser does not com\u00a1pute the partition function (normalization con\u00a1stant) for its distributions so the numbers it re\u00a1turns are not true probabilities.",
                    "sid": 70,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We noted there that if we replaced the \u00f4maxent inspired\u00f6 fea\u00a1ture with standard deleted interpolation smooth\u00a1ing, we took a signi.cant hit in performance.",
                    "sid": 71,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have now found several ways to overcome this problem, including some very ef.cient ways to compute partition functions for this class of mod\u00a1els.",
                    "sid": 72,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the end, however, this was not neces\u00a1sary, as we found that we could obtain equally good performance by \u00f4handcrafting\u00f6 our inter\u00a1polation smoothing rather than using the \u00f4obvi\u00a1ous\u00f6 method (which performs poorly).",
                    "sid": 73,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Secondly, as noted in [2], the parser encourages right branching with a \u00f4bonus\u00f6 multiplicative fac\u00a1tor of 1.2 for constituents that end at the right boundary of the sentence, and a penalty of 0.8 for those that do not.",
                    "sid": 74,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is replaced by explic\u00a1itly conditioning the events in the expansion of Equation 6 on whether or not the constituent is at the right boundary (barring sentence-.nal punctu\u00a1ation).",
                    "sid": 75,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Again, with proper attention to details, this can be known at the time the expansion is taking place.",
                    "sid": 76,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This modi.cation is much more complex than the multiplicative \u00f4hack,\u00f6 and it is not quite as good (we lose about 0.1% in precision/recall .gures), but it does allow us to compute true prob\u00a1abilities.",
                    "sid": 77,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The resulting parser strictly speaking de.nes a PCFG in that all of the extra conditioning in\u00a1formation could be included in the non-terminal\u00a1node labels (as we did with the head information in Figure 1).",
                    "sid": 78,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When a PCFG probability distribu\u00a1tion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum\u00a1ming to one) probability distribution over strings [5], thus making them appropriate for language models.",
                    "sid": 79,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also empirically checked that our in\u00a1dividual distributions (p(t Il, m, u, i), and p(h I t, l, m, u, i) from Equation 5 and p(L Il, t, h, m, u), p(M Il, t, h, m, u), and p(R Il, t, h, m, u) from Equation 5) sum to one for a large, random, se\u00a1lection of conditioning events2 As with [3], a subset of parses is computed with a non-lexicalized PCFG, and the most probable edges (using an empirically established thresh\u00a1old) have their probabilities recomputed accord\u00a1ing to the complete probability model of Equation 5.",
                    "sid": 80,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both searches are conducted using dynamic programming..",
                    "sid": 81,
                    "ssid": 34,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "experiments. ",
            "number": "4",
            "sents": [
                {
                    "text": "4.1 The Immediate-Bihead Language Model.",
                    "sid": 82,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The parser as described in the previous section was trained and tested on the data used in the pre\u00a1viously described grammar-based language mod\u00a1eling research [4,15].",
                    "sid": 83,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This data is from the Penn Wall Street Journal tree-bank [13], but modi.ed to make the text more \u00f4speech-like\u00f6.",
                    "sid": 84,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particu\u00a1lar: 1.",
                    "sid": 85,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "all punctuation is removed, 2.",
                    "sid": 86,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "no capitalization is used, 3.",
                    "sid": 87,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "all symbols and digits are replaced by the symbol N, and 2They should sum to one.",
                    "sid": 88,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We are just checking that there are no bugs in the code.",
                    "sid": 89,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Model Trigram Perplexity Grammar Interpolation C&J 167.14 158.28 148.90 Roark 167.02 152.26 137.26 Bihead 167.89 144.98 133.15 Table 2: Perplexity results for the immediate\u00a1bihead model 4.",
                    "sid": 90,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "all words except for the 10,000 most com\u00a1mon are replaced by the symbol UNK.",
                    "sid": 91,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As in previous work, .les F0 to F20 are used for training, F21F22 for development, and F23F24 for testing.",
                    "sid": 92,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results are given in Table 2.",
                    "sid": 93,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We refer to the current model as the bihead model.",
                    "sid": 94,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u00f4Bihead\u00f6 here emphasizes the already noted fact that in this model probabilities involve at most two lexical heads.",
                    "sid": 95,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As seen in Table 2, the immediate-bihead model with a perplexity of 144.98 outperforms both previous models, even though they use tri\u00a1grams of words in their probability estimates.",
                    "sid": 96,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also interpolated our parsing model with the trigram model (interpolation constant .36, as with the other models) and this model outper\u00a1forms the other interpolation models.",
                    "sid": 97,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note, how\u00a1ever, that because our parser does not de.ne prob\u00a1abilities for each word based upon previous words (as with trigram) it is not possible to do the inte\u00a1gration at the word level.",
                    "sid": 98,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Rather we interpolate the probabilities of the entire sentences.",
                    "sid": 99,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is a much less powerful technique than the word-level interpolation used by both C&J and Roark, but we still observe a signi.cant gain in performance.",
                    "sid": 100,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.2 The Immediate-Trihead Model.",
                    "sid": 101,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While the performance of the grammatical model is good, a look at sentences for which the tri\u00a1gram model outperforms it makes its limitations apparent.",
                    "sid": 102,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The sentences in question have noun phrases like \u00f4monday night football\u00f6 that trigram models eats up but on which our bihead parsing model performs less well.",
                    "sid": 103,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, consider the sentence \u00f4he watched monday night football\u00f6.",
                    "sid": 104,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The trigram model assigns this a probability of 1.",
                    "sid": 105,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 \u00c0105, while the grammar model gives it a probability of 2.",
                    "sid": 106,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "77 \u00c0107.",
                    "sid": 107,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To a .rst approxima\u00a1tion, this is entirely due to the difference in prob\u00a1 np nbar monday night football Figure 2: A noun-phrase with substructure ability of the noun-phrase.",
                    "sid": 108,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the tri\u00a1gram probability p(football Imonday, night) = 0.",
                    "sid": 109,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "366, and would have been 1.0 except that smoothing saved some of the probability for other things it might have seen but did not.",
                    "sid": 110,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Because the grammar model conditions in a different order, the closest equivalent probability would be that for \u00f4monday\u00f6, but in our model this is only con\u00a1ditioned on \u00f4football\u00f6 so the probability is much less biased, only 0.",
                    "sid": 111,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0306.",
                    "sid": 112,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Penn tree-bank base noun-phrases are .at, thus the head above \u00f4mon\u00a1day\u00f6 is \u00f4football\u00f6.)",
                    "sid": 113,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This immediately suggests creating a second model that captures some of the trigram-like probabilities that the immediate-bihead model misses.",
                    "sid": 114,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The most obvious extension would be to condition upon not just one\u00c6s parent\u00c6s head, but one\u00c6s grandparent\u00c6s as well.",
                    "sid": 115,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This does capture some of the information we would like, partic\u00a1ularly the case heads of noun-phrases inside of prepositional phrases.",
                    "sid": 116,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, in \u00f4united states of america\u00f6, the probability of \u00f4america\u00f6 is now conditioned not just on \u00f4of\u00f6 (the head of its parent) but also on \u00f4states\u00f6.",
                    "sid": 117,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unfortunately, for most of the cases where tri\u00a1gram really cleans up this revision would do lit\u00a1tle.",
                    "sid": 118,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, in \u00f4he watched monday night football\u00f6 \u00f4monday\u00f6 would now be conditioned upon \u00f4foot\u00a1ball\u00f6 and \u00f4watched.\u00f6 The addition of \u00f4watched\u00f6 is unlikely to make much difference, certainly compared to the boost trigram models get by, in effect, recognizing the complete name.",
                    "sid": 119,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is interesting to note, however, that virtu\u00a1ally all linguists believe that a noun-phrase like \u00f4monday night football\u00f6 has signi.cant substruc\u00a1ture \u00f9 e.g., it would look something like Figure 2.",
                    "sid": 120,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If we assume this tree-structure the two heads above \u00f4monday\u00f6 are \u00f4night\u00f6 and \u00f4football\u00f6 re\u00a1spectively, thus giving our trihead model the same power as the trigram for this case.",
                    "sid": 121,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ignoring some of the conditioning events, we now get a proba\u00a1bility p(h = monday Ii = night, j = football), which is much higher than the corresponding bi-head version p(h = monday Ii = football).",
                    "sid": 122,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The reader may remember that h is the head of the cur\u00a1rent constituent, while i is the head of its parent.",
                    "sid": 123,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We now de.ne j to be the grandparent head..",
                    "sid": 124,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We decided to adopt this structure, but to keep things simple we only changed the de.nition of \u00f4head\u00f6 for the distribution p(h It, l, m, u, i, j).",
                    "sid": 125,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus we adopted the following revised de.nition of head for constituents of base noun-phrases: For a pre-terminal (e.g., noun) con\u00a1stituent c of a base noun-phrase in which it is not the standard head (h)and which has as its right-sister another pre\u00a1terminal constituent d which is not it\u00a1self h, the head of c is the head of d.The sole exceptions to this rule are phrase-initial determiners and numbers which retain h as their heads.",
                    "sid": 126,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In effect this de.nition assumes that the sub\u00a1structure of all base noun-phrases is left branch\u00a1ing, as in Figure 2.",
                    "sid": 127,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is not true, but Lauer [11] shows that about two-thirds of all branching in base-noun-phrases is leftward.",
                    "sid": 128,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We believe we would get even better results if the parser could determine the true branching structure.",
                    "sid": 129,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then adopt the following de.nition of a grandparent-head feature j. 1.",
                    "sid": 130,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "if c is a noun phrase under a prepositional phrase, or is a pre-terminal which takes a revised head as de.ned above, then j is the grandparent head of c,else 2.",
                    "sid": 131,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "if c is a pre-terminal and is not next (in the production generating c) to the head of its parent (i)then j(c) is the head of the con\u00a1stituent next to c in the production in the di\u00a1rection of the head of that production, else 3.",
                    "sid": 132,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "j is a \u00f4none-of-the-above\u00f6 symbol.",
                    "sid": 133,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Case 1 now covers both \u00f4united states of amer\u00a1ica\u00f6 and \u00f4monday night football\u00f6 examples.",
                    "sid": 134,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Case 2 handles other .at constituents in Penn tree-bank style (e.g., quanti.er-phrases) for which we do not have a good analysis.",
                    "sid": 135,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Case three says that this feature is a no-op in all other situations.",
                    "sid": 136,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Model Trigram Perplexity Grammar Interpolation C&J 167.14 158.28 148.90 Roark 167.02 152.26 137.26 Bihead 167.89 144.98 133.15 Trihead 167.89 130.20 126.07 Table 3: Perplexity results for the immediate\u00a1trihead model The results for this model, again trained on F0\u00a1F20 and tested on F2324, are given in Figure 3 under the heading \u00f6Immediate-trihead model\u00f6.",
                    "sid": 137,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We see that the grammar perplexity is reduced to 130.20, a reduction of 10% over our .rst model, 14% over the previous best grammar model (152.26%), and 22% over the best of the above trigram models for the task (167.02).",
                    "sid": 138,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When we run the trigram and new grammar model in tandem we get a perplexity of 126.07, a reduction of 8% over the best previous tandem model and 24% over the best trigram model.",
                    "sid": 139,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.3 Discussion.",
                    "sid": 140,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One interesting fact about the immediate-trihead model is that of the 3761 sentences in the test cor\u00a1pus, on 2934, or about 75%, the grammar model assigns a higher probability to the sentence than does the trigram model.",
                    "sid": 141,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One might well ask what went \u00f4wrong\u00f6 with the remaining 25%?",
                    "sid": 142,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Why should the grammar model ever get beaten?",
                    "sid": 143,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Three possible reasons come to mind: 1.",
                    "sid": 144,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The grammar model is better but only by a small amount, and due to sparse data prob\u00a1lems occasionally the worse model will luck out and beat the better one.",
                    "sid": 145,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.",
                    "sid": 146,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The grammar model and the trigram model capture different facts about the distribution of words in the language, and for some set of sentences one distribution will perform bet\u00a1ter than the other.",
                    "sid": 147,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.",
                    "sid": 148,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The grammar model is, in some sense, al\u00a1ways better than the trigram model, but if the parser bungles the parse, then the grammar model is impacted very badly.",
                    "sid": 149,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Obviously the trigram model has no such Achilles\u00c6 heel.",
                    "sid": 150,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sentence Group Num.",
                    "sid": 151,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Labeled Labeled Precision Recall All Sentences 3761 84.6% 83.7% Grammar High 2934 85.7% 84.9% Trigram High 827 80.1% 79.0% Table 4: Precision/recall for sentences in which trigram/grammar models performed best We ask this question because what we should do to improve performance of our grammar-based language models depends critically on which of these explanations is correct: if (1) we should col\u00a1lect more data, if (2) we should just live with the tandem grammar-trigram models, and if (3) we should create better parsers.",
                    "sid": 152,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Based upon a few observations on sentences from the development corpus for which the tri\u00a1gram model gave higher probabilities we hypoth\u00a1esized that reason (3), bungled parses, is primary.",
                    "sid": 153,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To test this we performed the following experi\u00a1ment.",
                    "sid": 154,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We divide the sentences from the test cor\u00a1pus into two groups, ones for which the trigram model performs better, and the ones for which the grammar model does better.",
                    "sid": 155,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then collect labeled precision and recall statistics (the stan\u00a1dard parsing performance measures) separately for each group.",
                    "sid": 156,
                    "ssid": 75,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If our hypothesis is correct we ex\u00a1pect the \u00f4grammar higher\u00f6 group to have more ac\u00a1curate parses than the trigram-higher group as the poor parse would cause poor grammar perplexity for the sentence, which would then be worse than the trigram perplexity.",
                    "sid": 157,
                    "ssid": 76,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If either of the other two explanations were correct one would not expect much difference between the two groups.",
                    "sid": 158,
                    "ssid": 77,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The re\u00a1sults are shown in Table 4.",
                    "sid": 159,
                    "ssid": 78,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We see there that, for example, sentences for which the grammar model has the superior perplexity have average recall 5.9 (= 84.979.",
                    "sid": 160,
                    "ssid": 79,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0) percentage points higher than the sentences for which the trigram model performed better.",
                    "sid": 161,
                    "ssid": 80,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The gap for precision is 5.6.",
                    "sid": 162,
                    "ssid": 81,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This seems to support our hypothesis.",
                    "sid": 163,
                    "ssid": 82,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusion and future work. ",
            "number": "5",
            "sents": [
                {
                    "text": "We have presented two grammar-based language models, both of which signi.cantly improve upon both the trigram model baseline for the task (by 24% for the better of the two) and the best pre\u00a1vious grammar-based language model (by 14%).",
                    "sid": 164,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore we have suggested that improve\u00a1ment of the underlying parser should improve the model\u00c6s perplexity still further.",
                    "sid": 165,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We should note, however, that if we were deal\u00a1ing with standard Penn Tree-bank Wall-Street-Journal text, asking for better parsers would be easier said than done.",
                    "sid": 166,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While there is still some progress, it is our opinion that substantial im\u00a1provement in the state-of-the-art precision/recall .gures (around 90%) is unlikely in the near fu\u00a1ture.3 However, we are not dealing with stan\u00a1dard tree-bank text.",
                    "sid": 167,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As pointed out above, the text in question has been \u00f4speechi.ed\u00f6 by re\u00a1moving punctuation and capitalization, and \u00f4sim\u00a1pli.ed\u00f6 by allowing only a .xed vocabulary of 10,000 words (replacing all the rest by the sym\u00a1bol \u00f4UNK\u00f6), and replacing all digits and symbols by the symbol \u00f4N\u00f6.",
                    "sid": 168,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We believe that the resulting text grossly under-represents the useful grammatical information available to speech-recognition systems.",
                    "sid": 169,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we believe that information about rare or even truly unknown words would be useful.",
                    "sid": 170,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, when run on standard text, the parser uses ending information to guess parts of speech [3].",
                    "sid": 171,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Even if we had never encountered the word \u00f4show\u00a1boating\u00f6, the \u00f4ing\u00f6 ending tells us that this is almost certainly a progressive verb.",
                    "sid": 172,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is much harder to determine this about UNK.4 Secondly, while punctuation is not to be found in speech, prosody should give us something like equiva\u00a1lent information, perhaps even better.",
                    "sid": 173,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus sig\u00a1ni.cantly better parser performance on speech-derived data seems possible, suggesting that high-performance trigram-less language models may be within reach.",
                    "sid": 174,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We believe that the adaptation of prosodic information to parsing use is a worthy topic for future research.",
                    "sid": 175,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we have noted two objections to immediate-head language models: .rst, they complicate left-to-right search (since heads are often to the right of their children) and second, 3Furthermore, some of the newest wrinkles [8] use dis\u00a1criminative methods and thus do not de.ne language models at all, seemingly making them ineligible for the competition on a priori grounds.",
                    "sid": 176,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4To give the reader some taste for the dif.culties pre\u00a1sented by UNKs, we encourage you to try parsing the fol\u00a1lowing real example: \u00f4its supposedly unk unk unk a unk that makes one unk the unk of unk unk the unk radical unk of unk and unk and what in unk even seems like unk in unk\u00f6.",
                    "sid": 177,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "they cannot be tightly integrated with trigram models.",
                    "sid": 178,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The possibility of trigram-less language mod\u00a1els makes the second of these objections without force.",
                    "sid": 179,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Nor do we believe the .rst to be a per\u00a1manent disability.",
                    "sid": 180,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If one is willing to provide sub-optimal probability estimates as one proceeds left-to-right and then amend them upon seeing the true head, left-to-right processing and immediate-head parsing might be joined.",
                    "sid": 181,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that one of the cases where this might be worrisome, early words in a base noun-phrase could be conditioned upon a head which comes several words later, has been made signi.cantly less problematic by our revised de.nition of heads inside noun-phrases.",
                    "sid": 182,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We be\u00a1lieve that other such situations can be brought into line as well, thus again taming the search prob\u00a1lem.",
                    "sid": 183,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, this too is a topic for future re\u00a1search.",
                    "sid": 184,
                    "ssid": 21,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}