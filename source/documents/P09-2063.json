{
    "ID": "P09-2063",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "We propose a new specifically designed method for paraphrase generation based on MonteCarlo sampling and show how this algorithm is suitable for its task.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, the basic algorithm presented here leaves a lot of opportunities for future improvement.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, our algorithm does not constraint the scoring function in opposite to Viterbi based decoders.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is now possible to use some global features in paraphrase scoring functions.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This algorithm opens new outlooks for paraphrase generation and other natural language processing applications like statistical machine translation.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "A paraphrase generation system is a program which, given a source sentence, produces a different sentence with almost the same meaning.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Paraphrase generation is useful in applications to choose between different forms to keep the most appropriate one.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, automatic summary can be seen as a particular paraphrasing task (Barzilay and Lee, 2003) with the aim of selecting the shortest paraphrase.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Paraphrases can also be used to improve natural language processing (NLP) systems.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(CallisonBurch et al., 2006) improved machine translations by augmenting the coverage of patterns that can be translated.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to produce paraphrases, a promising approach is to see the paraphrase generation problem as a translation problem, where the target language is the same as the source language (Quirk et al., 2004; Bannard and CallisonBurch, 2005).",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A problem that has drawn less attention is the generation step which corresponds to the decoding step in SMT.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like MOSES (Koehn et al., 2007).",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The goal of a decoder is to find the best path in the lattice produced from a paraphrase table.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is basically achieved by using dynamic programming and especially the Viterbi algorithm associated with beam searching.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However decoding algorithms were designed for translation, not for paraphrase generation.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although left-to-right decoding is justified for translation, it may not be necessary for paraphrase generation.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A paraphrase generation tool usually starts with a sentence which may be very similar to some potential solution.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In other words, there is no need to \"translate\" all of the sentences.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, decoding may not be suitable for non-contiguous transformation rules.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, dynamic programming imposes an incremental scoring function to evaluate the quality of each hypothesis.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, it cannot capture some scattered syntactical dependencies.",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Improving on this major issue is a key point to improve paraphrase generation systems.",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This paper first presents an alternative to decoding that is based on transformation rule application in section 2.",
                    "sid": 25,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In section 3 we propose a paraphrase generation method for this paradigm based on an algorithm used in two-player games.",
                    "sid": 26,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Section 4 briefly explain experimental context and its associated protocol for evaluation of the proposed system.",
                    "sid": 27,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We compare the proposed algorithm with a baseline system in section 5.",
                    "sid": 28,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, in section 6, we point to future research tracks to improve paraphrase generation tools.",
                    "sid": 29,
                    "ssid": 29,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "statistical paraphrase generation using. ",
            "number": "2",
            "sents": [
                {
                    "text": "transformation rules The paraphrase generation problem can be seen as an exploration problem.",
                    "sid": 30,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We seek the best paraphrase according to a scoring function in a space 249 Proceedings of the ACLIJCNLP 2009 Conference Short Papers, pages 249\u2013252, Suntec, Singapore, 4 August 2009.",
                    "sid": 31,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Qc 2009 ACL and AFNLP to search by applying successive transformations.",
                    "sid": 32,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This space is composed of states connected by actions.",
                    "sid": 33,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An action is a transformation rule with a place where it applies in the sentence.",
                    "sid": 34,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "States are a sentence with a set of possible actions.",
                    "sid": 35,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Applying an action in a given state consists in transforming the sentence of the state and removing all rules that are no more applicable.",
                    "sid": 36,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our framework, each state, except the root, can be a final state.",
                    "sid": 37,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is modelised by adding a stop rule as a particular action.",
                    "sid": 38,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We impose the constraint that any transformed part of the source sentence cannot be transformed anymore.",
                    "sid": 39,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This paradigm is more approriate for paraphrase generation than the standard SMT approach in respect to several points: there is no need for left- to-right decoding because a transformation can be applied anywhere without order; there is no need to transform the whole of a sentence because each state is a final state; there is no need to keep the identity transformation for each phrase in the paraphrase table; the only domain knowledge needed is a generative model and a scoring function for final states; it is possible to mix different generative models because a statistical paraphrase table, an analogical solver and a paraphrase memory for instance; there is no constraint on the scoring function because it only scores final states.",
                    "sid": 40,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that the branching factor with a paraphrase table can be around thousand actions per states which makes the generation problem a difficult computational problem.",
                    "sid": 41,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hence we need an efficient generation algorithm.",
                    "sid": 42,
                    "ssid": 13,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "monte-carlo based paraphrase. ",
            "number": "3",
            "sents": [
                {
                    "text": "Generation UCT (Kocsis and Szepesv\u00e1ri, 2006) (Upper Confidence bound applied to Tree) is a MonteCarlo planning algorithm that have some interesting properties: it grows the search tree non-uniformly and favours the most promising sequences, without pruning branch; it can deal with high branching factor; it is an anytime algorithm and returns best solution found so far when interrupted; it does not require expert domain knowledge to evaluate states.",
                    "sid": 43,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These properties make it ideally suited for games with high branching factor and for which there is no strong evaluation function.",
                    "sid": 44,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the same reasons, this algorithm sounds interesting for paraphrase generation.",
                    "sid": 45,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, it does not put constraint on the scoring function.",
                    "sid": 46,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We propose a variation of the UCT algorithm for paraphrase generation named MCPG for MonteCarlo based Paraphrase Generation.",
                    "sid": 47,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The main part of the algorithm is the sampling step.",
                    "sid": 48,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An episode of this step is a sequence of states and actions, s1, a1, s2, a2, . . .",
                    "sid": 49,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", sT , from the root state to a final state.",
                    "sid": 50,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During an episode construction, there are two ways to select the action ai to perfom from a state si.",
                    "sid": 51,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the current state was already explored in a previous episode, the action is selected according to a compromise between exploration and exploitation.",
                    "sid": 52,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This compromise is computed using the UCBTunned formula (Auer et al., 2001) associated with the RAVE heuristic (Gelly and Silver, 2007).",
                    "sid": 53,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the current state is explored for the first time, its score is estimated using MonteCarlo sampling.",
                    "sid": 54,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In other word, to complete the episode, the actions ai, ai+1, . . .",
                    "sid": 55,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", aT \u22121, aT are selected randomly until a stop rule is drawn.",
                    "sid": 56,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "At the end of each episode, a reward is computed for the final state sT using a scoring function and the value of each (state, action) pair of the episode is updated.",
                    "sid": 57,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then, the algorithm computes an other episode with the new values.",
                    "sid": 58,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Periodically, the sampling step is stopped and the best action at the root state is selected.",
                    "sid": 59,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This action is then definitely applied and a sampling is restarted from the new root state.",
                    "sid": 60,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The action sequence is built incrementally and selected after being enough sampled.",
                    "sid": 61,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our experiments, we have chosen to stop sampling regularly after a fixed amount \u03b7 of episodes.",
                    "sid": 62,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our main adaptation of the original algorithm is in the (state, action) value updating procedure.",
                    "sid": 63,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the goal of the algorithm is to maximise a scoring function, we use the maximum reachable score from a state as value instead of the score expectation.",
                    "sid": 64,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This algorithm suits the paradigm proposed for paraphrase generation.",
                    "sid": 65,
                    "ssid": 23,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "experimental context. ",
            "number": "4",
            "sents": [
                {
                    "text": "This section describes the experimental context and the methodology followed to evaluate our statistical paraphrase generation tool.",
                    "sid": 66,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.1 Data.",
                    "sid": 67,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the experiment reported in section 5, we use one of the largest, multilingual, freely available aligned corpus, Europarl (Koehn, 2005).",
                    "sid": 68,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It consists of European parliament debates.",
                    "sid": 69,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We choose French as the language for paraphrases and English as the pivot language.",
                    "sid": 70,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this pair of languages, the corpus consists of 1, 487, 459 French sentences aligned with 1, 461, 429 English sentences.",
                    "sid": 71,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that the sentences in this corpus are long, with an average length of 30 words per French sentence and 27.1 for English.",
                    "sid": 72,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We randomly extracted 100 French sentences as a test corpus.",
                    "sid": 73,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.2 Language model and paraphrase table.",
                    "sid": 74,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Paraphrase generation tools based on SMT methods need a language model and a paraphrase table.",
                    "sid": 75,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both are computed on a training corpus.",
                    "sid": 76,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The language models we use are n-gram language models with back-off.",
                    "sid": 77,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use SRILM (Stolcke, 2002) with its default parameters for this pur SMT decoder.",
                    "sid": 78,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the MOSES decoder (Koehn et al., 2007) as a baseline.",
                    "sid": 79,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The MOSES scoring function is set by four weighting factors \u03b1\u03a6, \u03b1LM , \u03b1D , \u03b1W . Conventionally, these four weights are adjusted during a tuning step on a training corpus.",
                    "sid": 80,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The tuning step is inappropriate for paraphrase because there is no such tuning corpus available.",
                    "sid": 81,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We empirically set \u03b1\u03a6 = 1, \u03b1LM = 1, \u03b1D = 10 and \u03b1W = 0.",
                    "sid": 82,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hence, the scoring function (or reward function for MCPG) is equivalent to: R(f |f, I ) = p(f ) \u00d7 \u03a6(f |f , I ) where f and f are the source and target sentences, I a segmentation in phrases of f , p(f ) the language model score and \u03a6(f |f , I ) = pose.",
                    "sid": 83,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The length of the n-grams is five.",
                    "sid": 84,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "i\u2208I p(f |f i) the paraphrase table score.",
                    "sid": 85,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To build a paraphrase table, we use the construction method via a pivot language proposed in (Bannard and CallisonBurch, 2005).",
                    "sid": 86,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Three heuristics are used to prune the paraphrase table.",
                    "sid": 87,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first heuristic prunes any entry in the paraphrase table composed of tokens with a probability lower than a threshold E. The second, called pruning pivot heuristic, consists in deleting all pivot clusters larger than a threshold \u03c4 . The last heuristic keeps only the \u03ba most probable paraphrases for each source phrase in the final paraphrase table.",
                    "sid": 88,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this study, we empirically fix E = 10\u22125, \u03c4 = 200 and \u03ba = 10.",
                    "sid": 89,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.3 Evaluation Protocol.",
                    "sid": 90,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The MCPG algorithm needs two parameters.",
                    "sid": 91,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One is the number of episodes \u03b7 done before selecting the best action at root state.",
                    "sid": 92,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The other is k, an equivalence parameter which balances the exploration/exploitation compromise (Auer et al., 2001).",
                    "sid": 93,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We empirically set \u03b7 = 1, 000, 000 and k = 1, 000.",
                    "sid": 94,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our algorithm, note that identity paraphrase probabilities are biased: for each phrase it is equal to the probability of the most probable paraphrase.",
                    "sid": 95,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, as the source sentence is the best meaning preserved \"paraphrase\", a sentence cannot have a better score.",
                    "sid": 96,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hence, we use a slightly different scoring function: We developed a dedicated website to allow the human judges with some flexibility in workplaces \uf8eb R(f |f, I ) = min \uf8ec p (f ) n p(f i|f i) \uf8f6 , 1\uf8f7 \uf8ec i i \uf8f7 and evaluation periods.",
                    "sid": 97,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We retain the principle of the two-step evaluation, common in the machine \uf8ed p(f ) i\u2208I f i I=f ti p(f |f ) \uf8f8 translation domain and already used for paraphrase evaluation (Bannard and CallisonBurch, 2005).",
                    "sid": 98,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The question asked to the human evaluator for the syntactic task is: Is the following sentence in good French?",
                    "sid": 99,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The question asked to the human evaluator for the semantic task is: Do the following two sentences express the same thing?",
                    "sid": 100,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our experiments, each paraphrase was evaluated by two native French evaluators.",
                    "sid": 101,
                    "ssid": 36,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "comparison with a smt decoder. ",
            "number": "5",
            "sents": [
                {
                    "text": "In order to validate our algorithm for paraphrase generation, we compare it with an off-the-shelf Note that for this model, there is no need to know the identity transformations probability for unchanged part of the sentence.",
                    "sid": 102,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Results are presented in Table 1.",
                    "sid": 103,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Kappa statistics associated with the results are 0.84, 0.64 and 0.59 which are usually considered as a \"perfect\", \"substantial\" and \"moderate\" agreement.",
                    "sid": 104,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Results are close to evaluations from the baseline system.",
                    "sid": 105,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The main differences are from Kappa statistics which are lower for the MOSES system evaluation.",
                    "sid": 106,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Judges changed between the two experiments.",
                    "sid": 107,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We may wonder whether an evaluation with only two judges is reliable.",
                    "sid": 108,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This points to the ambiguity of any paraphrase definition.",
                    "sid": 109,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sy st e m M O S E S M C P G W ell for m ed (K ap pa ) 64 % (0.",
                    "sid": 110,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "57 ) 63 % (0.",
                    "sid": 111,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "84 ) M ea ni ng pr es er ve d (K ap pa ) 58 % (0.",
                    "sid": 112,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "48 ) 55 % (0.",
                    "sid": 113,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "64 ) W ell for m ed an d m ea ni ng pr es er ve d (K ap pa ) 50 % (0.",
                    "sid": 114,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "54 ) 49 % (0.",
                    "sid": 115,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "59 ) Table 1: Results of paraphrases evaluation for 100 sentences in French using English as the pivot language.",
                    "sid": 116,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Comparison between the baseline system MOSES and our algorithm MCPG.",
                    "sid": 117,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By doing this experiment, we have shown that our algorithm with a biased paraphrase table is state-of-the-art to generate paraphrases.",
                    "sid": 118,
                    "ssid": 17,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusions and further research. ",
            "number": "6",
            "sents": [
                {
                    "text": "In this paper, we have proposed a different paradigm and a new algorithm in NLP field adapted for statistical paraphrases generation.",
                    "sid": 119,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This method, based on large graph exploration by MonteCarlo sampling, produces results comparable with state-of-the-art paraphrase generation tools based on SMT decoders.",
                    "sid": 120,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The algorithm structure is flexible and generic enough to easily work with discontinous patterns.",
                    "sid": 121,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is also possible to mix various transformation methods to increase paraphrase variability.",
                    "sid": 122,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The rate of ill-formed paraphrase is high at 37%.",
                    "sid": 123,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The result analysis suggests an involvement of the non-preservation of the original meaning when a paraphrase is evaluated ill-formed.",
                    "sid": 124,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although the mesure is not statistically significant because the test corpus is too small, the same trend is also observed in other experiments.",
                    "sid": 125,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Improving on the language model issue is a key point to improve paraphrase generation systems.",
                    "sid": 126,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our algorithm can work with unconstraint scoring functions, in particular, there is no need for the scoring function to be incremental as for Viterbi based decoders.",
                    "sid": 127,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We are working to add, in the scoring function, a linguistic knowledge based analyzer to solve this problem.",
                    "sid": 128,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Because MCPG is based on a different paradigm, its output scores cannot be directly compared to MOSES scores.",
                    "sid": 129,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to prove the optimisation qualities of MCPG versus state-of-the-art decoders, we are transforming our paraphrase generation tool into a translation tool.",
                    "sid": 130,
                    "ssid": 12,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}