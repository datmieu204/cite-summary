{
    "ID": "W02-1011",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "We consider the problem of classifying doc\u00aduments not by topic, but by overall senti\u00adment, e.g., determining whether a review is positive or negative.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using movie re\u00adviews as data, we .nd that standard ma\u00adchine learning techniques de.nitively out\u00adperform human-produced baselines.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "How\u00adever, the three machine learning methods we employed (Naive Bayes, maximum en\u00adtropy classi.cation, and support vector ma\u00adchines) do not perform as well on sentiment classi.cation as on traditional topic-based categorization.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We conclude by examining factors that make the sentiment classi.ca\u00adtion problem more challenging.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Today, very large amounts of information are avail\u00adable in online documents.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As part of the e.ort to better organize this information for users, researchers have been actively investigating the problem of au\u00adtomatic text categorization.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The bulk of such work has focused on topical cat\u00adegorization, attempting to sort documents accord\u00ading to their subject matter (e.g., sports vs. poli\u00adtics).",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, recent years have seen rapid growth in online discussion groups and review sites (e.g., the New York Times\u2019 Books web page) where a cru\u00adcial characteristic of the posted articles is their senti\u00adment, or overall opinion towards the subject matter \u2014 for example, whether a product review is pos\u00aditive or negative.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Labeling these articles with their sentiment would provide succinct summaries to read\u00aders; indeed, these labels are part of the appeal and value-add of such sites as www.rottentomatoes.com, which both labels movie reviews that do not con\u00adtain explicit rating indicators and normalizes the di.erent rating schemes that individual reviewers use.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sentiment classi.cation would also be helpful in business intelligence applications (e.g. MindfulEye\u2019s Lexant system1) and recommender systems (e.g., Terveen et al.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1997), Tatemura (2000)), where user input and feedback could be quickly summarized; in\u00addeed, in general, free-form survey responses given in natural language format could be processed using sentiment categorization.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, there are also potential applications to message .ltering; for exam\u00adple, one might be able to use sentiment information to recognize and discard \u201c.ames\u201d(Spertus, 1997).",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we examine the e.ectiveness of ap\u00adplying machine learning techniques to the sentiment classi.cation problem.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A challenging aspect of this problem that seems to distinguish it from traditional topic-based classi.cation is that while topics are of\u00adten identi.able by keywords alone, sentiment can be expressed in a more subtle manner.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the sentence \u201cHow could anyone sit through this movie?\u201d contains no single word that is obviously negative.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(See Section 7 for more examples).",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, sentiment seems to require more understanding than the usual topic-based classi.cation.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "So, apart from presenting our results obtained via machine learning techniques, we also analyze the problem to gain a better under\u00adstanding of how di.cult it is.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "previous work. ",
            "number": "2",
            "sents": [
                {
                    "text": "This section brie.y surveys previous work on non-topic-based text categorization.",
                    "sid": 19,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One area of research concentrates on classifying documents according to their source or source style, with statistically-detected stylistic variation (Biber, 1988) serving as an important cue.",
                    "sid": 20,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Examples in\u00adclude author, publisher (e.g., the New York Times vs. The Daily News), native-language background, and \u201cbrow\u201d (e.g., highbrow vs. \u201cpopular\u201d, or lowbrow) (Mosteller and Wallace, 1984; ArgamonEngelson et 1http://www.mindfuleye.com/about/lexant.htm al., 1998; Tomokiyo and Jones, 2001; Kessler et al., 1997).",
                    "sid": 21,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Another, more related area of research is that of determining the genre of texts; subjective genres, such as \u201ceditorial\u201d, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002).",
                    "sid": 22,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other work explicitly attempts to .nd features indicating that subjective language is being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001).",
                    "sid": 23,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opin\u00adion, they do not address our speci.c classi.cation task of determining what that opinion actually is. Most previous research on sentiment-based classi\u00ad.cation has been at least partially knowledge-based.",
                    "sid": 24,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some of this work focuses on classifying the semantic orientation of individual words or phrases, using lin\u00adguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002).",
                    "sid": 25,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Past work on sentiment-based cat\u00adegorization of entire documents has often involved either the use of models inspired by cognitive lin\u00adguistics (Hearst, 1992; Sack, 1994) or the manual or semi-manual construction of discriminant-word lex\u00adicons (Huettner and Subasic, 2000; Das and Chen, 2001; Tong, 2001).",
                    "sid": 26,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Interestingly, our baseline exper\u00adiments, described in Section 4, show that humans may not always have the best intuition for choosing discriminating words.",
                    "sid": 27,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Turney\u2019s (2002) work on classi.cation of reviews is perhaps the closest to ours.2 He applied a spe\u00adci.c unsupervised learning technique based on the mutual information between document phrases and the words \u201cexcellent\u201d and \u201cpoor\u201d, where the mu\u00adtual information is computed using statistics gath\u00adered by a search engine.",
                    "sid": 28,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast, we utilize sev\u00aderal completely prior-knowledge-free supervised ma\u00adchine learning methods, with the goal of understand\u00ading the inherent di.culty of the task.",
                    "sid": 29,
                    "ssid": 11,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "the movie-review domain. ",
            "number": "3",
            "sents": [
                {
                    "text": "For our experiments, we chose to work with movie reviews.",
                    "sid": 30,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This domain is experimentally convenient because there are large online collections of such re\u00adviews, and because reviewers often summarize their overall sentiment with a machine-extractable rat\u00ading indicator, such as a number of stars; hence, we did not need to hand-label the data for supervised learning or evaluation purposes.",
                    "sid": 31,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also note that Turney (2002) found movie reviews to be the most 2Indeed, although our choice of title was completely independent of his, our selections were eerily similar.",
                    "sid": 32,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "di.cult of several domains for sentiment classi.ca\u00adtion, reporting an accuracy of 65.83% on a 120\u00addocument set (random-choice performance: 50%).",
                    "sid": 33,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But we stress that the machine learning methods and features we use are not speci.c to movie reviews, and should be easily applicable to other domains as long as su.cient training data exists.",
                    "sid": 34,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our data source was the Internet Movie Database (IMDb) archive of the rec.arts.movies.reviews newsgroup.3 We selected only reviews where the au\u00adthor rating was expressed either with stars or some numerical value (other conventions varied too widely to allow for automatic processing).",
                    "sid": 35,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ratings were automatically extracted and converted into one of three categories: positive, negative, or neutral.",
                    "sid": 36,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the work described in this paper, we concentrated only on discriminating between positive and nega\u00adtive sentiment.",
                    "sid": 37,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To avoid domination of the corpus by a small number of proli.c reviewers, we imposed a limit of fewer than 20 reviews per author per sen\u00adtiment category, yielding a corpus of 752 negative and 1301 positive reviews, with a total of 144 re\u00adviewers represented.",
                    "sid": 38,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This dataset will be available online at http://www.cs.cornell.edu/people/pabo/\u00admovie-review-data/ (the URL contains hyphens only around the word \u201creview\u201d).",
                    "sid": 39,
                    "ssid": 10,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "a closer look at the problem ",
            "number": "4",
            "sents": [
                {
                    "text": "Intuitions seem to di.er as to the di.culty of the sen\u00adtiment detection problem.",
                    "sid": 40,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An expert on using ma\u00adchine learning for text categorization predicted rela\u00adtively low performance for automatic methods.",
                    "sid": 41,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the other hand, it seems that distinguishing positive from negative reviews is relatively easy for humans, especially in comparison to the standard text catego\u00adrization problem, where topics can be closely related.",
                    "sid": 42,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One might also suspect that there are certain words people tend to use to express strong sentiments, so that it might su.ce to simply produce a list of such words by introspection and rely on them alone to classify the texts.",
                    "sid": 43,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To test this latter hypothesis, we asked two gradu\u00adate students in computer science to (independently) choose good indicator words for positive and nega\u00adtive sentiments in movie reviews.",
                    "sid": 44,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Their selections, shown in Figure 1, seem intuitively plausible.",
                    "sid": 45,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then converted their responses into simple decision procedures that essentially count the number of the proposed positive and negative words in a given doc\u00adument.",
                    "sid": 46,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We applied these procedures to uniformly-distributed data, so that the random-choice baseline result would be 50%.",
                    "sid": 47,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As shown in Figure 1, the 3http://reviews.imdb.com/Reviews/ Proposed word lists Accuracy Ties Human 1 positive: dazzling, brilliant, phenomenal, excellent, fantastic negative: suck, terrible, awful, unwatchable, hideous 58% 75% Human 2 positive: gripping, mesmerizing, riveting, spectacular, cool, awesome, thrilling, badass, excellent, moving, exciting negative: bad, cliched, sucks, boring, stupid, slow 64% 39% Figure 1: Baseline results for human word lists.",
                    "sid": 48,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Data: 700 positive and 700 negative reviews.",
                    "sid": 49,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Proposed word lists Accuracy Ties Human 3 + stats positive: love, wonderful, best, great, superb, still, beautiful negative: bad, worst, stupid, waste, boring, ?, ! 69% 16% Figure 2: Results for baseline using introspection and simple statistics of the data (including test data).",
                    "sid": 50,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "accuracy \u2014 percentage of documents classi.ed cor\u00adrectly \u2014 for the human-based classi.ers were 58% and 64%, respectively.4 Note that the tie rates \u2014 percentage of documents where the two sentiments were rated equally likely \u2014 are quite high5 (we chose a tie breaking policy that maximized the accuracy of the baselines).",
                    "sid": 51,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While the tie rates suggest that the brevity of the human-produced lists is a factor in the relatively poor performance results, it is not the case that size alone necessarily limits accuracy.",
                    "sid": 52,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Based on a very preliminary examination of frequency counts in the entire corpus (including test data) plus introspection, we created a list of seven positive and seven negative words (including punctuation), shown in Figure 2.",
                    "sid": 53,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As that .gure indicates, using these words raised the accuracy to 69%.",
                    "sid": 54,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Also, although this third list is of comparable length to the other two, it has a much lower tie rate of 16%.",
                    "sid": 55,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We further observe that some of the items in this third list, such as \u201c?\u201d or \u201cstill\u201d, would probably not have been proposed as possible candidates merely through introspection, although upon re.ection one sees their merit (the question mark tends to occur in sentences like \u201cWhat was the director thinking?\u201d; \u201cstill\u201d appears in sentences like \u201cStill, though, it was worth seeing\u201d).",
                    "sid": 56,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We conclude from these preliminary experiments that it is worthwhile to explore corpus-based tech\u00adniques, rather than relying on prior intuitions, to se\u00adlect good indicator features and to perform sentiment classi.cation in general.",
                    "sid": 57,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These experiments also pro\u00advide us with baselines for experimental comparison; in particular, the third baseline of 69% might actu\u00adally be considered somewhat di.cult to beat, since it was achieved by examination of the test data (al\u00adthough our examination was rather cursory; we do 4Later experiments using these words as features for machine learning methods did not yield better results.",
                    "sid": 58,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5This is largely due to 00 ties.",
                    "sid": 59,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "not claim that our list was the optimal set of four\u00adteen words).",
                    "sid": 60,
                    "ssid": 21,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "machine learning methods. ",
            "number": "5",
            "sents": [
                {
                    "text": "Our aim in this work was to examine whether it suf\u00ad.ces to treat sentiment classi.cation simply as a spe\u00adcial case of topic-based categorization (with the two \u201ctopics\u201d being positive sentiment and negative sen\u00adtiment), or whether special sentiment-categorization methods need to be developed.",
                    "sid": 61,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We experimented with three standard algorithms: Naive Bayes clas\u00adsi.cation, maximum entropy classi.cation, and sup\u00adport vector machines.",
                    "sid": 62,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The philosophies behind these three algorithms are quite di.erent, but each has been shown to be e.ective in previous text catego\u00adrization studies.",
                    "sid": 63,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To implement these machine learning algorithms on our document data, we used the following stan\u00addard bag-of-features framework.",
                    "sid": 64,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let {f1,...,fm} be a prede.ned set of m features that can appear in a document; examples include the word \u201cstill\u201d or the bigram \u201creally stinks\u201d.",
                    "sid": 65,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let ni(d) be the num\u00adber of times fi occurs in document d. Then, each document d is represented by the document vector i d := (n1(d),n2(d),...,nm(d)).",
                    "sid": 66,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.1 Naive Bayes.",
                    "sid": 67,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One approach to text classi.cation is to assign to a * given document d the class c= arg maxc P (c | d).",
                    "sid": 68,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We derive the Naive Bayes (NB) classi.er by .rst observing that by Bayes\u2019 rule, P (c)P (d | c) P (c | d)= , P (d) * where P (d) plays no role in selecting c. To estimate the term P (d | c), Naive Bayes decomposes it by as\u00adsuming the fi\u2019s are conditionally independent given d\u2019s class: m P (c)P (fi | c)ni (d) i=1 PNB(c | d) := . P (d) Our training method consists of relative-frequency estimation of P (c) and P (fi | c), using add-one smoothing.",
                    "sid": 69,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Despite its simplicity and the fact that its con\u00additional independence assumption clearly does not hold in real-world situations, Naive Bayes-based text categorization still tends to perform surprisingly well (Lewis, 1998); indeed, Domingos and Pazzani (1997) show that Naive Bayes is optimal for certain problem classes with highly dependent features.",
                    "sid": 70,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the other hand, more sophisticated algorithms might (and of\u00adten do) yield better results; we examine two such algorithms next.",
                    "sid": 71,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.2 Maximum Entropy.",
                    "sid": 72,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Maximum entropy classi.cation (MaxEnt, or ME, for short) is an alternative technique which has proven e.ective in a number of natural lan\u00adguage processing applications (Berger et al., 1996).",
                    "sid": 73,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Nigam et al.",
                    "sid": 74,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1999) show that it sometimes, but not always, outperforms Naive Bayes at standard text classi.cation.",
                    "sid": 75,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Its estimate of P (c | d) takes the fol\u00adlowing exponential form: L 1 PME(c | d) := expAi,cFi,c(d, c), Z(d) i where Z(d) is a normalization function.",
                    "sid": 76,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Fi,c is a fea\u00adture/class function for feature fi and class c, de.ned as follows:6 { ' 1,ni(d) > 0 and c= c Fi,c(d, c') := . 0 otherwise For instance, a particular feature/class function might .re if and only if the bigram \u201cstill hate\u201d ap\u00adpears and the document\u2019s sentiment is hypothesized to be negative.7 Importantly, unlike Naive Bayes, MaxEnt makes no assumptions about the relation\u00adships between features, and so might potentially per\u00adform better when conditional independence assump\u00adtions are not met.",
                    "sid": 77,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Ai,c\u2019s are feature-weight parameters; inspec\u00adtion of the de.nition of PME shows that a large Ai,c means that fi is considered a strong indicator for 6We use a restricted de.nition of feature/class func\u00adtions so that MaxEnt relies on the same sort of feature information as Naive Bayes.",
                    "sid": 78,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7The dependence on class is necessary for parameter induction.",
                    "sid": 79,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "See Nigam et al.",
                    "sid": 80,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1999) for additional moti\u00advation.",
                    "sid": 81,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "class c. The parameter values are set so as to max\u00adimize the entropy of the induced distribution (hence the classi.er\u2019s name) subject to the constraint that the expected values of the feature/class functions with respect to the model are equal to their expected values with respect to the training data: the under\u00adlying philosophy is that we should choose the model making the fewest assumptions about the data while still remaining consistent with it, which makes intu\u00aditive sense.",
                    "sid": 82,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use ten iterations of the improved iterative scaling algorithm (Della Pietra et al., 1997) for parameter training (this was a su.cient num\u00adber of iterations for convergence of training-data ac\u00adcuracy), together with a Gaussian prior to prevent over.tting (Chen and Rosenfeld, 2000).",
                    "sid": 83,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.3 Support Vector Machines.",
                    "sid": 84,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Support vector machines (SVMs) have been shown to be highly e.ective at traditional text categorization, generally outperforming Naive Bayes (Joachims, 1998).",
                    "sid": 85,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They are large-margin, rather than proba\u00adbilistic, classi.ers, in contrast to Naive Bayes and MaxEnt.",
                    "sid": 86,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the two-category case, the basic idea be\u00adhind the training procedure is to .nd a hyperplane, represented by vector wi, that not only separates the document vectors in one class from those in the other, but for which the separation, or margin, is as large as possible.",
                    "sid": 87,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This search corresponds to a con\u00adstrained optimization problem; letting cj E{1, -1} (corresponding to positive and negative) be the cor\u00adrect class of document dj , the solution can be written as L wi:= aj cj dij ,aj . 0, j where the aj \u2019s are obtained by solving a dual opti\u00admization problem.",
                    "sid": 88,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Those dij such that aj is greater than zero are called support vectors, since they are the only document vectors contributing to wi.",
                    "sid": 89,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Clas\u00adsi.cation of test instances consists simply of deter\u00admining which side of wi\u2019s hyperplane they fall on.",
                    "sid": 90,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used Joachim\u2019s (1999) SV M light package8 for training and testing, with all parameters set to their default values, after .rst length-normalizing the doc\u00adument vectors, as is standard (neglecting to normal\u00adize generally hurt performance slightly).",
                    "sid": 91,
                    "ssid": 31,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "evaluation. ",
            "number": "6",
            "sents": [
                {
                    "text": "6.1 Experimental Setup.",
                    "sid": 92,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used documents from the movie-review corpus described in Section 3.",
                    "sid": 93,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To create a data set with uni\u00adform class distribution (studying the e.ect of skewed 8http://svmlight.joachims.org Features # of features frequency or presence?",
                    "sid": 94,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "NB ME SVM (1) unigrams 16165 freq.",
                    "sid": 95,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "78.7 N/A 72.8 (2) unigrams \u201d pres.",
                    "sid": 96,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "81.0 80.4 82.9 (3) unigrams+bigrams 32330 pres.",
                    "sid": 97,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "80.6 80.8 82.7 (4) bigrams 16165 pres.",
                    "sid": 98,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "77.3 77.4 77.1 (5) unigrams+POS 16695 pres.",
                    "sid": 99,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "81.5 80.4 81.9 (6) adjectives 2633 pres.",
                    "sid": 100,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "77.0 77.7 75.1 (7) top 2633 unigrams 2633 pres.",
                    "sid": 101,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "80.3 81.0 81.4 (8) unigrams+position 22430 pres.",
                    "sid": 102,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "81.0 80.1 81.6 Figure 3: Average threefold cross-validation accuracies, in percent.",
                    "sid": 103,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Boldface: best performance for a given setting (row).",
                    "sid": 104,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recall that our baseline results ranged from 50% to 69%.",
                    "sid": 105,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "class distributions was out of the scope of this study), we randomly selected 700 positive-sentiment and 700 negative-sentiment documents.",
                    "sid": 106,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then divided this data into three equal-sized folds, maintaining bal\u00adanced class distributions in each fold.",
                    "sid": 107,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(We did not use a larger number of folds due to the slowness of the MaxEnt training procedure.)",
                    "sid": 108,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All results reported below, as well as the baseline results from Section 4, are the average threefold cross-validation results on this data (of course, the baseline algorithms had no parameters to tune).",
                    "sid": 109,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To prepare the documents, we automatically re\u00admoved the rating indicators and extracted the tex\u00adtual information from the original HTML docu\u00adment format, treating punctuation as separate lex\u00adical items.",
                    "sid": 110,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "No stemming or stoplists were used.",
                    "sid": 111,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One unconventional step we took was to attempt to model the potentially important contextual e.ect of negation: clearly \u201cgood\u201d and \u201cnot very good\u201d in\u00addicate opposite sentiment orientations.",
                    "sid": 112,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Adapting a technique of Das and Chen (2001), we added the tag NOT to every word between a negation word (\u201cnot\u201d, \u201cisn\u2019t\u201d, \u201cdidn\u2019t\u201d, etc.) and the .rst punctuation mark following the negation word.",
                    "sid": 113,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Preliminary ex\u00adperiments indicate that removing the negation tag had a negligible, but on average slightly harmful, ef\u00adfect on performance.)",
                    "sid": 114,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For this study, we focused on features based on unigrams (with negation tagging) and bigrams.",
                    "sid": 115,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Be\u00adcause training MaxEnt is expensive in the number of features, we limited consideration to (1) the 16165 unigrams appearing at least four times in our 1400\u00addocument corpus (lower count cuto.s did not yield signi.cantly di.erent results), and (2) the 16165 bi-grams occurring most often in the same data (the selected bigrams all occurred at least seven times).",
                    "sid": 116,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that we did not add negation tags to the bi-grams, since we consider bigrams (and n-grams in general) to be an orthogonal way to incorporate con\u00adtext.",
                    "sid": 117,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6.2 Results.",
                    "sid": 118,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Initial unigram results The classi.cation accu\u00adracies resulting from using only unigrams as fea\u00adtures are shown in line (1) of Figure 3.",
                    "sid": 119,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a whole, the machine learning algorithms clearly surpass the random-choice baseline of 50%.",
                    "sid": 120,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They also hand\u00adily beat our two human-selected-unigram baselines of 58% and 64%, and, furthermore, perform well in comparison to the 69% baseline achieved via limited access to the test-data statistics, although the im\u00adprovement in the case of SVMs is not so large.",
                    "sid": 121,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the other hand, in topic-based classi.cation, all three classi.ers have been reported to use bag-of-unigram features to achieve accuracies of 90% and above for particular categories (Joachims, 1998; Nigam et al., 1999)9 \u2014 and such results are for set\u00adtings with more than two classes.",
                    "sid": 122,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This provides suggestive evidence that sentiment categorization is more di.cult than topic classi.cation, which cor\u00adresponds to the intuitions of the text categoriza\u00adtion expert mentioned above.10 Nonetheless, we still wanted to investigate ways to improve our senti\u00adment categorization results; these experiments are reported below.",
                    "sid": 123,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Feature frequency vs. presence Recall that we represent each document d by a feature-count vector (n1(d),...,nm(d)).",
                    "sid": 124,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the de.nition of the 9Joachims (1998) used stemming and stoplists; in some of their experiments, Nigam et al.",
                    "sid": 125,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1999), like us, did not.",
                    "sid": 126,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "10We could not perform the natural experiment of at\u00adtempting topic-based categorization on our data because the only obvious topics would be the .lm being reviewed; unfortunately, in our data, the maximum number of re\u00adviews per movie is 27, too small for meaningful results.",
                    "sid": 127,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "MaxEnt feature/class functions Fi,c only re.ects the presence or absence of a feature, rather than directly incorporating feature frequency.",
                    "sid": 128,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to investi\u00adgate whether reliance on frequency information could account for the higher accuracies of Naive Bayes and SVMs, we binarized the document vectors, setting ni(d) to 1 if and only feature fi appears in d, and reran Naive Bayes and SV M light on these new vec\u00adtors.11 As can be seen from line (2) of Figure 3, better performance (much better performance for SVMs) is achieved by accounting only for fea\u00adture presence, not feature frequency.",
                    "sid": 129,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Interestingly, this is in direct opposition to the observations of McCallum and Nigam (1998) with respect to Naive Bayes topic classi.cation.",
                    "sid": 130,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We speculate that this in\u00addicates a di.erence between sentiment and topic cat\u00adegorization \u2014 perhaps due to topic being conveyed mostly by particular content words that tend to be repeated \u2014 but this remains to be veri.ed.",
                    "sid": 131,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In any event, as a result of this .nding, we did not incor\u00adporate frequency information into Naive Bayes and SVMs in any of the following experiments.",
                    "sid": 132,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Bigrams In addition to looking speci.cally for negation words in the context of a word, we also studied the use of bigrams to capture more context in general.",
                    "sid": 133,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that bigrams and unigrams are surely not conditionally independent, meaning that the feature set they comprise violates Naive Bayes\u2019 conditional-independence assumptions; on the other hand, recall that this does not imply that Naive Bayes will necessarily do poorly (Domingos and Paz\u00adzani, 1997).",
                    "sid": 134,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Line (3) of the results table shows that bigram information does not improve performance beyond that of unigram presence, although adding in the bi-grams does not seriously impact the results, even for Naive Bayes.",
                    "sid": 135,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.",
                    "sid": 136,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, comparing line (4) to line (2) shows that relying just on bigrams causes accuracy to decline by as much as 5.8 percentage points.",
                    "sid": 137,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hence, if context is in fact important, as our intuitions suggest, bigrams are not e.ective at cap\u00adturing it in our setting.",
                    "sid": 138,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "11Alternatively, we could have tried integrating fre\u00adquency information into MaxEnt.",
                    "sid": 139,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, feature/class functions are traditionally de.ned as binary (Berger et al., 1996); hence, explicitly incorporating frequencies would require di.erent functions for each count (or count bin), making training impractical.",
                    "sid": 140,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But cf.",
                    "sid": 141,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Nigam et al., 1999).",
                    "sid": 142,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Parts of speech We also experimented with ap\u00adpending POS tags to every word via Oliver Mason\u2019s Qtag program.12 This serves as a crude form of word sense disambiguation (Wilks and Stevenson, 1998): for example, it would distinguish the di.erent usages of \u201clove\u201d in \u201cI love this movie\u201d (indicating sentiment orientation) versus \u201cThis is a love story\u201d (neutral with respect to sentiment).",
                    "sid": 143,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the e.ect of this information seems to be a wash: as depicted in line (5) of Figure 3, the accuracy improves slightly for Naive Bayes but declines for SVMs, and the per\u00adformance of MaxEnt is unchanged.",
                    "sid": 144,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since adjectives have been a focus of previous work in sentiment detection (Hatzivassiloglou and Wiebe, 2000; Turney, 2002)13, we looked at the performance of using adjectives alone.",
                    "sid": 145,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Intuitively, we might ex\u00adpect that adjectives carry a great deal of informa\u00adtion regarding a document\u2019s sentiment; indeed, the human-produced lists from Section 4 contain almost no other parts of speech.",
                    "sid": 146,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Yet, the results, shown in line (6) of Figure 3, are relatively poor: the 2633 adjectives provide less useful information than uni\u00adgram presence.",
                    "sid": 147,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Indeed, line (7) shows that simply using the 2633 most frequent unigrams is a better choice, yielding performance comparable to that of using (the presence of) all 16165 (line (2)).",
                    "sid": 148,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This may imply that applying explicit feature-selection algo\u00adrithms on unigrams could improve performance.",
                    "sid": 149,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Position An additional intuition we had was that the position of a word in the text might make a dif\u00adference: movie reviews, in particular, might begin with an overall sentiment statement, proceed with a plot discussion, and conclude by summarizing the author\u2019s views.",
                    "sid": 150,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a rough approximation to deter\u00admining this kind of structure, we tagged each word according to whether it appeared in the .rst quar\u00adter, last quarter, or middle half of the document14 . The results (line (8)) didn\u2019t di.er greatly from using unigrams alone, but more re.ned notions of position might be more successful.",
                    "sid": 151,
                    "ssid": 60,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "discussion. ",
            "number": "7",
            "sents": [
                {
                    "text": "The results produced via machine learning tech\u00adniques are quite good in comparison to the human-generated baselines discussed in Section 4.",
                    "sid": 152,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In terms of relative performance, Naive Bayes tends to do the worst and SVMs tend to do the best, although the 12http://www.english.bham.ac.uk/sta./oliver/soft\u00adware/tagger/index.htm 13Turney\u2019s (2002) unsupervised algorithm uses bi-grams containing an adjective or an adverb.",
                    "sid": 153,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "14We tried a few other settings, e.g., .rst third vs. last third vs middle third, and found them to be less e.ective.",
                    "sid": 154,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "di.erences aren\u2019t very large.",
                    "sid": 155,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the other hand, we were not able to achieve ac\u00adcuracies on the sentiment classi.cation problem com\u00adparable to those reported for standard topic-based categorization, despite the several di.erent types of features we tried.",
                    "sid": 156,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unigram presence information turned out to be the most e.ective; in fact, none of the alternative features we employed provided consis\u00adtently better performance once unigram presence was incorporated.",
                    "sid": 157,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Interestingly, though, the superiority of presence information in comparison to frequency information in our setting contradicts previous obser\u00advations made in topic-classi.cation work (McCallum and Nigam, 1998).",
                    "sid": 158,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "What accounts for these two di.erences \u2014 dif\u00ad.culty and types of information proving useful \u2014 between topic and sentiment classi.cation, and how might we improve the latter?",
                    "sid": 159,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To answer these ques\u00adtions, we examined the data further.",
                    "sid": 160,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(All examples below are drawn from the full 2053-document cor\u00adpus.)",
                    "sid": 161,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As it turns out, a common phenomenon in the doc\u00aduments was a kind of \u201cthwarted expectations\u201d narra\u00adtive, where the author sets up a deliberate contrast to earlier discussion: for example, \u201cThis .lm should be brilliant.",
                    "sid": 162,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It sounds like a great plot, the actors are .rst grade, and the supporting cast is good as well, and Stallone is attempting to deliver a good performance.",
                    "sid": 163,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, it can\u2019t hold up\u201d or \u201cI hate the Spice Girls.",
                    "sid": 164,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "...[3 things the author hates about them]...",
                    "sid": 165,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Why I saw this movie is a really, really, really long story, but I did, and one would think I\u2019d despise every minute of it.",
                    "sid": 166,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But...",
                    "sid": 167,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Okay, I\u2019m really ashamed of it, but I enjoyed it.",
                    "sid": 168,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "I mean, I admit it\u2019s a really awful movie ...the ninth .oor of hell...The plot is such a mess that it\u2019s terrible.",
                    "sid": 169,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "15 But I loved it.\u201d In these examples, a human would easily detect the true sentiment of the review, but bag-of-features classi.ers would presumably .nd these instances dif\u00ad.cult, since there are many words indicative of the opposite sentiment to that of the entire review.",
                    "sid": 170,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Fun\u00addamentally, it seems that some form of discourse analysis is necessary (using more sophisticated tech\u00ad 15This phenomenon is related to another common theme, that of \u201ca good actor trapped in a bad movie\u201d: \u201cAN AMERICAN WEREWOLF IN PARIS is a failed at\u00adtempt...",
                    "sid": 171,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Julie Delpy is far too good for this movie.",
                    "sid": 172,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "She im\u00adbues Sera.ne with spirit, spunk, and humanity.",
                    "sid": 173,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This isn\u2019t necessarily a good thing, since it prevents us from relax\u00ading and enjoying AN AMERICAN WEREWOLF IN PARIS as a completely mindless, campy entertainment experience.",
                    "sid": 174,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Delpy\u2019s injection of class into an otherwise classless produc\u00adtion raises the specter of what this .lm could have been with a better script and a better cast ...",
                    "sid": 175,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "She was radiant, charismatic, and e.ective ....\u201d niques than our positional feature mentioned above), or at least some way of determining the focus of each sentence, so that one can decide when the author is talking about the .lm itself.",
                    "sid": 176,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Turney (2002) makes a similar point, noting that for reviews, \u201cthe whole is not necessarily the sum of the parts\u201d.)",
                    "sid": 177,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Further\u00admore, it seems likely that this thwarted-expectations rhetorical device will appear in many types of texts (e.g., editorials) devoted to expressing an overall opinion about some topic.",
                    "sid": 178,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hence, we believe that an important next step is the identi.cation of features indicating whether sentences are on-topic (which is a kind of co-reference problem); we look forward to addressing this challenge in future work.",
                    "sid": 179,
                    "ssid": 28,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments",
            "number": "",
            "sents": [
                {
                    "text": "We thank Joshua Goodman, Thorsten Joachims, Jon Kleinberg, Vikas Krishna, John La.erty, Jussi Myl\u00adlymaki, Phoebe Sengers, Richard Tong, Peter Tur\u00adney, and the anonymous reviewers for many valuable comments and helpful suggestions, and Hubie Chen and Tony Faradjian for participating in our baseline experiments.",
                    "sid": 180,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Portions of this work were done while the .rst author was visiting IBM Almaden.",
                    "sid": 181,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This pa\u00adper is based upon work supported in part by the Na\u00adtional Science Foundation under ITR/IM grant IIS\u00ad0081334.",
                    "sid": 182,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Any opinions, .ndings, and conclusions or recommendations expressed above are those of the authors and do not necessarily re.ect the views of the National Science Foundation.",
                    "sid": 183,
                    "ssid": 32,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}