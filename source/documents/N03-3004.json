{
    "ID": "N03-3004",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "This paper presents an unsupervised method for discriminating among the senses of a given target word based on the context in which it oc\u00adcurs.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instances of a word that occur in similar contexts are grouped together via McQuitty\u2019s Similarity Analysis, an agglomerative cluster\u00ading algorithm.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The context in which a target word occurs is represented by surface lexical features such as unigrams, bigrams, and sec\u00adond order co-occurrences.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This paper summa\u00adrizes our approach, and describes the results of a preliminary evaluation we have carried out using data from the SENSEVAL2 English lexi\u00adcal sample and the line corpus.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Word sense discrimination is the process of grouping or clustering together instances of written text that include similar usages of a given target word.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The instances that form a particular cluster will have used the target word in similar contexts and are therefore presumed to represent a related meaning.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This view follows from the strong con\u00adtextual hypothesis of (Miller and Charles, 1991), which states that two words are semantically similar to the ex\u00adtent that their contextual representations are similar.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Discrimination is distinct from the more common problem of word sense disambiguation in at least two respects.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, the number of possible senses a target word may have is usually not known in discrimination, while disambiguation is often viewed as a classi.cation problem where a word is assigned to one of several pre\u2013 existing possible senses.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, discrimination utilizes features and information that can be easily extracted from raw corpora, whereas disambiguation often relies on su\u00adpervised learning from sense\u2013tagged training examples.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the creation of sense\u2013tagged data is time con\u00adsuming and results in a knowledge acquisition bottleneck that severely limits the portability and scalability of tech\u00adniques that employ it.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Discrimination does not suffer from this problem since there is no expensive preprocess\u00ading, nor are any external knowledge sources or manually annotated data required.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The objective of this research is to extend previous work in discrimination by (Pedersen and Bruce, 1997), who developed an approach using agglomerative cluster\u00ading.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Their work relied on McQuitty\u2019s Similarity Anal\u00adysis using localized contextual features.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While the ap\u00adproach in this paper also adopts McQuitty\u2019s method, it is distinct in that it uses a larger number of features that occur both locally and globally in the instance being dis\u00adcriminated.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It also incorporates several ideas from later work by (Sch\u00a8utze, 1998), including the reliance on a sep\u00adarate \u201ctraining\u201d corpus of raw text from which to iden\u00adtify contextual features, and the use of second order co\u2013 occurrences (socs) as feature for discrimination.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our near term objectives for this research include de\u00adtermining to what extent different types of features im\u00adpact the accuracy of unsupervised discrimination.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We are also interested in assessing how different measures of similarity such as the matching coef.cient or the co\u00adsine affect overall performance.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Once we have re.ned our clustering techniques, we will incorporate them into a method that automatically assigns sense labels to discov\u00adered clusters by using information from a machine read\u00adable dictionary.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This paper continues with a more detailed discussion of the previous work that forms the foundation for our re\u00adsearch.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then present an overview of the features used to represent the context of a target word, and go on to de\u00adscribe an experimental evaluation using the SENSEVAL2 lexical sample data.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We close with a discussion of our re\u00adsults, a summary of related work, and an outline of our future directions.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "previous work. ",
            "number": "2",
            "sents": [
                {
                    "text": "The work in this paper builds upon two previous ap\u00adproaches to word sense discrimination, those of (Peder\u00adsen and Bruce, 1997) and (Sch\u00a8 utze, 1998).",
                    "sid": 23,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pedersen and Bruce developed a method based on agglomerative clus\u00adtering using McQuitty\u2019s Similarity Analysis (McQuitty, 1966), where the context of a target word is represented using localized contextual features such as collocations and part of speech tags that occur within one or two po\u00adsitions of the target word.",
                    "sid": 24,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pedersen and Bruce demon\u00adstrated that despite it\u2019s simplicity, McQuitty\u2019s method was more accurate than Ward\u2019s Method of Minimum Variance and the EM Algorithm for word sense discrimi\u00adnation.",
                    "sid": 25,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "McQuitty\u2019s method starts by assuming that each in\u00adstance is a separate cluster.",
                    "sid": 26,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It merges together the pair of clusters that have the highest average similarity value.",
                    "sid": 27,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This continues until a speci.ed number of clusters is found, or until the similarity measure between every pair of clusters is less than a prede.ned cutoff.",
                    "sid": 28,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pedersen and Bruce used a relatively small number of features, and em\u00adployed the matching coef.cient as the similarity measure.",
                    "sid": 29,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since we use a much larger number of features, we are ex\u00adperimenting with the cosine measure, which scales simi\u00adlarity based on the number of non\u2013zero features in each instance.",
                    "sid": 30,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By way of contrast, (Sch\u00a8utze, 1998) performs discrim\u00adination through the use of two different kinds of context vectors.",
                    "sid": 31,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The .rst is a word vector that is based on co\u2013 occurrence counts from a separate training corpus.",
                    "sid": 32,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each word in this corpus is represented by a vector made up of the words it co-occurs with.",
                    "sid": 33,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then, each instance in a test or evaluation corpus is represented by a vector that is the average of all the vectors of all the words that make up that instance.",
                    "sid": 34,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The context in which a target word occurs is thereby represented by second order co\u2013occurrences, which are words which co\u2013occur with the co\u2013occurrences of the target word.",
                    "sid": 35,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Discrimination is carried out by clus\u00adtering instance vectors using the EM Algorithm.",
                    "sid": 36,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The approach described in this paper proceeds as fol\u00adlows.",
                    "sid": 37,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Surface lexical features are identi.ed in a training corpus, which is made up of instances that consists of a sentence containing a given target word, plus one or two sentences to the left or right of it.",
                    "sid": 38,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similarly de.ned in\u00adstances in the test data are converted into vectors based on this feature set, and a similarity matrix is constructed using either the matching coef.cient or the cosine.",
                    "sid": 39,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There\u00adafter McQuitty\u2019s Similarity Analysis is used to group to\u00adgether instances based on the similarity of their context, and these are evaluated relative to a manually created gold standard.",
                    "sid": 40,
                    "ssid": 18,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "discrimination features. ",
            "number": "3",
            "sents": [
                {
                    "text": "We carry out discrimination based on surface lexical fea\u00adtures that require little or no preprocessing to identify.",
                    "sid": 41,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They consist of unigrams, bigrams, and second order co\u2013 occurrences.",
                    "sid": 42,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unigrams are single words that occur in the same con\u00adtext as a target word.",
                    "sid": 43,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Bag\u2013of\u2013words feature sets made up of unigrams have had a long history of success in text classi.cation and word sense disambiguation (Mooney, 1996), and we believe that despite creating quite a bit of noise can provide useful information for discrimination.",
                    "sid": 44,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Bigrams are pairs of words which occur together in the same context as the target word.",
                    "sid": 45,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They may include the target word, or they may not.",
                    "sid": 46,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We specify a win\u00addow of size .ve for bigrams, meaning that there may be up to three intervening words between the .rst and last word that make up the bigram.",
                    "sid": 47,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As such we are de.ning bigrams to be non\u2013consecutive word sequences, which could also be considered a kind of co\u2013occurrence feature.",
                    "sid": 48,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Peder\u00adsen, 2001).",
                    "sid": 49,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We believe this is because they capture mid\u00addle distance co\u2013occurrence relations between words that occur in the context of the target word.",
                    "sid": 50,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second order co\u2013occurrences are words that occur with co-occurrences of the target word.",
                    "sid": 51,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, suppose that line is the target word.",
                    "sid": 52,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given telephone line and tele\u00adphone bill, bill would be considered a second order co\u2013 occurrence of line since it occurs with telephone, a .rst order co\u2013occurrence of line.",
                    "sid": 53,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We de.ne a window size of .ve in identifying sec\u00adond order co\u2013occurrences, meaning that the .rst order co\u2013occurrence must be within .ve positions of the tar\u00adget word, and the second order co\u2013occurrence must be within .ve positions of the .rst order co\u2013occurrence.",
                    "sid": 54,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We only select those second order co\u2013occurrences which co\u2013 occur more than once with the .rst order co-occurrences which in turn co-occur more than once with the target word within the speci.ed window.",
                    "sid": 55,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We employ a stop list to remove high frequency non\u2013 content words from all of these features.",
                    "sid": 56,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unigrams that are included in the stop list are not used as features.",
                    "sid": 57,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A bi-gram is rejected if any word composing it is a stop word.",
                    "sid": 58,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second order co\u2013occurrences that are stop words or those that co\u2013occur with stop words are excluded from the fea\u00adture set.",
                    "sid": 59,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After the features have been identi.ed in the training data, all of the instances in the test data are converted athet atesttdedtdtt a a r into binary feature vectorsthat repre\u00adsent whether the features found in the training data have occurred in a particular test instance.",
                    "sid": 60,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to clus\u00adter these instances, we measure the pair\u2013wise similarities between them using matching and cosine coef.cients.",
                    "sid": 61,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These values are formatted in a N x Nsimilarity ma o tr rm trix such that cell contains the similarity measure m between instancesand . This information serves as the input to the clustering algorithm that groups together the most similar instances.",
                    "sid": 62,
                    "ssid": 22,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "experimental methodology. ",
            "number": "4",
            "sents": [
                {
                    "text": "We evaluate our method using two well known sources of sense\u2013tagged text.",
                    "sid": 63,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In supervised learning sense\u2013tagged text is used to induce a classi.er that is then applied to held out test data.",
                    "sid": 64,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, our approach is purely un\u00adsupervised and we only use the sense tags to carry out an automatic evaluation of the discovered clusters.",
                    "sid": 65,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We fol\u00adlow Sch\u00a8utze\u2019s strategy and use a \u201ctraining\u201d corpus only to extract features and ignore the sense tags.",
                    "sid": 66,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, we use subsets of the line data (Leacock et al., 1993) and the English lexical sample data from the SENSEVAL2 comparative exercise among word sense disambiguation systems (Edmonds and Cotton, 2001).",
                    "sid": 67,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The line data contains 4,146 instances, where each consists of two to three sentences where a single oc\u00adcurrence of line has been manually tagged with one of six possible senses.",
                    "sid": 68,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We randomly select 100 instances of each sense for test data, and 200 instances of each sense for training.",
                    "sid": 69,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This gives a total of 600 evaluation instances, and 1200 training instances.",
                    "sid": 70,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is done to test the quality of our discrimination method when senses are uniformly distributed and where no particular sense is dominant.",
                    "sid": 71,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The standard distribution of the SENSEVAL2 data consists of 8,611 training instances and 4,328 test in\u00adstances.",
                    "sid": 72,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each instance is made up of two to three sen\u00adtences where a single target word has been manually tagged with a sense (or senses) appropriate for that con\u00adtext.",
                    "sid": 73,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are 73 distinct target words found in this data; 29 nouns, 29 verbs, and 15 adjectives.",
                    "sid": 74,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most of these words have less than 100 test instances, and ap\u00adproximately twice that number of training examples.",
                    "sid": 75,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In general these are relatively small samples for an unsu\u00adpervised approach, but we are developing techniques to increase the amount of training data for this corpus auto\u00admatically.",
                    "sid": 76,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We .lter the SENSEVAL2 data in three different ways to prepare it for processing and evaluation.",
                    "sid": 77,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we in\u00adsure that it only includes instances whose actual sense is among the top .ve most frequent senses as observed in the training data for that word.",
                    "sid": 78,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We believe that this is an aggressive number of senses for a discrimination system to attempt, considering that (Pedersen and Bruce, 1997) experimented with 2 and 3 senses, and (Sch\u00a8 utze, 1998) made binary distinctions.",
                    "sid": 79,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, instances may have been assigned more than one correct sense by the human annotator.",
                    "sid": 80,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to simplify the evaluation process, we eliminate all but the most frequent of multiple correct answers.",
                    "sid": 81,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Third, the SENSEVAL2 data identi.es target words that are proper nouns.",
                    "sid": 82,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have elected not to use that in\u00adformation and have removed these P tags from the data.",
                    "sid": 83,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After carrying out these preprocessing steps, the number of training and test instances is 7,476 and 3,733.",
                    "sid": 84,
                    "ssid": 22,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "evaluation technique. ",
            "number": "5",
            "sents": [
                {
                    "text": "We specify an upper limit on the number of senses that McQuitty\u2019s algorithm can discover.",
                    "sid": 85,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In these experiments this value is .ve for the SENSEVAL2 data, and six for line.",
                    "sid": 86,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In future experiments we will specify even higher values, so that the algorithm is forced to create larger number of clusters with very few instances when the ac\u00adtual number of senses is smaller than the given cutoff.",
                    "sid": 87,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "About a third of the words in the SENSEVAL2 data have fewer than 5 senses, so even now the clustering algorithm is not always told the correct number of clusters it should .nd.",
                    "sid": 88,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Once the clusters are formed, we access the actual cor\u00adrect sense of each instance as found in the sense\u2013tagged text.",
                    "sid": 89,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This information is never utilized prior to evalua\u00adtion.",
                    "sid": 90,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the sense\u2013tagged text as a gold standard by which we can evaluate the discovered sense clusters.",
                    "sid": 91,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We assign sense tags to clusters such that the resulting accu\u00adracy is maximized.",
                    "sid": 92,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, suppose that .ve clusters (C1 \u2013 C5) have been discovered for a word with 100 instances, and that the number of instances in each cluster is 25, 20, 10, 25, and 20.",
                    "sid": 93,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Suppose that there are .ve actual senses (S1 \u2013 S5), and the number of instances for each sense is 20, 20, 20, 20, and 20.",
                    "sid": 94,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1 shows the resulting confusion matrix if the senses are assigned to clusters in numeric order.",
                    "sid": 95,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After this assignment is made, the accuracy of the clustering can be determined by .nding the sum of the diagonal, and dividing by the total number of instances, which in this case leads to accuracy of 10% (10/100).",
                    "sid": 96,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, clearly there are assignments of senses to clus\u00adters that would lead to better results.",
                    "sid": 97,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, the problem of assigning senses to clusters be\u00adcomes one of reordering the columns of the confusion such that the diagonal sum is maximized.",
                    "sid": 98,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This corre\u00adsponds to several well known problems, among them the Assignment Problem in Operations Research, and deter\u00admining the maximal matching of a bipartite graph.",
                    "sid": 99,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 2 shows the maximally accurate assignment of senses to clusters, which leads to accuracy of 70% (70/100).",
                    "sid": 100,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During evaluation we assign one cluster to at most one sense, and vice versa.",
                    "sid": 101,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When the number of discovered clusters is the same as the number of senses, then there is a 1 to 1 mapping between them.",
                    "sid": 102,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When the number of clusters is greater than the number of actual senses, then some clusters will be left unassigned.",
                    "sid": 103,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "And when the S1 S2 S3 S4 S5 C1: 5 20 0 0 0 25 C2: 10 0 5 0 5 20 C3: 0 0 0 0 10 10 C4: 0 0 15 5 5 25 C5: 5 0 0 15 0 20 20 20 20 20 20 100 Figure 1: Numeric Assignment S2 S1 S5 S3 S4 C1: 20 5 0 0 0 25 C2: 0 10 5 5 0 20 C3: 0 0 10 0 0 10 C4: 0 0 5 15 5 25 C5: 0 5 0 0 15 20 20 20 20 20 20 100 Figure 2: Maximally Accurate Assignment number of senses is greater than the number of clusters, some senses will not be assigned to any cluster.",
                    "sid": 104,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We determine the precision and recall based on this maximally accurate assignment of sense tags to clusters.",
                    "sid": 105,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Precision is de.ned as the number of instances that are clustered correctly divided by the number of instances clustered, while recall is the number of instances clus\u00adtered correctly over the total number of instances.",
                    "sid": 106,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To be clear, we do not believe that word sense discrim\u00adination must be carried out relative to a pre\u2013existing set of senses.",
                    "sid": 107,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In fact, one of the great advantages of an un\u00adsupervised approach is that it need not be relative to any particular set of senses.",
                    "sid": 108,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We carry out this evaluation tech\u00adnique in order to improve the performance of our cluster\u00ading algorithm, which we will then apply on text where sense\u2013tagged data is not available.",
                    "sid": 109,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An alternative means of evaluation is to have a hu\u00adman inspect the discovered clusters and judge them based on the semantic coherence of the instances that populate each cluster, but this is a more time consuming and sub\u00adjective method of evaluation that we will pursue in future.",
                    "sid": 110,
                    "ssid": 26,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "experimental results. ",
            "number": "6",
            "sents": [
                {
                    "text": "For each word in the SENSEVAL2 data and line, we con\u00adducted various experiments, each of which uses a differ\u00adent combination of measure of similarity and features.",
                    "sid": 111,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Features are identi.ed from the training data.",
                    "sid": 112,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our fea\u00adtures consist of unigrams, bigrams, or second order co\u2013 occurrences.",
                    "sid": 113,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We employ each of these three types of fea\u00adtures separately, and we also create a mixed set that is the union of all three sets.",
                    "sid": 114,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We convert each evaluation in\u00adstance into a feature vector, and then convert those into a similarity matrix using either the matching coef.cient or the cosine.",
                    "sid": 115,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 1 contains overall precision and recall for the nouns, verbs, and adjectives overall in the SENSEVAL\u00ad2 data, and for line.",
                    "sid": 116,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The SENSEVAL2 values are de\u00adrived from 29 nouns, 28 verbs, and 15 adjectives from the SENSEVAL2 data.",
                    "sid": 117,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The .rst column lists the part of speech, the second shows the feature, the third lists the measure of similarity, the fourth and the .fth show pre\u00adcision and recall, the sixth shows the percentage of the majority sense, and the .nal column shows the number of words in the given part of speech that gave accuracy greater than the percentage of the majority sense.",
                    "sid": 118,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The value of the majority sense is derived from the sense\u2013 tagged data we use in evaluation, but this is not infor\u00admation that we would presume to have available during actual clustering.",
                    "sid": 119,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 1: Experimental Results pos feat meas prec rec maj .maj noun soc cos 0.49 0.48 0.57 6/29 mat 0.54 0.52 0.57 7/29 big cos 0.53 0.50 0.57 5/29 mat 0.52 0.49 0.57 3/29 uni cos 0.50 0.49 0.57 7/29 mat 0.52 0.50 0.57 8/29 mix cos 0.50 0.48 0.57 6/29 mat 0.54 0.51 0.57 5/29 verb soc cos 0.51 0.49 0.51 11/28 mat 0.50 0.47 0.51 6/28 big cos 0.54 0.45 0.51 5/28 mat 0.53 0.43 0.51 5/28 uni cos 0.42 0.41 0.51 13/28 mat 0.43 0.41 0.51 9/28 mix cos 0.43 0.41 0.51 12/28 mat 0.42 0.41 0.51 7/28 adj soc cos 0.59 0.54 0.64 1/15 mat 0.59 0.55 0.64 1/15 big cos 0.56 0.51 0.64 0/15 mat 0.55 0.50 0.64 0/15 uni cos 0.55 0.50 0.64 1/15 mat 0.58 0.53 0.64 0/15 mix cos 0.50 0.44 0.64 0/15 mat 0.59 0.54 0.64 2/15 line soc cos 0.25 0.25 0.17 1/1 mat 0.23 0.23 0.17 1/1 big cos 0.19 0.18 0.17 1/1 mat 0.18 0.17 0.17 1/1 uni cos 0.21 0.21 0.17 1/1 mat 0.20 0.20 0.17 1/1 mix cos 0.21 0.21 0.17 1/1 mat 0.20 0.20 0.17 1/1 For the SENSEVAL2 data, on average the precision and recall of the clustering as determined by our evalu\u00adation method is less than that of the majority sense, re\u00adgardless of which features or measure are used.",
                    "sid": 120,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "How\u00adever, for nouns and verbs, a relatively signi.cant num\u00adber of individual words have precision and recall values higher than that of the majority sense.",
                    "sid": 121,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The adjectives are an exception to this, where words are very rarely dis\u00adambiguated more accurately than the percentage of the majority sense.",
                    "sid": 122,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, many of the adjectives have very high frequency majority senses, which makes this a dif.cult standard for an unsupervised method to reach.",
                    "sid": 123,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When examining the distribution of instances in clusters, we .nd that the algorithm tends to seek more balanced distributions, and is unlikely to create a single long clus\u00adter that would result in high accuracy for a word whose true distribution of senses is heavily skewed towards a single sense.",
                    "sid": 124,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also note that the precision and recall of the clus\u00adtering of the line data is generally better than that of the majority sense regardless of the features or measures em\u00adployed.",
                    "sid": 125,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We believe there are two explanations for this.",
                    "sid": 126,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, the number of training instances for the line data is signi.cantly higher (1200) than that of the SENSEVAL2 words, which typically have 100\u2013200 training instances per word.",
                    "sid": 127,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The number and quality of features identi.ed improves considerably with an increase in the amount of training data.",
                    "sid": 128,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, the amount of training data avail\u00adable for feature identi.cation is critically important.",
                    "sid": 129,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We believe that the SENSEVAL2 data could be augmented with training data taken from the World Wide Web, and we plan to pursue such approaches and see if our perfor\u00admance on the evaluation data improves as a result.",
                    "sid": 130,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "At this point we do not observe a clear advantage to using the cosine measure or matching coef.cient.",
                    "sid": 131,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This surprises us somewhat, as the number of features em\u00adployed is generally in the thousands, and the number of non\u2013zero features can be quite large.",
                    "sid": 132,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It would seem that simply counting the number of matching features would be inferior to the cosine measure, but this is not the case.",
                    "sid": 133,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This remains an interesting issue that we will continue to explore, with these and other measures of similarity.",
                    "sid": 134,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, there is not a single feature that does best in all parts of speech.",
                    "sid": 135,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second order co\u2013occurrences seem to do well with nouns and adjectives, while bigrams result in accurate clusters for verbs.",
                    "sid": 136,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also note that second order co\u2013occurrences do well with the line data.",
                    "sid": 137,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As yet we have drawn no conclusions from these results, but it is clearly a vital issue to investigate further.",
                    "sid": 138,
                    "ssid": 28,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "related work. ",
            "number": "7",
            "sents": [
                {
                    "text": "Unsupervised approaches to word sense discrimination have been somewhat less common in the computational linguistics literature, at least when compared to super\u00advised approaches to word sense disambiguation.",
                    "sid": 139,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There is a body of work at the intersection of super\u00advised and unsupervised approaches, which involves using a small amount of training data in order to automatically create more training data, in effect bootstrapping from the small sample of sense\u2013tagged data.",
                    "sid": 140,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The best example of such an approach is (Yarowsky, 1995), who proposes a method that automatically identi.es collocations that are indicative of the sense of a word, and uses those to itera\u00adtively label more examples.",
                    "sid": 141,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While our focus has been on Pedersen and Bruce, and on Sch\u00a8utze, there has been other work in purely unsuper\u00advised approaches to word sense discrimination.",
                    "sid": 142,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Fukumoto and Suzuki, 1999) describe a method for discriminating among verb senses based on determining which nouns co\u2013occur with the target verb.",
                    "sid": 143,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Collocations are extracted which are indicative of the sense of a verb based on a similarity measure they derive.",
                    "sid": 144,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Pantel and Lin, 2002) introduce a method known as Committee Based Clustering that discovers word senses.",
                    "sid": 145,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The words in the corpus are clustered based on their dis\u00adtributional similarity under the assumption that semanti\u00adcally similar words will have similar distributional char\u00adacteristics.",
                    "sid": 146,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, they use Pointwise Mutual In\u00adformation to .nd how close a word is to its context and then determine how similar the contexts are using the co\u00adsine coef.cient.",
                    "sid": 147,
                    "ssid": 9,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "future work. ",
            "number": "8",
            "sents": [
                {
                    "text": "Our long term goal is to develop a method that will as\u00adsign sense labels to clusters using information found in machine readable dictionaries.",
                    "sid": 148,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is an important prob\u00adlem because clusters as found in discrimination have no sense tag or label attached to them.",
                    "sid": 149,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While there are cer\u00adtainly applications for unlabeled sense clusters, having some indication of the sense of the cluster would bring discrimination and disambiguation closer together.",
                    "sid": 150,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will treat glosses as found in a dictionary as vectors that we project into the same space that is populated by in\u00adstances as we have already described.",
                    "sid": 151,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A cluster could be assigned the sense of the gloss whose vector it was most closely located to.",
                    "sid": 152,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This idea is based loosely on work by (Niwa and Nitta, 1994), who compare word co\u2013occurrence vectors derived from large corpora of text with co\u2013occurrence vectors based on the de.nitions or glosses of words in a ma\u00adchine readable dictionary.",
                    "sid": 153,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A co\u2013occurrence vector indi\u00adcates how often words are used with each other in a large corpora or in dictionary de.nitions.",
                    "sid": 154,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These vectors can be projected into a high dimensional space and used to mea\u00adsure the distance between concepts or words.",
                    "sid": 155,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Niwa and Nitta show that while the co\u2013occurrence data from a dic\u00adtionary has different characteristics that a co\u2013occurrence vector derived from a corpus, both provide useful infor\u00admation about how to categorize a word based on its mean\u00ading.",
                    "sid": 156,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our future work will mostly attempt to merge clus\u00adters found from corpora with meanings in dictionaries where presentation techniques like co\u2013occurrence vectors could be useful.",
                    "sid": 157,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are a number of smaller issues that we are inves\u00adtigating.",
                    "sid": 158,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We are also exploring a number of other types of features, as well as varying the formulation of the fea\u00adtures we are currently using.",
                    "sid": 159,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have already conducted a number of experiments that vary the window sizes em\u00adployed with bigrams and second order co\u2013occurrences, and will continue in this vein.",
                    "sid": 160,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We are also considering the use of other measures of similarity beyond the match\u00ading coef.cient and the cosine.",
                    "sid": 161,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We do not stem the train\u00ading data prior to feature identi.cation, nor do or employ fuzzy matching techniques when converting evaluation instances into feature vectors.",
                    "sid": 162,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, we believe both might lead to increased numbers of useful features being identi.ed.",
                    "sid": 163,
                    "ssid": 16,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusions. ",
            "number": "9",
            "sents": [
                {
                    "text": "We have presented an unsupervised method of word sense discrimination that employs a range of surface lexi\u00adcal features, and relies on similarity based clustering.",
                    "sid": 164,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have evaluated this method in an extensive experiment that shows that our method can achieve precision and re\u00adcall higher than the majority sense of a word for a reason\u00adably large number of cases.",
                    "sid": 165,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We believe that increases in the amount of training data employed in this method will yield to considerably improved results, and have outlined our plans to address this and several other issues.",
                    "sid": 166,
                    "ssid": 3,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgments. ",
            "number": "10",
            "sents": [
                {
                    "text": "This research is being conducted as a part of my M.S. the\u00adsis in Computer Science at the University of Minnesota, Duluth.",
                    "sid": 167,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "I am grateful to my thesis advisor, Dr. Ted Ped\u00adersen, for his help and guidance.",
                    "sid": 168,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "I have been fully supported by a National Science Foundation Faculty Early CAREER Development Award (#0092784) during the 2002\u20132003 academic year.",
                    "sid": 169,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "I would like to thank the Director of Computer Science Graduate Studies, Dr. Carolyn Crouch, and the Associate Vice Chancellor, Dr. Stephen Hedman, for their support in providing a travel award to attend the Student Research Workshop at HLTNAACL 2003.",
                    "sid": 170,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}