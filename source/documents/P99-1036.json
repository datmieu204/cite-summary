{
    "ID": "P99-1036",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the character types that con\u00ad stitute a word.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The point is quite simple: differ\u00ad ent character sets should be treated differently and the changes between character types are very im\u00ad portant because Japanese script has both ideograms like Chinese (kanji) and phonograms like English (katakana).",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The model can achieve 96.6% tag\u00ad ging accuracy if unknown words are correctly seg\u00ad mented.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "In Japanese, around 95% word segmentation ac\u00ad curacy is reported by using a word-based lan\u00ad guage model and the Viterbi-like dynamic program\u00ad ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Mat\u00ad sumoto, 1997).",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But there has been relatively little improve\u00ad ment in recent years because most of the remaining errors are due to unknown words.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We take the latter approach.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To improve word segmenta\u00ad tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "' The goal of our research is to assign a correct part of speech to unknown word as well as identifying it correctly.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we present a novel statistical model for Japanese unknown words.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It consists of a set of word models for each part of speech and word type.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We classified Japanese words into nine orthographic types based on the character types that constitute a word.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We find that by making different models for each word type, we can better model the length and spelling of unknown words.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the following sections, we first describe the lan\u00ad guage model used for Japanese word segmentation.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then describe a series of unknown word mod\u00ad els, from the baseline model to the one we propose.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we prove the effectiveness of the proposed model by experiment.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "word segmentation model. ",
            "number": "2",
            "sents": [
                {
                    "text": "2.1 Baseline Language Model and Search.",
                    "sid": 19,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Algorithm Let the input Japanese character sequence be C = c1 ... em, and segment it into word sequence W = w1 ...Wn 1 \u2022 The word segmentation task can be defined as finding the word segmentation W that max\u00ad imize the joint probability of word sequence given character sequence P(WIC).",
                    "sid": 20,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the maximiza\u00ad tion is carried out with fixed character sequence C, the word segmenter only has to maximize the joint probability of word sequence P(W).",
                    "sid": 21,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "W = argmaxP(WIC) = argmaxP(W) (1) w w We call P(W) the segmentation model.",
                    "sid": 22,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can use any type of word-based language model for P(W), such as word ngram and class-based ngram.",
                    "sid": 23,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used the word bigram model in this paper.",
                    "sid": 24,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "So, P(W) is approximated by the product of word hi\u00ad gram probabilities P(wilwi1)\u00b7 P(W) P(w1l<bos>) fr=2 P(wilwi-l)P( <eos>lwn) (2) Here, the special symbols <bos> and <eos> indi\u00ad cate the beginning and the end of a sentence, re\u00ad spectively.",
                    "sid": 25,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Basically, word bigram probabilities of the word segmentation model is estimated by computing the 1 In this paper, we define a word as a combination of its surface form and part of speech.",
                    "sid": 26,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Two words are considered to be equal only if they have the same surface form and part of speech.",
                    "sid": 27,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "277 Table 1: Examples of word bigrams including un\u00ad known word tags example \"(J)/no/particle <U-noun>\" will appear in the most frequent form of Japanese noun phrases \"A (J) B\", which corresponds to \"B of A\" in English.",
                    "sid": 28,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As Table 1 shows, word bigrams whose infrequent word bigram (f)/no/particle <U-noun> <U-verb> \\..., /shi/inflection <U-number> fil/yen/suffix <U-adjectival-verb> f.t Ina/inflection <U-adjective> It '/i/inflection <U-adverb> c /to/particle frequency 6783 1052 407 405 182 139 words are replaced with their corresponding part of speech-based unknown word tags are very important information source of the contexts where unknown words appears.",
                    "sid": 29,
                    "ssid": 11,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "unknown. ",
            "number": "3",
            "sents": [
                {
                    "text": "Word Model 3.1 Baseline Model The simplest unknown word model depends only on relative frequencies of the corresponding events in the word segmented training corpus, with appropri\u00ad ate smoothing techniques.",
                    "sid": 30,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The maximization search can be efficiently implemented by using the Viterbi\u00ad like dynamic programming procedure described in (Nagata, 1994).",
                    "sid": 31,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.2 Modification to Handle Unknown.",
                    "sid": 32,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Words To handle unknown words, we made a slight modi\u00ad fication in the above word segmentation model.",
                    "sid": 33,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have introduced unknown word tags <U-t> for each part of speech t. For example, <U-noun> and <U\u00ad verb> represents an unknown noun and an unknown verb, respectively.",
                    "sid": 34,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If Wi is an unknown word whose part of speech is t, the word bigram probability P(wilwi1) is ap\u00ad proximated as the product of word bigram probabil\u00ad ity P( <U-t>lwi_I) and the probability of Wi given it is an unknown word whose part of speech is t, P(wii<U-t> ).",
                    "sid": 35,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "P(wdwi1) = P( <U-t>lwi_I)P(wii<U-t>,Wi1) P( <U-t>lwi1)P(wii<U-t>) (3) the spelling.",
                    "sid": 36,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We think of an unknown word as a word having a special part of speech <UNK>.",
                    "sid": 37,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then, the unknown word model is formally defined as the joint probability of the character sequence wi = c1 ... ck if it is an unknown word.",
                    "sid": 38,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Without loss of generality, we decompose it into the product of word length probability and word spelling probability given its length, P(wii<UNK>) = P(c1 ... cki<UNK>) = P(ki<UNK>)P(c1 ... cklk, <UNK>) (4) where k is the length of the character sequence.",
                    "sid": 39,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We call P(ki<UNK>) the word length model, and P( c1 ...",
                    "sid": 40,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ck Ik, <UNK >) the word spelling model.",
                    "sid": 41,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to estimate the entropy of English, (Brown et al., 1992) approximated P(ki<UNK>) by a Poisson distribution whose parameter is the average word length A in the training corpus, and P(c1 ...",
                    "sid": 42,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ck lk, <UNK>) by the product of character zerogram probabilities.",
                    "sid": 43,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This means all characters in the character set are considered to be selected inde\u00ad pendently and uniformly.",
                    "sid": 44,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here, we made an assumption that the spelling P (c1 ..",
                    "sid": 45,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": ".ck I<UNK> ) k!",
                    "sid": 46,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ak ->.",
                    "sid": 47,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "k (5) of an unknown word solely depends on its part of speech and is independent of the previous word.",
                    "sid": 48,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is the same assumption made in the hidden Markov model, which is called output independence.",
                    "sid": 49,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The probabilities P( <U-t>lwi_I) can be esti\u00ad mated from the relative frequencies in the training corpus whose infrequent words are replaced with their corresponding unknown word tags based on their part of speeches 2 \u2022 Table 1 shows examples of word bigrams including unknown word tags.",
                    "sid": 50,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here, a word is represented by a list of surface form, pronunciation, and part of speech, which are delimited by a slash '/'.",
                    "sid": 51,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first 2 Throughout in this paper, we use the term \"infrequent words\" to represent words that appeared only once in the corpus.",
                    "sid": 52,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They are also called \"hapax legomena\" or \"hapax words\".",
                    "sid": 53,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is well known that the characteristics of hapax where p is the inverse of the number of characters in the character set.",
                    "sid": 54,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If we assume JIS-X-0208 is used as the Japanese character set, p = 1/6879.",
                    "sid": 55,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the Poisson distribution is a single parame\u00ad ter distribution with lower bound, it is appropriate to use it as a first order approximation to the word length distribution.",
                    "sid": 56,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But the Brown model has two problems.",
                    "sid": 57,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It assigns a certain amount of probability mass to zero-length words, and it is too simple to express morphology.",
                    "sid": 58,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For Japanese word segmentation and OCR error correction, (Nagata, 1996) proposed a modified ver\u00ad sion of the Brown model.",
                    "sid": 59,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Nagata also assumed the word length probability obeys the Poisson distribu\u00ad tion.",
                    "sid": 60,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But he moved the lower bound from zero to one.",
                    "sid": 61,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "legomena are similar to those of unknown words (Baayen and Sproat, 1996).",
                    "sid": 62,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "P(ki<UNK>) (A 1)k-1 - e-<>.- ) (6) (k- 1)!",
                    "sid": 63,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead of zerogram, He approximated the word spelling probability P(c1 ..",
                    "sid": 64,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": ".ckJk, <UNK>) by the product of word-based character bigram probabili\u00ad ties, regardless of word length.",
                    "sid": 65,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "P(cl\u00b7\u00b7\u00b7ckJk,<VNK>) P(cd<bow>) f1 =2 P(ciJci-l)P( <eow>Jck) (7) where <bow> and <eow> are special symbols that indicate the beginning and the end of a word.",
                    "sid": 66,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2 Correction of Word Spelling.",
                    "sid": 67,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Probabilities We find that Equation (7) assigns too little proba\u00ad bilities to long words (5 or more characters).",
                    "sid": 68,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is because the lefthand side of Equation (7) represents the probability of the string c1 ... ck in the set of all strings whose length are k, while the righthand side 0..",
                    "sid": 69,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 0 Word Length Distribution Probs from Raw Counts (hapax words) _..,_ EsUmates by Poisson (hapax words) -+--\u00b7",
                    "sid": 70,
                    "ssid": 41,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "6 \t10",
            "number": "4",
            "sents": [
                {
                    "text": "Wor Character represents the probability of the string in the set of all possible strings (from length zero to infinity).",
                    "sid": 71,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let Pb (c1 ...Ck I<UNK>) be the probability of character string c1 ...Ck estimated from the char\u00ad acter bigram model.",
                    "sid": 72,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "d Length Figure 1: Word length distribution of unknown words and its estimate by Poisson distribution Pb(cl ... cki<UNK>) = 0.5 UnknoWn Word Length Distribution kanji-+\u00ad P(c1 J<bow>) f1 =2 P(ciJci-l)P( <eow>Jck) (8) Let Pb(kJ<VNK>) be the sum of the probabilities of all strings which are generated by the character bigram model and whose length are k. More appro\u00ad priate estimate for P(c1 ..",
                    "sid": 73,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": ".ckJk, <UNK>) is, Pb(c1 ... ckJ<UNK>) P(cl\u00b7\u00b7\u00b7ckJk,<VNK>) Pb(kJ<VNK>) (9) But how can we estimate Pb(kl<VNK>)?",
                    "sid": 74,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is difficult to compute it directly, but we can get a rea\u00ad sonable estimate by considering the unigram case.",
                    "sid": 75,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If strings are generated by the character unigram model, the sum of the probabilities of all length k strings equals to the probability of the event that the end of word symbol <eow> is selected after a 0..",
                    "sid": 76,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 0 2 4 6 Word Character Length katakana -+--\u00b7 10 character other than <eow> is selected k - 1 times.",
                    "sid": 77,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pb(kl<VNK>)(1- P( <eow> ))k-l P( <eow>) (10) Throughout in this paper, we used Equation (9) to compute the word spelling probabilities.",
                    "sid": 78,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.3 Japanese Orthography and Word.",
                    "sid": 79,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Length Distribution In word segmentation, one of the major problems of the word length model of Equation (6) is the decom\u00ad position of unknown words.",
                    "sid": 80,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When a substring of an unknown word coincides with other word in the dic\u00ad tionary, it is very likely to be decomposed into the dictionary word and the remaining substring.",
                    "sid": 81,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We find the reason of the decomposition is that the word Figure 2: Word length distribution of kanji words and katakana words length model does not reflect the variation of the word length distribution resulting from the Japanese orthography.",
                    "sid": 82,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1shows the word length distribution of in\u00ad frequent words in the EDR corpus, and the estimate of word length distribution by Equation (6) whose parameter (.A = 4.8) is the average word length of infrequent words.",
                    "sid": 83,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The empirical and the estimated distributions agree fairly well.",
                    "sid": 84,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But the estimates by Poisson are smaller than empirical probabilities for shorter words ( <= 4 characters), and larger for longer words (>characters).",
                    "sid": 85,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is because we rep Table 2: Character type configuration of infrequent words in the EDR corpus Table 3: Examples of common character bigrams for each part of speech in the infrequent words pa rt of sp ee ch ch ar ac ter bi gr a m fre qu en cy no un nu m be r a dj e ct iv al v er b v er b ad je cti ve ad ve rb < e o w > <b o w > 1 S \" J < e o w > I t < e o w > L < e o w > < e o w > 13 43 4 8 4 3 2 7 2 1 3 69 63 resented all unknown words by one length model.",
                    "sid": 86,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 2 shows the word length distribution of words consists of only kanji characters and words consists of only katakana characters.",
                    "sid": 87,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It shows that the length of kanji words distributes around 3 char\u00ad acters, while that of katakana words distributes around 5 characters.",
                    "sid": 88,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The empirical word length dis\u00ad tribution of Figure 1 is, in fact, a weighted sum of these two distributions.",
                    "sid": 89,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the Japanese writing system, there are at least five different types of characters other than punc\u00ad tuation marks: kanji, hiragana, katakana, Roman alphabet, and Arabic numeral.",
                    "sid": 90,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Kanji which means 'Chinese character' is used for both Chinese origin words and Japanese words semantically equivalent to Chinese characters.",
                    "sid": 91,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hiragana and katakana are syllabaries: The former is used primarily for gram\u00ad matical function words, such as particles and inflec\u00ad tional endings, while the latter is used primarily to transliterate Western origin words.",
                    "sid": 92,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Roman alphabet is also used for Western origin words and acronyms.",
                    "sid": 93,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Arabic numeral is used for numbers.",
                    "sid": 94,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most Japanese words are written in kanji, while more recent loan words are written in katakana.",
                    "sid": 95,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "K atakana words are likely to be used for techni\u00ad cal terms, especially in relatively new fields like computer science.",
                    "sid": 96,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Kanji words are shorter than katakana words because kanji is based on a large (> 6,000) alphabet of ideograms while katakana is based on a small ( < 100) alphabet of phonograms.",
                    "sid": 97,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 shows the distribution of character type sequences that constitute the infrequent words in the EDR corpus.",
                    "sid": 98,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It shows approximately 65% of words are constituted by a single character type.",
                    "sid": 99,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Among the words that are constituted by more than two character types, only the kanjihiragana and hiraganakanji sequences are morphemes and others are compound words in a strict sense although they are identified as words in the EDR corpus 3 . Therefore, we classified Japanese words into 9 word types based on the character types that consti\u00ad tute a word: <sym>, <num>, <alpha>, <hira>, <kata>, and <kan> represent a sequence of sym\u00ad bols, numbers, alphabets, hiraganas, katakanas, and kanjis, respectively.",
                    "sid": 100,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "<kanhira> and <hirakan> represent a sequence of kanjis followed by hiraganas and that of hiraganas followed by kanjis, respec\u00ad tively.",
                    "sid": 101,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The rest are classified as <mise>.",
                    "sid": 102,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The resulting unknown word model is as follows.",
                    "sid": 103,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We first select the word type, then we select the length and spelling.",
                    "sid": 104,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "P(c1 ... cki<UNK>) = P( <WT>I<UNK> )P(ki<WT>, <UNK>) P(c1 ... ckik, <WT>, <UNK>) (11 ) 3.4 Part of Speech and Word.",
                    "sid": 105,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Morphology It is obvious that the beginnings and endings of words play an important role in tagging part of speech.",
                    "sid": 106,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 3 shows examples of common char\u00ad acter bigrams for each part of speech in the infre\u00ad quent words of the EDR corpus.",
                    "sid": 107,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first example in Table 3 shows that words ending in ' -' are likely to be nouns.",
                    "sid": 108,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This symbol typically appears at the end of transliterated Western origin words written in katakana.",
                    "sid": 109,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is natural to make a model for each part of speech.",
                    "sid": 110,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The resulting unknown word model is as follows.",
                    "sid": 111,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "P(c1 ... cki<U-t>) = P(ki<U-t> )P(c1 ... ckik, <U-t>) (12) By introducing the distinction of word type to the model of Equation(12),we can derive a more sophis\u00ad ticated unknown word model that reflects both word 3 When a Chinese character is used to represent a seman\u00ad tically equivalent Japanese verb, its root is written in the Chinese character and its inflectional suffix is written in hi\u00ad ragana.",
                    "sid": 112,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This results in kanjihiragana sequence.",
                    "sid": 113,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When a Chinese character is too difficult to read, it is transliterated in hiragana.",
                    "sid": 114,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This results in either hiraganakanji or kanji\u00ad hiragana sequence.",
                    "sid": 115,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "type and part of speech information.",
                    "sid": 116,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is the un\u00ad known word model we propose in this paper.",
                    "sid": 117,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It first selects the word type given the part of speech, then the word length and spelling.",
                    "sid": 118,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "P(c1 ... cki<U-t>) = P( <WT>I<U-t> )P(ki<WT>, <U-t>) P(c1 ... ckik, <WT>, <U-t>) (13) Table 4: The amount of training and test sets The first factor in the righthand side of Equa\u00ad tion (13) is estimated from the relative frequency of the corresponding events in the training corpus.",
                    "sid": 119,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "- ) = C( <WT>, <U-t>) t> C(<U-t>) (14) Where ad(c;, <WT>, <U-t>) +ad(c;ic;-1, <WT>, <U-t>) +a3f(ci) + a4f(cilci1) + as(1/V) (17) Here, C (\u00b7) represents the counts in the corpus.",
                    "sid": 120,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To estimate the probabilities of the combinations of word type and part of speech that did not appeared in the training corpus, we used the Witten-Bell method (Witten and Bell, 1991) to obtain an esti\u00ad mate for the sum of the probabilities of unobserved events.",
                    "sid": 121,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then redistributed this evenly among all unobserved events 4 . The second factor of Equation (13) is estimated from the Poisson distribution whose parameter A<WT>,<U-t> is the average length of words whose word type is <WT> and part of speech is <U-t>.",
                    "sid": 122,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "P(ki<WT>, <U-t>) = (.>.<WT>.<U-t> -1)k-t e-(>.<WT> <U-t> -1) (k-1)!",
                    "sid": 123,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "' If the combinations of word type and part of speech that did not appeared in the training corpus, we used the average word length of all words.",
                    "sid": 124,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To compute the third factor of Equation (13), we have to estimate the character bigram probabilities that are classified by word type and part of speech.",
                    "sid": 125,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Basically, they are estimated from the relative fre\u00ad quency of the character bigrams for each word type and part of speech.",
                    "sid": 126,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "f(c;ic;_1, <WT>, <U-t>) = C(<WT>,<U-t>,c;-t,c;) C( <WT>,<U-t>,c;-t) However, if we divide the corpus by the combina\u00ad tion of word type and part of speech, the amount of each training data becomes very small.",
                    "sid": 127,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, we linearly interpolated the following five probabili\u00ad ties (Jelinek and Mercer, 1980).",
                    "sid": 128,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "P(e;ic;-1,<WT>,<U-t>) = 4 The Witten-Bell method estimates the probability of ob\u00ad serving novel events to be r/(n+r), where n is the total num\u00ad ber of events seen previously, and r is the number of symbols that are distinct.",
                    "sid": 129,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The probability of the event observed c times is cf(n + r).",
                    "sid": 130,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "a1 +a2+a3+a4+as = 1.",
                    "sid": 131,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "f(ci, <WT>, <U-t>) and f(c;lc;-1, <WT>, <U-t>) are the relative frequen\u00ad cies of the character unigram and bigram for each word type and part of speech.",
                    "sid": 132,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "f(c;) and f(c;jc;_1) are the relative frequencies of the character unigram and bigram.",
                    "sid": 133,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "V is the number of characters (not to\u00ad kens but types) appeared in the corpus.",
                    "sid": 134,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4 Experiments.",
                    "sid": 135,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.1 Training and Test Data for the.",
                    "sid": 136,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Language Model We used the EDR Japanese Corpus Version 1.0 (EDR, 1991) to train the language model.",
                    "sid": 137,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is a manually word segmented and tagged corpus of ap\u00ad proximately 5.1 million words (208 thousand sen\u00ad tences).",
                    "sid": 138,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It contains a variety of Japanese sentences taken from newspapers, magazines, dictionaries, en\u00ad cyclopedias, textbooks, etc..",
                    "sid": 139,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this experiment, we randomly selected two sets of 100 thousand sentences.",
                    "sid": 140,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first 100 thousand sentences are used for training the language model.",
                    "sid": 141,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second 100 thousand sentences are used for test\u00ad ing.",
                    "sid": 142,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The remaining 8 thousand sentences are used as a heldout set for smoothing the parameters.",
                    "sid": 143,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the evaluation of the word segmentation ac\u00ad curacy, we randomly selected 5 thousand sentences from the test set of 100 thousand sentences.",
                    "sid": 144,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We call the first test set (100 thousand sentences) \"test set-1\" and the second test set (5 thousand sentences) \"test set-2\".",
                    "sid": 145,
                    "ssid": 75,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 4 shows the number of sentences, words, and characters of the training and test sets.",
                    "sid": 146,
                    "ssid": 76,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There were 94,680 distinct words in the training test.",
                    "sid": 147,
                    "ssid": 77,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We discarded the words whose frequency was one, and made a dictionary of 45,027 words.",
                    "sid": 148,
                    "ssid": 78,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Af\u00ad ter replacing the words whose frequency was one with the corresponding unknown word tags, there were 474,155 distinct word bigrams.",
                    "sid": 149,
                    "ssid": 79,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We discarded the bigrams with frequency one, and the remaining 175,527 bigrams were used in the word segmentation model.",
                    "sid": 150,
                    "ssid": 80,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As for the unknown word model, word-based char\u00ad acter bigrams are computed from the words with Table 5: Cross entropy (CE) per word and character perplexity (PP) of each unknown word model Part of Speech Estimation Accuracy 0.95 0.9 frequency one (49,653 words).",
                    "sid": 151,
                    "ssid": 81,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There were 3,120 dis\u00ad tinct character unigrams and 55,486 distinct char\u00ad acter bigrams.",
                    "sid": 152,
                    "ssid": 82,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We discarded the bigram with fre\u00ad quency one and remaining 20,775 bigrams were used.",
                    "sid": 153,
                    "ssid": 83,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There were 12,633 distinct character unigrams and 80,058 distinct character bigrams when we classified them for each word type and part of speech.",
                    "sid": 154,
                    "ssid": 84,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We discarded the bigrams with frequency one and re\u00ad 0.85 0.7 POS + WT + Poisson + bigram -+\u00ad POS + Poisson + bigram -+--\u00b7 maining 26,633 bigrams were used in the unknown word model.",
                    "sid": 155,
                    "ssid": 85,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Average word lengths for each word type and part of speech were also computed from the words with frequency one in the training set.",
                    "sid": 156,
                    "ssid": 86,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.2 Cross Entropy and Perplexity.",
                    "sid": 157,
                    "ssid": 87,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 5 shows the cross entropy per word and char\u00ad acter perplexity of three unknown word model.",
                    "sid": 158,
                    "ssid": 88,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first model is Equation (5), which is the combina . tion of Poisson distribution and character zerogram (Poisson + zerogram).",
                    "sid": 159,
                    "ssid": 89,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second model is the combination of Poisson distribution (Equation (6)) and character bigram (Equation (7)) (Poisson + hi\u00ad gram).",
                    "sid": 160,
                    "ssid": 90,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The third model is Equation (11), which is a set of word models trained for each word type (WT +Poisson+ bigram).",
                    "sid": 161,
                    "ssid": 91,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Cross entropy was computed over the words in test set-1 that were not found in the dictionary of the word segmentation model (56,121 words).",
                    "sid": 162,
                    "ssid": 92,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Character perplexity is more intu\u00ad itive than cross entropy because it shows the average number of equally probable characters out of 6,879 characters in JIS-X-0208.",
                    "sid": 163,
                    "ssid": 93,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 5 shows that by changing the word spelling model from zerogram to bigram, character perplex\u00ad ity is greatly reduced.",
                    "sid": 164,
                    "ssid": 94,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It also shows that by making a separate model for each word type, character per\u00ad plexity is reduced by an additional 45% (128 --+ 71).",
                    "sid": 165,
                    "ssid": 95,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This shows that the word type information is useful for modeling the morphology of Japanese words.",
                    "sid": 166,
                    "ssid": 96,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.3 Part of Speech Prediction Accuracy.",
                    "sid": 167,
                    "ssid": 97,
                    "kind_of_tag": "s"
                },
                {
                    "text": "without Context Figure 3 shows the part of speech prediction accu\u00ad racy of two unknown word model without context.",
                    "sid": 168,
                    "ssid": 98,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It shows the accuracies up to the top 10 candidates.",
                    "sid": 169,
                    "ssid": 99,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The first model is Equation (12), which is a set of word models trained for each part of speech (POS + Poisson + bigram).",
                    "sid": 170,
                    "ssid": 100,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The second model is Equa\u00ad tion (13), which is a set of word models trained for o.s51 ---:23 _.-J4L----'-5--6.L...----'7----'-8--9'-- J10 Rank Figure 3: Accuracy of part of speech estimation each part of speech and word type (POS + WT + Poisson + bigram).",
                    "sid": 171,
                    "ssid": 101,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The test words are the same 56,121 words used to compute the cross entropy.",
                    "sid": 172,
                    "ssid": 102,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since these unknown word models give the prob\u00ad ability of spelling for each part of speech P(wit), we used the empirical part of speech probability P(t) to compute the joint probability P(w, t).",
                    "sid": 173,
                    "ssid": 103,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The part of speech t that gives the highest joint probability is selected.",
                    "sid": 174,
                    "ssid": 104,
                    "kind_of_tag": "s"
                },
                {
                    "text": "i = argmp.xP(w, t) = P(t)P(wit) (18) The part of speech prediction accuracy of the first and the second model was 67.5% and 74.4%, respec\u00ad tively.",
                    "sid": 175,
                    "ssid": 105,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As Figure 3 shows, word type information improves the prediction accuracy significantly.",
                    "sid": 176,
                    "ssid": 106,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.4 Word Segmentation Accuracy.",
                    "sid": 177,
                    "ssid": 107,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).",
                    "sid": 178,
                    "ssid": 108,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let the number of words in the manually segmented corpus be Std, the number of words in the output of the word segmenter be Sys, and the number of matched words be M. Recall is defined as M/Std, and precision is defined as M/Sys.",
                    "sid": 179,
                    "ssid": 109,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since it is inconvenient to use both recall and precision all the time, we also use the F-measure to indicate the overall performance.",
                    "sid": 180,
                    "ssid": 110,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is calculated by F = (!3 2 + 1.0) X p X R (32 X p + R (19) where Pis precision, R is recall, and (3 is the relative importance given to recall over precision.",
                    "sid": 181,
                    "ssid": 111,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We set Table 6: Word segmentation accuracy of all words r e c pr ec F Po iss on +b igr a m W T +P oi ss on +b igr a m P O S + Po iss on +b igr a m P O S + W T + Po iss on + bi gr a m 94 .5 94 .4 94 .4 94 .6 93 .1 93 .8 93 .6 93 .7 93 .8 94 .1 94 .0 94 .1 Table 7: Word segmentation accuracy of unknown words r e c pr ec F Po iss on + bi gr a m W T + P oi ss o n + b i g r a m P O S + P o is s o n + b i g r a m P O S + W T + P o is s o n + bi g ra m 31 .8 45 .5 39 .7 42.",
                    "sid": 182,
                    "ssid": 112,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 65 .0 62 .0 61 .5 66 .4 42.",
                    "sid": 183,
                    "ssid": 113,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7 52 .5 48 .3 51.",
                    "sid": 184,
                    "ssid": 114,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4 f3 = 1.0 throughout this experiment.",
                    "sid": 185,
                    "ssid": 115,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, we put equal importance on recall and precision.",
                    "sid": 186,
                    "ssid": 116,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 6 shows the word segmentation accuracy of four unknown word models over test set-2.",
                    "sid": 187,
                    "ssid": 117,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Com\u00ad pared to the baseline model (Poisson+ bigram), by using word type and part of speech information, the precision of the proposed model (POS + WT + Pois\u00ad son + bigram) is improved by a modest 0.6%.",
                    "sid": 188,
                    "ssid": 118,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The impact of the proposed model is small because the out-of-vocabulary rate of test set-2 is only 3.1%.",
                    "sid": 189,
                    "ssid": 119,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To closely investigate the effect of the proposed unknown word model, we computed the word seg\u00ad mentation accuracy of unknown words.",
                    "sid": 190,
                    "ssid": 120,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 7 shows the results.",
                    "sid": 191,
                    "ssid": 121,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The accuracy of the proposed model (POS + WT + Poisson + bigram) is signif\u00ad icantly higher than the baseline model (Poisson + bigram).",
                    "sid": 192,
                    "ssid": 122,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recall is improved from 31.8% to 42.0% and precision is improved from 65.0% to 66.4%.",
                    "sid": 193,
                    "ssid": 123,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here, recall is the percentage of correctly seg\u00ad mented unknown words in the system output to the all unknown words in the test sentences.",
                    "sid": 194,
                    "ssid": 124,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Precision is the percentage of correctly segmented unknown words in the system's output to the all words that system identified as unknown words.",
                    "sid": 195,
                    "ssid": 125,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 8 shows the tagging accuracy of unknown words.",
                    "sid": 196,
                    "ssid": 126,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Notice that the baseline model (Poisson + bigram) cannot predict part of speech.",
                    "sid": 197,
                    "ssid": 127,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To roughly estimate the amount of improvement brought by the proposed model, we applied a simple tagging strat\u00ad egy to the output of the baseline model.",
                    "sid": 198,
                    "ssid": 128,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, words that include numbers are tagged as numbers, and others are tagged as nouns.",
                    "sid": 199,
                    "ssid": 129,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 8 shows that by using word type and part of speech information, recall is improved from 28.1% to 40.6% and precision is improved from 57.3% to 64.1%.",
                    "sid": 200,
                    "ssid": 130,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other than the usual recall/precision measures, we defined another precision (prec2 in Table 8), which roughly correspond to the tagging accuracy in English where word segmentation is trivial.",
                    "sid": 201,
                    "ssid": 131,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Prec2 is defined as the percentage of correctly tagged un\u00ad known words to the correctly segmented unknown words.",
                    "sid": 202,
                    "ssid": 132,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 8 shows that tagging precision is im\u00ad proved from 88.2% to 96.6%.",
                    "sid": 203,
                    "ssid": 133,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The tagging accuracy in context (96.6%) is significantly higher than that without context (74.4%).",
                    "sid": 204,
                    "ssid": 134,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This shows that the word bigrams using unknown word tags for each part of speech are useful to predict the part of speech.",
                    "sid": 205,
                    "ssid": 135,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "related work. ",
            "number": "5",
            "sents": [
                {
                    "text": "Since English uses spaces between words, unknown words can be identified by simple dictionary lookup.",
                    "sid": 206,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "So the topic of interest is part of speech estimation.",
                    "sid": 207,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some statistical model to estimate the part of speech of unknown words from the case of the first letter and the prefix and suffix is proposed (Weischedel et al., 1993; Brill, 1995; Ratnaparkhi, 1996; Mikheev, 1997).",
                    "sid": 208,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the contrary, since Asian languages like Japanese and Chinese do not put spaces between words, previous work on unknown word problem is focused on word segmentation; there are few studies estimating part of speech of unknown words in Asian languages.",
                    "sid": 209,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The cues used for estimating the part of speech of unknown words for Japanese in this paper are ba\u00ad sically the same for English, namely, the prefix and suffix of the unknown word as well as the previous and following part of speech.",
                    "sid": 210,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The contribution of this paper is in showing the fact that different char\u00ad acter sets behave differently in Japanese and a better word model can be made by using this fact.",
                    "sid": 211,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By introducing different length models based on character sets, the number of decomposition errors of unknown words are significantly reduced.",
                    "sid": 212,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In other words, the tendency of over-segmentation is cor\u00ad rected.",
                    "sid": 213,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the spelling model, especially the character bigrams in Equation (17) are hard to es\u00ad timate because of the data sparseness.",
                    "sid": 214,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is the main reason of the remaining under-segmented and over-segmented errors.",
                    "sid": 215,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To improve the unknown word model, feature\u00ad based approach such as the maximum entropy method (Ratnaparkhi, 1996) might be useful, be\u00ad cause we don't have to divide the training data into several disjoint sets (like we did by part of speech and word type) and we can incorporate more lin\u00ad guistic and morphological knowledge into the same probabilistic framework.",
                    "sid": 216,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We are thinking of re\u00ad implementing our unknown word model using the maximum entropy method as the next step of our research.",
                    "sid": 217,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 8: Part of speech tagging accuracy of unknown words (the last column represents the percentage of correctly tagged unknown words in the correctly segmented unknown words) r e c pr ec F pr ec 2 Po iss on +b igr a m W T + P o is s o n + b i g r a m P O S + P o is s o n + b i g r a m P O S + W T + P o is s o n + b i g r a m 28 .1 37 .7 37 .5 40 .6 57 .3 51 .5 58 .1 64 .1 37 .7 43 .5 45 .6 49 .7 8 8 . 2 8 7 . 9 9 4 . 3 9 6 . 6",
                    "sid": 218,
                    "ssid": 13,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusion. ",
            "number": "6",
            "sents": [
                {
                    "text": "We present a statistical model of Japanese unknown words using word morphology and word context.",
                    "sid": 219,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We find that Japanese words are better modeled by clas\u00ad sifying words based on the character sets (kanji, hi\u00ad ragana, katakana, etc.) and its changes.",
                    "sid": 220,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is because the different character sets behave differ\u00ad ently in many ways (historical etymology, ideogram vs. phonogram, etc.).",
                    "sid": 221,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both word segmentation ac\u00ad curacy and part of speech tagging accuracy are im\u00ad proved by treating them differently.",
                    "sid": 222,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}