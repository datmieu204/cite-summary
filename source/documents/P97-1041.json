{
    "ID": "P97-1041",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "This paper presents a trainable rule-based algorithm for performing word segmen\u00ad tation.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The algorithm provides a sim\u00ad ple, language-independent alternative to large-scale lexical-based segmenters requir\u00ad ing large amounts of knowledge engineer\u00ad ing.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a stand-alone segmenter, we show our algorithm to produce high performance Chinese segmentation.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, we show the transformation-based algorithm to be effective in improving the output of several existing word segmentation algo\u00ad rithms in three different languages.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "This paper presents a trainable rule-based algorithm for performing word segmentation.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our algorithm is effective both as a high-accuracy stand-alone seg\u00ad menter and as a postprocessor that improves the output of existing word segmentation algorithms.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the writing systems of many languages, includ\u00ad ing Chinese, Japanese, and Thai, words are not de\u00ad limited by spaces.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Determining the word bound\u00ad aries, thus tokenizing the text, is usually one of the first necessary processing steps, making tasks such as part-of-speech tagging and parsing possible.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A vari\u00ad ety of methods have recently been developed to per\u00ad form word segmentation and the results have been published widely.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 A major difficulty in evaluating segmentation al\u00ad gorithms is that there are no widely-accepted guide\u00ad lines as to what constitutes a word, and there is therefore no agreement on how to \"correctly\" seg\u00ad ment a text in an unsegmented language.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is 1Most published segmentation work has been done for Chinese.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "frequently mentioned in segmentation papers that native speakers of a language do not always agree about the \"correct\" segmentation and that the same text could be segmented into several very different (and equally correct) sets of words by different na\u00ad tive speakers.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sproat et al.(l996) and Wu and Fung (1994) give empirical results showing that an agree\u00ad ment rate between native speakers as low as 75% is common.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consequently, an algorithm which scores extremely well compared to one native segmentation may score dismally compared to other, equally \"cor\u00ad rect\" segmentations.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will discuss some other is\u00ad sues in evaluating word segmentation in Section 3.1.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One solution to the problem of multiple correct segmentations might be to establish specific guide\u00ad lines for what is and is not a word in unsegmented languages.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given these guidelines, all corpora could theoretically be uniformly segmented according to the same conventions, and we could directly compare existing methods on the same corpora.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While this approach has been successful in driving progress in NLP tasks such as part-of-speech tagging and pars\u00ad ing, there are valid arguments against adopting it for word segmentation.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, since word seg\u00ad mentation is merely a preprocessing task for a wide variety of further tasks such as parsing, information extraction, and information retrieval, different seg\u00ad mentations can be useful or even essential for the different tasks.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this sense, word segmentation is similar to speech recognition, in which a system must be robust enough to adapt to and recognize the mul\u00ad tiple speaker-dependent \"correct\" pronunciations of words.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In some cases, it may also be necessary to allow multiple \"correct\" segmentations of the same text, depending on the requirements of further pro\u00ad cessing steps.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, many algorithms use exten\u00ad sive domain-specific word lists and intricate name recognition routines as well as hard-coded morpho\u00ad logical analysis modules to produce a predetermined segmentation output.",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Modifying or retargeting an existing segmentation algorithm to produce a differ\u00ad ent segmentation can be difficult, especially if it is not clear what and where the systematic differences in segmentation are.",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is widely reported in word segmentation papers,2 that the greatest barrier to accurate word segmentation is in recognizing words that are not in the lexicon of the segmenter.",
                    "sid": 25,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Such a problem is de\u00ad pendent both on the source of the lexicon as well as the correspondence (in vocabulary) between the text in question and the lexicon.",
                    "sid": 26,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Wu and Fung (1994) demonstrate that segmentation accuracy is signifi\u00ad cantly higher when the lexicon is constructed using the same type of corpus as the corpus on which it is tested.",
                    "sid": 27,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We argue that rather than attempting to construct a single exhaustive lexicon or even a series of domain-specific lexica, it is more practical to de\u00ad velop a robust trainable means of compensating for lexicon inadequacies.",
                    "sid": 28,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, developing such an algorithm will allow us to perform segmentation in many different languages without requiring ex\u00ad tensive morphological resources and domain-specific lexica in any single language.",
                    "sid": 29,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For these reasons, we address the problem of word segmentation from a different direction.",
                    "sid": 30,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We intro\u00ad duce a rule-based algorithm which can produce an accurate segmentation of a text, given a rudimentary initial approximation to the segmentation.",
                    "sid": 31,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recog\u00ad nizing the utility of multiple correct segmentations of the same text, our algorithm also allows the output of a wide variety of existing segmentation algorithms to be adapted to different segmentation schemes.",
                    "sid": 32,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, our rule-based algorithm can also be used to supplement the segmentation of an existing al\u00ad gorithm in order to compensate for an incomplete lexicon.",
                    "sid": 33,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our algorithm is trainable and language in\u00ad dependent, so it can be used with any unsegmented l .nguage.",
                    "sid": 34,
                    "ssid": 34,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "transformation-based. ",
            "number": "2",
            "sents": [
                {
                    "text": "Segmentation The key component of our trainable segmenta\u00ad tion algorithm is Transformation-based Error-driven Learning, the corpus-based language processing method introduced by Brill (1993a).",
                    "sid": 35,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This technique provides a simple algorithm for learning a sequence of rules that can be applied to various NLP tasks.",
                    "sid": 36,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It differs from other common corpus-based methods in several ways.",
                    "sid": 37,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For one, it is weakly statistical, but not probabilistic; transformation-based approaches conseop11tly require far less training data than most \" e .i.i:;tical approaches.",
                    "sid": 38,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is rule-based, but relies on 2 See, for example, Sproat et al.",
                    "sid": 39,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1996)..",
                    "sid": 40,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "machine learning to acquire the rules, rather than expensive manual knowledge engineering.",
                    "sid": 41,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The rules produced can be inspected, which is useful for gain\u00ad ing insight into the nature of the rule sequence and for manual improvement and debugging of the se\u00ad quence.",
                    "sid": 42,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The learning algorithm also considers the entire training set at all learning steps, rather than decreasing the size of the training data as learning progresses, such as is the case in decision-tree in\u00ad duction (Quinlan, 1986).",
                    "sid": 43,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For a thorough discussion of transformation-based learning, see Ramshaw and Marcus (1996).",
                    "sid": 44,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Brill's work provides a proof of viability of transformation-based techniques in the form of a number of processors, including a (widely\u00ad distributed) part-of-speech tagger (Brill, 1994), a procedure for prepositional phrase attachment (Brill and Resnik, 1994), and a bracketing parser (Brill, 1993b).",
                    "sid": 45,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All of these provided performance comparable to or better than previous attempts.",
                    "sid": 46,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Transformation-based learning has also been suc\u00ad cessfully applied to text chunking (Ramshaw and Marcus, 1995), morphological disambiguation (Oflazer and Tur, 1996), and phrase parsing (Vilain and Day, 1996).",
                    "sid": 47,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.1 Training.",
                    "sid": 48,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Word segmentation can easily be cast as a transformation-based problem, which requires an initial model, a goal state into which we wish to transform the initial model (the \"gold standard\"), and a series of transformations to effect this improve\u00ad ment.",
                    "sid": 49,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The transformation-based algorithm involves applying and scoring all the possible rules to train\u00ad ing data and determining which rule improves the model the most.",
                    "sid": 50,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This rule is then applied to all ap\u00ad plicable sentences, and the process is repeated until no rule improves the score of the training data.",
                    "sid": 51,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this manner a sequence of rules is built for iteratively improving the initial model.",
                    "sid": 52,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Evaluation of the rule sequence is carried out on a test set of data which is independent of the training data.",
                    "sid": 53,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If we treat the output of an existing segmentation algorithm3 as the initial state and the desired seg\u00ad mentation as the goal state, we can perform a series of transformations on the initial state - removing ex\u00ad traneous boundaries and inserting new boundaries - to obtain a more accurate approximation of the goal state.",
                    "sid": 54,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We therefore need only define an appropriate rule syntax for transforming this initial approxima 3 The \"existing\" algorithm does not need to be a large or even accurate system; the algorithm can be arbi\u00ad trarily simple as long as it assigns some form of initial segmentation.",
                    "sid": 55,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "tion and prepare appropriate training data.",
                    "sid": 56,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our experiments, we obtained corpora which had been manually segmented by native or near\u00ad native speakers of Chinese and Thai.",
                    "sid": 57,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We divided the hand-segmented data randomly into training and test sets.",
                    "sid": 58,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Roughly 80% of the data was used to train the segmentation algorithm, and 20% was used as a blind test set to score the rules learned from the training data.",
                    "sid": 59,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to Chinese and Thai, we also performed segmentation experiments using a large corpus of English in which all the spaces had been removed from the texts.",
                    "sid": 60,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most of our English experiments were performed using training and test sets with roughly the same 8020 ratio, but in Sec\u00ad tion 3.4.3 we discuss results of English experiments with different amounts of training data.",
                    "sid": 61,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unfortu\u00ad nately, we could not repeat these experiments with Chinese and Thai due to the small amount of hand\u00ad segmented data available.",
                    "sid": 62,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.2 Rule syntax.",
                    "sid": 63,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are three main types of transformations which can act on the current state of an imperfect segmen\u00ad tation: \u2022 Insert - place a new boundary between two char\u00ad acters 3.1 Evaluation of segmentation.",
                    "sid": 64,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Despite the number of papers on the topic, the eval\u00ad uation and comparison of existing segmentation al\u00ad gorithms is virtually impossible.",
                    "sid": 65,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to the problem of multiple correct segmentations of the same texts, the comparison of algorithms is diffi\u00ad cult because of the lack of a single metric for re\u00ad porting scores.",
                    "sid": 66,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Two common measures of perfor\u00ad mance are recall and precision, where recall is de\u00ad fined as the percent of words in the hand-segmented text identified by the segmentation algorithm, and precision is defined as the percentage of words re\u00ad turned by the algorithm that also occurred in the hand-segmented text in the same position.",
                    "sid": 67,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The com\u00ad ponent recall and precision scores are then used to calculate an F-measure (Rijsbergen, 1979), where F = (1 + f3)P R/(/3P + R).",
                    "sid": 68,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper we will report all scores as a balanced F-measure (precision and recall weighted equally) with /3 = 1, such that F = 2PR/(P+ R) 3.2 Chinese.",
                    "sid": 69,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our Chinese experiments, the training set con\u00ad sisted of 2000 sentences (60187 words) from a Xin\u00ad hua news agency corpus; the test set was a separate set of 560 sentences (18783 words) from the same 5 \u2022 Delete - remove an existing boundary between corpus.",
                    "sid": 70,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We ran four experiments using this corpus, two characters \u2022 Slide- move an existing boundary from its cur\u00ad rent location between two characters to a loca\u00ad tion 1, 2, or 3 characters to the left or right4 In our syntax, Insert and Delete transformations can be triggered by any two adjacent characters (a bigram) and one character to the left or right of the bigram.",
                    "sid": 71,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Slide transformations can be triggered by a sequence of one, two, or three characters over which the boundary is to be moved.",
                    "sid": 72,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1 enumerates the 22 segmentation transformations we define.",
                    "sid": 73,
                    "ssid": 39,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "results. ",
            "number": "3",
            "sents": [
                {
                    "text": "With the above algorithm in place, we can use the training data to produce a rule sequence to augment an initial segmentation approximation in order to obtain a better approximation of the desired segmen\u00ad tation.",
                    "sid": 74,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Furthermore, since all the rules are purely character-based, a sequence can be learned for any character set and thus any language.",
                    "sid": 75,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used our rule-based algorithm to improve the word segmen\u00ad tation rate for several segmentation algorithms in three languages.",
                    "sid": 76,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022Note that a Slide transformation is equivalent to a Delete plus an Insert.",
                    "sid": 77,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "with four different algorithms providing the starting point for the learning of the segmentation transfor\u00ad mations.",
                    "sid": 78,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In each case, the rule sequence learned from the training set resulted in a significant im\u00ad provement in the segmentation of the test set.",
                    "sid": 79,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2.1 Character-as-word (CAW) A very simple initial segmentation for Chinese is to consider each character a distinct word.",
                    "sid": 80,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the average word length is quite short in Chinese, with most words containing only 1 or 2 characters,6 this character-as-word segmentation correctly iden\u00ad tified many one-character words and produced an initial segmentation score of F=40.3.",
                    "sid": 81,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While this is a low segmentation score, this segmentation algo\u00ad rithm identifies enough words to provide a reason\u00ad able initial segmentation approximation.",
                    "sid": 82,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In fact, the CAW algorithm alone has been shown (Buckley et al., 1996; Broglio et al., 1996) to be adequate to be used successfully in Chinese information retrieval.",
                    "sid": 83,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our algorithm learned 5903 transformations from the 2000 sentence training set.",
                    "sid": 84,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The 5903 transfor\u00ad mations applied to the test set improved the score from F=40.3 to 78.1, a 63.3% reduction in the error 5The Chinese texts were prepared by Tom Keenan.",
                    "sid": 85,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6 The average length of a word in our Chinese data was 1.60 characters.",
                    "sid": 86,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "R u l e Bo un da ry Ac tio n Tri gg eri ng Co nte xt A B < = > A B Ins ert (de let e) bet we en A an d B an y x B < = > X B Ins ert (de let e) bef ore an y B an y A y < = > A y Ins ert (de let e) aft er an y A an y A B C < = > A B c Ins ert (de let e) bet we en A an d B A N D Ins ert (de let e) bet we en Ba nd C an y J A B < = > J A B Ins ert (de let e) bet we en A an d B J to left of A . J A B < = > . J A B Ins ert (de let e) bet we en A an d B no J to lef t of A A B K < = > A B K Ins ert (de let e) bet we en A an d B K to rig ht of B A B . K <=> A B-.K Ins ert (de let e) bet we en A an d B no K to rig ht of B x A y < = > X A y M ov e fro m aft er A to be for e A an y x A B y <=> X ABy M ov e fro m aft er bi gr a m A B to be for e A B an y xA B C y {:::::} X ABCy M ov e fro m aft er tri gr a m A B C to be for e A B C an y Figure 1: Possible transformations.",
                    "sid": 87,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A, B, C, J, and K are specific characters; x and y can be any character.",
                    "sid": 88,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "-,J and -.K can be any character except J and K, respectively.",
                    "sid": 89,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "rate.",
                    "sid": 90,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is a very surprising and encouraging re\u00ad sult, in that, from a very naive initial approximation using no lexicon except that implicit from the train\u00ad ing data, our rule-based algorithm is able to produce a series of transformations with a high segmentation accuracy.",
                    "sid": 91,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2.2 Maximum matching (greedy) algorithm A common approach to word segmentation is to use a variation of the maximum matching algorithm, frequently referred to as the \"greedy algorithm.\"",
                    "sid": 92,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The greedy algorithm starts at the first character in a text and, using a word list for the language be\u00ad ing segmented, attempts to find the longest word in the list starting with that character.",
                    "sid": 93,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If a word is found, the maximum-matching algorithm marks a boundary at the end of the longest word, then be\u00ad gins the same longest match search starting at the character following the match.",
                    "sid": 94,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If no match is found in the word list, the greedy algorithm simply skips that character and begins the search starting at the next character.",
                    "sid": 95,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this manner, an initial segmen\u00ad tation can be obtained that is more informed than a simple character-as-word approach.",
                    "sid": 96,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We applied the maximum matching algorithm to the test set using a list of 57472 Chinese words from the NMSU CHSEG segmenter (described in the next section).",
                    "sid": 97,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This greedy algorithm produced an initial score of F=64.4.",
                    "sid": 98,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A sequence of 2897 transformations was learned \u00b7 from the training set; applied to the test set, they improved the score from F=64.4 to 84.9, a 57.8% error reduction.",
                    "sid": 99,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "From a simple Chinese word list, the rule-based algorithm was thus able to produce a. segmentation score comparable to segmentation al\u00ad gorithms developed with a large amount of domain knowledge (as we will see in the next section).",
                    "sid": 100,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This score was improved further when combin\u00ad ing the character-as-word (CAW) and the maximum matching algorithms.",
                    "sid": 101,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the maximum matching al\u00ad gorithm described above, when a sequence of char\u00ad acters occurred in the text, and no subset of the sequence was present in the word list, the entire sequence was treated as a single word.",
                    "sid": 102,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This of\u00ad ten resulted in words containing 10 or more char\u00ad acters, which is very unlikely in Chinese.",
                    "sid": 103,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this experiment, when such a sequence of characters was encountered, each of the characters was treated as a separate word, as in the CAW algorithm above.",
                    "sid": 104,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This variation of the greedy algorithm, using the same list of 57472 words, produced an initial score of F=82.9.",
                    "sid": 105,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A sequence of 2450 transformations was learned from the training set; applied to the test set, they improved the score from F=82.9 to 87.7, a 28.1% error reduction.",
                    "sid": 106,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The score produced using this variation of the maximum matching algorithm combined with a rule sequence (87.7) is nearly equal to the score produced by the NMSU segmenter seg\u00ad menter (87.9) discussed in the next section.",
                    "sid": 107,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2.3 NMSU segmenter The previous three experiments showed that our rule sequence algorithm can produce excellent seg\u00ad mentation results given very simple initial segmen\u00ad tation algorithms.",
                    "sid": 108,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, assisting in the adapta\u00ad tion of an existing algorithm to different segmenta\u00ad tion schemes, as discussed in Section 1, would most likely be performed with an already accurate, fully\u00ad developed algorithm.",
                    "sid": 109,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this experiment we demon strate that our algorithm can also improve the out\u00ad put of such a system.",
                    "sid": 110,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Chinese segmenter CHSEG developed at the Computing Research Laboratory at New Mexico State University is a complete system for high\u00ad accuracy Chinese segmentation (Jin, 1994).",
                    "sid": 111,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In ad\u00ad dition to an initial segmentation module that finds words in a text based on a list of Chinese words, CHSEG additionally contains specific modules for recognizing idiomatic expressions, derived words, Chinese person names, and foreign proper names.",
                    "sid": 112,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The accuracy of CHSEG on an 8.6MB corpus has been independently reported as F=84.0 (Ponte and Croft, 1996).",
                    "sid": 113,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(For reference, Ponte and Croft re\u00ad port scores of F=86.1 and 83.6 for their probabilis\u00ad tic Chinese segmentation algorithms trained on over 100MB of data.)",
                    "sid": 114,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On our test set, CHSEG produced a segmentation score of F=87.9.",
                    "sid": 115,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our rule-based algorithm learned a sequence of 1755 transformations from the training set; applied to the test set, they improved the score from 87.9 to 89.6, a 14.0% reduction in the error rate.",
                    "sid": 116,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our rule-based algorithm is thus able to produce an improvement to an existing high-performance sys\u00ad tem.",
                    "sid": 117,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 1 shows a summary of the four Chinese ex\u00ad periments.",
                    "sid": 118,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.3 Thai.",
                    "sid": 119,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While Thai is also an unsegmented language, the Thai writing system is alphabetic and the average word length is greater than Chinese.7 We would therefore expect that our character-based transfor\u00ad mations would not work as well with Thai, since a context of more than one character is necessary in many cases to make many segmentation decisions in alphabetic languages.",
                    "sid": 120,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Thai corpus consisted of texts8 from the Thai News Agency via NECTEC in Thailand.",
                    "sid": 121,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our experiment, the training set consisted of 3367 sen\u00ad tences ( 40937 words); the test set was a separate set of 1245 sentences (13724 words) from the same corpus.",
                    "sid": 122,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The initial segmentation was performed using the maximum matching algorithm, with a lexicon of 9933 Thai words from the word separation filter.",
                    "sid": 123,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "in Cttex,\u00b7 a Thai language Latex package.",
                    "sid": 124,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This greedy algorithm gave an initial segmentation score of F=48.2 on the test set.",
                    "sid": 125,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7 The average length of a word in our Thai data was.",
                    "sid": 126,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.01 characters.",
                    "sid": 127,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "8The Thai texts were manually segmented by Jo Tyler.",
                    "sid": 128,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our rule-based algorithm learned a sequence of 731 transformations which improved the score from 48.2 to 63.6, a 29.7% error reduction.",
                    "sid": 129,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While the alphabetic system is obviously harder to segment, we still see a significant reduction in the segmenter error rate using the transformation-based algorithm.",
                    "sid": 130,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Nevertheless, it is doubtful that a segmentation with a score of 63.6 would be useful in too many appli\u00ad cations, and this result will need to be significantly improved.",
                    "sid": 131,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.4 De-segmented English.",
                    "sid": 132,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although English is not an unsegmented language, the writing system is alphabetic like Thai and the average word length is similar.9 Since English lan\u00ad guage resources (e.g. word lists and morphological analyzers) are more readily available, it is instruc\u00ad tive to experiment with a de-segmented English cor\u00ad pus, that is, English texts in which the spaces have been removed and word boundaries are not explic\u00ad itly indicated.",
                    "sid": 133,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The following shows an example of an English sentence and its de-segmented version: About 20,000 years ago the last ice age ended.",
                    "sid": 134,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "About20,000yearsagothelasticeageended.",
                    "sid": 135,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results of such experiments can help us deter\u00ad mine which resources need to be compiled in order to develop a high-accuracy segmentation algorithm in unsegmented alphabetic languages such as Thai.",
                    "sid": 136,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, we are also able to provide a more detailed error analysis of the English segmentation (since the author can read English but not Thai).",
                    "sid": 137,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our English experiments were performed using a corpus of texts from the Wall Street Journal (WSJ).",
                    "sid": 138,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The training set consisted of 2675 sentences (64632 words) in which all the spaces had been removed; the test set was a separate set of 700 sentences (16318 words) from the same corpus (also with all spaces removed).",
                    "sid": 139,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.4.1 Maximum matching experiment For an initial experiment, segmentation was per\u00ad formed using the maximum matching algorithm, with a large lexicon of 34272 English words com\u00ad piled from the WSJ .10 In contrast to the low initial Thai score, the greedy algorithm gave an initial En\u00ad glish segmentation score of F=73.2.",
                    "sid": 140,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our rule-based algorithm learned a sequence of800 transformations, 9 The average length of a word in our English data was 4.46.",
                    "sid": 141,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "characters, compared to 5.01 for Thai and 1.60 for Chinese.",
                    "sid": 142,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "10 Note that the portion of the WSJ corpus used to compile the word list was independent of both the train\u00ad ing and test sets used in the segmentation experiments.",
                    "sid": 143,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ini tia l al go rit h m Ini tia l sc or e Ru les lea rn ed Im pr ov ed sc or e Er ro r re du cti on Ch ar acteras w or d M ax im u m m atc hi ng M ax im u m m atc hi ng + C A W N M SU se g m en ter 40 .3 64 .4 82 .9 87 .9 59 03 28 97 24 50 17 55 78 .1 84 .9 87 .7 89 .6 63 .3 % 57 .8 % 28 .1 % 14 .0 % Table 1: Chinese results.",
                    "sid": 144,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "which improved the score from 73.2 to 79.0, a 21.6% error reduction.",
                    "sid": 145,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The difference in the greedy scores for English and Thai demonstrates the dependence on the word list in the greedy algorithm.",
                    "sid": 146,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, an exper\u00ad iment in which we randomly removed half of the words from the English list reduced the performance of the greedy algorithm from 73.2 to 32.3; although this reduced English word list was nearly twice the size of the Thai word list (17136 vs. 9939), the longest match segmentation utilizing the list was much lower (32.3 vs. 48.2).",
                    "sid": 147,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Successive experiments in which we removed different random sets of half the words from the original list resulted in greedy algorithm performance of 39.2, 35.1, and 35.5.",
                    "sid": 148,
                    "ssid": 75,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Yet, despite the disparity in initial segmentation scores, the transformation sequences effect a significant er\u00ad ror reduction in all cases, which indicates that the transformation sequences are effectively able to com\u00ad pensate (to some extent) for weaknesses in the lexi\u00ad con.",
                    "sid": 149,
                    "ssid": 76,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 provides a summary of the results using the greedy algorithm for each of the three languages.",
                    "sid": 150,
                    "ssid": 77,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.4.2 Basic morphological segmentation experiment As mentioned above, lexical resources are more readily available for English than for Thai.",
                    "sid": 151,
                    "ssid": 78,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can use these resources to provide an informed ini\u00ad tial segmentation approximation separate from the greedy algorithm.",
                    "sid": 152,
                    "ssid": 79,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using our native knowledge of English as well as a short list of common English prefixes and suffixes, we developed a simple al\u00ad gorithm for initial segmentation of English which placed boundaries after any of the suffixes and before any of the prefixes, as well as segmenting punctua\u00ad tion characters.",
                    "sid": 153,
                    "ssid": 80,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In most cases, this simple approach was able to locate only one of the two necessary boundaries for recognizing full words, and the ini\u00ad tial score was understandably low, F=29.8.",
                    "sid": 154,
                    "ssid": 81,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Never\u00ad theless, even from this flawed initial approximation, our rule-based algorithm learned a sequence of 632 transformations which nearly doubled the word re\u00ad call, improving the score from 29.8 to 53.3, a 33.5% error reduction.",
                    "sid": 155,
                    "ssid": 82,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.4.3 Amount of training data Since we had a large amount of English data, we also performed a classic experiment to determine the effect the amount of training data had on the abil\u00ad ity of the rule sequences to improve segmentation.",
                    "sid": 156,
                    "ssid": 83,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We started with a training set only slightly larger than the test set, 872 sentences, and repeated the maximum matching experiment described in Section 3.4.1.",
                    "sid": 157,
                    "ssid": 84,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then incrementally increased the amount of training data and repeated the experiment.",
                    "sid": 158,
                    "ssid": 85,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results, summarized in Table 3, clearly indicate (not surprisingly) that more training sentences produce both a longer rule sequence and a larger error re\u00ad duction in the test data.",
                    "sid": 159,
                    "ssid": 86,
                    "kind_of_tag": "s"
                },
                {
                    "text": "T ra in in g se nte nc es R u l e s le ar ne d Im pr ov ed s c o r e E r r o r re du cti on 8 7 2 1 7 3 1 2 6 7 5 3 5 7 2 4 5 2 2 4 3 6 6 5 3 8 0 0 9 0 2 1 0 1 5 7 8 . 2 7 8 . 9 7 9 . 0 7 9 . 4 8 0 . 3 1 8 . 9 % 2 1 . 3 % 2 1 . 6 % 2 3 . 1 % 2 6 . 5 % Table 3: English training set sizes.",
                    "sid": 160,
                    "ssid": 87,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Initial score of test data (700 sentences) was 73.2.",
                    "sid": 161,
                    "ssid": 88,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.4.4 Error analysis Upon inspection of the English segmentation er\u00ad rors produced by both the maximum matching algo\u00ad rithm and the learned transformation sequences, one major category of errors became clear.",
                    "sid": 162,
                    "ssid": 89,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most appar\u00ad ent was the fact that the limited context transforma\u00ad tions were unable to recover from many errors intro\u00ad duced by the naive maximum matching algorithm.",
                    "sid": 163,
                    "ssid": 90,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, because the greedy algorithm always looks for the longest string of characters which can be a word, given the character sequence \"economicsi\u00ad tuation\", the greedy algorithm first recognized \"eco\u00ad nomics\" and several shorter words, segmenting the sequence as \"economics it u at io n\".",
                    "sid": 164,
                    "ssid": 91,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since our transformations consider only a single character of context, the learning algorithm was unable to patch the smaller segments back together to produce the desired output \"economic situation\".",
                    "sid": 165,
                    "ssid": 92,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In some cases, La ng ua ge Le xic on S l Z e Ini tia l s c o re R u l e s le ar ne d Im pr ov ed s c o r e Er ro r re du cti on Ch ine se C hi ne se ( w it h C A W ) T h ai En gli sh 5 7 4 7 2 5 7 4 7 2 9 9 3 9 3 4 2 7 2 6 4 . 4 8 2 . 9 4 8 . 2 7 3 . 2 2 8 9 7 2 4 5 0 7 3 1 8 0 0 8 4 . 9 8 7 . 7 6 3 . 6 7 9 . 0 57 .8 % 28 .1 % 29 .7 % 21 .6 % Table 2: Summary of maximum matching results.",
                    "sid": 166,
                    "ssid": 93,
                    "kind_of_tag": "s"
                },
                {
                    "text": "the transformations were able to recover some of the word, but were rarely able to produce the full desired output.",
                    "sid": 167,
                    "ssid": 94,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, in one case the greedy algo\u00ad rithm segmented \"humanactivity\" as \"humana c ti vi ty\".",
                    "sid": 168,
                    "ssid": 95,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The rule sequence was able to transform this into \"humana ctivity\", but was not able to produce the desired \"human activity\".",
                    "sid": 169,
                    "ssid": 96,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This suggests that both the greedy algorithm and the transformation learning algorithm need to have a more global word model, with the ability to recognize the impact of placing a boundary on the longer sequences of char\u00ad acters surrounding that point.",
                    "sid": 170,
                    "ssid": 97,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "discussion. ",
            "number": "4",
            "sents": [
                {
                    "text": "The results of these experiments demonstrate that a transformation-based rule sequence, supplement\u00ad ing a rudimentary initial approximation, can pro\u00ad duce accurate segmentation.",
                    "sid": 171,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, they are able to improve the performance of a wide range of segmentation algorithms, without requiring expen\u00ad sive knowledge engineering.",
                    "sid": 172,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Learning the rule se\u00ad quences can be achieved in a few hours and requires no language-specific knowledge.",
                    "sid": 173,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As discussed in Sec\u00ad tion 1, this simple algorithm could be used to adapt the output of an existing segmentation algorithm to different segmentation schemes as well as compen\u00ad sating for incomplete segmenter lexica, without re\u00ad quiring modifications to segmenters themselves.",
                    "sid": 174,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The rule-based algorithm we developed to improve word segmentation is very effective for segment\u00ad ing Chinese; in fact, the rule sequences combined with a very simple initial segmentation, such as that from a maximum matching algorithm, produce performance comparable to manually-developed seg\u00ad menters.",
                    "sid": 175,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As demonstrated by the experiment with the NMSU segmenter, the rule sequence algorithm can also be used to improve the output of an already highly-accurate segmenter, thus producing one of the best segmentation results reported in the litera\u00ad ture.",
                    "sid": 176,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to the excellent overall results in Chi\u00ad nese segmentation, we also showed the rule sequence algorithm to be very effective in improving segmen\u00ad tation in Thai, an alphabetic language.",
                    "sid": 177,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While the scores themselves were not as high as the Chinese performance, the error reduction was nevertheless very high, which is encouraging considering the sim\u00ad ple rule syntax used.",
                    "sid": 178,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The current state of our algo\u00ad rithm, in which only three characters are considered at a time, will understandably perform better with a language like Chinese than with an alphabetic lan\u00ad guage like Thai, where average word length is much greater.",
                    "sid": 179,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The simple syntax described in Section 2.2 can, however, be easily extended to consider larg r contexts to the left and the right of boundaries; this extension would necessarily come at a corresponding cost in learning speed since the size of the rule space searched during training would grow accordingly.",
                    "sid": 180,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the future, we plan to further investigate the ap\u00ad plication of our rule-based algorithm to alphabetic languages.",
                    "sid": 181,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Acknowledgements This work would not have been possible without the assistance and encour\u00ad agement of all the members of the MITRE Natural Language Group.",
                    "sid": 182,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This paper benefited greatly from discussions with and comments from Marc Vilain, Lynette Hirschman, Sam Bayer, and the anonymous rev1ewers.",
                    "sid": 183,
                    "ssid": 13,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}