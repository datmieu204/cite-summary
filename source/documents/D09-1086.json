{
    "ID": "D09-1086",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "We connect two scenarios in structured learning: adapting a parser trained on one corpus to another annotation style, and projecting syntactic annotations from one language to another.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We propose quasi- synchronous grammar (QG) features for these structured learning tasks.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, we score a aligned pair of source and target trees based on local features of the trees and the alignment.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our quasi-synchronous model assigns positive probability to any alignment of any trees, in contrast to a synchronous grammar, which would insist on some form of structural parallelism.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In monolingual dependency parser adaptation, we achieve high accuracy in translating among multiple annotation styles for the same sentence.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On the more difficult problem of cross-lingual parser projection, we learn a dependency parser for a target language by using bilingual text, an English parser, and automatic word alignments.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our experiments show that unsupervised QG projection improves on parses trained using only high- precision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, learning an unsupervised parser from raw target-language text alone.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2217The first author would like to thank the Center for Intelligent Information Retrieval at UMass Amherst.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We would also like to thank Noah Smith and Rebecca Hwa for helpful discussions and the anonymous reviewers for their suggestions for improving the paper.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "1.1 Parser Adaptation.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consider the problem of learning a dependency parser, which must produce a directed tree whose vertices are the words of a given sentence.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are many differing conventions for representing syntactic relations in dependency trees.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Say that we wish to output parses in the Prague style and so have annotated a small target corpus\u2014e.g., 100 sentences\u2014with those conventions.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A parser trained on those hundred sentences will achieve mediocre dependency accuracy (the proportion of words that attach to their correct parent).",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But what if we also had a large number of trees in the CoNLL style (the source corpus)?",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ideally they should help train our parser.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But unfortunately, a parser that learned to produce perfect CoNLL-style trees would, for example, get both links \u201cwrong\u201d when its coordination constructions were evaluated against a Prague-style gold standard (Figure 1).",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If it were just a matter of this one construction, the obvious solution would be to write a few rules by hand to transform the large source training corpus into the target style.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Suppose, however, that there were many more ways that our corpora differed.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then we would like to learn a statistical model to transform one style of tree into another.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We may not possess hand-annotated training data for this tree-to-tree transformation task.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That would require the two corpora to annotate some of the same sentences in different styles.",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But fortunately, we can automatically obtain a noisy form of the necessary paired-tree training data.",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A parser trained on the source corpus can parse the sentences in our target corpus, yielding trees (or more generally, probability distributions over trees) in the source style.",
                    "sid": 25,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will then learn a tree transformation model relating these noisy source trees to our known trees in the target style.",
                    "sid": 26,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "822 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 822\u2013831, Singapore, 67 August 2009.",
                    "sid": 27,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Qc 2009 ACL and AFNLP ( ( ( ( now or never now or never Prague Mel\u2019c\u02c7uk now ( ( ( ( or never now or never CoNLL MALT Figure 1: Four of the five logically possible schemes for annotating coordination show up in human-produced dependency treebanks.",
                    "sid": 28,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(The other possibility is a reverse Mel\u2019c\u02c7uk scheme.)",
                    "sid": 29,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These treebanks also differ on other conventions.",
                    "sid": 30,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This model should enable us to convert the original large source corpus to target style, giving us additional training data in the target style.",
                    "sid": 31,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.2 Parser Projection.",
                    "sid": 32,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For many target languages, however, we do not have the luxury of a large parsed \u201csource corpus\u201d in the language, even one in a different style or domain as above.",
                    "sid": 33,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, we may seek other forms of data to augment our small target corpus.",
                    "sid": 34,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One option would be to leverage unannotated text (McClosky et al., 2006; Smith and Eisner, 2007).",
                    "sid": 35,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But we can also try to transfer syntactic information from a parsed source corpus in another language.",
                    "sid": 36,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is an extreme case of out-of-domain data.",
                    "sid": 37,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This leads to the second task of this paper: learning a statistical model to transform a syntactic analysis of a sentence in one language into an analysis of its translation.",
                    "sid": 38,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Tree transformations are often modeled with synchronous grammars.",
                    "sid": 39,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Suppose we are given a sentence wl in the \u201csource\u201d language and its translation w into the \u201ctarget\u201d language.",
                    "sid": 40,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Their syntactic parses tl and t are presumably not independent, but will tend to have some parallel or at least correlated structure.",
                    "sid": 41,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "So we could jointly model the parses tl, t and the alignment a between them, with a model of the form p(t, a, tl | w, wl).Such a joint model captures how t, a, tl mu tually constrain each other, so that even partial knowledge of some of these three variables can help us to recover the others when training or decoding on bilingual text.",
                    "sid": 42,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This idea underlies a number of recent papers on syntax-based alignment (using t and tl to better recover a), grammar induction from bitext (using a to better recover t and tl), parser projection (using tl and a to better Figure 2: With the English tree and alignment provided by a parser and aligner at test time, the Chinese parser finds the correct dependencies (see \u00a76).",
                    "sid": 43,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A monolingual parser\u2019s incorrect edges are shown with dashed lines.",
                    "sid": 44,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "recover t), as well as full joint parsing (Smith and Smith, 2004; Burkett and Klein, 2008).",
                    "sid": 45,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we condition on the 1-best source tree tl.",
                    "sid": 46,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As for the alignment a, our models either condition on the 1-best alignment or integrate the alignment out.",
                    "sid": 47,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our models are thus of the form p(t | w, wl, tl, a) or, in the generative case, p(w, t, a | wl, tl).",
                    "sid": 48,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We intend to consider other for mulations in future work.",
                    "sid": 49,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "So far, this is very similar to the monolingual parser adaptation scenario, but there are a few key differences.",
                    "sid": 50,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the source and target sentences in the bitext are in different languages, there is no longer a trivial alignment between the words of the source and target trees.",
                    "sid": 51,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given word alignments, we could simply try to project dependency links in the source tree onto the target text.",
                    "sid": 52,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A link-by link projection, however, could result in invalid trees on the target side, with cycles or disconnected words.",
                    "sid": 53,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead, our models learn the necessary transformations that align and transform a source tree into a target tree by means of quasi- synchronous grammar (QG) features.",
                    "sid": 54,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 2 shows an example of bitext helping disambiguation when a parser is trained with only a small number of Chinese trees.",
                    "sid": 55,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With the help of the English tree and alignment, the parser is able to recover the correct Chinese dependencies using QG features.",
                    "sid": 56,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Incorrect edges from the monolingual parser are shown with dashed lines.",
                    "sid": 57,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(The bilingual parser corrects additional errors in the second half of this sentence, which has been removed to improve legibility.)",
                    "sid": 58,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The parser is able to recover the long-distance dependency from the first Chinese word (China) to the last (begun), while skipping over the intervening noun phrase that confused the undertrained monolingual parser.",
                    "sid": 59,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although, due to the auxiliary verb, \u201cChina\u201d and \u201cbegun\u201d are siblings in English and not in direct dependency, the QG features still leverage this indirect projection.",
                    "sid": 60,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.3 Plan of the Paper.",
                    "sid": 61,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We start by describing the features we use to augment conditional and generative parsers when scoring pairs of trees (\u00a72).",
                    "sid": 62,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then we discuss in turn monolingual (\u00a73) and cross-lingual (\u00a74) parser adaptation.",
                    "sid": 63,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we present experiments on cross-lingual parser projection in conditions when no target language trees are available for training (\u00a75) and when some trees are available (\u00a76).",
                    "sid": 64,
                    "ssid": 64,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "form of the model. ",
            "number": "2",
            "sents": [
                {
                    "text": "What should our model of source and target trees look like?",
                    "sid": 65,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our view, traditional approaches based on synchronous grammar are problematic both computationally and linguistically.",
                    "sid": 66,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Full inference takes O(n6) time or worse (depending on the grammar formalism).",
                    "sid": 67,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Yet synchronous models only consider a limited hypothesis space: e.g., parses must be projective, and alignments must decompose according to the recursive parse structure.",
                    "sid": 68,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(For example, two nodes can be aligned only if their respective parents are also aligned.)",
                    "sid": 69,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The synchronous model\u2019s probability mass function is also restricted to decompose in this way, so it makes certain conditional independence assumptions; put another way, it can evaluate only certain properties of the triple (t, a, tl).",
                    "sid": 70,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We instead model (t, a, tl) as an arbitrary graph that includes dependency links among the words of each sentence as well as arbitrary alignment links between the words of the two sentences.",
                    "sid": 71,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This permits non-synchronous and many-to-many alignments.",
                    "sid": 72,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The only hard constraint we impose is that the dependency links within each sentence must constitute a valid monolingual parse\u2014a directed projective spanning tree.1Given the two sentences w, wl, our probabil ity distribution over possible graphs considers local features of the parses, the alignment, and both jointly.",
                    "sid": 73,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, we learn what local syntactic configurations tend to occur in each language and how they correspond across languages.",
                    "sid": 74,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a result, we might learn that parses are \u201cmostly synchronous,\u201d but that there are some systematic cross-linguistic 1 Non-projective parsing would also be possible..",
                    "sid": 75,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "divergences and some instances of sloppy (non- parallel or inexact) translation.",
                    "sid": 76,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a).",
                    "sid": 77,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).",
                    "sid": 78,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All the models in this paper are conditioned on the source tree tl.",
                    "sid": 79,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Conditionally-trained models of adaptation and projection also condition on the target string w and its alignment a to wl and thus have the form p(t | w, wl, tl, a); the unsupervised, generative projection models in \u00a75 have the form p(w, t, a | wl, tl).",
                    "sid": 80,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The score s of a given tuple of trees, words, and alignment can thus be written as a dot product of weights w with features f and g: s(t, tl, a, w, wl) = ) wifi(t, w) i + ) wj gj (t, tl, a, w, wl) j The features f look only at target words and dependencies.",
                    "sid": 81,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the conditional models of \u00a73 and \u00a76, these features are those of an edge-factored dependency parser (McDonald et al., 2005).",
                    "sid": 82,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the generative models of \u00a75, f has the form of a dependency model with valence (Klein and Manning, 2004).",
                    "sid": 83,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All models, for instance, have a feature template that considers the parts of speech of a potential parent-child relation.",
                    "sid": 84,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order to benefit from the source language, we also need to include bilingual features g. When scoring a candidate target dependency link fromword x \u2192 y, these features consider the relation ship of their corresponding source words xl and yl.",
                    "sid": 85,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(The correspondences are determined by the alignment a.)",
                    "sid": 86,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, the source tree tl may contain the link xl \u2192 yl, which would cause a feature for monotonic projection to fire for the x \u2192 y edge.",
                    "sid": 87,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If, on the other hand, yl \u2192 xl \u2208 tl, a head-swapping feature fires.",
                    "sid": 88,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If xl = yl, i.e. x and y align to the same word, the same-word feature fires.",
                    "sid": 89,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similar features fire when xl and yl are in grandparent-grandchild, sibling, c-command, or none-of-the above relationships, or when y aligns to NULL.",
                    "sid": 90,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These alignment classes are called configurations (Smith and Eisner, 2006a, and following).",
                    "sid": 91,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When training is conditioned on the target words (see \u00a73 and \u00a76 below), we conjoin these configuration features with the part of speech and coarse part of speech of one or both of the source and target words, i.e. the feature template has from one to four tags.",
                    "sid": 92,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In conditional training, the exponentiated scores s are normalized by a constant: Z = (McDonald et al., 2005) on \u201csource\u201d domain data that followed one set of dependency conventions.",
                    "sid": 93,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then trained an edge-factored parser with QG features on a small amount of \u201ctarget\u201d domain data.",
                    "sid": 94,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The source parser outputs were produced for all target data, both training and test, so that fea t exp[s(t, tl, a, w, wl)].",
                    "sid": 95,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the generative model, the locally normalized generative process is explained in \u00a75.3.4.",
                    "sid": 96,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previous researchers have written fix-up rules to massage the projected links after the fact and learned a parser from the resulting trees (Hwa et al., 2005).",
                    "sid": 97,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead, our models learn the necessary transformations that align and transform a source tree into a target tree.",
                    "sid": 98,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other researchers have tackled the interesting task of learning parsers from unparsed bitext alone (Kuhn, 2004; Snyder et al., 2009); our methods take advantage of investments in high-resource languages such as English.",
                    "sid": 99,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In work most closely related to this paper, Ganchev et al.",
                    "sid": 100,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2009) constrain the posterior distribution over target-language dependencies to align to source dependencies some \u201creasonable\u201d proportion of the time (\u2248 70%, cf.",
                    "sid": 101,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2 in this paper).",
                    "sid": 102,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This approach performs well but cannot directly learn regular cross-language non-isomorphisms; for instance, some fixup rules for auxiliary verbs need to be introduced.",
                    "sid": 103,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, Huang et al.",
                    "sid": 104,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2009) use features, somewhat like QG configurations, on the shift-reduce actions in a monolingual, target- language parser.",
                    "sid": 105,
                    "ssid": 41,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "adaptation. ",
            "number": "3",
            "sents": [
                {
                    "text": "As discussed in \u00a71, the adaptation scenario is a special case of parser projection where the word alignments are one-to-one and observed.",
                    "sid": 106,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To test our handling of QG features, we performed ex periments in which training saw the correct parse trees in both source and target domains, and the mapping between them was simple and regular.",
                    "sid": 107,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also performed experiments where the source trees were replaced by the noisy output of a trained parser, making the mapping more complex and harder to learn.",
                    "sid": 108,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used the subset of the Penn Treebank from the CoNLL 2007 shared task and converted it to dependency representation while varying two parameters: (1) CoNLL vs. Prague coordination style (Figure 1), and (2) preposition the head vs. the child of its nominal object.",
                    "sid": 109,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We trained an edge-factored dependency parser tures for the target parser could refer to them.",
                    "sid": 110,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this task, we know what the gold-standard source language parses are for any given text, since we can produce them from the original Penn Treebank.",
                    "sid": 111,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can thus measure the contribution of adaptation loss alone, and the combined loss of imperfect source-domain parsing with adaptation (Table 1).",
                    "sid": 112,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When no target domain trees are available, we simply have the performance of the source domain parser on this out-of-domain data.",
                    "sid": 113,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Training a target-domain parser on as few as 10 sentences shows substantial improvements in accuracy.",
                    "sid": 114,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the \u201cgold\u201d conditions, where the target parser starts with perfect source trees, accuracy approaches 100%; in the realistic \u201cparse\u201d conditions, where the target-domain parser gets noisy source-domain parses, the improvements are quite significant but approach a lower ceiling imposed by the performance of the source parser.2 The adaptation problem in this section is a simple proof of concept of the QG approach; however, more complex and realistic adaptation problems exist.",
                    "sid": 115,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Monolingual adaptation is perhaps most obviously useful when the source parser is a black- box or rule-based system or is trained on unavailable data.",
                    "sid": 116,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One might still want to use such a parser in some new context, which might require new data or a new annotation standard.",
                    "sid": 117,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We are also interested in scenarios where we want to avoid expensive retraining on large reannotated treebanks.",
                    "sid": 118,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We would like a linguist to be able to annotate a few trees according to a hypothesized theory and then quickly use QG adaptation to get a parser for that theory.",
                    "sid": 119,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One example would be adapting a constituency parser to produce dependency parses.",
                    "sid": 120,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We have concentrated here on adapting between two dependency parse styles, in order to line up with the cross-lingual tasks to which we now turn.",
                    "sid": 121,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 In the diagonal cells, source and target styles match, so training the QG parser amounts to a \u201cstacking\u201d technique (Martins et al., 2008).",
                    "sid": 122,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The small training size and overregularization of the QG parser mildly hurts in-domain parsing performance.",
                    "sid": 123,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "% D e p e n d e n c y A c c u r a c y o n T a r g e t C o N LL Pr ep H ea d C o N LL Pr ep C hil d Pr a g ue Pr e p H e a d Pr a g ue Pr e p C hil d So urc e 0 10 100 0 10 100 0 10 100 0 10 100 Go ld Co NLL Pr ep He ad Par se Co NLL Pr ep He ad 1 0 0 99.6 99.6 89 .5 88.9 89.0 79.",
                    "sid": 124,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 96.9 97.8 71.",
                    "sid": 125,
                    "ssid": 20,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "85.9\t87.9",
            "number": "4",
            "sents": [
                {
                    "text": "90.",
                    "sid": 126,
                    "ssid": 1,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "95.0\t98.1",
            "number": "5",
            "sents": [
                {
                    "text": "82.",
                    "sid": 127,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 84.3 87.8 71.",
                    "sid": 128,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 92.7 95.4 65.",
                    "sid": 129,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 82.2 86.1 Go ld Co NLL Pr ep Ch ild Par se Co NLL Pr ep Ch ild 79.",
                    "sid": 130,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 96.6 97.3 71.",
                    "sid": 131,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 84.2 86.8 1 0 0 99.6 99.6 88 .1 87.5 88.0 71.",
                    "sid": 132,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 91.3 95.5 64.",
                    "sid": 133,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 80.7 84.9 89.",
                    "sid": 134,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 94.5 97.9 80.",
                    "sid": 135,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 83.5 86.1 Go ld Pr ag ue Pr ep He ad Par se Pr ag ue Pr ep He ad 90.",
                    "sid": 136,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 95.5 96.7 83.",
                    "sid": 137,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 87.1 87.4 71.",
                    "sid": 138,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 92.0 94.2 65.",
                    "sid": 139,
                    "ssid": 13,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "84.2\t85.9",
            "number": "6",
            "sents": [
                {
                    "text": "1 0 0 99.6 99.6 88 .5 88.3 88.0 79.",
                    "sid": 140,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6 97.4 98.1 70.",
                    "sid": 141,
                    "ssid": 2,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "86.4\t86.8",
            "number": "7",
            "sents": [
                {
                    "text": "Go ld Pr ag ue Pr ep Ch ild Par se Pr ag ue Pr ep Ch ild 71.",
                    "sid": 142,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 91.6 93.8 65.",
                    "sid": 143,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3 81.7 84.6 89.",
                    "sid": 144,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "9 95.6 96.4 81.",
                    "sid": 145,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 84.5 86.1 79.",
                    "sid": 146,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6 96.0 97.1 70.",
                    "sid": 147,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4 83.2 85.3 1 0 0 99.6 99.6 86 .9 86.1 86.8 Table 1: Adapting a parser to a new annotation style.",
                    "sid": 148,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We learn to parse in a \u201ctarget\u201d style (wide column label) given some number (narrow column label) of supervised target-style training sentences.",
                    "sid": 149,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As a font of additional features, all training and test sentences have already been augmented with parses in some \u201csource\u201d style (row label): either gold-standard parses (an oracle experiment) or else the output of a parser trained on 18k source trees (more realistic).",
                    "sid": 150,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If we have 0 training sentences, we simply output the source-style parse.",
                    "sid": 151,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But with 10 or 100 target-style training sentences, each off-diagonal block learns to adapt, mostly closing the gap with the diagonal block in the same column.",
                    "sid": 152,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the diagonal blocks, source and target styles match, and the QG parser degrades performance when acting as a \u201cstacked\u201d parser.",
                    "sid": 153,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4 Cross-Lingual Projection: Background.",
                    "sid": 154,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As in the adaptation scenario above, many syntactic structures can be transferred from one language to another.",
                    "sid": 155,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section, we evaluate the extent of this direct projection on a small handannotated corpus.",
                    "sid": 156,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In \u00a75, we will use a QG genera tive model to learn dependency parsers from bitext when there are no annotations in the target language.",
                    "sid": 157,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, in \u00a76,we show how QG features can augment a target-language parser trained on a small set of labeled trees.",
                    "sid": 158,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For syntactic annotation projection to work at all, we must hypothesize, or observe, that at least some syntactic structures are preserved in translation.",
                    "sid": 159,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hwa et al.",
                    "sid": 160,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2005) have called this intuition the Direct Correspondence Assumption (DCA, with slight notational changes): Given a pair of sentences w and wl that are translations of each other with syntactic structure t and tl, if nodes xl and yl of tl are aligned with nodes x and y of t, respectively, and if syntactic relationship R(xl, yl) holds in tl, then R(x, y) holds in t. The validity of this assumption clearly depends on the node-to-node alignment of the two trees.",
                    "sid": 161,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We again work in a dependency framework, where syntactic nodes are simply lexical items.",
                    "sid": 162,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This allows us to use existing work on word alignment.",
                    "sid": 163,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hwa et al.",
                    "sid": 164,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2005) tested the DCA under idealized conditions by obtaining hand-corrected dependency parse trees of a few hundred sentences of SpanishEnglish and ChineseEnglish bitext.",
                    "sid": 165,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They also used human-produced word alignments.",
                    "sid": 166,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "C or pu s Pr ec .[ %] Rec.[%] S pa ni sh (n o pu nc .) 6 4 . 3 2 8 . 4 7 2 . 0 3 0 . 8 C hi ne se (n o pu nc .) 6 5 . 1 1 1 . 1 6 8 . 2 1 1 . 5 Table 2: Precision and recall of direct dependency projection via one-to-one links alone.",
                    "sid": 167,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since their word alignments could be many-to- many, they gave a heuristic Direct Projection Algorithm (DPA) for resolving them into component dependency relations.",
                    "sid": 168,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It should be noted that this process introduced empty words into the projected target language tree and left words that are un- aligned to English detached from the tree; as a result, they measured performance in dependency F- score rather than accuracy.",
                    "sid": 169,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With manual English parses and word alignments, this DPA achieved 36.8% F-score in Spanish and 38.1% in Chinese.",
                    "sid": 170,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With Collins-model English parses and GIZA++ word alignments, F-score was 33.9% for Spanish and 26.3% for Chinese.",
                    "sid": 171,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Compare this to the Spanish attach-left baseline of 31.0% and the Chinese attach-right baselines of 35.9%.",
                    "sid": 172,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These discouragingly low numbers led them to write language- specific transformation rules to fix up the projected trees.",
                    "sid": 173,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After these rules were applied to the projections of automatic English parses, F-score was 65.7% for English and 52.4% for Chinese.",
                    "sid": 174,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While these F-scores were low, it is useful to look at a subset of the alignment: dependencies projected across one-to-one alignments before the heuristic fix-ups had a much higher precision, if lower recall, than Hwa et al.\u2019s final results.",
                    "sid": 175,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Us ing Hwa et al.\u2019s data, we calculated that the precision of projection to Spanish and Chinese via these one-to-one links was \u2248 65% (Table 2).",
                    "sid": 176,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There is clearly more information in these direct links than one would think from the F-scores.",
                    "sid": 177,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To exploit this information, however, we need to overcome the problems of (1) learning from partial trees, when not all target words are attached, and (2) learning in the presence of the still considerable noise in the projected one-to-one dependencies\u2014e.g., at least 28% error for Spanish non-punctuation dependencies.",
                    "sid": 178,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "What does this noise consist of?",
                    "sid": 179,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some errors reflect fairly arbitrary annotation conventions in treebanks, e.g. should the auxiliary verb govern the main verb or vice versa.",
                    "sid": 180,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Examples like this suggest that the projection problem contains the adaptation problem above.)",
                    "sid": 181,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Other errors arise from divergences in the complements required of certain head words.",
                    "sid": 182,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the GermanEnglish translation pair, with co-indexed words aligned,t p(t, w, a | tl, wl).",
                    "sid": 183,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We hope that this conditional EM training will drive the model to posit ap propriate syntactic relationships in the latent variable t, because\u2014thanks to the structure of the QG model\u2014that is the easiest way for it to exploit the extra information in tl, wl to help predict w.4 At test time, tl, wl are not made available, so we just use the trained model to find argmaxt p(t | w), backing off from the conditioning on tl, wl and summing over a. Below, we present the specific generative model (\u00a75.1) and some details of training (\u00a75.2).",
                    "sid": 184,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will then compare three approaches (\u00a75.3): \u00a75.3.2 a straight EM baseline (which does not condition on tl, wl at all) \u00a75.3.3 a \u201chard\u201d projection baseline (which naively projects tl, wl to derive direct supervision in the target language) \u00a75.3.4 our conditional EM approach above (which [an [den Libanon1]] denken2 \u2194 remember2 Lebanon1 makes tl, wl available to the learner for \u201csoft\u201d we would prefer that the preposition an attach to denken, even though the preposition\u2019s object Libanon aligns to a direct child of remember.",
                    "sid": 185,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In other words, we would like the grandparent parent-child chain of denken \u2192 an \u2192 Libanon to align to the parent-child pair of remember \u2192Lebanon.",
                    "sid": 186,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, naturally occurring bitexts con tain some number of free or erroneous translations.",
                    "sid": 187,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Machine translation researchers often seek to strike these examples from their training corpora; \u201cfree\u201d translations are not usually welcome from an MT system.",
                    "sid": 188,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 Unsupervised Cross-Lingual Projection.",
                    "sid": 189,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we consider the problem of parser projection when there are zero target-language trees available.",
                    "sid": 190,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As in much other work on unsupervised parsing, we try to learn a generative model that can predict target-language sentences.",
                    "sid": 191,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our novel contribution is to condition the probabilities of the generative actions on the dependency parse of a source-language translation.",
                    "sid": 192,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, our generative model is a quasi-synchronous grammar, exactly as in (Smith and Eisner, 2006a).3 When training on target sentences w, therefore, we tune the model parameters to maximize not t p(t, w) as in ordinary EM, but rather 3 Our task here is new; they used it for alignment..",
                    "sid": 193,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "indirect supervision via QG) 5.1 Generative Models.",
                    "sid": 194,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our base models of target-language syntax are generative dependency models that have achieved state-of-the art results in unsupervised dependency structure induction.",
                    "sid": 195,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The simplest version, called Dependency Model with Valence (DMV), has been used in isolation and in combination with other models (Klein and Manning, 2004; Smith and Eisner, 2006b).",
                    "sid": 196,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The DMV generates the right children, and then independently the left children, for each node in the dependency tree.",
                    "sid": 197,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Nodes correspond to words, which are represented by their part-of-speech tags.",
                    "sid": 198,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "At each step of generation, the DMV stochastically chooses whether to stop generating, conditioned on the currently generating head; whether it is generating to the right or left; and whether it has yet generated any children on that side.",
                    "sid": 199,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If it chooses to continue, it then 4 The contrastive estimation of Smith and Eisner (2005) also used a form of conditional EM, with similar motivation.",
                    "sid": 200,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They suggested that EM grammar induction, which learns to predict w, unfortunately learns mostly to predict lexical topic or other properties of the training sentences that do not strongly require syntactic latent variables.",
                    "sid": 201,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To focus EM on modeling the syntactic relationships, they conditioned the prediction of w on almost complete knowledge of the lexical items.",
                    "sid": 202,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similarly, we condition on a source translation of w. Furthermore, our QG model structure makes it easy for EM to learn to exploit the (explicitly represented) syntactic properties of that translation when predicting w. stochastically generates the tag of a new child, conditioned on the head.",
                    "sid": 203,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The parameters of the model are thus of the form p(stop | head, dir, adj) (1) p(child | head, dir) (2) where head and child are part-of-speech tags, dir \u2208 {left, right}, and adj, stop \u2208 {true, false}.",
                    "sid": 204,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ROOT is stipulated to generate a single right child.",
                    "sid": 205,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Bilingual configurations that condition on tl, wl (\u00a72) are incorporated into the generative process as in Smith and Eisner (2006a).",
                    "sid": 206,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When the model is generating a new child for word x, aligned to xl, it first chooses a configuration and then chooses a source word yl in that configuration.",
                    "sid": 207,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The child y is then generated, conditioned on its parent x, most recent sibling a, and its source analogue yl.",
                    "sid": 208,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.2 Details of EM Training.",
                    "sid": 209,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As in previous work on grammar induction, we learn the DMV from part-of-speech-tagged target- language text.",
                    "sid": 210,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use expectation maximization (EM) to maximize the likelihood of the data.",
                    "sid": 211,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the likelihood function is nonconvex in the unsupervised case, our choice of initial parameters can have a significant effect on the outcome.",
                    "sid": 212,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although we could also try many random starting points, the initializer in Klein and Manning (2004) performs quite well.",
                    "sid": 213,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The base dependency parser generates the right dependents of a head separately from the left dependents, which allows O(n3) dynamic programming for an n-word target sentence.",
                    "sid": 214,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the QG annotates nonterminals of the grammar with single nodes of tl, and we consider two nodes of tl when evaluating the above dependency configurations, QG parsing runs in O(n3m2) for an m-word source sentence.",
                    "sid": 215,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If, however, we restrict candidate senses for a target child c to come from links in an IBM Model 4 Viterbi alignment, we achieve O(n3k2), where k is the maximum number of possible words aligned to a given target languageword.",
                    "sid": 216,
                    "ssid": 75,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In practice, k \u00ab m, and parsing is not ap preciably slower than in the monolingual setting.",
                    "sid": 217,
                    "ssid": 76,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If all configurations were equiprobable, the source sentence would provide no information to the target.",
                    "sid": 218,
                    "ssid": 77,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our QG experiments, therefore, we started with a bias towards direct parent\u2013child links and a very small probability for breakages of locality.",
                    "sid": 219,
                    "ssid": 78,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The values of other configuration parameters seem, experimentally, less important for insuring accurate learning.",
                    "sid": 220,
                    "ssid": 79,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.3 Experiments.",
                    "sid": 221,
                    "ssid": 80,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our experiments compare learning on target language text to learning on parallel text.",
                    "sid": 222,
                    "ssid": 81,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the latter case, we compare learning from high-precision one-to-one alignments alone, to learning from all alignments using a QG.",
                    "sid": 223,
                    "ssid": 82,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.3.1 Corpora Our development and test data were drawn from the German TIGER and Spanish Cast3LB tree- banks as converted to projective dependencies for the CoNLL 2007 Shared Task (Brants et al., 2002; Civit Torruella and Mart\u00b4\u0131 Anton\u00b4\u0131n, 2002).5 Our training data were subsets of the 2006 Statistical Machine Translation Workshop Shared Task, in particular from the GermanEnglish and SpanishEnglish Europarl parallel corpora (Koehn, 2002).",
                    "sid": 224,
                    "ssid": 83,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Shared Task provided pre- built automatic GIZA++ word alignments, which we used to facilitate replicability.",
                    "sid": 225,
                    "ssid": 84,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since these word alignments do not contain posterior probabilities or null links, nor do they distinguish which links are in the IBM Model intersection, we treated all links as equally likely when learning the QG.",
                    "sid": 226,
                    "ssid": 85,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Target language words unaligned to any source language words were the only nodes allowed to align to NULL in QG derivations.",
                    "sid": 227,
                    "ssid": 86,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We parsed the English side of the bitext with the projective dependency parser described by McDonald et al.",
                    "sid": 228,
                    "ssid": 87,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2005) trained on the Penn Treebank \u00a7\u00a72\u201320.",
                    "sid": 229,
                    "ssid": 88,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Much previous work on unsupervisedgrammar induction has used gold-standard part of-speech tags (Smith and Eisner, 2006b; Klein and Manning, 2004; Klein and Manning, 2002).",
                    "sid": 230,
                    "ssid": 89,
                    "kind_of_tag": "s"
                },
                {
                    "text": "While there are no gold-standard tags for the Europarl bitext, we did train a conditional Markov 5 We made one change to the annotation conventions in German: in the dependencies provided, words in a noun phrase governed by a preposition were all attached to that preposition.",
                    "sid": 231,
                    "ssid": 90,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This meant that in the phrase das Kind (\u201cthe child\u201d) in, say, subject position, das was the child of Kind; but, in fu\u00a8 r das Kind (\u201cfor the child\u201d), das was the child of fu\u00a8 r. This seems to be a strange choice in converting from the TIGER constituency format, which does in fact annotate NPs inside PPs; we have standardized prepositions to govern only the head of the noun phrase.",
                    "sid": 232,
                    "ssid": 91,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We did not change any other annotation conventions to make them more like English.",
                    "sid": 233,
                    "ssid": 92,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the Spanish treebank, for instance, control verbs are the children of their verbal complements: in quiero decir (\u201cI want to say\u201d=\u201cI mean\u201d), quiero is the child of decir.",
                    "sid": 234,
                    "ssid": 93,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In German co- ordinations, the coordinands all attach to the first, but in English, they all attach to the last.",
                    "sid": 235,
                    "ssid": 94,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These particular divergences in annotation style hurt all of our models equally (since none of them have access to labeled trees).",
                    "sid": 236,
                    "ssid": 95,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These annotation divergences are one motivation for experiments below that include some target trees.",
                    "sid": 237,
                    "ssid": 96,
                    "kind_of_tag": "s"
                },
                {
                    "text": "B as eli ne s Depen dency accura cy [%] G er m an S p a n i s h M od ify pr ev . M od ify ne xt 1 8 . 2 2 7 . 5 28 .5 21 .4 E M Ha rd pr oj.",
                    "sid": 238,
                    "ssid": 97,
                    "kind_of_tag": "s"
                },
                {
                    "text": "H ar d p r oj . w / E M Q G w / E M 3 0 . 2 6 6 . 2 5 8 . 6 6 8 . 5 25 .6 59 .1 53 .0 64 .8 Table 3: Test accuracy with unsupervised training methods model tagger on a few thousand tagged sentences.",
                    "sid": 239,
                    "ssid": 98,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is the only supervised data we used in the target.",
                    "sid": 240,
                    "ssid": 99,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We created versions of each training corpus with the first thousand, ten thousand, and hundred thousand sentence pairs, each a prefix of the next.",
                    "sid": 241,
                    "ssid": 100,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the target-language-only baseline converged much more slowly, we used a version of the corpora with sentences 15 target words or fewer.",
                    "sid": 242,
                    "ssid": 101,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.3.2 Fully Unsupervised EM Using the target side of the bitext as training data, we initialized our model parameters as described in \u00a75.2 and ran EM.",
                    "sid": 243,
                    "ssid": 102,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We checked convergence ona development set and measured unlabeled depen dency accuracy on held-out test data.",
                    "sid": 244,
                    "ssid": 103,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We compare performance to simple attach-right and attach left baselines (Table 3).",
                    "sid": 245,
                    "ssid": 104,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For mostly head- final German, the \u201cmodify next\u201d baseline is better; for mostly head-initial Spanish, \u201cmodify previous\u201d wins.",
                    "sid": 246,
                    "ssid": 105,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Even after several hundred iterations, performance was slightly, but not significantly better than the baseline for German.",
                    "sid": 247,
                    "ssid": 106,
                    "kind_of_tag": "s"
                },
                {
                    "text": "EM training did not beat the baseline for Spanish.6 5.3.3 Hard Projection, Semi-Supervised EM The simplest approach to using the high-precision one-to-one word alignments is labeled \u201chard projection\u201d in the table.",
                    "sid": 248,
                    "ssid": 107,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We filtered the training corpus to find sentences where enough links were projected to completely determine a target language tree.",
                    "sid": 249,
                    "ssid": 108,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Of course, we needed to filter more than 1000 sentences of bitext to output 1000 training sentences in this way.",
                    "sid": 250,
                    "ssid": 109,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We simply perform supervised training with this subset, which is still quite noisy (\u00a74), and performance quickly 6 While these results are worse than those obtained previously for this model, the experiments in Klein and Manning (2004) and only used sentences of 10 words or fewer, without punctuation, and with gold-standard tags.",
                    "sid": 251,
                    "ssid": 110,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Punctuation in particular seems to trip up the initializer: since a sentence-final periods appear in most sentences, EM often decides to make it the head.",
                    "sid": 252,
                    "ssid": 111,
                    "kind_of_tag": "s"
                },
                {
                    "text": "plateaus.",
                    "sid": 253,
                    "ssid": 112,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Still, this method substantially improves over the baselines and unsupervised EM.",
                    "sid": 254,
                    "ssid": 113,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Restricting ourselves to fully projected trees seems a waste of information.",
                    "sid": 255,
                    "ssid": 114,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can also simply take all one-to-one projected links, impute expected counts for the remaining dependencies with EM, and update our models.",
                    "sid": 256,
                    "ssid": 115,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This approach (\u201chard projection with EM\u201d), however, performed worse than using only the fully projected trees.",
                    "sid": 257,
                    "ssid": 116,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In fact, only the first iteration of EM with this method made any improvement; afterwards, EM degraded accuracy further from the numbers in Table 3.",
                    "sid": 258,
                    "ssid": 117,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.3.4 Soft Projection: QG & Conditional EM The quasi-synchronous model used all of the alignments in re-estimating its parameters and performed significantly better than hard projection.",
                    "sid": 259,
                    "ssid": 118,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unlike EM on the target language alone, the QG\u2019s performance does not depend on a clever initial- izer for initial model weights\u2014all parameters of the generative model except for the QG configuration features were initialized to zero.",
                    "sid": 260,
                    "ssid": 119,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Setting the prior to prefer direct correspondence provides the necessary bias to initialize learning.",
                    "sid": 261,
                    "ssid": 120,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Error analysis showed that certain types of dependencies eluded the QG\u2019s ability to learn from bitext.",
                    "sid": 262,
                    "ssid": 121,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The Spanish treebank treats some verbal complements as the heads of main verbs and auxiliary verbs as the children of participles; the QG, following the English, learned the opposite dependency direction.",
                    "sid": 263,
                    "ssid": 122,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Spanish treebank conventions for punctuation were also a common source of errors.",
                    "sid": 264,
                    "ssid": 123,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In both German and Spanish, coordinations (a common bugbear for dependency grammars) were often mishandled: both treebanks attach the later coordinands and any conjunctions to the first coordinand; the reverse is true in English.",
                    "sid": 265,
                    "ssid": 124,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, in both German and Spanish, preposition attachments often led to errors, which is not surprising given the unlexicalized target-language grammars.",
                    "sid": 266,
                    "ssid": 125,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Rather than trying to adjudicate which dependencies are \u201cmere\u201d annotation conventions, it would be useful to test learned dependency models on some extrinsic task such as relation extraction or machine translation.",
                    "sid": 267,
                    "ssid": 126,
                    "kind_of_tag": "s"
                },
                {
                    "text": "6 Supervised Cross-Lingual Projection.",
                    "sid": 268,
                    "ssid": 127,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we consider the problem of parser projection when some target language trees are available.As in the adaptation case (\u00a73), we train a condi tional model (not a generative DMV) of the target tree given the target sentence, using the monolingual and bilingual QG features, including config urations conjoined with tags, outlined above (\u00a72).",
                    "sid": 269,
                    "ssid": 128,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For these experiments, we used the LDC\u2019s EnglishChinese Parallel Treebank (ECTB).",
                    "sid": 270,
                    "ssid": 129,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since manual word alignments also exist for a part of this corpus, we were able to measure the loss in accuracy (if any) from the use of an automatic language English dependency parser was trained Target only +Gold alignments +Source text on the Wall Street Journal, where it achieved 91% +Gold parses, alignments +Gold parses ever, it was only 80.3% accurate when applied to our task, the English side of the ECTB.7 After parsing the source side of the bitext, we train a parser on the annotated target side, usingQG features described above (\u00a72).",
                    "sid": 271,
                    "ssid": 130,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Both the mono lingual target-language parser and the projected parsers are trained to optimize conditional likelihood of the target trees tl with ten iterations of stochastic gradient ascent.",
                    "sid": 272,
                    "ssid": 131,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Figure 3, we plot the performance of the target-language parser on held-out bitext.",
                    "sid": 273,
                    "ssid": 132,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although projection performance is, not surprisingly, better if we know the true source trees at training and test time, even with the 1-best output of the source parser, QG features help produce a parser as accurate asq one trained on twice the amount of monolingual data.",
                    "sid": 274,
                    "ssid": 133,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In ablation experiments, we included bilingual features only for directly projected links, with no features for head-swapping, grandparents, etc. When using 1-best English parses, parsers trained only with direct-projection and monolingual features performed worse; when using gold English parses, parsers with direct- projection-only features performed better when trained with more Chinese trees.",
                    "sid": 275,
                    "ssid": 134,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7 Discussion.",
                    "sid": 276,
                    "ssid": 135,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The two related problems of parser adaptation and projection are often approached in different ways.",
                    "sid": 277,
                    "ssid": 136,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Many adaptation methods operate by simple augmentations of the target feature space, as we have done here (Daume III, 2007).",
                    "sid": 278,
                    "ssid": 137,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Parser projection, on the other hand, often uses a multi-stage pipeline 7 It would be useful to explore whether the techniques of.",
                    "sid": 279,
                    "ssid": 138,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u00a73 above could be used to improve English accuracy by domain adaptation.",
                    "sid": 280,
                    "ssid": 139,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In theory a model with QG features trained to perform well on Chinese should not suffer from an inaccurate, but consistent, English parser, but the results in Figure 3 indicate a significant benefit to be had from better English parsing or from joint ChineseEnglish inference.",
                    "sid": 281,
                    "ssid": 140,
                    "kind_of_tag": "s"
                },
                {
                    "text": "10 20 50 100 200 500 1000 2000 Training examples Figure 3: Parser projection with target trees.",
                    "sid": 282,
                    "ssid": 141,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using the true or 1-best parse trees in the source language is equivalent to having twice as much data in the target language.",
                    "sid": 283,
                    "ssid": 142,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Note that the penalty for using automatic alignments instead of gold alignments is negligible; in fact, using Source text alone is often higher than +Gold alignments.",
                    "sid": 284,
                    "ssid": 143,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using gold source trees, however, significantly outperforms using 1-best source trees.",
                    "sid": 285,
                    "ssid": 144,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Hwa et al., 2005).",
                    "sid": 286,
                    "ssid": 145,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The methods presented here move parser projection much closer in efficiency and simplicity to monolingual parsing.",
                    "sid": 287,
                    "ssid": 146,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We showed that augmenting a target parser with quasi-synchronous features can lead to significant improvements\u2014first in experiments with adapting to different dependency representations in English, and then in cross-language parser projection.",
                    "sid": 288,
                    "ssid": 147,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As with many domain adaptation problems, it is quite helpful to have some annotated target data, especially when annotation styles vary (Dredze et al., 2007).",
                    "sid": 289,
                    "ssid": 148,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our experiments show that unsupervised QG projection improves on parsers trained using only high-precision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, unsupervised EM.",
                    "sid": 290,
                    "ssid": 149,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When a small number of target-language parse trees is available, projection gives a boost equivalent to doubling the number of target trees.",
                    "sid": 291,
                    "ssid": 150,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The loss in performance from conditioning only on noisy 1-best source parses points to some natural avenues for improvement.",
                    "sid": 292,
                    "ssid": 151,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We are exploring methods that incorporate a packed parse forest on the source side and similar representations of uncertainty about alignments.",
                    "sid": 293,
                    "ssid": 152,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Building on our recent belief propagation work (Smith and Eisner, 2008), we can jointly infer two dependency trees and their alignment, under a joint distribu tion p(t, a, tl | w, wl) that evaluates the full graph of dependency and alignment edges.",
                    "sid": 294,
                    "ssid": 153,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}