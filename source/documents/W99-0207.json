{
    "ID": "W99-0207",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "In this paper we propose a corpus-based approach to anaphora resolution combin\u00ad ing a machine learning method and sta\u00ad tistical information.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, a decision tree trained on an annotated corpus determines the coreference relation of a given anaphor and antecedent candidates and is utilized as a filter in order to reduce the num\u00ad ber of potential candidates.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the sec\u00ad ond step, preference selection is achieved by taking into account the frequency infor\u00ad mation of coreferential and non-referential pairs tagged in the training corpus as well as distance features within the current dis\u00ad course.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Preliminary experiments concern\u00ad ing the resolution of Japanese pronouns in spoken-language dialogs result in a success rate of 80.6%.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Coreference information IS relevant for numerous NLP systems.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our interest in anaphora resolu\u00ad tion is based on the demand for machine translation systems to be able to translate (possibly omitted) anaphoric expressions in agreement with the mor\u00ad phosyntactic characteristics of the referred object in order to prevent contextual misinterpretations.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "So far various approaches1 to anaphora resolution have been proposed.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper a machine learn\u00ad ing approach (decision tree) is combined with a pref\u00ad erence selection method based on the frequency in\u00ad formation of non-/coreferential pairs tagged in the corpus as well as distance features within the cur\u00ad rent discourse.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The advantage of machine learning approaches is that they result in modular anaphora resolution sys\u00ad tems automatically trainable from a corpus with no 1See section 4 for a more detailed comparison with related research.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "or only a minimal amount of human intervention.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the case of decision trees, we do have to provide in\u00ad formation about possible antecedent indicators (syn\u00ad tactic, semantic, and pragmatic features) contained in the corpus, but the relevance of features for the resolution task is extracted automatically from the training data.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Machine learning approaches using decision trees proposed so far have focused on preference selection criteria directly derived from the decision tree re\u00ad sults.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The work described in (Conolly et al., 1994) utilized a decision tree capable of judging which one of two given anaphor-antecedent pairs is \"better\".",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Due to the lack of a strong assumption on \"transi\u00ad tivity\", however, this sorting algorithm is more like a greedy heuristic search as it may be unable to find the \"best\" solution.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The preference selection for a single antecedent in (Aone and Bennett, 1995) is based on the maximiza\u00ad tion of confidence values returned from a pruned de\u00ad cision tree for given anaphor-candidate pairs.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "How\u00ad ever, decision trees are characterized by an indepen\u00ad dent learning of specific features, i.e., relations be\u00ad tween single attributes cannot be obtained automat\u00ad ically.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Accordingly, the use of dependency factors for preference selection during decision tree train\u00ad ing requires that the artificially created attributes expressing these dependencies be defined.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, this not only extends human intervention into the automatic learning procedure (i.e., which dependen\u00ad cies are important?), but can also result in some drawbacks on the contextual adaptation of prefer\u00ad ence selection methods.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The preference selection in our approach is based on the combination of statistical frequency informa\u00ad tion and distance features in the discourse.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There\u00ad fore, our decision tree is not applied directly to the task of preference selection, but aims at the elimina\u00ad tion of irrelevant candidates based on the knowledge obtained from the training data.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The decision tree is trained on syntactic (lexi\u00ad cal word attributes), semantic, and primitive dis\u00ad course (distance, frequency) information and deter\u00ad mines the coreferential relation between an anaphor and antecedent candidate in the given context.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Irrel\u00ad evant antecedent candidates are filtered out, achiev\u00ad ing a noise reduction for the preference selection algorithm.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A preference value is assigned to each potential anaphor-candidate pair depending on the proportion of non-/coreferential occurrences of the pair in the training corpus (frequency ratio) and the relative position of both elements in the discourse (distance).",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The candidate with the maximal pref\u00ad erence value is resolved as the antecedent of the anaphoric expression.",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "corpus-based anaphora resolution. ",
            "number": "2",
            "sents": [
                {
                    "text": "In this section we introduce a new approach to anaphora resolution based on coreferential proper\u00ad ties automatically extracted from a training corpus.",
                    "sid": 25,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the first step, the decison tree filter is trained on the linguistic, discourse and coreference information annotated in the training corpus which is described in section 2.1.",
                    "sid": 26,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Data Coreference Analysis Preference Selection----1 r-----------1 ---------- 1 I 1 coreffilter I I I 1 1 I I I preference I igiii (\\ -\u00b7\u00a7i-iL2Ft:r i I I I N y I I I l l ------------ L----------J Figure 1: System outline The resolution system in Figure 1 applies the coreference filter (cf.",
                    "sid": 27,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "section 2.2) to all anaphor\u00ad candidate pairs (A; +C;j) found in the discourse his\u00ad tory.",
                    "sid": 28,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The detection of anaphoric expressions is out of the scope of this paper and just reduced to tags in our annotated corpus.",
                    "sid": 29,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Antecedent candidates are identified according to noun phrase part-of-speech tags.",
                    "sid": 30,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The reduced set (A; + C;k) forms the input of the preference algorithm which selects the most salient candidate Cp as described in section 2.3.",
                    "sid": 31,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Preliminary experiments are conducted for the task of pronominal anaphora resolution and the per\u00ad formance of our system is evaluated in section 3.",
                    "sid": 32,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.1 Data Corpus.",
                    "sid": 33,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our experiments we use the ATRITL Speech and Language Database (Takezawa et al., 1998) con\u00ad sisting of 500 Japanese spoken-language dialogs an\u00ad notated with coreferential tags.",
                    "sid": 34,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It includes nomi\u00ad nal, pronominal, and ellipsis annotations, whereby the anaphoric expressions used in our experiments are limited to those referring to nominal antecedents (nominal: 2160, pronominal: 526, ellipsis: 3843).",
                    "sid": 35,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Besides the anaphor type, we also include mor\u00ad phosyntactic information like stem form and inflec\u00ad tion attributes for each surface word as well as se\u00ad mantic codes for content words (Ohno and Haman\u00ad ishi, 1981) in this corpus.",
                    "sid": 36,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "rl: ;I;,IJ;O<i::-?",
                    "sid": 37,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": ".:\"l.(\u2022\u2022it\"o '-'71*'r!v\"t'.:\"l.(\\,\u2022JI:\"to [thank you very much] [City Hotel) \"Thank you for calling City Hotel.\" cl: Ll.., f/.ffi<P'll.'fi::l3\u2022\u2022i-til<, [hello) [I)[Hiroko Tanaka)[the name is) \"Hello, my name is Hiroko Tanaka.\"",
                    "sid": 38,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "-t\" 1:, t,Q)*'r !vQ)'H9 l..f \\,\u2022A.,\"t'Ti>'o [there) [hotel) [reservation][would like to have) \"I would like to make a reservation at your hotel.\" r2: :l5 jtQ)J5:tliiJQ);<.",
                    "sid": 39,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "JvalJiltit\"\"t'l..J: -?i>\u2022o [yottr) [name] [spelling) [can I have] \"Can you spell your name for me, please?",
                    "sid": 40,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "c2: li\u2022\u2022o 7\"1-.:r.-.:r.y:.:r.-Jr.{.:r.-\"t't\"o [yes) [T) [A] [N] [A] [K) [A] [be] '1t'sTANAKA.\" r3: li\u2022\u2022o +BI:::i:> :>I:.:\" J:fft\u2022\u2022-?::c-c.<.:\"l.(\\,\u2022;l:T o [yes] [tenth) [here] [arrival) [be) \"Okay, you will arrive here on the tenth, right?\"",
                    "sid": 41,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 2: Example dialog In the example dialog between the hotel reception (r) and a customer (c) listed in Figure 2 the proper noun (r1)\" 71*7Jv (City Hotel]\" is tagged as the antecedent of the pronoun (cl) \"-f (there]\" as well as the noun (c1) \"*T Jv (hotel]\".",
                    "sid": 42,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An exam\u00ad ple for ellipsis is the ommitted subject (c2)\"0[it]\" referring to (r2)\";l.\u00abJv (spelling]\".",
                    "sid": 43,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "According to the tagging guidelines used for our corpus an anaphoric tag refers to the most recent antecedent found in the dialog.",
                    "sid": 44,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, this an\u00ad tecedent might also refer to a previous one, e.g.",
                    "sid": 45,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(r3)\":: (here]\"-->(cl)\" -f[there]\"........",
                    "sid": 46,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(r1) \" 7 1 *7 Jv (City Hotel]\".",
                    "sid": 47,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, the transitive clo\u00ad sure between the anaphora and the first mention of the antecedent in the discourse history defines the set of positive examples, e.g.",
                    "sid": 48,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "( -f , 7 1 T Jv), whereas the nominal candidates outside the transi\u00ad tive closure are considered negative examples, e.g.",
                    "sid": 49,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "( -f , EB g:r), for coreferential relationships.",
                    "sid": 50,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Based on the corpus annotations we extract the frequency information of coreferential anaphor\u00ad antecedent pairs and non-referential pairs from the training data.",
                    "sid": 51,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each non-jcoreferential pair the occurrences of surface and stem form as well as se\u00ad mantic code combinations are counted.",
                    "sid": 52,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 1: Frequency data type a.na.phor candidate freq+ freq- ratio word\u00b7word gr; 7\"1'$7JV 6 0 1 i-J;t:, IB<P 0 111 ::J;t:, -tB 0 00.1 word sem ::J;t:, lshop} 33 33 0 semsem {demonstra.hvesJ {shopJ 51 18 0.48 In Table 1 some examples are given for pronoun anaphora, whereas the expressions \"{...}\" denote semantic classes assigned to the respective words.",
                    "sid": 53,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The values freq+, freq- and ratio and their usage are described in more detailed in section 2.3.",
                    "sid": 54,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, each dialog is subdivided into utter\u00ad ances consisting of one or more clauses.",
                    "sid": 55,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There\u00ad fore, distance features are available on the utterance, clause, candidate, and morpheme levels.",
                    "sid": 56,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For exam\u00ad ple, the distance values of the pronoun (r3)\".::.",
                    "sid": 57,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\"S [here)\" and the antecedent (rl)\" ';,!",
                    "sid": 58,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "T 1 j- Jv [City Hotel]\" in our sample dialog in Figure 2 are dutter=4, dclause=1, dcand=14, dmorph=40.",
                    "sid": 59,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.2 Coreference Analysis.",
                    "sid": 60,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To learn the coreference relations from our corpus An examination of our corpus gives rise to sus\u00ad picion that similarities to references in our training data might be useful for the identification of those antecedents.",
                    "sid": 61,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, we propose a preference se\u00ad lection scheme based on the combination of distance and frequency information.",
                    "sid": 62,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, utilizing statistical information about the frequency of coreferential anaphor-antecedent pairs (freq+) and non-referential pairs (freq-) extracted from the training data, we define the ratio of a given reference pair as follows4 : . { -6 : (freq+ = freq- = 0) ratzo = 1re +q _ 1 q \u00b7 otherwz.se we have chosen a C4.52 -like machine learning al\u00ad gorithm without pruning.",
                    "sid": 63,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The training attributes consist of lexical word attributes (surface word, stem form, part-of-speech, semantic code, morphological attributes) applied to the anaphor, antecedent can\u00ad didate, and clause predicate.",
                    "sid": 64,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition, features like attribute agreement, distance and frequency ra\u00ad tio are checked for each anaphor-candidate pair.",
                    "sid": 65,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The decision tree result consists of only two classes de\u00ad termining the coreference relation between the given anaphor-candidate pair.",
                    "sid": 66,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During anaphora resolution the decision tree is used as a module determining the coreferential prop\u00ad erty of each anaphor-candidate pair.",
                    "sid": 67,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each de\u00ad tected anaphoric expression a candidate list3 is cre\u00ad ated.",
                    "sid": 68,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The decision tree filter is then successively applied to all anaphor-candidate pairs.",
                    "sid": 69,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the decision tree results in the non-reference class, the candidate is judged as irrelevant and elim\u00ad inated from the list of potential antecedents forming the input of the preference selection algorithm.",
                    "sid": 70,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.3 Preference Selection.",
                    "sid": 71,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The primary order of candidates is given by their word distance from the anaphoric expression.",
                    "sid": 72,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A straightforward preference strategy we could choose is the selection ofthe most recent candidate (MRC) as the antecedent, i.e., the first element of the can\u00ad didate list.",
                    "sid": 73,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The success rate of this baseline test , however, is quite low as shown in section 3.",
                    "sid": 74,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But, this result does not mean that the recency factor is not important at aJl for the determination of saliency in this task.",
                    "sid": 75,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One reason for the bad per\u00ad formance is the application of the baseline test to the unfiltered set of-candidates resulting in the frequent selection of non-referential antecedents.",
                    "sid": 76,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Addition\u00ad ally, long-range references to candidates introduced first in the dialog are quite frequent in our data.",
                    "sid": 77,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 cf.",
                    "sid": 78,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(Quinlan, 1993) 3A list of noun phrase candidates preceding the anaphor element in the current discourse.",
                    "sid": 79,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "jreq+ + jreq \u00b7 The value of ratio is in the range of [-1, +1], whereby ratio = -1 in the case of exclusive non\u00ad referential relations and ratio = +1 in the case of exclusive coreferential relationships.",
                    "sid": 80,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In order for ref\u00ad erential pairs occurring in the training corpus with ratio = 0 to be preferred to those without frequency information, we slightly decrease the ratio value of the latter ones by a factor 6.",
                    "sid": 81,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As mentioned above the distance plays a crucial role in our selection method, too.",
                    "sid": 82,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We define a pref\u00ad erence value pref by normalizing the ratio value ac\u00ad cording to the distance dist given by the primary order of the candidates in the discourse.",
                    "sid": 83,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ratio pre I = dist The pre f value is calculated for each candidate and the precedence ordered list of candidates is resorted towards the maximization of the preference factor.",
                    "sid": 84,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similarly to the baseline test, the first element of the preferenced candidate list is chosen as the an\u00ad tecedent.",
                    "sid": 85,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The precedence order between candidates of the same confidence continues to remain so and thus a final decision is made in the case of a draw.",
                    "sid": 86,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The robustness of our approach is ensured by the definition of a backup strategy which ultimately se\u00ad lects one candidate occurring in the history in the case that all antecedent candidates are rejected by the decision tree filter.",
                    "sid": 87,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our experiments reported in section 3 we adopted the selection\u00b7of the dialog\u00ad initial candidate as the backup strategy.",
                    "sid": 88,
                    "ssid": 64,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "evaluation. ",
            "number": "3",
            "sents": [
                {
                    "text": "For the evaluation of the experimental results de\u00ad scribed in this section we use F-measure metrics cal\u00ad culated by the recall and precision of the system per\u00ad formance.",
                    "sid": 89,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Let l::t denote the total number of tagged 4 In order to keep the formula simple the frequency types are omitted (cf.",
                    "sid": 90,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 1) anaphor-antecedent pairs contained in the test data, Lt the number of these pairs passing the decision tree filter, and Lc the number of correctly selected antecedents.",
                    "sid": 91,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During evaluation we distinguish three classes: whether the correct antecedent is the first element of the candidate list (f), is in the candidate list (i), or is filtered out by the decision tree (o).",
                    "sid": 92,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The metrics F, recall (R) and precision (P) are defined as follows: L:=lfl ......................................................",
                    "sid": 93,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "F=2xPxR P+R L:=l!l+lil J L:=l!l+lil+lol 40.0L___ [ JL_ L_ 400_, ---' 100 200 300 t r a i n i n g s i z e ( d i a l o g ) In order to prove the feasibility of our approach we compare the four preference selection methods listed in Figure 3.",
                    "sid": 94,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "tagged corpus Figure 3: Preference selection experiments First, the baseline test MRC selects the most re\u00ad cent candidate as the antecedent of an anaphoric ex\u00ad pression.",
                    "sid": 95,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The necessity of the filter and preference selection components is shown by comparing the de\u00ad cision tree filter scheme DT (i.e., select the first el\u00ad ement of the filtered candidate list) and preference scheme PREF (i.e., resort the complete candidate list) against our combined method DT+PREF (i.e., resort the filtered candidate list).",
                    "sid": 96,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5-way cross-validation experiments are conducted for pronominal anaphora resolution.",
                    "sid": 97,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The selected antecedents are checked against the annotated cor\u00ad rect antecedents according to their morphosyntactic and semantic attributes.",
                    "sid": 98,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.1 Training Size.",
                    "sid": 99,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use varied numbers of training dialogs (50400) for the training of the decision tree and the extrac\u00ad tion of the frequency information from the corpus.",
                    "sid": 100,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Open tests are conducted on 100 non-training dialogs whereas closed tests use the training data for evalua\u00ad tion.",
                    "sid": 101,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results of the different preference selection methods are shown in Figure 4.",
                    "sid": 102,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The baseline test MRC succeeds in resolving only 43.9% of the most recent candidates correctly as the antecedent.",
                    "sid": 103,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The best F-measure rate for DT is 65.0% and for PREF the best rate is 78.1% whereas Figure 4: Training size versus performance the combination of both methods achieves a success rate of 80.6%.",
                    "sid": 104,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The PREF method seems to reach a plateau at around 300 dialogs which is borne out by the closed test reaching a maximum of 81.1%.",
                    "sid": 105,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Comparing the recall rate of DT (61.2%) and DT+PREF (75.9%) with the PREF result, we might conclude that the decision tree is not much of a help due to the side\u00ad effect of 11.8% of the correct antecedents being fil\u00ad tered out.",
                    "sid": 106,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, in contrast to the PREF algorithm, the DT method improves continuously according to the training size implying a lack of training data for the identification of potential candidates.",
                    "sid": 107,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Despite the sparse data the filtering method proves to be very effective.",
                    "sid": 108,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The average number of all candidates (his\u00ad tory) for a given anaphor in our open data is 39 can\u00ad didates which is reduced to 11 potential candidates by the decision tree filter resulting in a reduction rate of 71.8% (closed test: 81%).",
                    "sid": 109,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The number of trivial selection cases (only one candidate) increases from 2.7% (history) to 11.4% (filter; closed test: 21%).",
                    "sid": 110,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "On average, two candidates are skipped in the his\u00ad tory to select the correct antecedent.",
                    "sid": 111,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, the precision rates of DT (69.4%) and DT+PREF (86.0%) show that the utilization of the decision tree filter in combination with the statisti\u00ad cal preference selection gains a relative improvement of 9% towards the preference and 16% towards the filter method.",
                    "sid": 112,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additionally, the system proves to be quite robust, because the decision tree filters out all candidates in only 1% of the open test samples.",
                    "sid": 113,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Selecting the candidate first introduced in the dialog as a backup strategy shows the best performance due to the fre\u00ad quent dialog initial references contained in our data.",
                    "sid": 114,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 2: Frequency and distance dependency DT DT-no-dist DT-no-freq DT+PREF DT+PREF-no-dist recall precision F-measure 61.2 69.4 65.0 60.1 68.7 64.1 53.6 64.5 58.5 75.9 86.0 80.6 73.0 82.8 77.6 (filtered-out) 11.8 12.5 16.9 11.8 11.8 3.2 Feature Dependency.",
                    "sid": 115,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our approach frequency ratio and distance infor\u00ad mation plays a crucial role not only for the identi\u00ad fication of potential candidates during decision tree filtering, but also for the calculation of the prefer\u00ad ence value for each antecedent candidate.",
                    "sid": 116,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the first case these features are used indepen\u00ad dently to characterize the training samples whereas the preference selection method is based on the de\u00ad pendency between the frequency and distance values of the given anaphor-candidate pair in the context of the respective discourse.",
                    "sid": 117,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The relative importance of each factor is shown in Table 2.",
                    "sid": 118,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we compare our decision tree filter DT to those methods that do not use either frequency (DT\u00ad no-freq) or distance ( D T -no-dist) information.",
                    "sid": 119,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Fre\u00ad quency information does appear to be more relevant for the identification of potential candidates than distance features extracted from the training corpus.",
                    "sid": 120,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The recall performance of DT-no-freq decreases by 7.6% whereas DT-no-dist is only 1.1% below the re\u00ad sult of the original DT filter5 . Moreover, the number of correct antecedents not passing the filter increases by 5.1% (DT-no-freq) and 0.7% (DT-no-dist).",
                    "sid": 121,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the distance factor proves to be quite important as a preference criterion.",
                    "sid": 122,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Relying only on the frequency ratio as the preference value, the re\u00ad call performance of DT+PREF-no-dist is only 73.0%, down 2.9% of the original DT+PREF method.",
                    "sid": 123,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The effectiveness of our approach is not only based on the usage of single antecedent indicators ex\u00ad tracted from the corpus, but also on the combination of these features for the selection of the most prefer\u00ad able candidate in the context of the given discourse.",
                    "sid": 124,
                    "ssid": 36,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "related research. ",
            "number": "4",
            "sents": [
                {
                    "text": "Due to the characteristics of the underlying data used in these experiments a comparison involving absolute numbers to previous approaches gives us less evidence.",
                    "sid": 125,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the difficulty of our task can be verified according to the baseline experiment 5So far we have considered the decision tree filter just as a black-box tool.",
                    "sid": 126,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Further investigations on tree struc\u00ad tures, however, should give us more evidence about the relative importance of the respective features.",
                    "sid": 127,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).",
                    "sid": 128,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Resolving pro\u00ad nouns in English technical manuals to the most re\u00ad cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an\u00ad tecedent (cf.",
                    "sid": 129,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "section 3).",
                    "sid": 130,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen\u00ad sive in the cost of human effort at development time and limited ability to scale to new domains, more re\u00ad cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.",
                    "sid": 131,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similarly to them we do not use any sentence parsing or structural analysis, but just rely on morphosyn\u00ad tactic and semantic word information.",
                    "sid": 132,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Moreover, clues are used about the grammatical and pragmatic functions of expressions as in (Grosz et al., 1995), (Strube, 1998), \u00b7or (Azzam et al., 1998) as well as rule-based empirical approaches like (Nakaiwa and Shirai, 1996) or (Murata and Nagao, 1997), to determine the most salient referent.",
                    "sid": 133,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These kinds of manually defined scoring heuristics, how\u00ad ever, involve quite an amount of human intervention which is avoided in machine learning approaches.",
                    "sid": 134,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As briefly noted in section 1, the work described in (Conolly et al., 1994) and (Aone and Bennett, 1995) differs from our approach according to the us\u00ad age of the decision tree in the resolution task.",
                    "sid": 135,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In (Conolly et al., 1994) a decision tree is trained on a small number of 15 features concerning anaphor type, grammatical function, recency, morphosyntac\u00ad tic agreement and subsuming concepts.",
                    "sid": 136,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given two anaphor-candidate pairs the system judges which is \"better\".",
                    "sid": 137,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, due to the lack of a strong assumption on \"transitivity\" this sorting algorithm may be unable to find the \"best\" solution.",
                    "sid": 138,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Based on discourse markers extracted from lexical, syntactic, and semantic processing, the approach of (Aone and Bennett, 1995) uses 66 unary and bi\u00ad nary attributes (lexical, syntactic, semantic, posi\u00ad tion, matching category, topic) during decision tree training.",
                    "sid": 139,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The confidence values returned from the pruned decision tree are utilized as a saliency mea\u00ad sure for each anaphor-candidate pair in order to se lect a single antecedent.",
                    "sid": 140,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, we use depen\u00ad dency factors for preference selection which cannot be learned automatically because of the indepen\u00ad dent learning of specific features during decision tree training.",
                    "sid": 141,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, our decision tree is not applied directly to the task of preference selection, but only used as a filter to reduce the number of potential candidates for preference selection.",
                    "sid": 142,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In addition to salience preference, a statistically modeled lexical preference is exploited in (Dagan et al., 1995) by comparing the conditional probabili\u00ad ties of co-occurrence patterns given the occurrence of candidates.",
                    "sid": 143,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Experiments, however, are carried out on computer manual texts with mainly intra\u00ad sentential references.",
                    "sid": 144,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This kind of data is also char\u00ad acterized by the avoidance of disambiguities and only short discourse units, which prohibits almost any long-range references.",
                    "sid": 145,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast to this re\u00ad search, our results show that the distance factor in addition to corpus-based frequency information is quite relevant for the selection of the most salient candidate in our task.",
                    "sid": 146,
                    "ssid": 22,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusion. ",
            "number": "5",
            "sents": [
                {
                    "text": "In this paper we proposed a corpus-based anaphora resolution method combining an automatic learning algorithm for coreferential relationships with statis\u00ad tical preference selection in the discourse context.",
                    "sid": 147,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We proved the applicability of our approach to pro\u00ad noun resolution achieving a resolution accuracy of 86.0% (precision) and 75.9% (recall) for Japanese pronouns despite the limitation of sparse data.",
                    "sid": 148,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Im\u00ad provements in these results can be expected by in\u00ad creasing the training data as well as utilizing more sophisticated linguistic knowledge (structural anal\u00ad ysis of utterances, etc.) and discourse information (extra-sentential knowledge, etc.) which should lead to a rise of the decision tree filter performance.",
                    "sid": 149,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Preliminary experiments with nominal reference and ellipsis resolution showed promising results, too.",
                    "sid": 150,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We plan to incorporate this approach in multi\u00ad lingual machine translation which enables us to han\u00ad dle a variety of referential relations in order to im\u00ad prove the translation quality.",
                    "sid": 151,
                    "ssid": 5,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "acknowledgement",
            "number": "",
            "sents": [
                {
                    "text": "We would like to thank Hitoshi Nishimura (ATR) for his programming support and Hideki Tanaka (ATR) for helpful personal communications.",
                    "sid": 152,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}