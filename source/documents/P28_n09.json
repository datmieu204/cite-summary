{
    "ID": "P28_n09",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "We present a novel method to improve word alignment quality and eventoally the translation performance by producing and combining complementary word align\u00ad ments for low-resource languages.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead of focusing on the improvement of a single set of word alignments, we generate mul\u00ad tiple sets of diversified alignments based on different motivations, such as linguis\u00ad tic knowledge, morphology and heuris\u00ad tics.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We demonstrate this approach on an English-to-Pashto translation task by com\u00ad bining the alignments obtained from syn\u00ad tactic reordering, stemming, and partial words.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The combined alignment outper\u00ad forms the baseline alignment, with signif\u00ad icantly higher F-scores and better transla\u00ad tion performance.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Word alignment usually serves as the starting point and foundation for a statistical machine translation (SMT) system.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It has received a signif\u00ad icant amount of research over the years, notably in (Brown eta!., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007; Hermjakob, 2009).",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They all focused on the improvement of word alignment models.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this work, we leverage existing align\u00ad ers and generate multiple sets of word alignments based on complementary information, then com\u00ad bine them to get the final alignment for phrase training.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The resource required for this approach is little, compared to what is needed to build a rea\u00ad sonable discriminative alignment model, for ex\u00ad ample.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1bis makes the approach especially ap\u00ad pealing for SMT on low-resource languages.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Most of the research on alignment combination in the past has focused on how to combine the alignments from two different directions, source\u00ad to-target and target-to-source.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Usually people start from the intersection of two sets of alignments, and gradually add links in the union based on certain heuristics, as in (Koehn et a!., 2003), to achieve a better balance compared to using either intersection {high precision) or union {high recall).",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In{Ayan and Dorr, 2006) a maximum entropy ap\u00ad proach was proposed to combine multiple align\u00ad ments based on a set of linguistic and alignment features.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A different approach was presented in (Deng and Zhou, 2009), which again concentrated on the combination of two sets of alignments, but with a different criterion.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It tries to maximize the number of phrases that can be extracted in the combined alignments.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A greedy search method was utilized and it achieved higher translation per\u00ad formance than the baseline.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More recently, an alignment selection approach was proposed in (Huang, 2009), which com\u00ad putes confidence scores for each liuk and prones the links from multiple sets of alignments using a handpicked threshold.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The alignments used in that work were generated from different align\u00ad ers {HMM, block model, and maximum entropy model).",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There is no need for a predetermined threshold as used in (Huang, 2009).",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Also, we utilize var\u00ad ious knowledge sources to eurich the alignments instead of using different aligners.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our strategy is to diversify and then combine in order to catch any complementary information captured in the word alignments for low-resource languages.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The rest of the paper is organized as follows.",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "22 Proceedings uf the ACL 2010 C01'!{erence Short Papers, pages 2226, Uppsala, Sweden, 1116 July 2010.",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "@2010 Association for Computational Unguistics We present three different sets of alignments in Section 2 for an English-to-Pashto MT task.",
                    "sid": 25,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Section 3, we propose the alignment combination algorithm.",
                    "sid": 26,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The experimental results are reported in Section 4.",
                    "sid": 27,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We conclude the paper in Section 5.",
                    "sid": 28,
                    "ssid": 28,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "diversified word alignments. ",
            "number": "2",
            "sents": [
                {
                    "text": "We take an English-to-Pashto MT task as an exam\u00ad ple and create three sets of additional alignments on top of the baseline alignment.",
                    "sid": 29,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.1 Syntactic Reordering.",
                    "sid": 30,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pashto is a subject-object-verb (SOV) language, which puts verbs after objects.",
                    "sid": 31,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "People have pro\u00ad posed different syntaetic rules to pre-reorder SOY languages, either based on a constituent parse tree (Dn\\bek and Yarowsky, 2004; Wang eta!., 2007) or dependency parse tree (Xu eta!., 2009).",
                    "sid": 32,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this work, we apply syntactic reordering for verb phrases (VP) based on the English constituent parse.",
                    "sid": 33,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The VP-based reordering rule we apply in the work is: where VBuepresents VB, VBD,VBG, VBN, VBPandVBZ.",
                    "sid": 34,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Figure I, we show the reference alignment between an English sentence and the correspond\u00ad ing Pashto translation, where E is the original En\u00ad glish sentence, P is the Pashto sentence (in ro\u00ad manized text), and E' is the English sentence after reordering.",
                    "sid": 35,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As we can see, after the VP-based re\u00ad ordering, the alignment between the two sentences becomes monotone, which makes it easier for the aligner to get the alignment correct.",
                    "sid": 36,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During the reordering of English sentences, we store the in\u00ad dex changes for the English words.",
                    "sid": 37,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "After getting the alignment trained on the reordered English and original Pashto sentence pairs, we map the English words back to the original order, along with the learned alignment links.",
                    "sid": 38,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this way, the align\u00ad ment is ready to be combined with the baseline alignment and any other alternatives.",
                    "sid": 39,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.2 Stemming.",
                    "sid": 40,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Pashto is one of the morphologically rich lan\u00ad guages.",
                    "sid": 41,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Inaddition to the linguistic koowledge ap\u00ad plied in the syntaetic reordering described above, we also utilize morphological analysis by applying stemming on both the English and Pashto sides.",
                    "sid": 42,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For English, we use Porter stemming (Porter, \u2022 Figure 1: Alignment before/after VP-based re\u00ad ordering.",
                    "sid": 43,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1980), a widely applied algorithm to remove the common morphological and inflexional endings from words in English.",
                    "sid": 44,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For Pashto, we utilize a morphological decompostion algorithm that has been shown to be effective for Arabic speech recognition (Xiang et a!., 2006).",
                    "sid": 45,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We start from a fixed set of affixes with 8 prefixes and 21 suffixes.",
                    "sid": 46,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The prefixes and suffixes are stripped off from the Pashto words under the two constraints:(!)",
                    "sid": 47,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Longest matched affixes first; (2) Remaining stem must be at least two characters long.",
                    "sid": 48,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.3 Partial Word.",
                    "sid": 49,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For low-resource languages, we usually suffer from the data sparsity issue.",
                    "sid": 50,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.",
                    "sid": 51,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is similar to the stenuning method, but is more heoristics\u00ad based, and does not rely on a set of available af\u00ad fixes.",
                    "sid": 52,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "With the same motivation, we keep the first 4 characters of each English and Pashto word to generate one more alternative for the word align\u00ad ment.",
                    "sid": 53,
                    "ssid": 25,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "confidence-based alignment. ",
            "number": "3",
            "sents": [
                {
                    "text": "Combination Now we describe the algorithm to combine mul\u00ad tiple sets of word alignments based on weighted confidence scores.",
                    "sid": 54,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Suppose a;;k is an alignment link in the i-th set of alignments between the j -th source word and k-th target word in sentence pair (S,T).",
                    "sid": 55,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similar to (Huang, 2009), we define the confidence of a;;k as c(a;;kiS,T) = Vq\u20222t(a;;kiS,T)qt2s(a;;kiT, 8), (1) where the source-to-target link posterior probabil\u00ad ity and the target-to-source link posterior probability qt2s(<>i;kiT, S) is defined similarly.",
                    "sid": 56,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "p;(tkis;) is the lexical translation probability between source word s; and target word tk in the i-th set of align\u00ad ments.",
                    "sid": 57,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our alignment combination algorithm is as fol\u00ad lows.",
                    "sid": 58,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1.",
                    "sid": 59,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each candidate link a;k gets soft votes from.",
                    "sid": 60,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "N sets of alignments via weighted confidence scores: N v(a;kiS,T) = Lw; oc(a;;kiS,T), (3) i=l where the weight w; for each set of alignment can be optimized under various criteria.",
                    "sid": 61,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this work, we tune it on a hand-aligned de\u00ad velopment set to maximize the alignment F\u00ad score.",
                    "sid": 62,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.",
                    "sid": 63,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All candidates are sorted by soft votes in de\u00ad.",
                    "sid": 64,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "scending order and evaluated sequentially.",
                    "sid": 65,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A candidate link a;k is included if one of the following is true: \u2022 Neither s; nor tk is aligned so far; \u2022 s1 is not aligned and its left or right neighboring word is aligned to tk so far; \u2022 tk is not aligned and its left or right neighboring word is aligned to s1 so far.",
                    "sid": 66,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.",
                    "sid": 67,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Repeat scanning all candidate links until no.",
                    "sid": 68,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "more links can be added.",
                    "sid": 69,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this way, those alignment links with higher confidence scores have higher priority to be in\u00ad cluded in the combined alignment.",
                    "sid": 70,
                    "ssid": 17,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "experiments. ",
            "number": "4",
            "sents": [
                {
                    "text": "4.1 Baseline.",
                    "sid": 71,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our training data contains around 70K English\u00ad Pashto sentence pairs released under the DARPA TRANSTAC project, with about 900K words on the English side.",
                    "sid": 72,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The baseline is a phrase-based MT system similar to (Koehn et a!., 2003).",
                    "sid": 73,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf).",
                    "sid": 74,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The decoding weights are optimized with minimum error rate training (MERT) (Och, 2003) to maximize BLEU scores (Papineni eta!., 2002).",
                    "sid": 75,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are 2028 sen\u00ad tences in the toning set and I019 sentences in the test set, both with one reference.",
                    "sid": 76,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use another !50 sentence pairs as a heldout hand-aligned set to measure the word alignment quality.",
                    "sid": 77,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The three sets of alignments described in Section 2 are gen\u00ad erated on the same training data separately with GIZA++ and enhanced by gdf as for the baseline alignment.",
                    "sid": 78,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997).",
                    "sid": 79,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.2 Improvement in Word Alignment.",
                    "sid": 80,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Table I we show the precision, recall and F\u00ad score of each set of word alignments for the !50- sentence set.",
                    "sid": 81,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Using partial word provides the high\u00ad est F-score among all individual alignments.",
                    "sid": 82,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The F-score is 5% higher than for the baseline align\u00ad ment.",
                    "sid": 83,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The VP-based reordering itself does not im\u00ad prove the F-score, which could be due to the parse errors on the conversational training data.",
                    "sid": 84,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We ex\u00ad periment with three options {co, c1.",
                    "sid": 85,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "c2) when com\u00ad bining the baseline and reordering-based align\u00ad ments.",
                    "sid": 86,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Inco, the weights w; and confidence scores c(a;;kiS,T) in Eq.",
                    "sid": 87,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(3) are all set to 1.",
                    "sid": 88,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In c1, we set confidence scores to I, while toning the weights with hill climbing to maximize the F\u00ad score on a hand-aligned toning set.",
                    "sid": 89,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In c2 , we com\u00ad pute the confidence scores as in Eq. {I) and tune the weights as in c1.",
                    "sid": 90,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The numbers in Table I show the effectiveness of having both weights and con\u00ad fidence scores during the combination.",
                    "sid": 91,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similarly, we combine the baseline with each of the other sets of alignments using c2.",
                    "sid": 92,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They all result in significantly higher F-scores.",
                    "sid": 93,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also generate alignments on VP-reordered partial words (X in Table I) and compared B +X and B + V + P. The better results with B + V + P show the benefit of keeping the alignments as di\u00ad versified as possible before the combination.",
                    "sid": 94,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Fi\u00ad nally, we compare the proposed alignment combi\u00ad nation c2 with the heoristics-based method (gdj), where the latter starts from the intersection of all 4 sets of alignments and then applies grow-diagonal\u00ad final (Koehn et a!., 2003) based on the links in the union.",
                    "sid": 95,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The proposed combination approach on B + V + S + P results in close to 7% higher F\u00ad scores than the baseline and also 2% higher than gdf We also notice that its higher F-score is I Alignment I Comb I Links I Phrase I BLEU I mainly due to the higher precision, which should Baseline 963K 565K 12.67 result from the consideration of confidence scores.",
                    "sid": 96,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "v 965K 624K 12.82 I Alignment I Comb I p I R I F I s 915K 692K 13.04 p 906K 716K 13.30 Baseline 0.6923 0.6414 0.6659 X 911K 689K 13.00 v 0.6934 0.6388 0.6650 B+V co 870K 890K 13.20 s 0.7376 0.6495 0.6907 B+V Cl 865K 899K 13.32 p 0.7665 0.6643 0.7118 B+V C2 874K 879K 13.60 X 0.7615 0.6641 0.7095 B+S C2 864K 948K 13.41 B+V co 0.7639 0.6312 0.6913 B+P C2 863K 942K 13.40 B+V Cl 0.7645 0.6373 0.6951 B+X C2 871K 905K 13.37 B+V C2 0.7895 0.6505 0.7133 B+V+P C2 880K 914K 13.60 B+S C2 0.7942 0.6553 0.7181 B+V+S+P cat 3749K 1258K 13.01 B+P C2 0.8006 0.6612 0.7242 B+V+S+P gdf 1021K 653K 13.14 B+X C2 0.7827 0.6670 0.7202 B+V+S+P C2 907K 771K 13.73 B+V+P C2 0.7912 0.6755 0.7288B+V+S+P gdf 0.7238 0.7042 0.7138 Table 2: Improvement in BLEU scores (B: base B+V+S+P C2 0.7906 0.6852 0.7342 line; V: VPhased reordering; S: stemming; P: partial word; X: VP-reordered partial word).",
                    "sid": 97,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table I: Alignment precision, recall and F-score (B: baseline; V: VP-based reordering; S: stem\u00ad ming; P: partial word; X: VP-reordered partial word).",
                    "sid": 98,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.3 Improvement in MT Performance.",
                    "sid": 99,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Table 2, we show the corresponding BLEU scores on the test set for the systems built on each set of word alignment in Table I. Similar to the observation from Table I, c2 outperforms co and C1> and B + V + S + P with c2 outperforms B + V + S + P with gdf We also ran one ex\u00ad perinlent in which we concatenated all 4 sets of alignments into one big set (shown as cat).",
                    "sid": 100,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Over\u00ad all, the BLEU score with confidence-based com\u00ad bination was increased by Ipoint compared to the baseline, 0.6 compared to gdf, and 0.7 compared to cat.",
                    "sid": 101,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All results are statistically significant with p < 0.05 using the sign-test described in (Collins eta!., 2005).",
                    "sid": 102,
                    "ssid": 32,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}