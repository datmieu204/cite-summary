{
    "ID": "D12-1046",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previous work often used a pipeline method \u2013 Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "For Asian languages such as Japanese and Chinese that do not contain explicitly marked word boundaries, word segmentation is an important first step for many subsequent language processing tasks, such as POS tagging, parsing, semantic role labeling, and various applications.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previous studies for POS tagging and syntax parsing on these languages sometimes assume that gold standard word segmentation information is provided, which is not the real scenario.",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In a fully automatic system, a pipeline approach is often adopted, where raw sentences are first segmented into word sequences, then POS tagging and parsing are performed.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This kind of approach suffers from error propagation.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, word segmentation errors will result in tagging and parsing errors.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Additionally, early modules cannot use information from subsequent modules.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Intuitively a joint model that performs the three tasks together should help the system make the best decisions.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we propose a unified model for joint Chinese word segmentation, POS tagging, and parsing.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Three sub-models are independently trained using the state-of-the-art methods.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We do not use the joint inference algorithm for training because of the high complexity caused by the large amount of parameters.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use linear chain Conditional Random Fields (CRFs) (Lafferty et al., 2001) to train the word segmentation model and POS tagging model, and averaged perceptron (Collins, 2002) to learn the parsing model.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "During decoding, parameters of each sub-model are scaled to represent its importance in the joint model.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our decoding algorithm is an extension of CYK parsing.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Initially, weights of all possible words together with their POS tags are calculated.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When searching the parse tree, the word and POS tagging features are dynamically generated and the transition information of POS tagging is considered in the span merge operation.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Experiments are conducted on Chinese Tree Bank (CTB) 5 dataset, which is widely used for Chinese word segmentation, POS tagging and parsing.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We compare our proposed joint model with the pipeline system, both built using the state-of-the-art sub- models.",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also propose an evaluation metric to 501 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 501\u2013511, Jeju Island, Korea, 12\u201314 July 2012.",
                    "sid": 24,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Qc 2012 Association for Computational Linguistics word segmentation errors.",
                    "sid": 25,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our experimental results show that the joint model significantly outperforms the pipeline method based on the state-of-the-art sub-models.",
                    "sid": 26,
                    "ssid": 26,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "related work. ",
            "number": "2",
            "sents": [
                {
                    "text": "There is very limited previous work on joint Chinese word segmentation, POS tagging, and parsing.",
                    "sid": 27,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al., 2010), cascaded linear model (Jiang et al., 2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), reranking (Jiang et al., 2008b).",
                    "sid": 28,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These joint models form joint decoding using them.",
                    "sid": 29,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section, we first describe the three sub-models and then the joint decoding algorithm.",
                    "sid": 30,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Methods for Chinese word segmentation can be broadly categorized into character based and word based models.",
                    "sid": 31,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previous studies showed that character-based models are more effective to detect out-of-vocabulary words while word-based models are more accurate to predict in-vocabulary words (Zhang et al., 2006).",
                    "sid": 32,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here, we use order-0 semi- Markov model (Sarawagi and Cohen, 2004) to take advantages of both approaches.",
                    "sid": 33,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More specifically, given a sentence x = showed about 0.2 \u2212 1% F-score improvement over c , c , . . .",
                    "sid": 34,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", c (where c is the ith Chinese character, the pipeline method.",
                    "sid": 35,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recently, joint tagging and de1 2 l i pendency parsing has been studied as well (Li et al., 2011; Lee et al., 2011).",
                    "sid": 36,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009).",
                    "sid": 37,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In (Jiang et al., 2009), additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing task using the pipeline framework.",
                    "sid": 38,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).",
                    "sid": 39,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew.",
                    "sid": 40,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Different from that work, we use a discriminative model, which benefits from large amounts of features and is easier to deal with unknown words.",
                    "sid": 41,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Another main difference is that, besides segmentation and parsing, we also incorporate the POS tagging model into the CYK parsing framework.",
                    "sid": 42,
                    "ssid": 16,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "methods. ",
            "number": "3",
            "sents": [
                {
                    "text": "For a given Chinese sentence, our task is to generate the word sequence, its POS tag sequence, and the parse tree (constituent parsing).",
                    "sid": 43,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A joint model is expected to make more optimal decisions than a pipeline approach; however, such a model will be too complex and it is difficult to estimate model parameters.",
                    "sid": 44,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore we do not perform joint inference for training.",
                    "sid": 45,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Instead, we develop three individl is the sentence length), the character-based mod el assigns each character with a word boundary tag.",
                    "sid": 46,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Here we use the BCDIES tag set, which achieved the best official performance (Zhao and Kit, 2008): B, C, D, E denote the first, second, third, and last character of a multi-character word respectively, I denotes the other characters, and S denotes the single character word.",
                    "sid": 47,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the same character- based feature templates as in the best official system, shown in Table 1 (1.11.3), including character unigram and bigram features, and transition features.",
                    "sid": 48,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Linear chain CRFs are used for training.",
                    "sid": 49,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Feature templates in the word-based model are shown in Table 1 (1.41.6), including word features, sub-word features, and character bigrams within words.",
                    "sid": 50,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The word feature is activated if a predicted word w is in the vocabulary (i.e., appears in training data).",
                    "sid": 51,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Subword(w) is the longest in-vocabulary word within w. To use word features, we adopt a K- best reranking approach.",
                    "sid": 52,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The top K candidate segmentation results for each training sample are generated using the character-based model, and the gold segmentation is added if it is not in the candidate set.",
                    "sid": 53,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the Maximum Entropy (ME) model to learn the weights of word features such that the probability of the gold candidate is maximal.",
                    "sid": 54,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A problem arises when combining the two models and using it in joint segmentation and parsing, since the linear chain used in the character-based model is incompatible with CYK parsing model and the word-based model due to the transition informa (1.1) ci\u22122 yi , ci\u22121 yi , ci yi , ci+1 yi , ci+2 yi (1.2) ci\u22121 ci yi , ci ci+1 yi , ci\u22121 ci+1 yi (1.3) yi\u22121 yi Word Level Feature Templates (1.4) word w (1.5) subword(w) (1.6) character bigrams within w Table 1: Feature templates for word segmentation.",
                    "sid": 55,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ci is the ith character in the sentence, yi is its label, w is a predicted word.",
                    "sid": 56,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "tion.",
                    "sid": 57,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Thus, we slightly modify the linear chain CRF- s by fixing the weights of transition features during training and testing.",
                    "sid": 58,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, weights of impossible transition features (e.g., B\u2192B) are set to \u2212\u221e, and weights of the other transition features (e.g., E\u2192B) are set to 0.",
                    "sid": 59,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this way, the transition feature could be neglected in testing for two reasons.",
                    "sid": 60,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, all illegal label assignments are prohibited in prediction, s but no syntax annotations, such as the People\u2019s Daily corpus and SIGHAN bakeoff corpora (Jin and Chen, 2008).",
                    "sid": 61,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Such data can only be used to train POS tag- gers, but not for training the parsing model.",
                    "sid": 62,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Often using a larger training set will result in a better POS tagger.",
                    "sid": 63,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, the state-of-the-art POS tagging systems are often trained by sequence labeling models, not parsing models.",
                    "sid": 64,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u22122 i i Table 2: Feature templates for POS tagging.",
                    "sid": 65,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "wi is the ith word in the sentence, ti is its POS tag.",
                    "sid": 66,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For a word w, ince their weights are \u2212\u221e; second, because weights cj (w) is its jth character, c\u2212j (w) is the last j character, of legal transition features are 0, they do not affect the prediction at all.",
                    "sid": 67,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the following, transition features are excluded.",
                    "sid": 68,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Now we can use order-0 semi Markov model as the hybrid model.",
                    "sid": 69,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We define the score of a word as the sum of the weights of all the features within the word.",
                    "sid": 70,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Formally, the score of a multi-character word w = ci , . . .",
                    "sid": 71,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", cj is defined as: scoreseg (x, i, j) = \u03b8C RF \u00b7 fC RF (x, yi = B) + . . .",
                    "sid": 72,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "+\u03b8C RF \u00b7 fC RF (x, yj = E) + \u03b8M E \u00b7 fM E (x, i, j) \u2261 \u03b8seg fseg (x, i, j) (1) where fC RF and fM E are the feature vectors in the character and word based models respectively, and \u03b8C RF , \u03b8M E are their corresponding weight vectors.",
                    "sid": 73,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For simplicity, we denote \u03b8seg = \u03b8C RF \u2295M E , fseg = fC RF \u2295M E , where \u03b8C RF \u2295M E means the concatenation of \u03b8C RF and \u03b8M E . Scores for single character words are defined similarly.",
                    "sid": 74,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These word scores will be used in the joint segmentation and parsing task Section 3.4.",
                    "sid": 75,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.2 POS Tagging Model.",
                    "sid": 76,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Though syntax parsing model can directly predict the POS tag itself, we choose not to use this, but use an independent POS tagger for two reasons.",
                    "sid": 77,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, and l(w) is its length.",
                    "sid": 78,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The POS tagging problem is to assign a POS tagt \u2208 T to each word in a sentence.",
                    "sid": 79,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also use lin ear chain CRFs for POS tagging.",
                    "sid": 80,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Feature templates shown in Table 2 are the same as those in (Qian et al., 2010), which have been shown effective on CTB corpus.",
                    "sid": 81,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Three feature sets are considered: (i) word level features, including surrounding word uni- grams, bigrams, and word length; (ii) character level features, such as the first and last characters in the words; (iii) transition features.",
                    "sid": 82,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.3 Parsing Model.",
                    "sid": 83,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We choose discriminative models for parsing since it is easy to handle unknown words by simply adding character level features.",
                    "sid": 84,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008).",
                    "sid": 85,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this study, we use averaged perceptron algorithm for parameter estimation since it is easier to implement and has competitive performance.",
                    "sid": 86,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A Context Free Grammar (CFG) consists of (i) a set of terminals; (ii) a set of nonterminals {N k }; (i ii) a designated start symbol ROOT; and (iv) a set of rules, {r = N i \u2192 \u03b6 j }, where \u03b6 j is a sequence ofterminals and nonterminals.",
                    "sid": 87,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the parsing task, ter N P NR NN NR NN Shanghai customs Chongming office CP top state IP VP bottom state N P VV NT CP VP NP VV NT VV Last year realized N P Last year realized NR Shanghai NN_NR NN NR_NN Figure 2: Unary rule normalization.",
                    "sid": 88,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Nonterminal-yield unary chains are collapsed to single unary rules.",
                    "sid": 89,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Identity unary rules are added to spans that have no unary rule.",
                    "sid": 90,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "customs NR NN Chongming office tom states; (ii) top state features ftop (i, j, x, N i,j ); (iii) unary rule features funary (i, j, x, runary ), which extract the transition information from bottom s Figure 1: Parse tree binarization tates to top states; (iv) binary rule features fbinary (i, j, k, x, rbinary = N i,j \u2192 N i,k \u22121 + N k,r ), minals are the words, and nonterminals are the POS tags and phrase types.",
                    "sid": 91,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, nonterminal is named state for short.",
                    "sid": 92,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A parse tree T of sentence x can be factorized into several one-level subtrees, where N i,k\u22121, N k,r are the top states of the left and right children.",
                    "sid": 93,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The score function for a sentence x with parse tree T is defined as: score(x, T ) = each corresponding to a rule r. In practice, binarization of rules is necessary to \u03b8bottom \u00b7 fbottom (i, j, x, N i,j ) obtain cubic parsing time.",
                    "sid": 94,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, the right hand N i,j \u2208T side of each rule should contain no more than 2 states.",
                    "sid": 95,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used right branching binarization, as illustrated in Figure 1.",
                    "sid": 96,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We did not use parent annotation, since we found it degraded the performance + \u2211 N i,j \u2208T + \u2211 unary \u03b8top \u00b7 ftop (i, j, x, N i,j ) \u03b8unary \u00b7 funary (i, j, x, runary ) in our experiments (shown in Section 4).",
                    "sid": 97,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We used the same preprocessing step as (Harper and Huang, 2009), collapsing all the allowed nonterminal-yield unary chains to single unary rules.",
                    "sid": 98,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, all s ri,j \u2208T + \u2211 i,j,k \u2208T \u03b8binary \u00b7 fbinary (i, j, x, rbinary ) pans in the binarized trees contain no more than one unary rules.",
                    "sid": 99,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To facilitate decoding, we unify the form of spans so that each span contains exactly one u- nary rule.",
                    "sid": 100,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is done by adding identity unary rules where \u03b8bottom , \u03b8top , \u03b8unary , \u03b8binary are the weight vectors of the four feature sets.",
                    "sid": 101,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given the training corpus {(xi , T\u02dc )}, the learning task is to estimate the weight vectors so that for each (N \u2192 N ) to spans that have no unary rule.",
                    "sid": 102,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These sentence xi , the gold standard tree T\u02dc achieves the identity unary rules will be removed in evaluation.",
                    "sid": 103,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hence, there are two states of a span: the top state N and the bottom state N that correspond to the left and right hand of the unary rule runary = N \u2192 N respectively, as shown in Figure 2.",
                    "sid": 104,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 3 lists the feature templates we use for parsing.",
                    "sid": 105,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are 4 feature sets: (i) bottom state features fbottom (i, j, x, N i,j ), which depend on the botmaximal score among all the possible trees.",
                    "sid": 106,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The per ceptron algorithm is guaranteed to find the solution if it exists.",
                    "sid": 107,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.4 Joint Decoding.",
                    "sid": 108,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The three models described above are separately trained to make parameter estimation feasible as well as optimize each individual component.",
                    "sid": 109,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In test (3.",
                    "sid": 110,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1) Bi na ry rul e te mp lat es N \u2192 N l + N r Xl Xm \u22121 Xr len l len r Xl Xm Xr lenl lenr Xl Xm \u22121 Xr wo rd m\u2212 1 (R OO T) Xl + Xm Xr wordm (ROOT) (3.",
                    "sid": 111,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2) Un ary rul e te mp lat es N \u2192 N (3.",
                    "sid": 112,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3) Bo tto m sta te te mp lat es Xl len Xr len Xl \u22122 Xl \u22121 Xr +1 len Xl\u22121 Xr+1 Xr+2 len w ll w lr X l le n wll wlr Xr len Xl Xr wll len Xl Xr wlr len wordl wordr Xl Xr len wordl wordr Xl Xr X l\u2212 1 X l (L E A F) Xl+1 Xl (LEAF) Xl wordl (LEAF) Xl wll (LEAF) Xl+a Xr+b len wordl+a wordr+b \u22121 \u2264 a, b \u2264 1 (3.",
                    "sid": 113,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3) To p sta te te mp lat es X l\u2212 1 X l (L E A F) Xl+1 Xl (LEAF) Xl wordl (LEAF) Xl wll (LEAF) Xl+a Xr+b len wordl+a wordr+b \u22121 \u2264 a, b \u2264 1 Table 3: Feature templates for parsing, where X can be word, first and last character of word, first and last character bigram of word, POS tag.",
                    "sid": 114,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Xl+a /Xr\u2212a denotes the first/last a X in the span, while Xl\u2212a /Xr+a denotes the a X left/right to span.",
                    "sid": 115,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Xm is the first X of right child, and Xm\u22121 is the last X of the left child.",
                    "sid": 116,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "len, lenl , lenr denote the length of the span, left child and right child respectively.",
                    "sid": 117,
                    "ssid": 75,
                    "kind_of_tag": "s"
                },
                {
                    "text": "wl is the length of word.",
                    "sid": 118,
                    "ssid": 76,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ROOT/LEAF means the template can only generate the features for the root/initial span.",
                    "sid": 119,
                    "ssid": 77,
                    "kind_of_tag": "s"
                },
                {
                    "text": "ing, we perform joint decoding to combine information from the three models.",
                    "sid": 120,
                    "ssid": 78,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Parameters of word segmentation (\u03b8seg ), POS tagging (\u03b8pos ), and parsing models (\u03b8parse = \u03b8bottom\u2295top\u2295 unary\u2295bianry ) are s caled by three positive hyper-parameters \u03b1, \u03b2, and \u03b3 respectively, which control their contribution in the joint model.",
                    "sid": 121,
                    "ssid": 79,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If \u03b1 >> \u03b2 >> \u03b3, then the joint model is equivalent to a pipeline model, in which there is no feedback from downstream models to upstream ones.",
                    "sid": 122,
                    "ssid": 80,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For well tuned hyper-parameters, we expect that segmentation and POS tagging results can be improved by parsing information.",
                    "sid": 123,
                    "ssid": 81,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The hyper- parameters are tuned on development data.",
                    "sid": 124,
                    "ssid": 82,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the following sections, for simplicity we drop \u03b1, \u03b2, \u03b3, and just use \u03b8seg , \u03b8pos , \u03b8parse to represent the scaled parameters.",
                    "sid": 125,
                    "ssid": 83,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The basic idea of our decoding algorithm is to extend the CYK parsing algorithm so that it can deal with transition features in POS tagging and segmentation scores in word segmentation.",
                    "sid": 126,
                    "ssid": 84,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.4.1 Algorithm The joint decoding algorithm is shown in Algorithm 1.",
                    "sid": 127,
                    "ssid": 85,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a sentence x = c1, . . .",
                    "sid": 128,
                    "ssid": 86,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", cl , Line 0 calculates the scores of all possible words in the sentence using Eq(1).",
                    "sid": 129,
                    "ssid": 87,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are l(l + 1)/2 word candidates in total.",
                    "sid": 130,
                    "ssid": 88,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Surrounding words are important features for POS tagging and parsing; however, they are unavailable because segmentation is incomplete before parsing.",
                    "sid": 131,
                    "ssid": 89,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, we adopt pseudo surrounding features by simply fixing the context words as the single most likely ones.",
                    "sid": 132,
                    "ssid": 90,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a word candidate wi,j from ci to cj , its previous word s\u2032 is the rightmost one in the best word sequence of c1, . . .",
                    "sid": 133,
                    "ssid": 91,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", ci\u22121, which can be obtained by dynamic programming.",
                    "sid": 134,
                    "ssid": 92,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recursively, the second word left to wi,j is the previousword of s\u2032.",
                    "sid": 135,
                    "ssid": 93,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The next word of wi,j is defined similar ly.",
                    "sid": 136,
                    "ssid": 94,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Line 1, we use bidirectional Viterbi decoding to obtain all the surrounding words.",
                    "sid": 137,
                    "ssid": 95,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the forward direction, the algorithm starts from the first character boundary to the last, and finds the best previous word for the ith character boundary bi . In the backward direction, the algorithm starts from right to left, and finds the best next word of each bi .In Line 2, for each word candidate, we can calcu late the score of each POS tag using state features in the POS tagging model, since the context words are available now.",
                    "sid": 138,
                    "ssid": 96,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The score function of word wi,j with POS tag t is: scoreseg\u2295pos (x, i, j, t) = scoreseg (x, i, j) + \u03b8pos \u00b7 fpos (x, wi,j , t) (2) In Line 3, POS tags of surrounding words can be obtained similarly using bidirectional decoding.",
                    "sid": 139,
                    "ssid": 97,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Algorithm 1 Joint Word Segmentation, POS tagging, and Parsing Algorithm Input: Sentence x = c1 , . . .",
                    "sid": 140,
                    "ssid": 98,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", cl , beam size B, scaled word segmentation model, POS tagging model and parsing model.",
                    "sid": 141,
                    "ssid": 99,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Output: Word sequence, POS tag sequence, and parse tree 0: \u22000 \u2264 i \u2264 j \u2264 l \u2212 1, calculate scoreseg (x, i, j) using Equation (1) 1: For each character boundary bi , 0 \u2264 i \u2264 l, get the best previous and next words of bi using bidirectional Viterbi decoding 2: \u22000 \u2264 i \u2264 j \u2264 l \u2212 1, t \u2208 T , calculate scoreseg\u2295pos (x, i, j, t) using Equation (2) 3: \u2200bi , 0 \u2264 i \u2264 l, t \u2208 T , get the best POS tags of words left/right to bi using bidirectional viterbi decoding.",
                    "sid": 142,
                    "ssid": 100,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4: For each word candidate wi,j , 0 \u2264 i \u2264 j \u2264 l \u2212 1 5: For each bottom state N , POS tag t \u2208 T \u2701 step 1 (Line 57): get bottom states 6: scorebottom (x, i, j, wi,j , t, N ) = scoreseg\u2295pos (x, i, j, t) + \u03b8bottom \u00b7 fbottom (x, i, j, wi,j , t, N ) 7: Keep B best scorebottom . 8: For each top state N \u2701 step 2 (Line 89): get top states 9: scoretop (x, i, j, wi,j , t, N ) = maxN {scorebottom (x, i, j, wi,j , t, N ) + \u03b8top \u00b7 ftop (x, i, j, wi,j , t, N ) +\u03b8unary \u00b7 funary (x, i, j, wi,j , t, N \u2192 N )} 10: for i = 0, . . .",
                    "sid": 143,
                    "ssid": 101,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", l \u2212 1 do 11: for width = 1, . . .",
                    "sid": 144,
                    "ssid": 102,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", l \u2212 1 do 12: j = i + width 13: for k = i + 1, . . .",
                    "sid": 145,
                    "ssid": 103,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", j do 14: scorebottom (x, i, j, w, t, N ) = maxl,r {scoretop (x, i, k \u2212 1, wl , tl , N l ) + scoretop (x, k, j, wr , tr , N r ) +\u03b8binary \u00b7 fbinary (x, i, j, k, w, t, N \u2192 N r + N r ) + \u03b8pos \u00b7 fpos (tlast \u2192 tf irst ) l r +\u03b8bottom fbottom (x, i, j, w, t, N )} 15: Keep B best scorebottom \u2701 step 1 (Line 1415): get bottom states 16: For each top state N \u2701 step 2 (Line 1617): get top states 17: scoretop (x, i, j, w, t, N ) = maxN {scorebottom (x, i, j, w, t, N ) +\u03b8unary \u00b7 funary (x, i, j, w, t, N \u2192 N )} 18: end for 19: end for 20: end for L i n e 0 1 2 3 6 9 1 4 1 5 To tal Bo un d( w.r .t. l) Co mp lex ity l2 l2 |T |l2 |T |2 l2 |T |M l2 B M l2 l3 M B2 B M l2 l 3 M B 2 Table 4: Complexity Analysis of Algorithm 1.",
                    "sid": 146,
                    "ssid": 104,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, for wi,j with POS tag t, we use Viterbi algorithm to search the optimal POS tags of its left and right words.",
                    "sid": 147,
                    "ssid": 105,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In Lines 49, each word was initialized as a basic span.",
                    "sid": 148,
                    "ssid": 106,
                    "kind_of_tag": "s"
                },
                {
                    "text": "A span structure in the joint model is a 6tuple: S(i, j, w, t, N , N ), where i, j are the boundary indices, w, t are the word sequence and POS sequence within the span respectively, and N , N are the bottom and top states.",
                    "sid": 149,
                    "ssid": 107,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are two types of surrounding n-grams: one is inside the span, for example, the first word of a span, which can be obtained from w; the other is outside the span, for example, the previous word of a span, which is obtained from the pseudo context information.",
                    "sid": 150,
                    "ssid": 108,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The score of a basic span depends on its corresponding word and POS pair score, and the weights of the active state and unary features.",
                    "sid": 151,
                    "ssid": 109,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To avoid enumerating the combination of the bottom and top states, initialization for each span is divided into 2 steps.",
                    "sid": 152,
                    "ssid": 110,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the first step, the score of every bottom state is calculated using bottom state features, and only the B best states are maintained (see Line 67).",
                    "sid": 153,
                    "ssid": 111,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the second step, top state features and unary rule features are used to get the score of each top state (Line 9), and only the top B states are preserved.",
                    "sid": 154,
                    "ssid": 112,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Similarly, there are two steps in the merge operation: S(i, j, w, t, N , N ) = Sl (i, k, wl , tl , Nl , Nl ) + Sr (k + 1, j, wr , tr , Nr , Nr ).",
                    "sid": 155,
                    "ssid": 113,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The score of the bottom state N is calculated using binary features fbinary (x, i, j, k, w, t, N \u2192 N r + N r ), bottom statefeatures fbottom (x, i, j, w, t, N ), and POS tag transi tion features that depend on the boundary POS tags of Sl and Sr . See Line 14 of Algorithm 1, where For joint word segmentation and POS tagging, a word is correctly predicted if both the boundaries and the POS tag are correctly identified.",
                    "sid": 156,
                    "ssid": 114,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For joint segmentation, POS tagging, and parsing task, when calculating the bracket scores using existing parseval tools, we need to consider possible word segmentation errors.",
                    "sid": 157,
                    "ssid": 115,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To do this, we add the word boundary information in states \u2013 a bracket is correct only if tlast f irst l and tr are the POS tags of the last word in the left child span and the first word in the right child span respectively.",
                    "sid": 158,
                    "ssid": 116,
                    "kind_of_tag": "s"
                },
                {
                    "text": "3.4.2 Complexity analysis Given a sentence of length l, the complexity for each line of Algorithm 1 is listed in Table 4, where |T | is the size of POS tag set, M is the number of states, and B is the beam size.",
                    "sid": 159,
                    "ssid": 117,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "experiments. ",
            "number": "4",
            "sents": [
                {
                    "text": "4.1 Data.",
                    "sid": 160,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "its boundaries, label and word segmentation are all correct.",
                    "sid": 161,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One example is shown in Figure 3.",
                    "sid": 162,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Notice that identity unary rules are removed during evaluation.",
                    "sid": 163,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The basic spans are characters, not words, because the number of words in reference and prediction may be different.",
                    "sid": 164,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "POS tags are removed since they do not affect the bracket scores.",
                    "sid": 165,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If the segmentation is perfect, then the bracket scores of the modified tree are exactly the same as the original tree.",
                    "sid": 166,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is similar to evaluating parsing performance on speech transcripts with automatic sentence segmentation (Roark et al., 2006).",
                    "sid": 167,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For comparison with other systems, we use the CTNP B5 corpus, which has been studied for Chinese word NP(0,2,5) segmentation, POS tagging and parsing.",
                    "sid": 168,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We use the standard train/develop/test split of the data.",
                    "sid": 169,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "DetailsNR NN - - - - are shown in Table 5.",
                    "sid": 170,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Shanghai office Shanghai office Table 5: Training, development, and test data of CTB 5.",
                    "sid": 171,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.2 Evaluation Metric.",
                    "sid": 172,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluate system performance on the individual tasks, as well as the joint tasks.1 For word segmentation, three metrics are used for evaluation: precision (P), recall (R), and F-score (F) defined by 2PR/(P+R).",
                    "sid": 173,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Precision is the percentage of correct words in the system output.",
                    "sid": 174,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Recall is the percentage of words in gold standard annotations that are correctly predicted.",
                    "sid": 175,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For parsing, we use the standard parseval evaluation metrics: bracketing precision, recall and F-score.",
                    "sid": 176,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "1 Note that the joint task refers to automatic segmentation and tagging/parsing.",
                    "sid": 177,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It can be achieved using a pipeline system or our joint decoding method.",
                    "sid": 178,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 3: Boundary information is added to states to calculate the bracket scores in the face of word segmentation errors.",
                    "sid": 179,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Left: the original parse tree, Right: the converted parse tree.",
                    "sid": 180,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The numbers in the brackets are the indices of the character boundaries based on word segmentation.",
                    "sid": 181,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.3 Parameter Estimation.",
                    "sid": 182,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We train three submodels using the gold features, that is, POS tagger is trained using the perfect segmentation, and parser is trained using perfect segmentation and POS tags.",
                    "sid": 183,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some studies reported that better performance may be achieved by training subsequent models using representative output of the preceding models (Che et al., 2009).",
                    "sid": 184,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Hence for comparison we trained another parser using automatically generated POS tags obtained from 10-fold cross validation, but did not find significant difference between these two parsers when testing on the perfectly segmented development dataset.",
                    "sid": 185,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore we use the parser trained with perfect POS tags for the joint task.",
                    "sid": 186,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Three hyper-parameters, \u03b1, \u03b2, and \u03b3, are tuned on development data using a heuristic search.",
                    "sid": 187,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Parameters that achieved the best joint parsing result are selected.",
                    "sid": 188,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the search, we fixed \u03b3 = 1 and varied \u03b1, \u03b2.",
                    "sid": 189,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we set \u03b2 = 1, and enumerate \u03b1 = 1 , 1 , 1, 2, . . .",
                    "sid": 190,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", and choose the best \u03b1\u2217.",
                    "sid": 191,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then, System P R F (Jiang et al., 2008b) - - 97.74 (Jiang et al., 2008a) - - 97.85 (Kruengkrai et al., 2009) 97.46 98.29 97.87 (Zhang and Clark, 2010) - - 97.78 (Zhang and Clark, 2011) - - 97.78 (Sun, 2011) - - 98.17 Ours (w/o transition features) 97.45 98.24 97.85 Ours (with transition features) 97.44 98.23 97.84 4 2 we set \u03b1 = \u03b1\u2217 and vary \u03b2 = 1 , 1 , 1, 2, . . .",
                    "sid": 192,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": ", and 4 2 select the best \u03b2\u2217.",
                    "sid": 193,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 6 lists the parameters we used for training the submodels, as well as the hyper-parameters for joint decoding.",
                    "sid": 194,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "M od el Pa ra me ter V a l u e Ch ara cte r ba se d wo rd se gm ent or Ga uss ian pri or # Fe atu re 0 . 0 1 3, 8 7 5, 8 0 2 W or d ba se d wo rd se gm ent or Ga uss ian pri or # Fe atu re 0 . 0 1 3 1 2 , 5 3 3 PO S tag ger Ga uss ian pri or # Fe atu re 0.1 48, 60 8,8 02 Pa rse r Ite rat ion Nu mb er # Fe atu re 10 49, 36 9,8 43 Joi nt H y p e r p a r a m e t e r \u03b1 H y p e r p a r a m e t e r \u03b2 H y p e r p a r a m e t e r \u03b3 B e a m S i z e B 4 0.5 1 20 Table 6: Parameters used in our system.",
                    "sid": 195,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.4 Experimental Results.",
                    "sid": 196,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this section we first show that our sub-models are better than or comparable to state-of-the-art systems, and then the joint model is superior to the pipeline approach.",
                    "sid": 197,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.4.1 Evaluating Sub-models Table 7 shows word segmentation results using our word segmentation submodel, in comparison to a few state-of-the-art systems.",
                    "sid": 198,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our segmentor, we show results for two variants: one removes transition features as described in Section 3.1, the other uses CRFs to learn the weights of transition features.",
                    "sid": 199,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can see that our system is competitive with all the others except Sun\u2019s that used additional idiom resources.",
                    "sid": 200,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our two word segmentors have similar performance.",
                    "sid": 201,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the one without transition features can be naturally integrated into the joint system, we use it in the following joint tasks.",
                    "sid": 202,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 7: Word segmentation results.",
                    "sid": 203,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the POS tagging only task that takes gold standard word segmentation as input, we have two systems.",
                    "sid": 204,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One uses the linear chain CRFs as described in Section 3.2, the other is obtained using the parser described in Section 3.3 \u2013 the parser generates POS tag hypotheses when POS tag features are not used.",
                    "sid": 205,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The POS tagging accuracy is 95.53% and 95.10% using these two methods respectively.",
                    "sid": 206,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The better performance from the former system may be because the local label dependency is more helpful for POS tagging than the long distance dependencies that might be noisy.",
                    "sid": 207,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This result also confirms our choice of using an independent POS tagger for the sub-model, rather than relying on a parser for POS tagging.",
                    "sid": 208,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, since there are no reported results for this setup, we demonstrate the competence of our POS tagger using the joint word segmentation and POS tagging task.",
                    "sid": 209,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 8 shows the performance of a few systems along with ours, all using the pipeline approach where automatic segmentation is followed by POS tagging.",
                    "sid": 210,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can see that our POS tagger is comparable to the others.",
                    "sid": 211,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "System P R F (Jiang et al., 2008b) - - 93.37 (Jiang et al., 2008a) - - 93.41 (Kruengkrai et al., 2009) 93.28 94.07 93.67 (Zhang and Clark, 2010) - - 93.67 (Zhang and Clark, 2011) - - 93.67 (Sun, 2011) - - 94.02 Ours (pipeline) 93.10 93.96 93.53 Table 8: Results for the joint word segmentation and POS tagging task.",
                    "sid": 212,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For parsing, Table 9 presents the parsing result on gold standard segmented sentence.",
                    "sid": 213,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Notice that the result of (Harper and Huang, 2009; Zhang and Clark, 2011) are not directly comparable to ours, as they used a different data split.",
                    "sid": 214,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The best published system result on CTB5 is Petrov and Klein\u2019s, which used PCFG with latent Variables.",
                    "sid": 215,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our system performs better mainly because it benefits from a large amount of features.",
                    "sid": 216,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 10: Results for the joint segmentation, tagging, and parsing task using pipeline and joint models.",
                    "sid": 217,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 9: Parsing results using gold standard word segmentation.",
                    "sid": 218,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For our parser, besides the model described in Section 3.3, we tried two variations: one does not use the automatic POS tag features, the other one is learned on the parent annotated training data.",
                    "sid": 219,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results in Table 9 show that there is a performance degradation when using parent annotation.",
                    "sid": 220,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This may be due to the introduction of a large number of states, resulting in sparse features.",
                    "sid": 221,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also notice that with the help of the POS tag information, even automatically generated, the parser gained 0.9% improvement in F-score.",
                    "sid": 222,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This demonstrates the advantage of using a better independent POS tagger and incorporating it in parsing.",
                    "sid": 223,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally Table 10 shows the results for the three tasks using our joint decoding method in comparison to the pipeline method.",
                    "sid": 224,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can see that the joint model outperforms the pipeline one.",
                    "sid": 225,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is mainly because of a better parsing module as well as joint decoding.",
                    "sid": 226,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the table we also include results of (Jiang et al., 2009), which is the only reported joint parsing result we found using the same data split on CTB5.",
                    "sid": 227,
                    "ssid": 68,
                    "kind_of_tag": "s"
                },
                {
                    "text": "They achieved 80.28% parsing F-score using automatic word segmentation.",
                    "sid": 228,
                    "ssid": 69,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Their adapted system Jiang09+ leveraged additional corpus to improve Chinese word segmentation, resulting in an F- score of 81.07%.",
                    "sid": 229,
                    "ssid": 70,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our system has better performance than these.",
                    "sid": 230,
                    "ssid": 71,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4.5 Error Analysis.",
                    "sid": 231,
                    "ssid": 72,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We compared the results from the pipeline and our joint decoding systems in order to understand the impact of the joint model on word segmentation and POS tagging.",
                    "sid": 232,
                    "ssid": 73,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We notice that the joint model tend to generate more words than the pipeline model.",
                    "sid": 233,
                    "ssid": 74,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, \u201c\u5df4\u5c14\u4e00\u884c\u201d is one word in the pipeline model, but correctly segmented as two words \u201c\u5df4\u5c14/\u4e00\u884c\u201d in the joint model.",
                    "sid": 234,
                    "ssid": 75,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This tendency of seg mentation also makes it fail to recognize some long words, especially OOV words.",
                    "sid": 235,
                    "ssid": 76,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, \u201c\u4e8b \u5b9e\u4e0a\u201d is segmented as \u201c\u4e8b\u5b9e/\u4e0a\u201d.",
                    "sid": 236,
                    "ssid": 77,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the data set, we find that, the joint model corrected 10 missing boundaries over the pipeline method, and introduced 3 false positive segmentation errors.",
                    "sid": 237,
                    "ssid": 78,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the analysis of POS tags, we only examined the words that are correctly segmented by both the pipeline and the joint models.",
                    "sid": 238,
                    "ssid": 79,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Table 11 shows the increase and decrease of error patterns of the joint model over the pipeline POS tagger.",
                    "sid": 239,
                    "ssid": 80,
                    "kind_of_tag": "s"
                },
                {
                    "text": "An error pat tern \u201cX \u2192 Y\u201d means that the word whose true tag is \u2018X\u2019 is assigned a tag \u2018Y\u2019.",
                    "sid": 240,
                    "ssid": 81,
                    "kind_of_tag": "s"
                },
                {
                    "text": "All the patterns are ranked in descending order of the reduction/increase of the error number.",
                    "sid": 241,
                    "ssid": 82,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can see that the joint model has a clear advantage in the disambiguation of {VV, NN}and {DEG, DEC}, which results in the overall im proved performance.",
                    "sid": 242,
                    "ssid": 83,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In contrast, the joint method performs worse on ambiguous POS pairs such as{N N, N R}.",
                    "sid": 243,
                    "ssid": 84,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This observation is similar to those re ported by (Li et al., 2011; Hatori et al., 2011).",
                    "sid": 244,
                    "ssid": 85,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusion. ",
            "number": "5",
            "sents": [
                {
                    "text": "In this paper, we proposed a new algorithm for joint Chinese word segmentation, POS tagging, and parsing.",
                    "sid": 245,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our algorithm is an extension of the CYK err or pa tte rn # \u2193 err or pa tte rn # \u2191 N N \u2192 V V 47 19 N N \u2192 N R 15 12 V V \u2192 N N 42 13 N R \u2192 N N 7 5 D E G \u2192 D EC 23 10 J J \u2192 P 1 4 N N \u2192 J J 29 8 N N \u2192 D T 2 4 D EC \u2192 D E G 11 4 P \u2192 V V 3 2 J J \u2192 N N 12 4 A D \u2192 N N 1 2 Table 11: POS tagging error patterns.",
                    "sid": 246,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "# means the error number of the corresponding pattern made by the pipeline tagging model.",
                    "sid": 247,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2193 and \u2191 mean the error number reduced or increased by the joint model.",
                    "sid": 248,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "parsing method.",
                    "sid": 249,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The sub-models are independently trained for the three tasks to reduce model complexity and optimize individual sub-models.",
                    "sid": 250,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our experiments demonstrate the advantage of the joint models.",
                    "sid": 251,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the future work, we will compare this joint model to the pipeline approach that uses multiple candidates or soft decisions in the early modules.",
                    "sid": 252,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will also investigate methods for joint learning as well as ways to speed up the joint decoding algorithm.",
                    "sid": 253,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Acknowledgments The authors thank Zhongqiang Huang for his help with experiments.",
                    "sid": 254,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This work is partly supported by DARPA under Contract No.",
                    "sid": 255,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "HR001112-C-0016.",
                    "sid": 256,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.",
                    "sid": 257,
                    "ssid": 13,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}