{
    "ID": "P03-1035",
    "sections": [
        {
            "text": "abstract",
            "number": 0,
            "sents": [
                {
                    "text": "This paper presents a Chinese word segmentation system that uses improved source- channel models of Chinese sentence generation.",
                    "sid": 1,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Chinese words are defined as one of the following four types: lexicon words, morphologically derived words, factoids, and named entities.",
                    "sid": 2,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our system provides a unified approach to the four fundamental features of word-level Chinese language processing: (1) word segmentation, (2) morphological analysis, (3) factoid detection, and (4) named entity recognition.",
                    "sid": 3,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The performance of the system is evaluated on a manually annotated test set, and is also compared with several state-of- the-art systems, taking into account the fact that the definition of Chinese words often varies from system to system.",
                    "sid": 4,
                    "ssid": 4,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "introduction",
            "number": "1",
            "sents": [
                {
                    "text": "Chinese word segmentation is the initial step of many Chinese language processing tasks, and has attracted a lot of attention in the research community.",
                    "sid": 5,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is a challenging problem due to the fact that there is no standard definition of Chinese words.",
                    "sid": 6,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In this paper, we define Chinese words as one of the following four types: entries in a lexicon, morphologically derived words, factoids, and named entities.",
                    "sid": 7,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then present a Chinese word segmentation system which provides a solution to the four fundamental problems of word-level Chinese language processing: word segmentation, morphological analysis, factoid detection, and named entity recognition (NER).",
                    "sid": 8,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are no word boundaries in written Chinese text.",
                    "sid": 9,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, unlike English, it may not be desirable to separate the solution to word segmentation from the solutions to the other three problems.",
                    "sid": 10,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ideally, we would like to propose a unified approach to all the four problems.",
                    "sid": 11,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The unified approach we used in our system is based on the improved source-channel models of Chinese sentence generation, with two components: a source model and a set of channel models.",
                    "sid": 12,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The source model is used to estimate the generative probability of a word sequence, in which each word belongs to one word type.",
                    "sid": 13,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each word type, a channel model is used to estimate the generative probability of a character string given the word type.",
                    "sid": 14,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "So there are multiple channel models.",
                    "sid": 15,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We shall show in this paper that our models provide a statistical framework to corporate a wide variety linguistic knowledge and statistical models in a unified way.",
                    "sid": 16,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We evaluate the performance of our system using an annotated test set.",
                    "sid": 17,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also compare our system with several state-of-the-art systems, taking into account the fact that the definition of Chinese words often varies from system to system.",
                    "sid": 18,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the rest of this paper: Section 2 discusses previous work.",
                    "sid": 19,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Section 3 gives the detailed definition of Chinese words.",
                    "sid": 20,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Sections 4 to 6 describe in detail the improved source-channel models.",
                    "sid": 21,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Section 8 describes the evaluation results.",
                    "sid": 22,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Section 9 presents our conclusion.",
                    "sid": 23,
                    "ssid": 23,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "previous work. ",
            "number": "2",
            "sents": [
                {
                    "text": "Many methods of Chinese word segmentation have been proposed: reviews include (Wu and Tseng, 1993; Sproat and Shih, 2001).",
                    "sid": 24,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These methods can be roughly classified into dictionary-based methods and statistical-based methods, while many state-of- the-art systems use hybrid approaches.",
                    "sid": 25,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In dictionary-based methods (e.g. Cheng et al., 1999), given an input character string, only words that are stored in the dictionary can be identified.",
                    "sid": 26,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The performance of these methods thus depends to 1 We would like to thank Ashley Chang, JianYun Nie, Andi Wu and Ming Zhou for many useful discussions, and for comments on.",
                    "sid": 27,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "earlier versions of this paper.",
                    "sid": 28,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We would also like to thank Xiaoshan Fang, Jianfeng Li, Wenfeng Yang and Xiaodan Zhu for their help with evaluating our system.",
                    "sid": 29,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "a large degree upon the coverage of the dictionary, which unfortunately may never be complete because new words appear constantly.",
                    "sid": 30,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, in addition to the dictionary, many systems also contain special components for unknown word identification.",
                    "sid": 31,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In particular, statistical methods have been widely applied because they utilize a probabilistic or cost-based scoring mechanism, instead of the dictionary, to segment the text.",
                    "sid": 32,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These methods however, suffer from three drawbacks.",
                    "sid": 33,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, some of these methods (e.g. Lin et al., 1993) identify unknown words without identifying their types.",
                    "sid": 34,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For instance, one would identify a string as a unit, but not identify whether it is a person name.",
                    "sid": 35,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This is not always sufficient.",
                    "sid": 36,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, the probabilistic models used in these methods (e.g. Teahan et al., 2000) are trained on a segmented corpus which is not always available.",
                    "sid": 37,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Third, the identified unknown words are likely to be linguistically implausible (e.g. Dai et al., 1999), and additional manual checking is needed for some subsequent tasks such as parsing.",
                    "sid": 38,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We believe that the identification of unknown words should not be defined as a separate problem from word segmentation.",
                    "sid": 39,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These two problems are better solved simultaneously in a unified approach.",
                    "sid": 40,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One example of such approaches is Sproat et al.",
                    "sid": 41,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1996), which is based on weighted finite-state transducers (FSTs).",
                    "sid": 42,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our approach is motivated by the same inspiration, but is based on a different mechanism: the improved source-channel models.",
                    "sid": 43,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As we shall see, these models provide a more flexible framework to incorporate various kinds of lexical and statistical information.",
                    "sid": 44,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Some types of unknown words that are not discussed in Sproat\u2019s system are dealt with in our system.",
                    "sid": 45,
                    "ssid": 22,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "chinese words. ",
            "number": "3",
            "sents": [
                {
                    "text": "There is no standard definition of Chinese words \u2013 linguists may define words from many aspects (e.g. Packard, 2000), but none of these definitions will completely line up with any other.",
                    "sid": 46,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Fortunately, this may not matter in practice because the definition that is most useful will depend to a large degree upon how one uses and processes these words.",
                    "sid": 47,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We define Chinese words in this paper as one of the following four types: (1) entries in a lexicon (lexicon words below), (2) morphologically derived words, (3) factoids, and (4) named entities, because these four types of words have different function- alities in Chinese language processing, and are processed in different ways in our system.",
                    "sid": 48,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the plausible word segmentation for the sentence in Figure 1(a) is as shown.",
                    "sid": 49,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Figure 1(b) is the output of our system, where words of different types are processed in different ways: (a) \u670b\u53cb\u4eec/\u5341\u4e8c\u70b9\u4e09\u5341\u5206/\u9ad8\u9ad8\u5174\u5174/\u5230/\u674e\u4fca\u751f/\u6559\u6388/\u5bb6/ \u5403\u996d (Friends happily go to professor Li Junsheng\u2019s home for lunch at twelve thirty.)",
                    "sid": 50,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(b) [\u670b\u53cb+\u4eec MA_S] [\u5341\u4e8c\u70b9\u4e09\u5341\u5206 12:30 TIME] [\u9ad8\u5174 MR_AABB] [\u5230] [\u674e\u4fca\u751f PN] [\u6559\u6388] [\u5bb6] [\u5403\u996d] Figure 1: (a) A Chinese sentence.",
                    "sid": 51,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Slashes indicate word boundaries.",
                    "sid": 52,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(b) An output of our word segmentation system.",
                    "sid": 53,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Square brackets indicate word boundaries.",
                    "sid": 54,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "+ indicates a morpheme boundary.",
                    "sid": 55,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 For lexicon words, word boundaries are detected.",
                    "sid": 56,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 For morphologically derived words, their morphological patterns are detected, e.g. \u670b\u53cb \u4eec \u2018friend+s\u2019 is derived by affixation of the plural affix \u4eec to the noun \u670b\u53cb (MA_S indicates a suffixation pattern), and \u9ad8\u9ad8\u5174\u5174 \u2018happily\u2019 is a reduplication of \u9ad8\u5174 \u2018happy\u2019 (MR_AABB indicates an AABB reduplication pattern).",
                    "sid": 57,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 For factoids, their types and normalized forms are detected, e.g. 12:30 is the normalized form of the time expression \u5341\u4e8c\u70b9\u4e09\u5341 \u5206 (TIME indicates a time expression).",
                    "sid": 58,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 For named entities, their types are detected, e.g. \u674e\u4fca\u751f \u2018Li Junsheng\u2019 is a person name (PN indicates a person name).",
                    "sid": 59,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our system, we use a unified approach to detecting and processing the above four types of words.",
                    "sid": 60,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "This approach is based on the improved source-channel models described below.",
                    "sid": 61,
                    "ssid": 16,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "improved source-channel models. ",
            "number": "4",
            "sents": [
                {
                    "text": "Let S be a Chinese sentence, which is a character string.",
                    "sid": 62,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all possible word segmentations W, we will choose the most likely one W* which achieves the highest conditional probability P(W|S): W* = argmaxw P(W|S).",
                    "sid": 63,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "According to Bayes\u2019 decision rule and dropping the constant denominator, we can equivalently perform the following maximization: W * \uf03d arg max P(W )P(S | W ) .",
                    "sid": 64,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1) W Following the Chinese word definition in Section 3, we define word class C as follows: (1) Each lexicon Word class Class model Linguistic Constraints Lexicon word (LW) P(S|LW)=1 if S forms a word lexicon entry, 0 otherwise.",
                    "sid": 65,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Word lexicon Morphologically derived word (MW) P(S|MW)=1 if S forms a morph lexicon entry, 0 otherwise.",
                    "sid": 66,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Morph-lexicon Person name (PN) Character bigram family name list, Chinese PN patterns Location name (LN) Character bigram LN keyword list, LN lexicon, LN abbr.",
                    "sid": 67,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "list Organization name (ON) Word class bigram ON keyword list, ON abbr.",
                    "sid": 68,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "list Transliteration names (FN) Character bigram transliterated name character list Factoid2 (FT) P(S|FT)=1 if S can be parsed using a factoid grammar G, 0 otherwise Figure 2.",
                    "sid": 69,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Class models Factoid rules (presented by FSTs).",
                    "sid": 70,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "word is defined as a class; (2) each morphologically derived word is defined as a class; (3) each type of factoids is defined as a class, e.g. all time expressions belong to a class TIME; and (4) each type of named entities is defined as a class, e.g. all person names belong to a class PN.",
                    "sid": 71,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We therefore convert the word segmentation W into a word class sequence C. Eq. 1 can then be rewritten as: C * \uf03d arg max P(C )P(S | C) .",
                    "sid": 72,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2) C Eq. 2 is the basic form of the source-channel models for Chinese word segmentation.",
                    "sid": 73,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The models assume that a Chinese sentence S is generated as follows: First, a person chooses a sequence of concepts (i.e., word classes C) to output, according to the probability distribution P(C); then the person attempts to express each concept by choosing a sequence of characters, according to the probability distribution P(S|C).",
                    "sid": 74,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The source-channel models can be interpreted in another way as follows: P(C) is a stochastic model estimating the probability of word class sequence.",
                    "sid": 75,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It indicates, given a context, how likely a word class occurs.",
                    "sid": 76,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, person names are more likely to occur before a title such as \u6559\u6388 \u2018professor\u2019.",
                    "sid": 77,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "So P(C) is also referred to as context model afterwards.",
                    "sid": 78,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "P(S|C) is a generative model estimating how likely a character string is generated given a word class.",
                    "sid": 79,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the character string \u674e\u4fca\u751f is more likely to be a person name than \u91cc\u4fca\u751f \u2018Li Junsheng\u2019 because \u674e is a common family name in China while \u91cc is not.",
                    "sid": 80,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "So P(S|C) is also referred to as class model afterwards.",
                    "sid": 81,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our system, we use the improved source-channel models, which contains one context model (i.e., a trigram language model in our case) and a set of class models of different types, each of which is for one class of words, as shown in Figure 2.",
                    "sid": 82,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Although Eq. 2 suggests that class model probability and context model probability can be combined through simple multiplication, in practice some weighting is desirable.",
                    "sid": 83,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "There are two reasons.",
                    "sid": 84,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, some class models are poorly estimated, owing to the sub-optimal assumptions we make for simplicity and the insufficiency of the training corpus.",
                    "sid": 85,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Combining the context model probability with poorly estimated class model probabilities according to Eq. 2 would give the context model too little weight.",
                    "sid": 86,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, as seen in Figure 2, the class models of different word classes are constructed in different ways (e.g. name entity models are n-gram models trained on corpora, and factoid models are compiled using linguistic knowledge).",
                    "sid": 87,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, the quantities of class model probabilities are likely to have vastly different dynamic ranges among different word classes.",
                    "sid": 88,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "One way to balance these probability quantities is to add several class model weight CW, each for one word class, to adjust the class model probability P(S|C) to P(S|C)CW.",
                    "sid": 89,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our experiments, these class model weights are determined empirically to optimize the word segmentation performance on a development set.",
                    "sid": 90,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given the source-channel models, the procedure of word segmentation in our system involves two steps: First, given an input string S, all word candidates are generated (and stored in a lattice).",
                    "sid": 91,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Each candidate is tagged with its word class and the class 2 In our system, we define ten types of factoid: date, time (TIME), percentage, money, number (NUM), measure, email, phone.",
                    "sid": 92,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "number, and WWW.",
                    "sid": 93,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "model probability P(S\u2019|C), where S\u2019 is any substring of S. Second, Viterbi search is used to select (from the lattice) the most probable word segmentation (i.e. word class sequence C*) according to Eq.",
                    "sid": 94,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2).",
                    "sid": 95,
                    "ssid": 34,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "class model probabilities. ",
            "number": "5",
            "sents": [
                {
                    "text": "Given an input string S, all class models in Figure 2 are applied simultaneously to generate word class candidates whose class model probabilities are assigned using the corresponding class models: \u2022 Lexicon words: For any substring S\u2019 \uf0cd S, we assume P(S\u2019|C) = 1 and tagged the class as lexicon word if S\u2019 forms an entry in the word lexicon, P(S\u2019|C) = 0 otherwise.",
                    "sid": 96,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 Morphologically derived words: Similar to lexicon words, but a morph-lexicon is used instead of the word lexicon (see Section 5.1).",
                    "sid": 97,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "\u2022 Factoids: For each type of factoid, we compile a set of finite-state grammars G, represented as FSTs.",
                    "sid": 98,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For all S\u2019 \uf0cd S, if it can be parsed using G, we assume P(S\u2019|FT) = 1, and tagged S\u2019 as a factoid candidate.",
                    "sid": 99,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As the example in Figure 1 shows, \u5341\u4e8c\u70b9\u4e09\u5341\u5206 is a factoid (time) candidate with the class model probability P(\u5341\u4e8c \u70b9\u4e09\u5341\u5206|TIME) =1, and \u5341\u4e8c and \u4e09\u5341 are also factoid (number) candidates, with P(\u5341\u4e8c |NUM) = P(\u4e09\u5341|NUM) =1 \u2022 Named entities: For each type of named entities, we use a set of grammars and statistical models to generate candidates as described in Section 5.2.",
                    "sid": 100,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.1 Morphologically derived words.",
                    "sid": 101,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our system, the morphologically derived words are generated using five morphological patterns: (1) affixation: \u670b\u53cb \u4eec (friend - plural) \u2018friends\u2019; (2) reduplication: \u9ad8\u5174 \u2018happy\u2019 \ufffd \u9ad8\u9ad8\u5174\u5174 \u2018happily\u2019; (3) merging: \u4e0a\u73ed \u2018on duty\u2019 + \u4e0b\u73ed \u2018off duty\u2019 \ufffd\u4e0a \u4e0b\u73ed \u2018on-off duty\u2019; (4) head particle (i.e. expressions that are verb + comp): \u8d70 \u2018walk\u2019 + \u51fa\u53bb \u2018out\u2019 \ufffd \u8d70\u51fa\u53bb \u2018walk out\u2019; and (5) split (i.e. a set of expressions that are separate words at the syntactic level but single words at the semantic level): \u5403\u4e86\u996d \u2018already ate\u2019, where the bi-character word \u5403\u996d \u2018eat\u2019 is split by the particle \u4e86 \u2018already\u2019.",
                    "sid": 102,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is difficult to simply extend the well-known techniques for English (i.e., finite-state morphology) to Chinese due to two reasons.",
                    "sid": 103,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, Chinese mor morphological rules are not as \u2018general\u2019 as their English counterparts.",
                    "sid": 104,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, English plural nouns can be in general generated using the rule \u2018noun + s \ufffd plural noun\u2019.",
                    "sid": 105,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "But only a small subset of Chinese nouns can be pluralized (e.g. \u670b\u53cb\u4eec) using its Chinese counterpart \u2018noun + \u4eec \ufffd plural noun\u2019 whereas others (e.g. \u5357\u74dc \u2018pumpkins\u2019) cannot.",
                    "sid": 106,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, the operations required by Chinese morphological analysis such as copying in reduplication, merging and splitting, cannot be implemented using the current finite-state networks3.",
                    "sid": 107,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our solution is the extended lexicalization.",
                    "sid": 108,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We simply collect all morphologically derived word forms of the above five types and incorporate them into the lexicon, called morph lexicon.",
                    "sid": 109,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The procedure involves three steps: (1) Candidate generation.",
                    "sid": 110,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is done by applying a set of morphological rules to both the word lexicon and a large corpus.",
                    "sid": 111,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the rule \u2018noun + \u4eec \ufffd plural noun\u2019 would generate candidates like \u670b\u53cb\u4eec.",
                    "sid": 112,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2) Statistical filtering.",
                    "sid": 113,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each candidate, we obtain a set of statistical features such as frequency, mutual information, left/right context dependency from a large corpus.",
                    "sid": 114,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then use an information gain-like metric described in (Chien, 1997; Gao et al., 2002) to estimate how likely a candidate is to form a morphologically derived word, and remove \u2018bad\u2019 candidates.",
                    "sid": 115,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The basic idea behind the metric is that a Chinese word should appear as a stable sequence in the corpus.",
                    "sid": 116,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, the components within the word are strongly correlated, while the components at both ends should have low correlations with words outside the sequence.",
                    "sid": 117,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(3) Linguistic selection.",
                    "sid": 118,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We finally manually check the remaining candidates, and construct the morph-lexicon, where each entry is tagged by its morphological pattern.",
                    "sid": 119,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.2 Named entities.",
                    "sid": 120,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We consider four types of named entities: person names (PN), location names (LN), organization names (ON), and transliterations of foreign names (FN).",
                    "sid": 121,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al.",
                    "sid": 122,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1996) also studied such problems (with the same.",
                    "sid": 123,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "example) and uses weighted FSTs to deal with the affixation.",
                    "sid": 124,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "linguists and are represented as FSTs) to generate only those \u2018most likely\u2019 candidates.",
                    "sid": 125,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, each of the generated candidates is assigned a class model probability.",
                    "sid": 126,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "These class models are defined as generative models which are respectively estimated on their corresponding named entity lists using maximum likelihood estimation (MLE), together with smoothing methods4.",
                    "sid": 127,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We will describe briefly the constraints and the class models below.",
                    "sid": 128,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.2.1 Chinese person names There are two main constraints.",
                    "sid": 129,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1) PN patterns: We assume that a Chinese PN consists of a family name F and a given name G, and is of the pattern F+G. Both F and G are of one or two characters long.",
                    "sid": 130,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2) Family name list: We only consider PN candidates that begin with an F stored in the family name list (which contains 373 entries in our system).",
                    "sid": 131,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given a PN candidate, which is a character string S\u2019, the class model probability P(S\u2019|PN) is computed by a character bigram model as follows: (1) Generate the family name sub-string SF, with the probability P(SF|F); (2) Generate the given name sub-string SG, with the probability P(SG|G) (or P(SG1|G1)); and (3) Generate the second given name, with the probability P(SG2|SG1,G2).",
                    "sid": 132,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, the generative probability of the string \u674e\u4fca\u751f given that it is a PN would be estimated as P(\u674e\u4fca\u751f|PN) = P(\u674e|F)P(\u4fca|G1)P(\u751f|\u4fca,G2).",
                    "sid": 133,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.2.2 Location names Unlike PNs, there are no patterns for LNs.",
                    "sid": 134,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We assume that a LN candidate is generated given S\u2019 (which is less than 10 characters long), if one of the following conditions is satisfied: (1) S\u2019 is an entry in the LN list (which contains 30,000 LNs); (2) S\u2019 ends in a keyword in a 120-entry LN keyword list such as \u5e02 \u2018city\u20195.",
                    "sid": 135,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The probability P(S\u2019|LN) is computed by a character bigram model.",
                    "sid": 136,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consider a string \u4e4c\u82cf\u91cc\u6c5f \u2018Wusuli river\u2019.",
                    "sid": 137,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is a LN candidate because it ends in a LN keyword \u6c5f \u2018river\u2019.",
                    "sid": 138,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The generative probability of the string given it is a LN would be estimated as P(\u4e4c\u82cf\u91cc\u6c5f |LN) = P( \u4e4c |<LN>) P( \u82cf|\u4e4c ) P( \u91cc |\u82cf ) P( \u6c5f |\u91cc ) 4 The detailed description of these models are in Sun et al..",
                    "sid": 139,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2002), which also describes the use of cache model and the way the abbreviations of LN and ON are handled.",
                    "sid": 140,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 For a better understanding, the constraint is a simplified.",
                    "sid": 141,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "version of that used in our system.",
                    "sid": 142,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "P(</LN>|\u6c5f), where <LN> and </LN> are symbols denoting the beginning and the end of a LN, respectively.",
                    "sid": 143,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.2.3 Organization names ONs are more difficult to identify than PNs and LNs because ONs are usually nested named entities.",
                    "sid": 144,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consider an ON \u4e2d\u56fd \u56fd\u9645\u822a \u7a7a \u516c \u53f8 \u2018Air China Corporation\u2019; it contains an LN \u4e2d\u56fd \u2018China\u2019.",
                    "sid": 145,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Like the identification of LNs, an ON candidate is only generated given a character string S\u2019 (less than 15 characters long), if it ends in a keyword in a 1,355-entry ON keyword list such as \u516c\u53f8 \u2018corporation\u2019.",
                    "sid": 146,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To estimate the generative probability of a nested ON, we introduce word class segmentations of S\u2019, C, as hidden variables.",
                    "sid": 147,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In principle, the ON class model recovers P(S\u2019|ON) over all possible C: P(S\u2019|ON) = \u2211CP(S\u2019,C|ON) = \u2211CP(C|ON)P(S\u2019|C, ON).",
                    "sid": 148,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since P(S\u2019|C,ON) = P(S\u2019|C), we have P(S\u2019|ON) = \u2211CP(C|ON) P(S\u2019|C).",
                    "sid": 149,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We then assume that the sum is approximated by a single pair of terms P(C*|ON)P(S\u2019|C*), where C* is the most probable word class segmentation discovered by Eq. 2.",
                    "sid": 150,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, we also use our system to find C*, but the source- channel models are estimated on the ON list.",
                    "sid": 151,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consider the earlier example.",
                    "sid": 152,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Assuming that C* = LN/\u56fd\u9645/\u822a\u7a7a/\u516c\u53f8, where \u4e2d\u56fd is tagged as a LN, the probability P(S\u2019|ON) would be estimated using a word class bigram model as: P(\u4e2d\u56fd\u56fd\u9645\u822a\u7a7a\u516c\u53f8 |ON) \u2248 P(LN/\u56fd\u9645/\u822a\u7a7a/\u516c\u53f8|ON) P(\u4e2d\u56fd|LN) = P(LN|<ON>)P(\u56fd\u9645|LN)P(\u822a\u7a7a|\u56fd\u9645)P(\u516c\u53f8|\u822a\u7a7a) P(</ON>|\u516c\u53f8)P(\u4e2d\u56fd |LN), where P(\u4e2d\u56fd|LN) is the class model probability of \u4e2d\u56fd given that it is a LN, <ON> and </ON> are symbols denoting the beginning and the end of a ON, respectively.",
                    "sid": 153,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5.2.4 Transliterations of foreign names As described in Sproat et al.",
                    "sid": 154,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.",
                    "sid": 155,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since FNs can be of any length and their original pronunciation is effectively unlimited, the recognition of such names is tricky.",
                    "sid": 156,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Fortunately, there are only a few hundred Chinese characters that are particularly common in transliterations.",
                    "sid": 157,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, an FN candidate would be generated given S\u2019, if it contains only characters stored in a transliterated name character list (which contains 618 Chinese characters).",
                    "sid": 158,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The probability P(S\u2019|FN).",
                    "sid": 159,
                    "ssid": 64,
                    "kind_of_tag": "s"
                },
                {
                    "text": "is estimated using a character bigram model.",
                    "sid": 160,
                    "ssid": 65,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Notice that in our system a FN can be a PN, a LN, or an ON, depending on the context.",
                    "sid": 161,
                    "ssid": 66,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then, given a FN candidate, three named entity candidates, each for one category, are generated in the lattice, with the class probabilities P(S\u2019|PN)=P(S\u2019|LN)=P(S\u2019|ON)= P(S\u2019|FN).",
                    "sid": 162,
                    "ssid": 67,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In other words, we delay the determination of its type until decoding where the context model is used.",
                    "sid": 163,
                    "ssid": 68,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "context model estimation. ",
            "number": "6",
            "sents": [
                {
                    "text": "This section describes the way the class model probability P(C) (i.e. trigram probability) in Eq. 2 is estimated.",
                    "sid": 164,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Ideally, given an annotated corpus, where each sentence is segmented into words which are tagged by their classes, the trigram word class probabilities can be calculated using MLE, together with a backoff schema (Katz, 1987) to deal with the sparse data problem.",
                    "sid": 165,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Unfortunately, building such annotated training corpora is very expensive.",
                    "sid": 166,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our basic solution is the bootstrapping approach described in Gao et al.",
                    "sid": 167,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2002).",
                    "sid": 168,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It consists of three steps: (1) Initially, we use a greedy word segmentor6 to annotate the corpus, and obtain an initial context model based on the initial annotated corpus; (2) we re-annotate the corpus using the obtained models; and (3) retrain the context model using the re-annotated corpus.",
                    "sid": 169,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Steps 2 and 3 are iterated until the performance of the system converges.",
                    "sid": 170,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In the above approach, the quality of the context model depends to a large degree upon the quality of the initial annotated corpus, which is however not satisfied due to two problems.",
                    "sid": 171,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, the greedy segmentor cannot deal with the segmentation ambiguities, and even after iterations, these ambiguities can only be partially resolved.",
                    "sid": 172,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, many factoids and named entities cannot be identified using the greedy word segmentor which is based on the dictionary.",
                    "sid": 173,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To solve the first problem, we use two methods to resolve segmentation ambiguities in the initial segmented training data.",
                    "sid": 174,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We classify word segmentation ambiguities into two classes: overlap ambiguity (OA), and combination ambiguity (CA).",
                    "sid": 175,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Consider a character string ABC, if it can be seg 6 The greedy word segmentor is based on a forward maximum.",
                    "sid": 176,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "matching (FMM) algorithm: It processes through the sentence from left to right, taking the longest match with the lexicon entry at each point.",
                    "sid": 177,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "mented into two words either as AB/C or A/BC depending on different context, ABC is called an overlap ambiguity string (OAS).",
                    "sid": 178,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "If a character string AB can be segmented either into two words, A/B, or as one word depending on different context.",
                    "sid": 179,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "AB is called a combination ambiguity string (CAS).",
                    "sid": 180,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To resolve OA, we identify all OASs in the training data and replace them with a single token <OAS>.",
                    "sid": 181,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "By doing so, we actually remove the portion of training data that are likely to contain OA errors.",
                    "sid": 182,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "To resolve CA, we select 70 high-frequent two-char- acter CAS (e.g. \u624d\u80fd \u2018talent\u2019 and \u624d/\u80fd \u2018just able\u2019).",
                    "sid": 183,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For each CAS, we train a binary classifier (which is based on vector space models) using sentences that contains the CAS segmented manually.",
                    "sid": 184,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then for each occurrence of a CAS in the initial segmented training data, the corresponding classifier is used to determine whether or not the CAS should be segmented.",
                    "sid": 185,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For the second problem, though we can simply use the finite-state machines described in Section 5 (extended by using the longest-matching constraint for disambiguation) to detect factoids in the initial segmented corpus, our method of NER in the initial step (i.e. step 1) is a little more complicated.",
                    "sid": 186,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we manually annotate named entities on a small subset (call seed set) of the training data.",
                    "sid": 187,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Then, we obtain a context model on the seed set (called seed model).",
                    "sid": 188,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We thus improve the context model which is trained on the initial annotated training corpus by interpolating it with the seed model.",
                    "sid": 189,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Finally, we use the improved context model in steps 2 and 3 of the bootstrapping.",
                    "sid": 190,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our experiments show that a relatively small seed set (e.g., 10 million characters, which takes approximately three weeks for 4 persons to annotate the NE tags) is enough to get a good improved context model for initialization.",
                    "sid": 191,
                    "ssid": 28,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "evaluation. ",
            "number": "7",
            "sents": [
                {
                    "text": "To conduct a reliable evaluation, a manually annotated test set was developed.",
                    "sid": 192,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The text corpus contains approximately half million Chinese characters that have been proofread and balanced in terms of domain, styles, and times.",
                    "sid": 193,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Before we annotate the corpus, several questions have to be answered: (1) Does the segmentation depend on a particular lexicon?",
                    "sid": 194,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(2) Should we assume a single correct segmentation for a sentence?",
                    "sid": 195,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(3) What are the evaluation criteria?",
                    "sid": 196,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "(4) How to perform a fair comparison across different systems?",
                    "sid": 197,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "W o r d System segmentation Factoid P% R% P% R% P % PN R% P % LN R% P% ON R% 1 FM M 8 3 . 7 9 2 . 7 2 Bas elin e 8 4 . 4 9 3 . 8 3 2 + Fact oid 8 9 . 9 9 5 . 5 8 4.",
                    "sid": 198,
                    "ssid": 7,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4 8 0.",
                    "sid": 199,
                    "ssid": 8,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 4 3 + PN 9 4 . 1 9 6 . 7 8 4.",
                    "sid": 200,
                    "ssid": 9,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 8 0.",
                    "sid": 201,
                    "ssid": 10,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 8 1.",
                    "sid": 202,
                    "ssid": 11,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 90.0 5 4 + LN 9 4 . 7 9 7 . 0 8 4.",
                    "sid": 203,
                    "ssid": 12,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 8 0.",
                    "sid": 204,
                    "ssid": 13,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 8 6.",
                    "sid": 205,
                    "ssid": 14,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4 90.0 7 9.",
                    "sid": 206,
                    "ssid": 15,
                    "kind_of_tag": "s"
                },
                {
                    "text": "4 86.0 6 5 + ON 9 6 . 3 9 7 . 4 8 5.",
                    "sid": 207,
                    "ssid": 16,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 8 0.",
                    "sid": 208,
                    "ssid": 17,
                    "kind_of_tag": "s"
                },
                {
                    "text": "0 8 7.",
                    "sid": 209,
                    "ssid": 18,
                    "kind_of_tag": "s"
                },
                {
                    "text": "5 90.0 8 9.",
                    "sid": 210,
                    "ssid": 19,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2 85.4 81.4 65.6 Table 1: system results As described earlier, it is more useful to define words depending on how the words are used in real applications.",
                    "sid": 211,
                    "ssid": 20,
                    "kind_of_tag": "s"
                },
                {
                    "text": "In our system, a lexicon (containing 98,668 lexicon words and 59,285 morphologically derived words) has been constructed for several applications, such as Asian language input and web search.",
                    "sid": 212,
                    "ssid": 21,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, we annotate the text corpus based on the lexicon.",
                    "sid": 213,
                    "ssid": 22,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, we segment each sentence as much as possible into words that are stored in our lexicon, and tag only the new words, which otherwise would be segmented into strings of one-character words.",
                    "sid": 214,
                    "ssid": 23,
                    "kind_of_tag": "s"
                },
                {
                    "text": "When there are multiple seg mentations for a sentence, we keep only one that contains the least number of words.",
                    "sid": 215,
                    "ssid": 24,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The annotated test set contains in total 247,039 tokens (including 205,162 lexicon/morph-lexicon words, 4,347 PNs, 5,311 LNs, 3,850 ONs, and 6,630 factoids, etc.) Our system is measured through multiple precision-recall (P/R) pairs, and F-measures (F\u03b2=1, which is defined as 2PR/(P+R)) for each word class.",
                    "sid": 216,
                    "ssid": 25,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Since the annotated test set is based on a particular lexicon, some of the evaluation measures are meaningless when we compare our system to other systems that use different lexicons.",
                    "sid": 217,
                    "ssid": 26,
                    "kind_of_tag": "s"
                },
                {
                    "text": "So in comparison with different systems, we consider only the precision-recall of NER and the number of OAS errors (i.e. crossing brackets) because these measures are lexicon independent and there is always a single unambiguous answer.",
                    "sid": 218,
                    "ssid": 27,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The training corpus for context model contains approximately 80 million Chinese characters from various domains of text such as newspapers, novels, magazines etc. The training corpora for class models are described in Section 5.",
                    "sid": 219,
                    "ssid": 28,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7.1 System results.",
                    "sid": 220,
                    "ssid": 29,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Our system is designed in the way that components such as factoid detector and NER can be \u2018switched on or off\u2019, so that we can investigate the relative contribution of each component to the overall word segmentation performance.",
                    "sid": 221,
                    "ssid": 30,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The main results are shown in Table 1.",
                    "sid": 222,
                    "ssid": 31,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For comparison, we also include in the table (Row 1) the results of using the greedy segmentor (FMM) described in Section 6.",
                    "sid": 223,
                    "ssid": 32,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Row 2 shows the baseline results of our system, where only the lexicon is used.",
                    "sid": 224,
                    "ssid": 33,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is interesting to find, in Rows 1 and 2, that the dictionary-based methods already achieve quite good recall, but the precisions are not very good because they cannot identify correctly unknown words that are not in the lexicon such factoids and named entities.",
                    "sid": 225,
                    "ssid": 34,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also find that even using the same lexicon, our approach that is based on the improved source-channel models outperforms the greedy approach (with a slight but statistically significant different i.e., P < 0.01 according to the t test) because the use of context model resolves more ambiguities in segmentation.",
                    "sid": 226,
                    "ssid": 35,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The most promising property of our approach is that the source-channel models provide a flexible framework where a wide variety of linguistic knowledge and statistical models can be combined in a unified way.",
                    "sid": 227,
                    "ssid": 36,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As shown in Rows 3 to 6, when components are switched on in turn by activating corresponding class models, the overall word segmentation performance increases consistently.",
                    "sid": 228,
                    "ssid": 37,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also conduct an error analysis, showing that 86.2% of errors come from NER and factoid detection, although the tokens of these word types consist of only 8.7% of all that are in the test set.",
                    "sid": 229,
                    "ssid": 38,
                    "kind_of_tag": "s"
                },
                {
                    "text": "7.2 Comparison with other systems.",
                    "sid": 230,
                    "ssid": 39,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We compare our system \u2013 henceforth SCM, with other two Chinese word segmentation systems7: 7 Although the two systems are widely accessible in mainland.",
                    "sid": 231,
                    "ssid": 40,
                    "kind_of_tag": "s"
                },
                {
                    "text": "China, to our knowledge no standard evaluations on Chinese word segmentation of the two systems have been published by press time.",
                    "sid": 232,
                    "ssid": 41,
                    "kind_of_tag": "s"
                },
                {
                    "text": "More comprehensive comparisons (with other well- known systems) and detailed error analysis form one area of our future work.",
                    "sid": 233,
                    "ssid": 42,
                    "kind_of_tag": "s"
                },
                {
                    "text": "System # OAS LN PN ON Errors P % R % F\u03b2=1 P % R % F\u03b2=1 P % R % F\u03b2=1 MSWS 63 93.5 44.2 60.0 90.7 74.4 81.8 64.2 46.9 60.0 LCWS 49 85.4 72.0 78.2 94.5 78.1 85.6 71.3 13.1 22.2 SCM 7 87.6 86.4 87.0 83.0 89.7 86.2 79.9 61.7 69.6 Table 2.",
                    "sid": 234,
                    "ssid": 43,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Comparison results 1.",
                    "sid": 235,
                    "ssid": 44,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The MSWS system is one of the best available.",
                    "sid": 236,
                    "ssid": 45,
                    "kind_of_tag": "s"
                },
                {
                    "text": "products.",
                    "sid": 237,
                    "ssid": 46,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is released by Microsoft\u00ae (as a set of Windows APIs).",
                    "sid": 238,
                    "ssid": 47,
                    "kind_of_tag": "s"
                },
                {
                    "text": "MSWS first conducts the word breaking using MM (augmented by heuristic rules for disambiguation), then conducts factoid detection and NER using rules.",
                    "sid": 239,
                    "ssid": 48,
                    "kind_of_tag": "s"
                },
                {
                    "text": "2.",
                    "sid": 240,
                    "ssid": 49,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The LCWS system is one of the best research.",
                    "sid": 241,
                    "ssid": 50,
                    "kind_of_tag": "s"
                },
                {
                    "text": "systems in mainland China.",
                    "sid": 242,
                    "ssid": 51,
                    "kind_of_tag": "s"
                },
                {
                    "text": "It is released by Beijing Language University.",
                    "sid": 243,
                    "ssid": 52,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The system works similarly to MSWS, but has a larger dictionary containing more PNs and LNs.",
                    "sid": 244,
                    "ssid": 53,
                    "kind_of_tag": "s"
                },
                {
                    "text": "As mentioned above, to achieve a fair comparison, we compare the above three systems only in terms of NER precision-recall and the number of OAS errors.",
                    "sid": 245,
                    "ssid": 54,
                    "kind_of_tag": "s"
                },
                {
                    "text": "However, we find that due to the different annotation specifications used by these systems, it is still very difficult to compare their results automatically.",
                    "sid": 246,
                    "ssid": 55,
                    "kind_of_tag": "s"
                },
                {
                    "text": "For example, \u5317\u4eac\u5e02\u653f\u5e9c \u2018Beijing city government\u2019 has been segmented inconsistently as \u5317\u4eac\u5e02/\u653f\u5e9c \u2018Beijing city\u2019 + \u2018government\u2019 or \u5317\u4eac/ \u5e02\u653f\u5e9c \u2018Beijing\u2019 + \u2018city government\u2019 even in the same system.",
                    "sid": 247,
                    "ssid": 56,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Even worse, some LNs tagged in one system are tagged as ONs in another system.",
                    "sid": 248,
                    "ssid": 57,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Therefore, we have to manually check the results.",
                    "sid": 249,
                    "ssid": 58,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We picked 933 sentences at random containing 22,833 words (including 329 PNs, 617 LNs, and 435 ONs) for testing.",
                    "sid": 250,
                    "ssid": 59,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also did not differentiate.",
                    "sid": 251,
                    "ssid": 60,
                    "kind_of_tag": "s"
                },
                {
                    "text": "LNs and ONs in evaluation.",
                    "sid": 252,
                    "ssid": 61,
                    "kind_of_tag": "s"
                },
                {
                    "text": "That is, we only checked the word boundaries of LNs and ONs and treated both tags exchangeable.",
                    "sid": 253,
                    "ssid": 62,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The results are shown in Table 2.",
                    "sid": 254,
                    "ssid": 63,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We can see that in this small test set SCM achieves the best overall performance of NER and the best performance of resolving OAS.",
                    "sid": 255,
                    "ssid": 64,
                    "kind_of_tag": "s"
                }
            ]
        },
        {
            "text": "conclusion. ",
            "number": "8",
            "sents": [
                {
                    "text": "The contributions of this paper are threefold.",
                    "sid": 256,
                    "ssid": 1,
                    "kind_of_tag": "s"
                },
                {
                    "text": "First, we formulate the Chinese word segmentation problem as a set of correlated problems, which are better solved simultaneously, including word breaking, morphological analysis, factoid detection and NER.",
                    "sid": 257,
                    "ssid": 2,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Second, we present a unified approach to these problems using the improved source-channel models.",
                    "sid": 258,
                    "ssid": 3,
                    "kind_of_tag": "s"
                },
                {
                    "text": "The models provide a simple statistical framework to incorporate a wide variety of linguistic knowledge and statistical models in a unified way.",
                    "sid": 259,
                    "ssid": 4,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Third, we evaluate the system\u2019s performance on an annotated test set, showing very promising results.",
                    "sid": 260,
                    "ssid": 5,
                    "kind_of_tag": "s"
                },
                {
                    "text": "We also compare our system with several state-of-the-art systems, taking into account the fact that the definition of Chinese words varies from system to system.",
                    "sid": 261,
                    "ssid": 6,
                    "kind_of_tag": "s"
                },
                {
                    "text": "Given the comparison results, we can say with confidence that our system achieves at least the performance of state-of-the-art word segmentation systems.",
                    "sid": 262,
                    "ssid": 7,
                    "kind_of_tag": "s"
                }
            ]
        }
    ]
}